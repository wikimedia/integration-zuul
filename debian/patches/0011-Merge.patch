From: root <root@test-random-wikisaur.tk>
Date: Thu, 21 Jul 2016 16:11:28 +0100
Subject: Merge

Change-Id: I3e3febffc7e1250261a8d33b7ace060b5154de82
---
 .gitignore                                         |   30 +-
 .gitreview                                         |   10 +-
 .mailmap                                           |    8 +-
 .testr.conf                                        |    8 +-
 MANIFEST.in                                        |   14 +-
 NEWS.rst                                           |  116 +-
 README.rst                                         |   42 +-
 TESTING.rst                                        |  184 +-
 debian/changelog                                   |  268 +-
 debian/compat                                      |    2 +-
 debian/control                                     |  204 +-
 debian/copyright                                   |   64 +-
 debian/dirs                                        |   12 +-
 debian/examples                                    |    8 +-
 debian/gbp.conf                                    |   20 +-
 debian/links                                       |   24 +-
 debian/manpages                                    |    2 +-
 debian/patches/0001-wmf-soften-requirements.patch  |   88 +-
 ...repository-configuration-lock-is-released.patch |   98 +-
 ...003-Replace-python-shebang-with-python2.7.patch |  374 +-
 ...ner-Implement-cache-no-hardlinks-argument.patch |  184 +-
 ...cloner-recognizes-bare-repos-in-cache-dir.patch |  244 +-
 debian/patches/series                              |   10 +-
 debian/postinst                                    |  134 +-
 debian/prerm                                       |   92 +-
 debian/pydist-overrides                            |    4 +-
 debian/rules                                       |   94 +-
 debian/source/format                               |    2 +-
 debian/source/lintian-overrides                    |   10 +-
 debian/source/options                              |    2 +-
 debian/watch                                       |    4 +-
 debian/zuul-merger.default                         |   14 +-
 debian/zuul-merger.init                            |  290 +-
 debian/zuul.default                                |   30 +-
 debian/zuul.init                                   |  338 +-
 debian/zuul.lintian-overrides                      |   44 +-
 debian/zuul.triggers                               |   14 +-
 doc/Makefile                                       |  306 +-
 doc/source/client.rst                              |  102 +-
 doc/source/cloner.rst                              |  230 +-
 doc/source/conf.py                                 |  470 +-
 doc/source/connections.rst                         |  182 +-
 doc/source/gating.rst                              |  942 +--
 doc/source/index.rst                               |   66 +-
 doc/source/launchers.rst                           |  770 +-
 doc/source/merger.rst                              |  120 +-
 doc/source/reporters.rst                           |  124 +-
 doc/source/statsd.rst                              |  178 +-
 doc/source/triggers.rst                            |  312 +-
 doc/source/zuul.rst                                | 2058 ++---
 etc/clonemap.yaml-sample                           |   32 +-
 etc/layout.yaml-sample                             |  134 +-
 etc/logging.conf-sample                            |   88 +-
 etc/status/.gitignore                              |    2 +-
 etc/status/.jshintignore                           |    2 +-
 etc/status/.jshintrc                               |   42 +-
 etc/status/README.rst                              |   54 +-
 etc/status/fetch-dependencies.sh                   |   46 +-
 etc/status/public_html/index.html                  |   74 +-
 etc/status/public_html/jquery.zuul.js              | 1820 ++--
 etc/status/public_html/status-basic.json-sample    |  356 +-
 .../public_html/status-openstack.json-sample       |  624 +-
 etc/status/public_html/styles/zuul.css             |  114 +-
 etc/status/public_html/zuul.app.js                 |  208 +-
 etc/zuul.conf-sample                               |   84 +-
 requirements.txt                                   |   32 +-
 setup.cfg                                          |   64 +-
 setup.py                                           |   44 +-
 test-requirements.txt                              |   28 +-
 tests/base.py                                      | 2718 +++---
 tests/cmd/test_cloner.py                           |  112 +-
 tests/fixtures/clonemap.yaml                       |   10 +-
 tests/fixtures/custom_functions.py                 |    4 +-
 ...tom_functions_live_reconfiguration_functions.py |    4 +-
 .../fixtures/gerrit/simple_query_pagination_new_1  |    8 +-
 .../fixtures/gerrit/simple_query_pagination_new_2  |    6 +-
 .../fixtures/gerrit/simple_query_pagination_old_1  |    8 +-
 .../fixtures/gerrit/simple_query_pagination_old_2  |    6 +-
 .../fixtures/gerrit/simple_query_pagination_old_3  |    2 +-
 tests/fixtures/layout-bad-queue.yaml               |  148 +-
 tests/fixtures/layout-cloner.yaml                  |   90 +-
 .../layout-connections-multiple-gerrits.yaml       |   74 +-
 .../layout-connections-multiple-voters.yaml        |   40 +-
 tests/fixtures/layout-delayed-repo-init.yaml       |  104 +-
 tests/fixtures/layout-disable-at.yaml              |   42 +-
 tests/fixtures/layout-dont-ignore-deletes.yaml     |   32 +-
 tests/fixtures/layout-footer-message.yaml          |   68 +-
 tests/fixtures/layout-idle.yaml                    |   24 +-
 tests/fixtures/layout-ignore-dependencies.yaml     |   56 +-
 .../layout-live-reconfiguration-add-job.yaml       |   76 +-
 .../layout-live-reconfiguration-del-project.yaml   |   42 +-
 .../layout-live-reconfiguration-failed-job.yaml    |   50 +-
 .../layout-live-reconfiguration-functions.yaml     |   74 +-
 .../layout-live-reconfiguration-shared-queue.yaml  |  124 +-
 tests/fixtures/layout-merge-failure.yaml           |  112 +-
 tests/fixtures/layout-merge-queues.yaml            |   50 +-
 tests/fixtures/layout-mutex.yaml                   |   50 +-
 tests/fixtures/layout-no-jobs.yaml                 |   86 +-
 tests/fixtures/layout-no-timer.yaml                |   56 +-
 tests/fixtures/layout-rate-limit.yaml              |   64 +-
 tests/fixtures/layout-repo-deleted.yaml            |  104 +-
 .../layout-requirement-current-patchset.yaml       |   40 +-
 tests/fixtures/layout-requirement-email.yaml       |   74 +-
 tests/fixtures/layout-requirement-newer-than.yaml  |   78 +-
 tests/fixtures/layout-requirement-older-than.yaml  |   78 +-
 tests/fixtures/layout-requirement-open.yaml        |   40 +-
 .../layout-requirement-reject-username.yaml        |   72 +-
 tests/fixtures/layout-requirement-reject.yaml      |   88 +-
 tests/fixtures/layout-requirement-status.yaml      |   40 +-
 tests/fixtures/layout-requirement-username.yaml    |   74 +-
 tests/fixtures/layout-requirement-vote.yaml        |   78 +-
 tests/fixtures/layout-requirement-vote1.yaml       |   78 +-
 tests/fixtures/layout-requirement-vote2.yaml       |   78 +-
 tests/fixtures/layout-skip-if.yaml                 |   58 +-
 tests/fixtures/layout-smtp.yaml                    |   50 +-
 tests/fixtures/layout-swift.yaml                   |  118 +-
 tests/fixtures/layout-tags.yaml                    |   84 +-
 tests/fixtures/layout-timer-smtp.yaml              |   46 +-
 tests/fixtures/layout-timer.yaml                   |   56 +-
 tests/fixtures/layout-zuultrigger-enqueued.yaml    |  106 +-
 tests/fixtures/layout-zuultrigger-merged.yaml      |  108 +-
 tests/fixtures/layout.yaml                         |  522 +-
 tests/fixtures/layouts/bad_gerrit_missing.yaml     |   36 +-
 tests/fixtures/layouts/bad_merge_failure.yaml      |   80 +-
 tests/fixtures/layouts/bad_misplaced_ref.yaml      |   26 +-
 tests/fixtures/layouts/bad_pipelines1.yaml         |    4 +-
 tests/fixtures/layouts/bad_pipelines10.yaml        |   16 +-
 tests/fixtures/layouts/bad_pipelines2.yaml         |   14 +-
 tests/fixtures/layouts/bad_pipelines3.yaml         |   16 +-
 tests/fixtures/layouts/bad_pipelines4.yaml         |   20 +-
 tests/fixtures/layouts/bad_pipelines5.yaml         |   22 +-
 tests/fixtures/layouts/bad_pipelines6.yaml         |   22 +-
 tests/fixtures/layouts/bad_pipelines7.yaml         |   12 +-
 tests/fixtures/layouts/bad_pipelines8.yaml         |   12 +-
 tests/fixtures/layouts/bad_pipelines9.yaml         |   18 +-
 tests/fixtures/layouts/bad_projects1.yaml          |   20 +-
 tests/fixtures/layouts/bad_projects2.yaml          |   20 +-
 tests/fixtures/layouts/bad_reject.yaml             |   42 +-
 tests/fixtures/layouts/bad_swift.yaml              |   56 +-
 tests/fixtures/layouts/bad_template1.yaml          |   40 +-
 tests/fixtures/layouts/bad_template2.yaml          |   46 +-
 tests/fixtures/layouts/bad_template3.yaml          |   20 +-
 tests/fixtures/layouts/good_connections1.conf      |   84 +-
 tests/fixtures/layouts/good_connections1.yaml      |   36 +-
 tests/fixtures/layouts/good_layout.yaml            |  204 +-
 tests/fixtures/layouts/good_merge_failure.yaml     |  106 +-
 tests/fixtures/layouts/good_require_approvals.yaml |   72 +-
 tests/fixtures/layouts/good_swift.yaml             |   64 +-
 tests/fixtures/layouts/good_template1.yaml         |   34 +-
 tests/fixtures/layouts/zuul_default.conf           |   72 +-
 tests/fixtures/tags_custom_functions.py            |    4 +-
 .../zuul-connections-multiple-gerrits.conf         |   84 +-
 tests/fixtures/zuul-connections-same-gerrit.conf   |   84 +-
 tests/fixtures/zuul.conf                           |   72 +-
 tests/test_change_matcher.py                       |  308 +-
 tests/test_clonemapper.py                          |  168 +-
 tests/test_cloner.py                               | 1182 +--
 tests/test_connection.py                           |  170 +-
 tests/test_daemon.py                               |  126 +-
 tests/test_gerrit.py                               |  160 +-
 tests/test_layoutvalidator.py                      |  156 +-
 tests/test_merger_repo.py                          |  158 +-
 tests/test_model.py                                |  128 +-
 tests/test_reporter.py                             |   92 +-
 tests/test_requirements.py                         |  854 +-
 tests/test_scheduler.py                            | 8788 ++++++++++----------
 tests/test_source.py                               |   50 +-
 tests/test_stack_dump.py                           |   68 +-
 tests/test_trigger.py                              |  102 +-
 tests/test_webapp.py                               |  170 +-
 tests/test_zuultrigger.py                          |  284 +-
 tools/trigger-job.py                               |  154 +-
 tools/zuul-changes.py                              |   88 +-
 tox.ini                                            |   82 +-
 zuul/change_matcher.py                             |  264 +-
 zuul/cmd/__init__.py                               |  192 +-
 zuul/cmd/client.py                                 |  608 +-
 zuul/cmd/cloner.py                                 |  328 +-
 zuul/cmd/merger.py                                 |  224 +-
 zuul/cmd/server.py                                 |  466 +-
 zuul/connection/__init__.py                        |  142 +-
 zuul/connection/gerrit.py                          |  974 +--
 zuul/connection/smtp.py                            |  126 +-
 zuul/exceptions.py                                 |   52 +-
 zuul/launcher/gearman.py                           | 1068 +--
 zuul/layoutvalidator.py                            |  688 +-
 zuul/lib/clonemapper.py                            |  156 +-
 zuul/lib/cloner.py                                 |  412 +-
 zuul/lib/connections.py                            |  132 +-
 zuul/lib/swift.py                                  |  336 +-
 zuul/merger/client.py                              |  246 +-
 zuul/merger/merger.py                              |  710 +-
 zuul/merger/server.py                              |  230 +-
 zuul/model.py                                      | 2764 +++---
 zuul/reporter/__init__.py                          |  296 +-
 zuul/reporter/gerrit.py                            |  104 +-
 zuul/reporter/smtp.py                              |  110 +-
 zuul/rpcclient.py                                  |  168 +-
 zuul/rpclistener.py                                |  318 +-
 zuul/scheduler.py                                  | 4322 +++++-----
 zuul/source/__init__.py                            |  130 +-
 zuul/source/gerrit.py                              |  714 +-
 zuul/trigger/__init__.py                           |   92 +-
 zuul/trigger/gerrit.py                             |  234 +-
 zuul/trigger/timer.py                              |  188 +-
 zuul/trigger/zuultrigger.py                        |  296 +-
 zuul/version.py                                    |   40 +-
 zuul/webapp.py                                     |  260 +-
 208 files changed, 25670 insertions(+), 25670 deletions(-)

diff --git a/.gitignore b/.gitignore
index b59cb77..39ad27c 100644
--- a/.gitignore
+++ b/.gitignore
@@ -1,15 +1,15 @@
-*.sw?
-*.egg
-*.egg-info
-*.pyc
-.test
-.testrepository
-.tox
-.venv
-AUTHORS
-build/*
-ChangeLog
-config
-doc/build/*
-zuul/versioninfo
-dist/
+*.sw?
+*.egg
+*.egg-info
+*.pyc
+.test
+.testrepository
+.tox
+.venv
+AUTHORS
+build/*
+ChangeLog
+config
+doc/build/*
+zuul/versioninfo
+dist/
diff --git a/.gitreview b/.gitreview
index c41c274..c443b1b 100644
--- a/.gitreview
+++ b/.gitreview
@@ -1,5 +1,5 @@
-[gerrit]
-host=gerrit.wikimedia.org
-port=29418
-project=integration/zuul.git
-defaultbranch=debian/precise-wikimedia
+[gerrit]
+host=gerrit.wikimedia.org
+port=29418
+project=integration/zuul.git
+defaultbranch=debian/precise-wikimedia
diff --git a/.mailmap b/.mailmap
index 18221d4..d9215f5 100644
--- a/.mailmap
+++ b/.mailmap
@@ -1,4 +1,4 @@
-# Format is:
-# <preferred e-mail> <other e-mail 1>
-# <preferred e-mail> <other e-mail 2>
-Zhongyue Luo <zhongyue.nah@intel.com> <lzyeval@gmail.com>
+# Format is:
+# <preferred e-mail> <other e-mail 1>
+# <preferred e-mail> <other e-mail 2>
+Zhongyue Luo <zhongyue.nah@intel.com> <lzyeval@gmail.com>
diff --git a/.testr.conf b/.testr.conf
index 5433c07..a6b2a79 100644
--- a/.testr.conf
+++ b/.testr.conf
@@ -1,4 +1,4 @@
-[DEFAULT]
-test_command=OS_STDOUT_CAPTURE=${OS_STDOUT_CAPTURE:-1} OS_STDERR_CAPTURE=${OS_STDERR_CAPTURE:-1} OS_LOG_CAPTURE=${OS_LOG_CAPTURE:-1} ${PYTHON:-python} -m subunit.run discover -t ./ tests $LISTOPT $IDOPTION
-test_id_option=--load-list $IDFILE
-test_list_option=--list
+[DEFAULT]
+test_command=OS_STDOUT_CAPTURE=${OS_STDOUT_CAPTURE:-1} OS_STDERR_CAPTURE=${OS_STDERR_CAPTURE:-1} OS_LOG_CAPTURE=${OS_LOG_CAPTURE:-1} ${PYTHON:-python} -m subunit.run discover -t ./ tests $LISTOPT $IDOPTION
+test_id_option=--load-list $IDFILE
+test_list_option=--list
diff --git a/MANIFEST.in b/MANIFEST.in
index 74fc557..de0d96c 100644
--- a/MANIFEST.in
+++ b/MANIFEST.in
@@ -1,7 +1,7 @@
-include AUTHORS
-include ChangeLog
-
-exclude .gitignore
-exclude .gitreview
-
-global-exclude *.pyc
+include AUTHORS
+include ChangeLog
+
+exclude .gitignore
+exclude .gitreview
+
+global-exclude *.pyc
diff --git a/NEWS.rst b/NEWS.rst
index 5fef40a..1f783f7 100644
--- a/NEWS.rst
+++ b/NEWS.rst
@@ -1,58 +1,58 @@
-Since 2.0.0:
-
-* The push_change_refs option which specified that Zuul refs should be
-  pushed to Gerrit has been removed.
-
-* Git merge operations are now performed in a separate process.  Run
-  at least one instance of the ``zuul-merger`` program which is now
-  included.  Any number of Zuul-Mergers may be run in order to
-  distribute the work of speculatively merging changes into git and
-  serving the results to test workers.  This system is designed to
-  scale out to many servers, but one instance may be co-located with
-  the Zuul server in smaller deployments.  Several configuration
-  options have moved from the ``zuul`` section to ``merger``.
-
-Since 1.3.0:
-
-* The Jenkins launcher is replaced with Gearman launcher.  An internal
-  Gearman server is provided, and there is a Gearman plugin for
-  Jenkins, so migration to the new system should be fairly
-  straightforward.  See the Launchers section of the documentation for
-  details.
-
-* The custom parameter function signature has changed.  It now takes a
-  QueueItem as the first argument, rather than the Change.  The
-  QueueItem has the full context for why the change is being run
-  (including the pipeline, items ahead and behind, etc.).  The Change
-  is still available via the "change" attribute on the QueueItem.  The
-  second argument is now the Job that is about to be run, and the
-  parameter dictionary is shifted to the third position.
-
-* The ZUUL_SHORT_* parameters have been removed (the same
-  functionality may be achieved with a custom parameter function that
-  matches all jobs).
-
-* Multiple triggers are now supported (currently Gerrit and a simple
-  Timer trigger are supported).  Your layout.yaml file will need to
-  change to add the key "gerrit:" inside of the "triggers:" list to
-  specify a Gerrit trigger (and facilitate adding other kinds of
-  triggers later).  See the sample layout.yaml and Zuul section of the
-  documentation.
-
-* Some statsd keys have changed in a backwards incompatible way:
-  * The counters and timers of the form zuul.job.{name} is now split
-    into several keys of the form:
-    zuul.pipeline.{pipeline-name}.job.{job-name}.{result}
-  * Job names in statsd keys now have the '_' character substituted
-    for the '.' character.
-
-* The layout.yaml structure has changed to introduce configurable
-  reporters. This requires restructuring the start/success/failure
-  actions to include a dictionary of reporters and their parameters.
-  See reporters in the docs and layout.yaml-sample.
-
-* The zuul_url configuration option is required in zuul.conf.  It
-  specifies the URL of the git repositories that should be used by
-  workers when fetching Zuul refs and is passed to the workers as the
-  ZUUL_URL parameter.  It should probably be set to
-  "http://zuul-host-name/p/".
+Since 2.0.0:
+
+* The push_change_refs option which specified that Zuul refs should be
+  pushed to Gerrit has been removed.
+
+* Git merge operations are now performed in a separate process.  Run
+  at least one instance of the ``zuul-merger`` program which is now
+  included.  Any number of Zuul-Mergers may be run in order to
+  distribute the work of speculatively merging changes into git and
+  serving the results to test workers.  This system is designed to
+  scale out to many servers, but one instance may be co-located with
+  the Zuul server in smaller deployments.  Several configuration
+  options have moved from the ``zuul`` section to ``merger``.
+
+Since 1.3.0:
+
+* The Jenkins launcher is replaced with Gearman launcher.  An internal
+  Gearman server is provided, and there is a Gearman plugin for
+  Jenkins, so migration to the new system should be fairly
+  straightforward.  See the Launchers section of the documentation for
+  details.
+
+* The custom parameter function signature has changed.  It now takes a
+  QueueItem as the first argument, rather than the Change.  The
+  QueueItem has the full context for why the change is being run
+  (including the pipeline, items ahead and behind, etc.).  The Change
+  is still available via the "change" attribute on the QueueItem.  The
+  second argument is now the Job that is about to be run, and the
+  parameter dictionary is shifted to the third position.
+
+* The ZUUL_SHORT_* parameters have been removed (the same
+  functionality may be achieved with a custom parameter function that
+  matches all jobs).
+
+* Multiple triggers are now supported (currently Gerrit and a simple
+  Timer trigger are supported).  Your layout.yaml file will need to
+  change to add the key "gerrit:" inside of the "triggers:" list to
+  specify a Gerrit trigger (and facilitate adding other kinds of
+  triggers later).  See the sample layout.yaml and Zuul section of the
+  documentation.
+
+* Some statsd keys have changed in a backwards incompatible way:
+  * The counters and timers of the form zuul.job.{name} is now split
+    into several keys of the form:
+    zuul.pipeline.{pipeline-name}.job.{job-name}.{result}
+  * Job names in statsd keys now have the '_' character substituted
+    for the '.' character.
+
+* The layout.yaml structure has changed to introduce configurable
+  reporters. This requires restructuring the start/success/failure
+  actions to include a dictionary of reporters and their parameters.
+  See reporters in the docs and layout.yaml-sample.
+
+* The zuul_url configuration option is required in zuul.conf.  It
+  specifies the URL of the git repositories that should be used by
+  workers when fetching Zuul refs and is passed to the workers as the
+  ZUUL_URL parameter.  It should probably be set to
+  "http://zuul-host-name/p/".
diff --git a/README.rst b/README.rst
index ff4d938..4d3d88d 100644
--- a/README.rst
+++ b/README.rst
@@ -1,21 +1,21 @@
-Zuul
-====
-
-Zuul is a trunk gating system developed for the OpenStack Project.
-
-Contributing
-------------
-
-To browse the latest code, see: https://git.openstack.org/cgit/openstack-infra/zuul/tree/
-To clone the latest code, use `git clone git://git.openstack.org/openstack-infra/zuul`
-
-Bugs are handled at: https://storyboard.openstack.org/#!/project/679
-
-Code reviews are, as you might expect, handled by gerrit. The gerrit they
-use is http://review.openstack.org
-
-Use `git review` to submit patches (after creating a gerrit account that links to your launchpad account). Example::
-
-    # Do your commits
-    $ git review
-    # Enter your username if prompted
+Zuul
+====
+
+Zuul is a trunk gating system developed for the OpenStack Project.
+
+Contributing
+------------
+
+To browse the latest code, see: https://git.openstack.org/cgit/openstack-infra/zuul/tree/
+To clone the latest code, use `git clone git://git.openstack.org/openstack-infra/zuul`
+
+Bugs are handled at: https://storyboard.openstack.org/#!/project/679
+
+Code reviews are, as you might expect, handled by gerrit. The gerrit they
+use is http://review.openstack.org
+
+Use `git review` to submit patches (after creating a gerrit account that links to your launchpad account). Example::
+
+    # Do your commits
+    $ git review
+    # Enter your username if prompted
diff --git a/TESTING.rst b/TESTING.rst
index 56f2fbb..31ab7b3 100644
--- a/TESTING.rst
+++ b/TESTING.rst
@@ -1,92 +1,92 @@
-===========================
-Testing Your OpenStack Code
-===========================
-------------
-A Quickstart
-------------
-
-This is designed to be enough information for you to run your first tests.
-Detailed information on testing can be found here: https://wiki.openstack.org/wiki/Testing
-
-*Install pip*::
-
-  [apt-get | yum] install python-pip
-More information on pip here: http://www.pip-installer.org/en/latest/
-
-*Use pip to install tox*::
-
-  pip install tox
-
-Run The Tests
--------------
-
-*Navigate to the project's root directory and execute*::
-
-  tox
-Note: completing this command may take a long time (depends on system resources)
-also, you might not see any output until tox is complete.
-
-Information about tox can be found here: http://testrun.org/tox/latest/
-
-
-Run The Tests in One Environment
---------------------------------
-
-Tox will run your entire test suite in the environments specified in the project tox.ini::
-
-  [tox]
-
-  envlist = <list of available environments>
-
-To run the test suite in just one of the environments in envlist execute::
-
-  tox -e <env>
-so for example, *run the test suite in py26*::
-
-  tox -e py26
-
-Run One Test
-------------
-
-To run individual tests with tox::
-
-  tox -e <env> -- path.to.module.Class.test
-
-For example, to *run the basic Zuul test*::
-
-  tox -e py27 -- tests.test_scheduler.TestScheduler.test_jobs_launched
-
-To *run one test in the foreground* (after previously having run tox
-to set up the virtualenv)::
-
-  .tox/py27/bin/python -m testtools.run tests.test_scheduler.TestScheduler.test_jobs_launched
-
-List Failing Tests
-------------------
-
-  .tox/py27/bin/activate
-  testr failing --list
-
-Hanging Tests
--------------
-
-The following will run each test in turn and print the name of the
-test as it is run::
-
-  . .tox/py27/bin/activate
-  testr run --subunit | subunit2pyunit
-
-You can compare the output of that to::
-
-  python -m testtools.run discover --list
-
-Need More Info?
----------------
-
-More information about testr: https://wiki.openstack.org/wiki/Testr
-
-More information about nose: https://nose.readthedocs.org/en/latest/
-
-
-More information about testing OpenStack code can be found here:
-https://wiki.openstack.org/wiki/Testing
+===========================
+Testing Your OpenStack Code
+===========================
+------------
+A Quickstart
+------------
+
+This is designed to be enough information for you to run your first tests.
+Detailed information on testing can be found here: https://wiki.openstack.org/wiki/Testing
+
+*Install pip*::
+
+  [apt-get | yum] install python-pip
+More information on pip here: http://www.pip-installer.org/en/latest/
+
+*Use pip to install tox*::
+
+  pip install tox
+
+Run The Tests
+-------------
+
+*Navigate to the project's root directory and execute*::
+
+  tox
+Note: completing this command may take a long time (depends on system resources)
+also, you might not see any output until tox is complete.
+
+Information about tox can be found here: http://testrun.org/tox/latest/
+
+
+Run The Tests in One Environment
+--------------------------------
+
+Tox will run your entire test suite in the environments specified in the project tox.ini::
+
+  [tox]
+
+  envlist = <list of available environments>
+
+To run the test suite in just one of the environments in envlist execute::
+
+  tox -e <env>
+so for example, *run the test suite in py26*::
+
+  tox -e py26
+
+Run One Test
+------------
+
+To run individual tests with tox::
+
+  tox -e <env> -- path.to.module.Class.test
+
+For example, to *run the basic Zuul test*::
+
+  tox -e py27 -- tests.test_scheduler.TestScheduler.test_jobs_launched
+
+To *run one test in the foreground* (after previously having run tox
+to set up the virtualenv)::
+
+  .tox/py27/bin/python -m testtools.run tests.test_scheduler.TestScheduler.test_jobs_launched
+
+List Failing Tests
+------------------
+
+  .tox/py27/bin/activate
+  testr failing --list
+
+Hanging Tests
+-------------
+
+The following will run each test in turn and print the name of the
+test as it is run::
+
+  . .tox/py27/bin/activate
+  testr run --subunit | subunit2pyunit
+
+You can compare the output of that to::
+
+  python -m testtools.run discover --list
+
+Need More Info?
+---------------
+
+More information about testr: https://wiki.openstack.org/wiki/Testr
+
+More information about nose: https://nose.readthedocs.org/en/latest/
+
+
+More information about testing OpenStack code can be found here:
+https://wiki.openstack.org/wiki/Testing
diff --git a/debian/changelog b/debian/changelog
index 1dd1e6b..73b80d7 100644
--- a/debian/changelog
+++ b/debian/changelog
@@ -1,134 +1,134 @@
-zuul (2.1.0-151-g30a433b-wmf1precise1) precise-wikimedia; urgency=medium
-
-  * Bump upstream version
-  * Should deal with circular dependencies which caused a complete deadlock of
-    the Zuul scheduler: https://phabricator.wikimedia.org/T129938
-  * Fix typo in previous changelog entry
-  * Make myself the Maintainer
-  * Patch maintenance:
-  * wmf-soften-requirements:
-    - apscheduler 3.0.x is now shipped via dh_virtualenv
-      Note 3.1.0 fails with old setuptools
-    - soften pbr, 1.1.0 is just for tests
-  * Update-merge-status-after-merge-merge-is-submitted
-    - merged upstream
-  * Revert-Fix-passing-labels-to-Gerrit-when-they-are-no
-    - no more needed with upstream "connections" refactoring
-  * Rebased the couple Zuul cloner patches
-  * Bump GitPython 2.0.3..2.0.5
-
- -- Antoine Musso <hashar@free.fr>  Tue, 31 May 2016 11:05:32 +0200
-
-zuul (2.1.0-95-g66c8e52-wmf1precise1) precise-wikimedia; urgency=medium
-
-  * Slowly catch up with upstream
-  * 0008-wmf-pin-python-daemon-2.1.0 is no more needed.
-
- -- Antoine Musso <hashar@free.fr>  Mon, 30 May 2016 13:55:52 +0200
-
-zuul (2.1.0-60-g1cc37f7-wmf4precise1) precise-wikimedia; urgency=critical
-
-  * 0008-wmf-pin-python-daemon-2.1.0.patch
-    Solves https://phabricator.wikimedia.org/T119741
-
- -- Antoine Musso <hashar@free.fr>  Thu, 26 Nov 2015 23:23:51 +0100
-
-zuul (2.1.0-60-g1cc37f7-wmf3precise1) precise-wikimedia; urgency=medium
-
-  * 0007-zuul-cloner-recognizes-bare-repos-in-cache-dir.patch
-    upstream version: https://review.openstack.org/#/c/249207/2
-
- -- Antoine Musso <hashar@free.fr>  Tue, 24 Nov 2015 16:56:26 +0100
-
-zuul (2.1.0-60-g1cc37f7-wmf2precise1) precise-wikimedia; urgency=medium
-
-  * Refresh patch 0005-Cloner-Implement-cache-no-hardlinks-argument.patch to
-    latest upstream version https://review.openstack.org/#/c/117626/11
-    Closes https://phabricator.wikimedia.org/T97106
-
- -- Antoine Musso <hashar@free.fr>  Fri, 13 Nov 2015 14:15:50 +0100
-
-zuul (2.1.0-60-g1cc37f7-wmf1precise1) precise-wikimedia; urgency=medium
-
-  * Bump upstream from 3ebedde to 1cc37f7
-
-  * Rebased patches and dropped patches merged upstream
-      0002-Merger-ensure_cloned-now-looks-for-.git.patch
-      0007-Remove-lockfile-from-requirements.patch
-
-  * Dependencies changes:
-    Babel            1.3 -> 2.1.1
-    lockfile      0.10.2 -> 0.11.0
-    python-daemon  2.0.5 -> 2.0.6
-    + gear 0.5.8
-
- -- Antoine Musso <hashar@free.fr>  Mon, 02 Nov 2015 12:01:46 +0100
-
-zuul (2.0.0-327-g3ebedde-wmf3precise1) precise-wikimedia; urgency=medium
-
-  * 0008-Revert-Fix-passing-labels-to-Gerrit-when-they-are-no
-    Makes Zuul to send code-review and verified label votes using the shortcuts
-    --verified and --code-review since --label is case sensitive.
-   See https://phabricator.wikimedia.org/T106596
-
- -- Antoine Musso <hashar@free.fr>  Thu, 23 Jul 2015 14:11:53 +0000
-
-zuul (2.0.0-327-g3ebedde-wmf2precise1) precise-wikimedia; urgency=medium
-
-  * 0007-Remove-lockfile-from-requirements.patch
-  * python-daemon 2.x shipped via the venv
-
- -- Antoine Musso <hashar@free.fr>  Tue, 21 Jul 2015 21:27:42 +0200
-
-zuul (2.0.0-327-g3ebedde-wmf1precise1) precise-wikimedia; urgency=low
-
-  * Bump upstream 5984adc..3ebedde
-  * python-daemon to >= 2.0.4
-
- -- Antoine Musso <hashar@free.fr>  Tue, 21 Jul 2015 16:29:03 +0200
-
-
-zuul (2.0.0-306-g5984adc-wmf1precise1) precise-wikimedia; urgency=low
-
-  * Bump GitPython from 0.3.2.RC1 to 0.3.2.1. Fetched by dh_virtualenv.
-
- -- Antoine Musso <hashar@free.fr>  Mon, 20 Jul 2015 16:10:02 +0200
-
-zuul (2.0.0-304-g685ca22-wmf2precise1) precise-wikimedia; urgency=low
-
-  * T97106 zuul-cloner should use hard links when fetching from cache-dir
-    Incorporate patch https://review.openstack.org/#/c/117626/4 which makes
-    zuul-cloner to use hardlinks from git cache.
-
- -- Antoine Musso <hashar@free.fr>  Wed, 29 Apr 2015 15:42:21 +0200
-
-zuul (2.0.0-304-g685ca22-wmf1precise1) precise-wikimedia; urgency=low
-
-  * Snapshot from upstream, target is Wikimedia Ubuntu Precise which has some
-    updated packages for python modules packages.
-  * Adjust debian/gbp.conf to match our repository layout
-  * Bump debian/compat from 8 to 9
-  * Bump standard from 3.9.3 to 3.9.7
-
-  * Build for python 2.7 so we can get rid of the OrderDict and argparse
-    python modules which are only needed for python 2.6.
-  * Skip building sphinx doc entirely due to sphinx.programoutput not being
-    able to find the shipped commands (zuul, zuul-merger, zuul-cloner)
-  * Add dh-virtualenv to bring in additional dependencies not available in
-    Wikimedia Ubuntu Precise. Require an unreleased snapshot of dh-virtualenv
-    0.9, see:
-        https://phabricator.wikimedia.org/T91631
-        https://github.com/spotify/dh-virtualenv/issues/78
-
-  * Add init script for the zuul-merger
-  * Keep the zuul user on purge and thus delete noop debian/postrm
-  * Improve the init scripts to recognize START_DAEMON and do not start
-    daemons.
-
- -- Antoine Musso <hashar@free.fr>  Sat, 07 Mar 2015 21:00:13 +0100
-
-zuul (1.3.0-1) UNRELEASED; urgency=low
-
-  * Initial release (Closes: #705844)
-
- -- Paul Belanger <paul.belanger@polybeacon.com>  Thu, 31 Oct 2013 16:57:26 +0000
+zuul (2.1.0-151-g30a433b-wmf1precise1) precise-wikimedia; urgency=medium
+
+  * Bump upstream version
+  * Should deal with circular dependencies which caused a complete deadlock of
+    the Zuul scheduler: https://phabricator.wikimedia.org/T129938
+  * Fix typo in previous changelog entry
+  * Make myself the Maintainer
+  * Patch maintenance:
+  * wmf-soften-requirements:
+    - apscheduler 3.0.x is now shipped via dh_virtualenv
+      Note 3.1.0 fails with old setuptools
+    - soften pbr, 1.1.0 is just for tests
+  * Update-merge-status-after-merge-merge-is-submitted
+    - merged upstream
+  * Revert-Fix-passing-labels-to-Gerrit-when-they-are-no
+    - no more needed with upstream "connections" refactoring
+  * Rebased the couple Zuul cloner patches
+  * Bump GitPython 2.0.3..2.0.5
+
+ -- Antoine Musso <hashar@free.fr>  Tue, 31 May 2016 11:05:32 +0200
+
+zuul (2.1.0-95-g66c8e52-wmf1precise1) precise-wikimedia; urgency=medium
+
+  * Slowly catch up with upstream
+  * 0008-wmf-pin-python-daemon-2.1.0 is no more needed.
+
+ -- Antoine Musso <hashar@free.fr>  Mon, 30 May 2016 13:55:52 +0200
+
+zuul (2.1.0-60-g1cc37f7-wmf4precise1) precise-wikimedia; urgency=critical
+
+  * 0008-wmf-pin-python-daemon-2.1.0.patch
+    Solves https://phabricator.wikimedia.org/T119741
+
+ -- Antoine Musso <hashar@free.fr>  Thu, 26 Nov 2015 23:23:51 +0100
+
+zuul (2.1.0-60-g1cc37f7-wmf3precise1) precise-wikimedia; urgency=medium
+
+  * 0007-zuul-cloner-recognizes-bare-repos-in-cache-dir.patch
+    upstream version: https://review.openstack.org/#/c/249207/2
+
+ -- Antoine Musso <hashar@free.fr>  Tue, 24 Nov 2015 16:56:26 +0100
+
+zuul (2.1.0-60-g1cc37f7-wmf2precise1) precise-wikimedia; urgency=medium
+
+  * Refresh patch 0005-Cloner-Implement-cache-no-hardlinks-argument.patch to
+    latest upstream version https://review.openstack.org/#/c/117626/11
+    Closes https://phabricator.wikimedia.org/T97106
+
+ -- Antoine Musso <hashar@free.fr>  Fri, 13 Nov 2015 14:15:50 +0100
+
+zuul (2.1.0-60-g1cc37f7-wmf1precise1) precise-wikimedia; urgency=medium
+
+  * Bump upstream from 3ebedde to 1cc37f7
+
+  * Rebased patches and dropped patches merged upstream
+      0002-Merger-ensure_cloned-now-looks-for-.git.patch
+      0007-Remove-lockfile-from-requirements.patch
+
+  * Dependencies changes:
+    Babel            1.3 -> 2.1.1
+    lockfile      0.10.2 -> 0.11.0
+    python-daemon  2.0.5 -> 2.0.6
+    + gear 0.5.8
+
+ -- Antoine Musso <hashar@free.fr>  Mon, 02 Nov 2015 12:01:46 +0100
+
+zuul (2.0.0-327-g3ebedde-wmf3precise1) precise-wikimedia; urgency=medium
+
+  * 0008-Revert-Fix-passing-labels-to-Gerrit-when-they-are-no
+    Makes Zuul to send code-review and verified label votes using the shortcuts
+    --verified and --code-review since --label is case sensitive.
+   See https://phabricator.wikimedia.org/T106596
+
+ -- Antoine Musso <hashar@free.fr>  Thu, 23 Jul 2015 14:11:53 +0000
+
+zuul (2.0.0-327-g3ebedde-wmf2precise1) precise-wikimedia; urgency=medium
+
+  * 0007-Remove-lockfile-from-requirements.patch
+  * python-daemon 2.x shipped via the venv
+
+ -- Antoine Musso <hashar@free.fr>  Tue, 21 Jul 2015 21:27:42 +0200
+
+zuul (2.0.0-327-g3ebedde-wmf1precise1) precise-wikimedia; urgency=low
+
+  * Bump upstream 5984adc..3ebedde
+  * python-daemon to >= 2.0.4
+
+ -- Antoine Musso <hashar@free.fr>  Tue, 21 Jul 2015 16:29:03 +0200
+
+
+zuul (2.0.0-306-g5984adc-wmf1precise1) precise-wikimedia; urgency=low
+
+  * Bump GitPython from 0.3.2.RC1 to 0.3.2.1. Fetched by dh_virtualenv.
+
+ -- Antoine Musso <hashar@free.fr>  Mon, 20 Jul 2015 16:10:02 +0200
+
+zuul (2.0.0-304-g685ca22-wmf2precise1) precise-wikimedia; urgency=low
+
+  * T97106 zuul-cloner should use hard links when fetching from cache-dir
+    Incorporate patch https://review.openstack.org/#/c/117626/4 which makes
+    zuul-cloner to use hardlinks from git cache.
+
+ -- Antoine Musso <hashar@free.fr>  Wed, 29 Apr 2015 15:42:21 +0200
+
+zuul (2.0.0-304-g685ca22-wmf1precise1) precise-wikimedia; urgency=low
+
+  * Snapshot from upstream, target is Wikimedia Ubuntu Precise which has some
+    updated packages for python modules packages.
+  * Adjust debian/gbp.conf to match our repository layout
+  * Bump debian/compat from 8 to 9
+  * Bump standard from 3.9.3 to 3.9.7
+
+  * Build for python 2.7 so we can get rid of the OrderDict and argparse
+    python modules which are only needed for python 2.6.
+  * Skip building sphinx doc entirely due to sphinx.programoutput not being
+    able to find the shipped commands (zuul, zuul-merger, zuul-cloner)
+  * Add dh-virtualenv to bring in additional dependencies not available in
+    Wikimedia Ubuntu Precise. Require an unreleased snapshot of dh-virtualenv
+    0.9, see:
+        https://phabricator.wikimedia.org/T91631
+        https://github.com/spotify/dh-virtualenv/issues/78
+
+  * Add init script for the zuul-merger
+  * Keep the zuul user on purge and thus delete noop debian/postrm
+  * Improve the init scripts to recognize START_DAEMON and do not start
+    daemons.
+
+ -- Antoine Musso <hashar@free.fr>  Sat, 07 Mar 2015 21:00:13 +0100
+
+zuul (1.3.0-1) UNRELEASED; urgency=low
+
+  * Initial release (Closes: #705844)
+
+ -- Paul Belanger <paul.belanger@polybeacon.com>  Thu, 31 Oct 2013 16:57:26 +0000
diff --git a/debian/compat b/debian/compat
index ec63514..3b0923b 100644
--- a/debian/compat
+++ b/debian/compat
@@ -1 +1 @@
-9
+9
diff --git a/debian/control b/debian/control
index 7ecb5e4..176d6a3 100644
--- a/debian/control
+++ b/debian/control
@@ -1,102 +1,102 @@
-Source: zuul
-Maintainer: Antoine Musso <hashar@free.fr>
-Uploaders: Antoine Musso <hashar@free.fr>
-Section: python
-Priority: optional
-Build-Depends: debhelper (>= 9),
- python-virtualenv (>= 1.11),
- dh-virtualenv (>= 0.9),
- openstack-pkg-tools,
- python2.7,
-## Following was Build-Depends-Indep
-#
-# Dependencies comes from upstream sources, they are not available under
-# Wikimedia Ubuntu Precise distribution and will be provided via dh-virtualenv.
-#
-###################################
-# requirements.txt
-###################################
- python-pbr (>= 0.5.21),
-#Build in python 2.7:
-#python-argparse,
- python-yaml (>= 3.1.0),
- python-paste,
- python-webob,
- python-paramiko,
-# python-git (= 0.3.2.RC1-1),
-# python-gitdb,
-# python-smmap,
-#Build in python 2.7:
-#python-orderdict
-#python-daemon,
- python-extras,
-#python-statsd (>= 1.0.0, <3.0)
- python-voluptuous (>= 0.7),
- python-gear (>= 0.5.4),
-#python-apscheduler (>= 2.1.1),
-#python-prettytable (>= 0.6),
- python-tz,
-#python-babel (>= 1.0),
- python-six (>= 1.6.0),
-###################################
-# test-requirements.txt
-###################################
-#python-hacking (>= 0.9.2),
-#python-coverage (>= 3.6),
-#python-sphinx (>= 1.0.7+dfsg),
-#python-sphinxcontrib.blockdiag (>= 0.5.5),  added by dh-virtualenv
-#python-discover,
-#python-fixtures (>= 0.3.14),
-#python-keystoneclient (>= 0.4.2),
- python-subunit,
-#python-swiftclient (>= 1.6),
-#python-testrepository (>= 0.0.17),
-#python-testtools (>= 0.9.32),
-#python-sphinxcontrib.programoutput    added by dh-virtualenv
-###################################
-# Debian packaging dependencies
-###################################
- python-nose
-Standards-Version: 3.9.6
-X-Python-Version: = 2.7
-Homepage: https://launchpad.net/zuul
-Vcs-Git: git://github.com/pabelanger/zuul-deb.git
-Vcs-Browser: https://github.com/pabelanger/zuul-deb
-
-Package: zuul
-# any since dh-virtualenv includes python
-Architecture: any
-Pre-Depends: dpkg (>= 1.16.1),
- python2.7,
- ${misc:Pre-Depends}
-Depends: adduser,
- libjs-jquery,
- libjs-underscore,
- python-pbr (>= 0.5.21),
- python-yaml (>= 3.1.0),
- python-paste,
- python-webob,
- python-paramiko,
-# python-git (= 0.3.2.RC1-1),
-# python-gitdb,
-# python-smmap,
-# python-daemon,
- python-extras,
- python-voluptuous (>= 0.7),
- python-gear (>= 0.5.4),
-#python-apscheduler (>= 2.1.1),
- python-six (>= 1.6.0),
- python-tz,
- ${misc:Depends},
- ${python:Depends},
-# shlibs since dh-virtualenv includes python
- ${shlibs:Depends}
-Suggests:
- jenkins (>= 1.426),
-Description: Trunk gating system
- Zuul is a program that is used to gate the source code repository of a project
- so that changes are only merged if they pass tests.
- .
- The main component of Zuul is the scheduler. It receives events related to
- proposed changes (currently from Gerrit), triggers tests based on those events
- (currently on Jenkins), and reports back.
+Source: zuul
+Maintainer: Antoine Musso <hashar@free.fr>
+Uploaders: Antoine Musso <hashar@free.fr>
+Section: python
+Priority: optional
+Build-Depends: debhelper (>= 9),
+ python-virtualenv (>= 1.11),
+ dh-virtualenv (>= 0.9),
+ openstack-pkg-tools,
+ python2.7,
+## Following was Build-Depends-Indep
+#
+# Dependencies comes from upstream sources, they are not available under
+# Wikimedia Ubuntu Precise distribution and will be provided via dh-virtualenv.
+#
+###################################
+# requirements.txt
+###################################
+ python-pbr (>= 0.5.21),
+#Build in python 2.7:
+#python-argparse,
+ python-yaml (>= 3.1.0),
+ python-paste,
+ python-webob,
+ python-paramiko,
+# python-git (= 0.3.2.RC1-1),
+# python-gitdb,
+# python-smmap,
+#Build in python 2.7:
+#python-orderdict
+#python-daemon,
+ python-extras,
+#python-statsd (>= 1.0.0, <3.0)
+ python-voluptuous (>= 0.7),
+ python-gear (>= 0.5.4),
+#python-apscheduler (>= 2.1.1),
+#python-prettytable (>= 0.6),
+ python-tz,
+#python-babel (>= 1.0),
+ python-six (>= 1.6.0),
+###################################
+# test-requirements.txt
+###################################
+#python-hacking (>= 0.9.2),
+#python-coverage (>= 3.6),
+#python-sphinx (>= 1.0.7+dfsg),
+#python-sphinxcontrib.blockdiag (>= 0.5.5),  added by dh-virtualenv
+#python-discover,
+#python-fixtures (>= 0.3.14),
+#python-keystoneclient (>= 0.4.2),
+ python-subunit,
+#python-swiftclient (>= 1.6),
+#python-testrepository (>= 0.0.17),
+#python-testtools (>= 0.9.32),
+#python-sphinxcontrib.programoutput    added by dh-virtualenv
+###################################
+# Debian packaging dependencies
+###################################
+ python-nose
+Standards-Version: 3.9.6
+X-Python-Version: = 2.7
+Homepage: https://launchpad.net/zuul
+Vcs-Git: git://github.com/pabelanger/zuul-deb.git
+Vcs-Browser: https://github.com/pabelanger/zuul-deb
+
+Package: zuul
+# any since dh-virtualenv includes python
+Architecture: any
+Pre-Depends: dpkg (>= 1.16.1),
+ python2.7,
+ ${misc:Pre-Depends}
+Depends: adduser,
+ libjs-jquery,
+ libjs-underscore,
+ python-pbr (>= 0.5.21),
+ python-yaml (>= 3.1.0),
+ python-paste,
+ python-webob,
+ python-paramiko,
+# python-git (= 0.3.2.RC1-1),
+# python-gitdb,
+# python-smmap,
+# python-daemon,
+ python-extras,
+ python-voluptuous (>= 0.7),
+ python-gear (>= 0.5.4),
+#python-apscheduler (>= 2.1.1),
+ python-six (>= 1.6.0),
+ python-tz,
+ ${misc:Depends},
+ ${python:Depends},
+# shlibs since dh-virtualenv includes python
+ ${shlibs:Depends}
+Suggests:
+ jenkins (>= 1.426),
+Description: Trunk gating system
+ Zuul is a program that is used to gate the source code repository of a project
+ so that changes are only merged if they pass tests.
+ .
+ The main component of Zuul is the scheduler. It receives events related to
+ proposed changes (currently from Gerrit), triggers tests based on those events
+ (currently on Jenkins), and reports back.
diff --git a/debian/copyright b/debian/copyright
index 70c22e3..ead118e 100644
--- a/debian/copyright
+++ b/debian/copyright
@@ -1,32 +1,32 @@
-Format: http://www.debian.org/doc/packaging-manuals/copyright-format/1.0/
-Upstream-Name: Zuul
-Upstream-Contact: OpenStack Continuous Integration Administrators <openstack-ci-admins@lists.launchpad.net>
-Source: http://launchpad.net/zuul
-
-Files: *
-Copyright: 2011-2013, OpenStack, LLC.
-           2012, Hewlett-Packard Development Company, L.P.
-           2013, Antoine "hashar" Musso
-           2013, OpenStack Foundation
-           2013, Timo "Krinkle" Tijhof
-           2013, Wikimedia Foundation Inc.
-License: Apache
-
-Files: debian/*
-Copyright: 2012-2013 Paul Belanger <paul.belanger@polybeacon.com>
-           2013-2015 Antoine Musso <hashar@free.fr>
-           2013-2015 Wikimedia Foundation Inc.
-License: Apache
-
-License: Apache
- Licensed under the Apache License, Version 2.0 (the "License"); you may
- not use this file except in compliance with the License. You may obtain
- a copy of the License at
- .
-      http://www.apache.org/licenses/LICENSE-2.0
- .
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- License for the specific language governing permissions and limitations
- under the License.
+Format: http://www.debian.org/doc/packaging-manuals/copyright-format/1.0/
+Upstream-Name: Zuul
+Upstream-Contact: OpenStack Continuous Integration Administrators <openstack-ci-admins@lists.launchpad.net>
+Source: http://launchpad.net/zuul
+
+Files: *
+Copyright: 2011-2013, OpenStack, LLC.
+           2012, Hewlett-Packard Development Company, L.P.
+           2013, Antoine "hashar" Musso
+           2013, OpenStack Foundation
+           2013, Timo "Krinkle" Tijhof
+           2013, Wikimedia Foundation Inc.
+License: Apache
+
+Files: debian/*
+Copyright: 2012-2013 Paul Belanger <paul.belanger@polybeacon.com>
+           2013-2015 Antoine Musso <hashar@free.fr>
+           2013-2015 Wikimedia Foundation Inc.
+License: Apache
+
+License: Apache
+ Licensed under the Apache License, Version 2.0 (the "License"); you may
+ not use this file except in compliance with the License. You may obtain
+ a copy of the License at
+ .
+      http://www.apache.org/licenses/LICENSE-2.0
+ .
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+ WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+ License for the specific language governing permissions and limitations
+ under the License.
diff --git a/debian/dirs b/debian/dirs
index 731fe5f..154846a 100644
--- a/debian/dirs
+++ b/debian/dirs
@@ -1,6 +1,6 @@
-/etc/zuul
-/var/lib/zuul
-/var/lib/zuul/git
-/var/log/zuul
-/var/run/zuul
-/var/run/zuul-merger
+/etc/zuul
+/var/lib/zuul
+/var/lib/zuul/git
+/var/log/zuul
+/var/run/zuul
+/var/run/zuul-merger
diff --git a/debian/examples b/debian/examples
index 9a5a799..22a2431 100644
--- a/debian/examples
+++ b/debian/examples
@@ -1,4 +1,4 @@
-etc/layout.yaml-sample
-etc/logging.conf-sample
-etc/zuul.conf-sample
-
+etc/layout.yaml-sample
+etc/logging.conf-sample
+etc/zuul.conf-sample
+
diff --git a/debian/gbp.conf b/debian/gbp.conf
index 3568935..13babf7 100644
--- a/debian/gbp.conf
+++ b/debian/gbp.conf
@@ -1,10 +1,10 @@
-[git-buildpackage]
-pristine-tar = False
-compression = gzip
-
-ignore-branch = True
-
-# source and /debian/ are in the same tree and we have a branch per repository.
-# To avoid having to pass -git-debian-branch when checking out branches, just
-# point to HEAD which has the appropriate /debian/
-debian-branch = HEAD
+[git-buildpackage]
+pristine-tar = False
+compression = gzip
+
+ignore-branch = True
+
+# source and /debian/ are in the same tree and we have a branch per repository.
+# To avoid having to pass -git-debian-branch when checking out branches, just
+# point to HEAD which has the appropriate /debian/
+debian-branch = HEAD
diff --git a/debian/links b/debian/links
index 8c9b27e..ce1abd6 100644
--- a/debian/links
+++ b/debian/links
@@ -1,12 +1,12 @@
-# Overwrite jquery.js from upstream tarball with a link to jquery.js
-# provided by jQuery Debian package
-/usr/share/javascript/jquery/jquery.js usr/share/doc/zuul/html/_static/jquery.js
-
-# Overwrite underscore.js from upstream tarball with a link to underscore.min.js
-# provided by Underscore Debian package
-/usr/share/javascript/underscore/underscore.min.js usr/share/doc/zuul/html/_static/underscore.js
-
-/usr/share/python/zuul/bin/zuul usr/bin/zuul
-/usr/share/python/zuul/bin/zuul-cloner usr/bin/zuul-cloner
-/usr/share/python/zuul/bin/zuul-merger usr/bin/zuul-merger
-/usr/share/python/zuul/bin/zuul-server usr/bin/zuul-server
+# Overwrite jquery.js from upstream tarball with a link to jquery.js
+# provided by jQuery Debian package
+/usr/share/javascript/jquery/jquery.js usr/share/doc/zuul/html/_static/jquery.js
+
+# Overwrite underscore.js from upstream tarball with a link to underscore.min.js
+# provided by Underscore Debian package
+/usr/share/javascript/underscore/underscore.min.js usr/share/doc/zuul/html/_static/underscore.js
+
+/usr/share/python/zuul/bin/zuul usr/bin/zuul
+/usr/share/python/zuul/bin/zuul-cloner usr/bin/zuul-cloner
+/usr/share/python/zuul/bin/zuul-merger usr/bin/zuul-merger
+/usr/share/python/zuul/bin/zuul-server usr/bin/zuul-server
diff --git a/debian/manpages b/debian/manpages
index 0d670ea..67424f5 100644
--- a/debian/manpages
+++ b/debian/manpages
@@ -1 +1 @@
-build/docs/man/*
+build/docs/man/*
diff --git a/debian/patches/0001-wmf-soften-requirements.patch b/debian/patches/0001-wmf-soften-requirements.patch
index 71062c7..5539df9 100644
--- a/debian/patches/0001-wmf-soften-requirements.patch
+++ b/debian/patches/0001-wmf-soften-requirements.patch
@@ -1,44 +1,44 @@
-From: Antoine Musso <hashar@free.fr>
-Date: Mon, 4 Nov 2013 12:17:58 +0100
-Subject: wmf: soften requirements
-
-WebOb and paramiko have a minimal version which is enforced by OpenStack
-but is probably not really needed.
-
-ordereddict and argparse are included in python 2.7 and we do not care
-about python 2.6.
-
-apscheduler 3.1.0 fail with old setuptools
-https://github.com/agronholm/apscheduler/issues/123
-
-Change-Id: Ie2bffccd0b4aa8b505a5c7de8174a42d4395d9d7
----
- requirements.txt | 9 ++++-----
- 1 file changed, 4 insertions(+), 5 deletions(-)
-
-diff --git a/requirements.txt b/requirements.txt
-index 77ac0a5..0c85d7a 100644
---- a/requirements.txt
-+++ b/requirements.txt
-@@ -1,17 +1,16 @@
--pbr>=1.1.0
-+pbr>=0.5.21
- 
- PyYAML>=3.1.0
- Paste
--WebOb>=1.2.3
--paramiko>=1.8.0,<2.0.0
-+WebOb
-+paramiko<2.0.0
- GitPython>=0.3.3
--ordereddict
- python-daemon>=2.0.4,<2.1.0
- extras
- statsd>=1.0.0,<3.0
- voluptuous>=0.7
- gear>=0.5.7,<1.0.0
--apscheduler>=3.0
-+apscheduler>=3.0,<3.1.0
- PrettyTable>=0.6,<0.8
- babel>=1.0
- six>=1.6.0
+From: Antoine Musso <hashar@free.fr>
+Date: Mon, 4 Nov 2013 12:17:58 +0100
+Subject: wmf: soften requirements
+
+WebOb and paramiko have a minimal version which is enforced by OpenStack
+but is probably not really needed.
+
+ordereddict and argparse are included in python 2.7 and we do not care
+about python 2.6.
+
+apscheduler 3.1.0 fail with old setuptools
+https://github.com/agronholm/apscheduler/issues/123
+
+Change-Id: Ie2bffccd0b4aa8b505a5c7de8174a42d4395d9d7
+---
+ requirements.txt | 9 ++++-----
+ 1 file changed, 4 insertions(+), 5 deletions(-)
+
+diff --git a/requirements.txt b/requirements.txt
+index 77ac0a5..0c85d7a 100644
+--- a/requirements.txt
++++ b/requirements.txt
+@@ -1,17 +1,16 @@
+-pbr>=1.1.0
++pbr>=0.5.21
+ 
+ PyYAML>=3.1.0
+ Paste
+-WebOb>=1.2.3
+-paramiko>=1.8.0,<2.0.0
++WebOb
++paramiko<2.0.0
+ GitPython>=0.3.3
+-ordereddict
+ python-daemon>=2.0.4,<2.1.0
+ extras
+ statsd>=1.0.0,<3.0
+ voluptuous>=0.7
+ gear>=0.5.7,<1.0.0
+-apscheduler>=3.0
++apscheduler>=3.0,<3.1.0
+ PrettyTable>=0.6,<0.8
+ babel>=1.0
+ six>=1.6.0
diff --git a/debian/patches/0002-Ensure-the-repository-configuration-lock-is-released.patch b/debian/patches/0002-Ensure-the-repository-configuration-lock-is-released.patch
index 52db537..e290311 100644
--- a/debian/patches/0002-Ensure-the-repository-configuration-lock-is-released.patch
+++ b/debian/patches/0002-Ensure-the-repository-configuration-lock-is-released.patch
@@ -1,49 +1,49 @@
-From: Ori Livneh <ori@wikimedia.org>
-Date: Tue, 13 Jan 2015 20:42:09 -0800
-Subject: Ensure the repository configuration lock is released
-
-When zuul initializes a fresh clone of a git repository, it attempts to set
-the user.name and user.email configuration options in .git/config. This job is
-the responsibility of GitConfigParser, which is currently broken, because
-GitConfigParser.write() acquires a lock but does not release it.  The lock is
-released in the object's __del__ method, which is invoked when the object is
-about to be dereferenced. This is not a reliable means of ensuring the
-lock is released, because it can break if there is a circular reference keeping
-the object alive, or if another GitConfigParser object for the same
-repository is initiated while a reference to the existing one is still held.
-The author of GitPython appears to still be struggling with this as of
-commit a05e49d2419 of GitPython[0].  Fortunately, we don't have to chase this
-down: we can just clear the lock when we know we're done, rather than hope that
-__del__ gets called.
-
-  [0]: https://github.com/gitpython-developers/GitPython/commit/a05e49d2419
-
-Change-Id: Ie869f0b147577d3b2a0b57afba4474ee07753210
----
- zuul/merger/merger.py | 13 ++++++++++++-
- 1 file changed, 12 insertions(+), 1 deletion(-)
-
-diff --git a/zuul/merger/merger.py b/zuul/merger/merger.py
-index c6ae35d..fed8394 100644
---- a/zuul/merger/merger.py
-+++ b/zuul/merger/merger.py
-@@ -70,7 +70,18 @@ class Repo(object):
-         if self.username:
-             repo.config_writer().set_value('user', 'name',
-                                            self.username)
--        repo.config_writer().write()
-+        config_writer = repo.config_writer()
-+        try:
-+            # GitConfigParser.write() acquires a lock but does not release it.
-+            # The lock is released in the object's __del__ method, which is
-+            # invoked when the object is about to be dereferenced. This is not
-+            # a reliable means of ensuring the lock is released, because it can
-+            # break if there is a circular reference keeping the object alive,
-+            # or if another GitConfigParser object for the same repository is
-+            # initiated while a reference to the existing one is still held.
-+            config_writer.write()
-+        finally:
-+            config_writer._lock._release_lock()
-         self._initialized = True
- 
-     def isInitialized(self):
+From: Ori Livneh <ori@wikimedia.org>
+Date: Tue, 13 Jan 2015 20:42:09 -0800
+Subject: Ensure the repository configuration lock is released
+
+When zuul initializes a fresh clone of a git repository, it attempts to set
+the user.name and user.email configuration options in .git/config. This job is
+the responsibility of GitConfigParser, which is currently broken, because
+GitConfigParser.write() acquires a lock but does not release it.  The lock is
+released in the object's __del__ method, which is invoked when the object is
+about to be dereferenced. This is not a reliable means of ensuring the
+lock is released, because it can break if there is a circular reference keeping
+the object alive, or if another GitConfigParser object for the same
+repository is initiated while a reference to the existing one is still held.
+The author of GitPython appears to still be struggling with this as of
+commit a05e49d2419 of GitPython[0].  Fortunately, we don't have to chase this
+down: we can just clear the lock when we know we're done, rather than hope that
+__del__ gets called.
+
+  [0]: https://github.com/gitpython-developers/GitPython/commit/a05e49d2419
+
+Change-Id: Ie869f0b147577d3b2a0b57afba4474ee07753210
+---
+ zuul/merger/merger.py | 13 ++++++++++++-
+ 1 file changed, 12 insertions(+), 1 deletion(-)
+
+diff --git a/zuul/merger/merger.py b/zuul/merger/merger.py
+index c6ae35d..fed8394 100644
+--- a/zuul/merger/merger.py
++++ b/zuul/merger/merger.py
+@@ -70,7 +70,18 @@ class Repo(object):
+         if self.username:
+             repo.config_writer().set_value('user', 'name',
+                                            self.username)
+-        repo.config_writer().write()
++        config_writer = repo.config_writer()
++        try:
++            # GitConfigParser.write() acquires a lock but does not release it.
++            # The lock is released in the object's __del__ method, which is
++            # invoked when the object is about to be dereferenced. This is not
++            # a reliable means of ensuring the lock is released, because it can
++            # break if there is a circular reference keeping the object alive,
++            # or if another GitConfigParser object for the same repository is
++            # initiated while a reference to the existing one is still held.
++            config_writer.write()
++        finally:
++            config_writer._lock._release_lock()
+         self._initialized = True
+ 
+     def isInitialized(self):
diff --git a/debian/patches/0003-Replace-python-shebang-with-python2.7.patch b/debian/patches/0003-Replace-python-shebang-with-python2.7.patch
index 014feea..0b3379a 100644
--- a/debian/patches/0003-Replace-python-shebang-with-python2.7.patch
+++ b/debian/patches/0003-Replace-python-shebang-with-python2.7.patch
@@ -1,187 +1,187 @@
-From: Antoine Musso <hashar@free.fr>
-Date: Tue, 10 Mar 2015 12:48:13 +0100
-Subject: Replace python shebang with python2.7
-
-The Precise package is only meant for python2.7, get rid of some lintian
-errors by pointing to python2.7.
-
-Change-Id: I68b805d0e1b49971063ed98ccc187f4456f9501d
----
- setup.py                      | 2 +-
- tests/base.py                 | 2 +-
- tests/test_cloner.py          | 2 +-
- tests/test_layoutvalidator.py | 2 +-
- tests/test_merger_repo.py     | 2 +-
- tests/test_requirements.py    | 2 +-
- tests/test_scheduler.py       | 2 +-
- tests/test_webapp.py          | 2 +-
- tests/test_zuultrigger.py     | 2 +-
- tools/trigger-job.py          | 2 +-
- tools/zuul-changes.py         | 2 +-
- zuul/cmd/__init__.py          | 2 +-
- zuul/cmd/client.py            | 2 +-
- zuul/cmd/cloner.py            | 2 +-
- zuul/cmd/merger.py            | 2 +-
- zuul/cmd/server.py            | 2 +-
- 16 files changed, 16 insertions(+), 16 deletions(-)
-
-diff --git a/setup.py b/setup.py
-index 70c2b3f..4ec20a6 100644
---- a/setup.py
-+++ b/setup.py
-@@ -1,4 +1,4 @@
--#!/usr/bin/env python
-+#!/usr/bin/env python2.7
- # Copyright (c) 2013 Hewlett-Packard Development Company, L.P.
- #
- # Licensed under the Apache License, Version 2.0 (the "License");
-diff --git a/tests/base.py b/tests/base.py
-index 405caa0..4f41be9 100755
---- a/tests/base.py
-+++ b/tests/base.py
-@@ -1,4 +1,4 @@
--#!/usr/bin/env python
-+#!/usr/bin/env python2.7
- 
- # Copyright 2012 Hewlett-Packard Development Company, L.P.
- #
-diff --git a/tests/test_cloner.py b/tests/test_cloner.py
-index 137c157..1ce4a0a 100644
---- a/tests/test_cloner.py
-+++ b/tests/test_cloner.py
-@@ -1,4 +1,4 @@
--#!/usr/bin/env python
-+#!/usr/bin/env python2.7
- 
- # Copyright 2012 Hewlett-Packard Development Company, L.P.
- # Copyright 2014 Wikimedia Foundation Inc.
-diff --git a/tests/test_layoutvalidator.py b/tests/test_layoutvalidator.py
-index 3dc3234..99732a5 100644
---- a/tests/test_layoutvalidator.py
-+++ b/tests/test_layoutvalidator.py
-@@ -1,4 +1,4 @@
--#!/usr/bin/env python
-+#!/usr/bin/env python2.7
- 
- # Copyright 2013 OpenStack Foundation
- #
-diff --git a/tests/test_merger_repo.py b/tests/test_merger_repo.py
-index 454f3cc..55d16a0 100644
---- a/tests/test_merger_repo.py
-+++ b/tests/test_merger_repo.py
-@@ -1,4 +1,4 @@
--#!/usr/bin/env python
-+#!/usr/bin/env python2.7
- 
- # Copyright 2012 Hewlett-Packard Development Company, L.P.
- # Copyright 2014 Wikimedia Foundation Inc.
-diff --git a/tests/test_requirements.py b/tests/test_requirements.py
-index 3ae56ad..202010c 100644
---- a/tests/test_requirements.py
-+++ b/tests/test_requirements.py
-@@ -1,4 +1,4 @@
--#!/usr/bin/env python
-+#!/usr/bin/env python2.7
- 
- # Copyright 2012-2014 Hewlett-Packard Development Company, L.P.
- #
-diff --git a/tests/test_scheduler.py b/tests/test_scheduler.py
-index fe7c7cc..cbf1495 100755
---- a/tests/test_scheduler.py
-+++ b/tests/test_scheduler.py
-@@ -1,4 +1,4 @@
--#!/usr/bin/env python
-+#!/usr/bin/env python2.7
- 
- # Copyright 2012 Hewlett-Packard Development Company, L.P.
- #
-diff --git a/tests/test_webapp.py b/tests/test_webapp.py
-index b127c51..8a88261 100644
---- a/tests/test_webapp.py
-+++ b/tests/test_webapp.py
-@@ -1,4 +1,4 @@
--#!/usr/bin/env python
-+#!/usr/bin/env python2.7
- 
- # Copyright 2014 Hewlett-Packard Development Company, L.P.
- # Copyright 2014 Rackspace Australia
-diff --git a/tests/test_zuultrigger.py b/tests/test_zuultrigger.py
-index 0d52fc9..49b79ad 100644
---- a/tests/test_zuultrigger.py
-+++ b/tests/test_zuultrigger.py
-@@ -1,4 +1,4 @@
--#!/usr/bin/env python
-+#!/usr/bin/env python2.7
- 
- # Copyright 2014 Hewlett-Packard Development Company, L.P.
- #
-diff --git a/tools/trigger-job.py b/tools/trigger-job.py
-index dff4e3f..4651d7d 100755
---- a/tools/trigger-job.py
-+++ b/tools/trigger-job.py
-@@ -1,4 +1,4 @@
--#!/usr/bin/env python
-+#!/usr/bin/env python2.7
- # Copyright 2013 OpenStack Foundation
- #
- # Licensed under the Apache License, Version 2.0 (the "License"); you may
-diff --git a/tools/zuul-changes.py b/tools/zuul-changes.py
-index 9dbf504..d825ef1 100755
---- a/tools/zuul-changes.py
-+++ b/tools/zuul-changes.py
-@@ -1,4 +1,4 @@
--#!/usr/bin/env python
-+#!/usr/bin/env python2.7
- # Copyright 2013 OpenStack Foundation
- # Copyright 2015 Hewlett-Packard Development Company, L.P.
- #
-diff --git a/zuul/cmd/__init__.py b/zuul/cmd/__init__.py
-index 2902c50..7ee7900 100644
---- a/zuul/cmd/__init__.py
-+++ b/zuul/cmd/__init__.py
-@@ -1,4 +1,4 @@
--#!/usr/bin/env python
-+#!/usr/bin/env python2.7
- # Copyright 2012 Hewlett-Packard Development Company, L.P.
- # Copyright 2013 OpenStack Foundation
- #
-diff --git a/zuul/cmd/client.py b/zuul/cmd/client.py
-index 59ac419..e31f467 100644
---- a/zuul/cmd/client.py
-+++ b/zuul/cmd/client.py
-@@ -1,4 +1,4 @@
--#!/usr/bin/env python
-+#!/usr/bin/env python2.7
- # Copyright 2012 Hewlett-Packard Development Company, L.P.
- # Copyright 2013 OpenStack Foundation
- #
-diff --git a/zuul/cmd/cloner.py b/zuul/cmd/cloner.py
-index c616aa1..a922a34 100755
---- a/zuul/cmd/cloner.py
-+++ b/zuul/cmd/cloner.py
-@@ -1,4 +1,4 @@
--#!/usr/bin/env python
-+#!/usr/bin/env python2.7
- #
- # Copyright 2014 Antoine "hashar" Musso
- # Copyright 2014 Wikimedia Foundation Inc.
-diff --git a/zuul/cmd/merger.py b/zuul/cmd/merger.py
-index df215fd..5f51ee6 100644
---- a/zuul/cmd/merger.py
-+++ b/zuul/cmd/merger.py
-@@ -1,4 +1,4 @@
--#!/usr/bin/env python
-+#!/usr/bin/env python2.7
- # Copyright 2012 Hewlett-Packard Development Company, L.P.
- # Copyright 2013-2014 OpenStack Foundation
- #
-diff --git a/zuul/cmd/server.py b/zuul/cmd/server.py
-index b1cd050..8861972 100755
---- a/zuul/cmd/server.py
-+++ b/zuul/cmd/server.py
-@@ -1,4 +1,4 @@
--#!/usr/bin/env python
-+#!/usr/bin/env python2.7
- # Copyright 2012 Hewlett-Packard Development Company, L.P.
- # Copyright 2013 OpenStack Foundation
- #
+From: Antoine Musso <hashar@free.fr>
+Date: Tue, 10 Mar 2015 12:48:13 +0100
+Subject: Replace python shebang with python2.7
+
+The Precise package is only meant for python2.7, get rid of some lintian
+errors by pointing to python2.7.
+
+Change-Id: I68b805d0e1b49971063ed98ccc187f4456f9501d
+---
+ setup.py                      | 2 +-
+ tests/base.py                 | 2 +-
+ tests/test_cloner.py          | 2 +-
+ tests/test_layoutvalidator.py | 2 +-
+ tests/test_merger_repo.py     | 2 +-
+ tests/test_requirements.py    | 2 +-
+ tests/test_scheduler.py       | 2 +-
+ tests/test_webapp.py          | 2 +-
+ tests/test_zuultrigger.py     | 2 +-
+ tools/trigger-job.py          | 2 +-
+ tools/zuul-changes.py         | 2 +-
+ zuul/cmd/__init__.py          | 2 +-
+ zuul/cmd/client.py            | 2 +-
+ zuul/cmd/cloner.py            | 2 +-
+ zuul/cmd/merger.py            | 2 +-
+ zuul/cmd/server.py            | 2 +-
+ 16 files changed, 16 insertions(+), 16 deletions(-)
+
+diff --git a/setup.py b/setup.py
+index 70c2b3f..4ec20a6 100644
+--- a/setup.py
++++ b/setup.py
+@@ -1,4 +1,4 @@
+-#!/usr/bin/env python
++#!/usr/bin/env python2.7
+ # Copyright (c) 2013 Hewlett-Packard Development Company, L.P.
+ #
+ # Licensed under the Apache License, Version 2.0 (the "License");
+diff --git a/tests/base.py b/tests/base.py
+index 405caa0..4f41be9 100755
+--- a/tests/base.py
++++ b/tests/base.py
+@@ -1,4 +1,4 @@
+-#!/usr/bin/env python
++#!/usr/bin/env python2.7
+ 
+ # Copyright 2012 Hewlett-Packard Development Company, L.P.
+ #
+diff --git a/tests/test_cloner.py b/tests/test_cloner.py
+index 137c157..1ce4a0a 100644
+--- a/tests/test_cloner.py
++++ b/tests/test_cloner.py
+@@ -1,4 +1,4 @@
+-#!/usr/bin/env python
++#!/usr/bin/env python2.7
+ 
+ # Copyright 2012 Hewlett-Packard Development Company, L.P.
+ # Copyright 2014 Wikimedia Foundation Inc.
+diff --git a/tests/test_layoutvalidator.py b/tests/test_layoutvalidator.py
+index 3dc3234..99732a5 100644
+--- a/tests/test_layoutvalidator.py
++++ b/tests/test_layoutvalidator.py
+@@ -1,4 +1,4 @@
+-#!/usr/bin/env python
++#!/usr/bin/env python2.7
+ 
+ # Copyright 2013 OpenStack Foundation
+ #
+diff --git a/tests/test_merger_repo.py b/tests/test_merger_repo.py
+index 454f3cc..55d16a0 100644
+--- a/tests/test_merger_repo.py
++++ b/tests/test_merger_repo.py
+@@ -1,4 +1,4 @@
+-#!/usr/bin/env python
++#!/usr/bin/env python2.7
+ 
+ # Copyright 2012 Hewlett-Packard Development Company, L.P.
+ # Copyright 2014 Wikimedia Foundation Inc.
+diff --git a/tests/test_requirements.py b/tests/test_requirements.py
+index 3ae56ad..202010c 100644
+--- a/tests/test_requirements.py
++++ b/tests/test_requirements.py
+@@ -1,4 +1,4 @@
+-#!/usr/bin/env python
++#!/usr/bin/env python2.7
+ 
+ # Copyright 2012-2014 Hewlett-Packard Development Company, L.P.
+ #
+diff --git a/tests/test_scheduler.py b/tests/test_scheduler.py
+index fe7c7cc..cbf1495 100755
+--- a/tests/test_scheduler.py
++++ b/tests/test_scheduler.py
+@@ -1,4 +1,4 @@
+-#!/usr/bin/env python
++#!/usr/bin/env python2.7
+ 
+ # Copyright 2012 Hewlett-Packard Development Company, L.P.
+ #
+diff --git a/tests/test_webapp.py b/tests/test_webapp.py
+index b127c51..8a88261 100644
+--- a/tests/test_webapp.py
++++ b/tests/test_webapp.py
+@@ -1,4 +1,4 @@
+-#!/usr/bin/env python
++#!/usr/bin/env python2.7
+ 
+ # Copyright 2014 Hewlett-Packard Development Company, L.P.
+ # Copyright 2014 Rackspace Australia
+diff --git a/tests/test_zuultrigger.py b/tests/test_zuultrigger.py
+index 0d52fc9..49b79ad 100644
+--- a/tests/test_zuultrigger.py
++++ b/tests/test_zuultrigger.py
+@@ -1,4 +1,4 @@
+-#!/usr/bin/env python
++#!/usr/bin/env python2.7
+ 
+ # Copyright 2014 Hewlett-Packard Development Company, L.P.
+ #
+diff --git a/tools/trigger-job.py b/tools/trigger-job.py
+index dff4e3f..4651d7d 100755
+--- a/tools/trigger-job.py
++++ b/tools/trigger-job.py
+@@ -1,4 +1,4 @@
+-#!/usr/bin/env python
++#!/usr/bin/env python2.7
+ # Copyright 2013 OpenStack Foundation
+ #
+ # Licensed under the Apache License, Version 2.0 (the "License"); you may
+diff --git a/tools/zuul-changes.py b/tools/zuul-changes.py
+index 9dbf504..d825ef1 100755
+--- a/tools/zuul-changes.py
++++ b/tools/zuul-changes.py
+@@ -1,4 +1,4 @@
+-#!/usr/bin/env python
++#!/usr/bin/env python2.7
+ # Copyright 2013 OpenStack Foundation
+ # Copyright 2015 Hewlett-Packard Development Company, L.P.
+ #
+diff --git a/zuul/cmd/__init__.py b/zuul/cmd/__init__.py
+index 2902c50..7ee7900 100644
+--- a/zuul/cmd/__init__.py
++++ b/zuul/cmd/__init__.py
+@@ -1,4 +1,4 @@
+-#!/usr/bin/env python
++#!/usr/bin/env python2.7
+ # Copyright 2012 Hewlett-Packard Development Company, L.P.
+ # Copyright 2013 OpenStack Foundation
+ #
+diff --git a/zuul/cmd/client.py b/zuul/cmd/client.py
+index 59ac419..e31f467 100644
+--- a/zuul/cmd/client.py
++++ b/zuul/cmd/client.py
+@@ -1,4 +1,4 @@
+-#!/usr/bin/env python
++#!/usr/bin/env python2.7
+ # Copyright 2012 Hewlett-Packard Development Company, L.P.
+ # Copyright 2013 OpenStack Foundation
+ #
+diff --git a/zuul/cmd/cloner.py b/zuul/cmd/cloner.py
+index c616aa1..a922a34 100755
+--- a/zuul/cmd/cloner.py
++++ b/zuul/cmd/cloner.py
+@@ -1,4 +1,4 @@
+-#!/usr/bin/env python
++#!/usr/bin/env python2.7
+ #
+ # Copyright 2014 Antoine "hashar" Musso
+ # Copyright 2014 Wikimedia Foundation Inc.
+diff --git a/zuul/cmd/merger.py b/zuul/cmd/merger.py
+index df215fd..5f51ee6 100644
+--- a/zuul/cmd/merger.py
++++ b/zuul/cmd/merger.py
+@@ -1,4 +1,4 @@
+-#!/usr/bin/env python
++#!/usr/bin/env python2.7
+ # Copyright 2012 Hewlett-Packard Development Company, L.P.
+ # Copyright 2013-2014 OpenStack Foundation
+ #
+diff --git a/zuul/cmd/server.py b/zuul/cmd/server.py
+index b1cd050..8861972 100755
+--- a/zuul/cmd/server.py
++++ b/zuul/cmd/server.py
+@@ -1,4 +1,4 @@
+-#!/usr/bin/env python
++#!/usr/bin/env python2.7
+ # Copyright 2012 Hewlett-Packard Development Company, L.P.
+ # Copyright 2013 OpenStack Foundation
+ #
diff --git a/debian/patches/0004-Cloner-Implement-cache-no-hardlinks-argument.patch b/debian/patches/0004-Cloner-Implement-cache-no-hardlinks-argument.patch
index 905027c..66aa6f7 100644
--- a/debian/patches/0004-Cloner-Implement-cache-no-hardlinks-argument.patch
+++ b/debian/patches/0004-Cloner-Implement-cache-no-hardlinks-argument.patch
@@ -1,92 +1,92 @@
-From: Timo Tijhof <krinklemail@gmail.com>
-Date: Thu, 28 Aug 2014 23:08:38 +0200
-Subject: Cloner: Implement cache-no-hardlinks argument
-
-The cache directory is prefixing with 'file://' which forces in
-git-clone to always copy files. This is intended for OpenStack usage.
-
-By default, git has a feature to hardlink files if the source
-and destination on the same disk. This saves space and speeds up
-the cloning process.
-
-This exposes that feature to users of zuul-cloner.
-
-Change-Id: Ib8796f9dc5ccfeac4a3c9b3d30629c7db0f84739
----
- doc/source/cloner.rst |  5 +++++
- zuul/cmd/cloner.py    |  5 +++++
- zuul/lib/cloner.py    | 11 ++++++++---
- 3 files changed, 18 insertions(+), 3 deletions(-)
-
-diff --git a/doc/source/cloner.rst b/doc/source/cloner.rst
-index 70577cc..c0ca990 100644
---- a/doc/source/cloner.rst
-+++ b/doc/source/cloner.rst
-@@ -108,3 +108,8 @@ repository has all the information in the upstream repository.
- The default for ``--cache-dir`` is taken from the environment variable
- ``ZUUL_CACHE_DIR``. A value provided explicitly on the command line
- overrides the environment variable setting.
-+
-+The ``--cache-no-hardlinks`` option can be used to force git to
-+always copy git objects from the cache directory. By default, if
-+the cache directory is on the same disk as the workspace, git-clone
-+will hardlink git objects to speed up the process and save space.
-diff --git a/zuul/cmd/cloner.py b/zuul/cmd/cloner.py
-index a922a34..63825e4 100755
---- a/zuul/cmd/cloner.py
-+++ b/zuul/cmd/cloner.py
-@@ -57,6 +57,10 @@ class Cloner(zuul.cmd.ZuulApp):
-                                   'Can also be set via ZUUL_CACHE_DIR '
-                                   'environment variable.'
-                                   ))
-+        parser.add_argument('--cache-no-hardlinks', dest='cache_no_hardlinks',
-+                            action='store_true',
-+                            help=('force git-clone to never use hardlinks when'
-+                                  'fetching from the cache directory.'))
-         parser.add_argument('git_base_url',
-                             help='reference repo to clone from')
-         parser.add_argument('projects', nargs='+',
-@@ -145,6 +149,7 @@ class Cloner(zuul.cmd.ZuulApp):
-             clone_map_file=self.args.clone_map_file,
-             project_branches=project_branches,
-             cache_dir=self.args.cache_dir,
-+            cache_no_hardlinks=self.args.cache_no_hardlinks,
-         )
-         cloner.execute()
- 
-diff --git a/zuul/lib/cloner.py b/zuul/lib/cloner.py
-index f0235a6..3a12a40 100644
---- a/zuul/lib/cloner.py
-+++ b/zuul/lib/cloner.py
-@@ -29,7 +29,8 @@ class Cloner(object):
- 
-     def __init__(self, git_base_url, projects, workspace, zuul_branch,
-                  zuul_ref, zuul_url, branch=None, clone_map_file=None,
--                 project_branches=None, cache_dir=None):
-+                 project_branches=None, cache_dir=None,
-+                 cache_no_hardlinks=None):
- 
-         self.clone_map = []
-         self.dests = None
-@@ -37,6 +38,7 @@ class Cloner(object):
-         self.branch = branch
-         self.git_url = git_base_url
-         self.cache_dir = cache_dir
-+        self.cache_no_hardlinks = cache_no_hardlinks
-         self.projects = projects
-         self.workspace = workspace
-         self.zuul_branch = zuul_branch or ''
-@@ -74,8 +76,11 @@ class Cloner(object):
-         if (self.cache_dir and
-             os.path.exists(git_cache) and
-             not repo_is_cloned):
--            # file:// tells git not to hard-link across repos
--            git_cache = 'file://%s' % git_cache
-+
-+            if self.cache_no_hardlinks:
-+                # file:// tells git not to hard-link across repos
-+                git_cache = 'file://%s' % git_cache
-+
-             self.log.info("Creating repo %s from cache %s",
-                           project, git_cache)
-             new_repo = git.Repo.clone_from(git_cache, dest)
+From: Timo Tijhof <krinklemail@gmail.com>
+Date: Thu, 28 Aug 2014 23:08:38 +0200
+Subject: Cloner: Implement cache-no-hardlinks argument
+
+The cache directory is prefixing with 'file://' which forces in
+git-clone to always copy files. This is intended for OpenStack usage.
+
+By default, git has a feature to hardlink files if the source
+and destination on the same disk. This saves space and speeds up
+the cloning process.
+
+This exposes that feature to users of zuul-cloner.
+
+Change-Id: Ib8796f9dc5ccfeac4a3c9b3d30629c7db0f84739
+---
+ doc/source/cloner.rst |  5 +++++
+ zuul/cmd/cloner.py    |  5 +++++
+ zuul/lib/cloner.py    | 11 ++++++++---
+ 3 files changed, 18 insertions(+), 3 deletions(-)
+
+diff --git a/doc/source/cloner.rst b/doc/source/cloner.rst
+index 70577cc..c0ca990 100644
+--- a/doc/source/cloner.rst
++++ b/doc/source/cloner.rst
+@@ -108,3 +108,8 @@ repository has all the information in the upstream repository.
+ The default for ``--cache-dir`` is taken from the environment variable
+ ``ZUUL_CACHE_DIR``. A value provided explicitly on the command line
+ overrides the environment variable setting.
++
++The ``--cache-no-hardlinks`` option can be used to force git to
++always copy git objects from the cache directory. By default, if
++the cache directory is on the same disk as the workspace, git-clone
++will hardlink git objects to speed up the process and save space.
+diff --git a/zuul/cmd/cloner.py b/zuul/cmd/cloner.py
+index a922a34..63825e4 100755
+--- a/zuul/cmd/cloner.py
++++ b/zuul/cmd/cloner.py
+@@ -57,6 +57,10 @@ class Cloner(zuul.cmd.ZuulApp):
+                                   'Can also be set via ZUUL_CACHE_DIR '
+                                   'environment variable.'
+                                   ))
++        parser.add_argument('--cache-no-hardlinks', dest='cache_no_hardlinks',
++                            action='store_true',
++                            help=('force git-clone to never use hardlinks when'
++                                  'fetching from the cache directory.'))
+         parser.add_argument('git_base_url',
+                             help='reference repo to clone from')
+         parser.add_argument('projects', nargs='+',
+@@ -145,6 +149,7 @@ class Cloner(zuul.cmd.ZuulApp):
+             clone_map_file=self.args.clone_map_file,
+             project_branches=project_branches,
+             cache_dir=self.args.cache_dir,
++            cache_no_hardlinks=self.args.cache_no_hardlinks,
+         )
+         cloner.execute()
+ 
+diff --git a/zuul/lib/cloner.py b/zuul/lib/cloner.py
+index f0235a6..3a12a40 100644
+--- a/zuul/lib/cloner.py
++++ b/zuul/lib/cloner.py
+@@ -29,7 +29,8 @@ class Cloner(object):
+ 
+     def __init__(self, git_base_url, projects, workspace, zuul_branch,
+                  zuul_ref, zuul_url, branch=None, clone_map_file=None,
+-                 project_branches=None, cache_dir=None):
++                 project_branches=None, cache_dir=None,
++                 cache_no_hardlinks=None):
+ 
+         self.clone_map = []
+         self.dests = None
+@@ -37,6 +38,7 @@ class Cloner(object):
+         self.branch = branch
+         self.git_url = git_base_url
+         self.cache_dir = cache_dir
++        self.cache_no_hardlinks = cache_no_hardlinks
+         self.projects = projects
+         self.workspace = workspace
+         self.zuul_branch = zuul_branch or ''
+@@ -74,8 +76,11 @@ class Cloner(object):
+         if (self.cache_dir and
+             os.path.exists(git_cache) and
+             not repo_is_cloned):
+-            # file:// tells git not to hard-link across repos
+-            git_cache = 'file://%s' % git_cache
++
++            if self.cache_no_hardlinks:
++                # file:// tells git not to hard-link across repos
++                git_cache = 'file://%s' % git_cache
++
+             self.log.info("Creating repo %s from cache %s",
+                           project, git_cache)
+             new_repo = git.Repo.clone_from(git_cache, dest)
diff --git a/debian/patches/0005-zuul-cloner-recognizes-bare-repos-in-cache-dir.patch b/debian/patches/0005-zuul-cloner-recognizes-bare-repos-in-cache-dir.patch
index 01df976..44da101 100644
--- a/debian/patches/0005-zuul-cloner-recognizes-bare-repos-in-cache-dir.patch
+++ b/debian/patches/0005-zuul-cloner-recognizes-bare-repos-in-cache-dir.patch
@@ -1,122 +1,122 @@
-From: Antoine Musso <hashar@free.fr>
-Date: Tue, 24 Nov 2015 14:11:05 +0100
-Subject: zuul-cloner: recognizes bare repos in cache dir
-
-A git cache populated with non-bare repositories (ie workspace checked
-out) has a couple issues:
-
-* the working copy checkout is unneeded for caching purposes and
-  uselessly consumes disk space.  That makes disk images larger than
-  strictly needed.
-
-* checked out files might conflict with repositories names. git bare
-  clones are suffixed with '.git' which reduces the risk.
-
-Recognize bare clones before non-bare clones.
-Short circuit the logic when neither is found.
-Rename git_cache to repo_cache to avoid confusion since it now varies.
-
-Cover bare repos suffixed with '.git' are properly recognized by
-inspecing the logging message emitted when the cache is hit.
-
-Change-Id: I3d5177787de5c4d629969cee1f715d692275ad9a
----
- tests/test_cloner.py | 23 +++++++++++++++++++++++
- zuul/lib/cloner.py   | 39 ++++++++++++++++++++++++---------------
- 2 files changed, 47 insertions(+), 15 deletions(-)
-
-diff --git a/tests/test_cloner.py b/tests/test_cloner.py
-index 1ce4a0a..2f5f8a7 100644
---- a/tests/test_cloner.py
-+++ b/tests/test_cloner.py
-@@ -15,6 +15,7 @@
- # License for the specific language governing permissions and limitations
- # under the License.
- 
-+import fixtures
- import logging
- import os
- import shutil
-@@ -118,6 +119,28 @@ class TestCloner(ZuulTestCase):
-         self.worker.release()
-         self.waitUntilSettled()
- 
-+    def test_recognize_bare_cache(self):
-+        cache_root = os.path.join(self.test_root, "cache")
-+        upstream_repo_path = os.path.join(self.upstream_root, 'org/project1')
-+        cache_bare_path = os.path.join(cache_root, 'org/project1.git')
-+        cache_repo = git.Repo.clone_from(upstream_repo_path, cache_bare_path,
-+                                         bare=True)
-+        self.assertTrue(type(cache_repo.bare), msg='Cache repo is bare')
-+
-+        log_fixture = self.useFixture(fixtures.FakeLogger(level=logging.INFO))
-+        cloner = zuul.lib.cloner.Cloner(
-+            git_base_url=self.upstream_root,
-+            projects=['org/project1'],
-+            workspace=self.workspace_root,
-+            zuul_branch='HEAD',
-+            zuul_ref='HEAD',
-+            zuul_url=self.git_root,
-+            cache_dir=cache_root
-+        )
-+        cloner.execute()
-+        self.assertIn('Creating repo org/project1 from cache file://%s' % (
-+                      cache_bare_path), log_fixture.output)
-+
-     def test_one_branch(self):
-         self.worker.hold_jobs_in_build = True
- 
-diff --git a/zuul/lib/cloner.py b/zuul/lib/cloner.py
-index 3a12a40..77f52fa 100644
---- a/zuul/lib/cloner.py
-+++ b/zuul/lib/cloner.py
-@@ -71,25 +71,34 @@ class Cloner(object):
-     def cloneUpstream(self, project, dest):
-         # Check for a cached git repo first
-         git_cache = '%s/%s' % (self.cache_dir, project)
-+        git_cache_bare = '%s.git' % (git_cache)
-         git_upstream = '%s/%s' % (self.git_url, project)
-+
-         repo_is_cloned = os.path.exists(os.path.join(dest, '.git'))
--        if (self.cache_dir and
--            os.path.exists(git_cache) and
--            not repo_is_cloned):
--
--            if self.cache_no_hardlinks:
--                # file:// tells git not to hard-link across repos
--                git_cache = 'file://%s' % git_cache
--
--            self.log.info("Creating repo %s from cache %s",
--                          project, git_cache)
--            new_repo = git.Repo.clone_from(git_cache, dest)
--            self.log.info("Updating origin remote in repo %s to %s",
--                          project, git_upstream)
--            new_repo.remotes.origin.config_writer.set('url', git_upstream)
--        else:
-+
-+        repo_cache = None
-+        if (self.cache_dir and not repo_is_cloned):
-+            if os.path.exists(git_cache_bare):
-+                repo_cache = git_cache_bare
-+            elif os.path.exists(git_cache):
-+                repo_cache = git_cache
-+
-+            if repo_cache:
-+                if self.cache_no_hardlinks:
-+                    # file:// tells git not to hard-link across repos
-+                    repo_cache = 'file://%s' % repo_cache
-+
-+                self.log.info("Creating repo %s from cache %s",
-+                              project, repo_cache)
-+                new_repo = git.Repo.clone_from(repo_cache, dest)
-+                self.log.info("Updating origin remote in repo %s to %s",
-+                              project, git_upstream)
-+                new_repo.remotes.origin.config_writer.set('url', git_upstream)
-+
-+        if not repo_cache:
-             self.log.info("Creating repo %s from upstream %s",
-                           project, git_upstream)
-+
-         repo = Repo(
-             remote=git_upstream,
-             local=dest,
+From: Antoine Musso <hashar@free.fr>
+Date: Tue, 24 Nov 2015 14:11:05 +0100
+Subject: zuul-cloner: recognizes bare repos in cache dir
+
+A git cache populated with non-bare repositories (ie workspace checked
+out) has a couple issues:
+
+* the working copy checkout is unneeded for caching purposes and
+  uselessly consumes disk space.  That makes disk images larger than
+  strictly needed.
+
+* checked out files might conflict with repositories names. git bare
+  clones are suffixed with '.git' which reduces the risk.
+
+Recognize bare clones before non-bare clones.
+Short circuit the logic when neither is found.
+Rename git_cache to repo_cache to avoid confusion since it now varies.
+
+Cover bare repos suffixed with '.git' are properly recognized by
+inspecing the logging message emitted when the cache is hit.
+
+Change-Id: I3d5177787de5c4d629969cee1f715d692275ad9a
+---
+ tests/test_cloner.py | 23 +++++++++++++++++++++++
+ zuul/lib/cloner.py   | 39 ++++++++++++++++++++++++---------------
+ 2 files changed, 47 insertions(+), 15 deletions(-)
+
+diff --git a/tests/test_cloner.py b/tests/test_cloner.py
+index 1ce4a0a..2f5f8a7 100644
+--- a/tests/test_cloner.py
++++ b/tests/test_cloner.py
+@@ -15,6 +15,7 @@
+ # License for the specific language governing permissions and limitations
+ # under the License.
+ 
++import fixtures
+ import logging
+ import os
+ import shutil
+@@ -118,6 +119,28 @@ class TestCloner(ZuulTestCase):
+         self.worker.release()
+         self.waitUntilSettled()
+ 
++    def test_recognize_bare_cache(self):
++        cache_root = os.path.join(self.test_root, "cache")
++        upstream_repo_path = os.path.join(self.upstream_root, 'org/project1')
++        cache_bare_path = os.path.join(cache_root, 'org/project1.git')
++        cache_repo = git.Repo.clone_from(upstream_repo_path, cache_bare_path,
++                                         bare=True)
++        self.assertTrue(type(cache_repo.bare), msg='Cache repo is bare')
++
++        log_fixture = self.useFixture(fixtures.FakeLogger(level=logging.INFO))
++        cloner = zuul.lib.cloner.Cloner(
++            git_base_url=self.upstream_root,
++            projects=['org/project1'],
++            workspace=self.workspace_root,
++            zuul_branch='HEAD',
++            zuul_ref='HEAD',
++            zuul_url=self.git_root,
++            cache_dir=cache_root
++        )
++        cloner.execute()
++        self.assertIn('Creating repo org/project1 from cache file://%s' % (
++                      cache_bare_path), log_fixture.output)
++
+     def test_one_branch(self):
+         self.worker.hold_jobs_in_build = True
+ 
+diff --git a/zuul/lib/cloner.py b/zuul/lib/cloner.py
+index 3a12a40..77f52fa 100644
+--- a/zuul/lib/cloner.py
++++ b/zuul/lib/cloner.py
+@@ -71,25 +71,34 @@ class Cloner(object):
+     def cloneUpstream(self, project, dest):
+         # Check for a cached git repo first
+         git_cache = '%s/%s' % (self.cache_dir, project)
++        git_cache_bare = '%s.git' % (git_cache)
+         git_upstream = '%s/%s' % (self.git_url, project)
++
+         repo_is_cloned = os.path.exists(os.path.join(dest, '.git'))
+-        if (self.cache_dir and
+-            os.path.exists(git_cache) and
+-            not repo_is_cloned):
+-
+-            if self.cache_no_hardlinks:
+-                # file:// tells git not to hard-link across repos
+-                git_cache = 'file://%s' % git_cache
+-
+-            self.log.info("Creating repo %s from cache %s",
+-                          project, git_cache)
+-            new_repo = git.Repo.clone_from(git_cache, dest)
+-            self.log.info("Updating origin remote in repo %s to %s",
+-                          project, git_upstream)
+-            new_repo.remotes.origin.config_writer.set('url', git_upstream)
+-        else:
++
++        repo_cache = None
++        if (self.cache_dir and not repo_is_cloned):
++            if os.path.exists(git_cache_bare):
++                repo_cache = git_cache_bare
++            elif os.path.exists(git_cache):
++                repo_cache = git_cache
++
++            if repo_cache:
++                if self.cache_no_hardlinks:
++                    # file:// tells git not to hard-link across repos
++                    repo_cache = 'file://%s' % repo_cache
++
++                self.log.info("Creating repo %s from cache %s",
++                              project, repo_cache)
++                new_repo = git.Repo.clone_from(repo_cache, dest)
++                self.log.info("Updating origin remote in repo %s to %s",
++                              project, git_upstream)
++                new_repo.remotes.origin.config_writer.set('url', git_upstream)
++
++        if not repo_cache:
+             self.log.info("Creating repo %s from upstream %s",
+                           project, git_upstream)
++
+         repo = Repo(
+             remote=git_upstream,
+             local=dest,
diff --git a/debian/patches/series b/debian/patches/series
index 34f7472..2e5875f 100644
--- a/debian/patches/series
+++ b/debian/patches/series
@@ -1,5 +1,5 @@
-0001-wmf-soften-requirements.patch
-0002-Ensure-the-repository-configuration-lock-is-released.patch
-0003-Replace-python-shebang-with-python2.7.patch
-0004-Cloner-Implement-cache-no-hardlinks-argument.patch
-0005-zuul-cloner-recognizes-bare-repos-in-cache-dir.patch
+0001-wmf-soften-requirements.patch
+0002-Ensure-the-repository-configuration-lock-is-released.patch
+0003-Replace-python-shebang-with-python2.7.patch
+0004-Cloner-Implement-cache-no-hardlinks-argument.patch
+0005-zuul-cloner-recognizes-bare-repos-in-cache-dir.patch
diff --git a/debian/postinst b/debian/postinst
index 6b02c01..5998528 100644
--- a/debian/postinst
+++ b/debian/postinst
@@ -1,67 +1,67 @@
-#!/bin/sh
-# postinst script for #PACKAGE#
-#
-# see: dh_installdeb(1)
-
-set -e
-
-# summary of how this script can be called:
-#        * <postinst> `configure' <most-recently-configured-version>
-#        * <old-postinst> `abort-upgrade' <new version>
-#        * <conflictor's-postinst> `abort-remove' `in-favour' <package>
-#          <new-version>
-#        * <postinst> `abort-remove'
-#        * <deconfigured's-postinst> `abort-deconfigure' `in-favour'
-#          <failed-install-package> <version> `removing'
-#          <conflicting-package> <version>
-# for details, see http://www.debian.org/doc/debian-policy/ or
-# the debian-policy package
-
-
-case "$1" in
-	configure)
-		# add zuul user
-		if ! getent passwd zuul > /dev/null ; then
-			echo 'Adding system user for Zuul' 1>&2
-			adduser --system --group --quiet \
-				--home /var/lib/zuul \
-				--no-create-home --disabled-login \
-				--gecos "Zuul trunk gating daemon" \
-				zuul
-		fi
-
-		chown zuul:zuul /etc/zuul
-		chown zuul:zuul /var/lib/zuul
-		chown zuul:adm /var/log/zuul
-
-		chmod 0755 /etc/zuul
-		chmod 0755 /var/lib/zuul
-		chmod 0755 /var/log/zuul
-
-		# compile the installed modules, not the whole hierarchy since there
-		# are symlink back to python2.7 built in modules
-		pycompile --package='zuul' /usr/share/python/zuul/lib/python2.7/site-packages
-
-		# Populate python2.7 in Zuul virtualenv
-		# Make sure it is properly up-to-date even if the trigger is supposed
-		# to take care of it.
-		if ! cmp -s /usr/bin/python2.7 /usr/share/python/zuul/bin/python2.7; then
-			cp -a /usr/bin/python2.7 /usr/share/python/zuul/bin
-		fi
-	;;
-
-	abort-upgrade|abort-remove|abort-deconfigure)
-	;;
-
-	*)
-		echo "postinst called with unknown argument \`$1'" >&2
-		exit 1
-	;;
-esac
-
-# dh_installdeb will replace this with shell code automatically
-# generated by other debhelper scripts.
-
-#DEBHELPER#
-
-exit 0
+#!/bin/sh
+# postinst script for #PACKAGE#
+#
+# see: dh_installdeb(1)
+
+set -e
+
+# summary of how this script can be called:
+#        * <postinst> `configure' <most-recently-configured-version>
+#        * <old-postinst> `abort-upgrade' <new version>
+#        * <conflictor's-postinst> `abort-remove' `in-favour' <package>
+#          <new-version>
+#        * <postinst> `abort-remove'
+#        * <deconfigured's-postinst> `abort-deconfigure' `in-favour'
+#          <failed-install-package> <version> `removing'
+#          <conflicting-package> <version>
+# for details, see http://www.debian.org/doc/debian-policy/ or
+# the debian-policy package
+
+
+case "$1" in
+	configure)
+		# add zuul user
+		if ! getent passwd zuul > /dev/null ; then
+			echo 'Adding system user for Zuul' 1>&2
+			adduser --system --group --quiet \
+				--home /var/lib/zuul \
+				--no-create-home --disabled-login \
+				--gecos "Zuul trunk gating daemon" \
+				zuul
+		fi
+
+		chown zuul:zuul /etc/zuul
+		chown zuul:zuul /var/lib/zuul
+		chown zuul:adm /var/log/zuul
+
+		chmod 0755 /etc/zuul
+		chmod 0755 /var/lib/zuul
+		chmod 0755 /var/log/zuul
+
+		# compile the installed modules, not the whole hierarchy since there
+		# are symlink back to python2.7 built in modules
+		pycompile --package='zuul' /usr/share/python/zuul/lib/python2.7/site-packages
+
+		# Populate python2.7 in Zuul virtualenv
+		# Make sure it is properly up-to-date even if the trigger is supposed
+		# to take care of it.
+		if ! cmp -s /usr/bin/python2.7 /usr/share/python/zuul/bin/python2.7; then
+			cp -a /usr/bin/python2.7 /usr/share/python/zuul/bin
+		fi
+	;;
+
+	abort-upgrade|abort-remove|abort-deconfigure)
+	;;
+
+	*)
+		echo "postinst called with unknown argument \`$1'" >&2
+		exit 1
+	;;
+esac
+
+# dh_installdeb will replace this with shell code automatically
+# generated by other debhelper scripts.
+
+#DEBHELPER#
+
+exit 0
diff --git a/debian/prerm b/debian/prerm
index 8bb26b3..ef64603 100644
--- a/debian/prerm
+++ b/debian/prerm
@@ -1,46 +1,46 @@
-#!/bin/sh
-# prerm script for #PACKAGE#
-#
-# see: dh_installdeb(1)
-
-set -e
-
-# summary of how this script can be called:
-#        * <prerm> `remove'
-#        * <old-prerm> `upgrade' <new-version>
-#        * <new-prerm> `failed-upgrade' <old-version>
-#        * <conflictor's-prerm> `remove' `in-favour' <package> <new-version>
-#        * <deconfigured's-prerm> `deconfigure' `in-favour'
-#          <package-being-installed> <version> `removing'
-#          <conflicting-package> <version>
-# for details, see http://www.debian.org/doc/debian-policy/ or
-# the debian-policy package
-
-
-case "$1" in
-    remove|upgrade|deconfigure)
-		# clean the installed modules, not the whole hierarchy since there
-		# are symlink back to python2.7 built in modules
-		pyclean --package='zuul' /usr/share/python/zuul/lib/python2.7/site-packages
-
-		# Left over by dh-virtualenv trigger
-		if [ -e /usr/share/python/zuul/bin/python2.7,orig ]; then
-			rm /usr/share/python/zuul/bin/python2.7,orig
-		fi
-    ;;
-
-    failed-upgrade)
-    ;;
-
-    *)
-        echo "prerm called with unknown argument \`$1'" >&2
-        exit 1
-    ;;
-esac
-
-# dh_installdeb will replace this with shell code automatically
-# generated by other debhelper scripts.
-
-#DEBHELPER#
-
-exit 0
+#!/bin/sh
+# prerm script for #PACKAGE#
+#
+# see: dh_installdeb(1)
+
+set -e
+
+# summary of how this script can be called:
+#        * <prerm> `remove'
+#        * <old-prerm> `upgrade' <new-version>
+#        * <new-prerm> `failed-upgrade' <old-version>
+#        * <conflictor's-prerm> `remove' `in-favour' <package> <new-version>
+#        * <deconfigured's-prerm> `deconfigure' `in-favour'
+#          <package-being-installed> <version> `removing'
+#          <conflicting-package> <version>
+# for details, see http://www.debian.org/doc/debian-policy/ or
+# the debian-policy package
+
+
+case "$1" in
+    remove|upgrade|deconfigure)
+		# clean the installed modules, not the whole hierarchy since there
+		# are symlink back to python2.7 built in modules
+		pyclean --package='zuul' /usr/share/python/zuul/lib/python2.7/site-packages
+
+		# Left over by dh-virtualenv trigger
+		if [ -e /usr/share/python/zuul/bin/python2.7,orig ]; then
+			rm /usr/share/python/zuul/bin/python2.7,orig
+		fi
+    ;;
+
+    failed-upgrade)
+    ;;
+
+    *)
+        echo "prerm called with unknown argument \`$1'" >&2
+        exit 1
+    ;;
+esac
+
+# dh_installdeb will replace this with shell code automatically
+# generated by other debhelper scripts.
+
+#DEBHELPER#
+
+exit 0
diff --git a/debian/pydist-overrides b/debian/pydist-overrides
index c182094..34d310f 100644
--- a/debian/pydist-overrides
+++ b/debian/pydist-overrides
@@ -1,2 +1,2 @@
-python_daemon python-daemon
-statsd python-statsd
+python_daemon python-daemon
+statsd python-statsd
diff --git a/debian/rules b/debian/rules
index a1fc9e9..d085b28 100755
--- a/debian/rules
+++ b/debian/rules
@@ -1,47 +1,47 @@
-#!/usr/bin/make -f
-
-# Uncomment this to turn on verbose mode.
-export DH_VERBOSE=1
-
-UPSTREAM_GIT = git://github.com/openstack-infra/zuul.git
-
-include /usr/share/openstack-pkg-tools/pkgos.make
-
-# Make pbr to use the version from the Debian changelog
-export OSLO_PACKAGE_VERSION=$(DEBVERS)
-
-%:
-	dh $@ --with python-virtualenv
-
-override_dh_auto_build:
-	dh_auto_build
-	# Ubuntu Precise is missing blockdiag/pillow. Skip doc
-	#PYTHONPATH=. http_proxy='.' https_proxy='.' $(MAKE) -C doc html man BUILDDIR=$(CURDIR)/build/docs
-
-override_dh_auto_clean:
-	dh_auto_clean
-	rm -rf *.egg*
-
-override_dh_installinit:
-	dh_installinit --name=zuul
-	dh_installinit --name=zuul-merger
-
-override_dh_virtualenv:
-	# Needs dh_virtualenv 0.9+ for --use-system-packages
-	# The .pyc are generated in postinst
-	PIP_NO_COMPILE=1 PYTHONDONTWRITEBYTECODE=true VIRTUALENV_VERBOSE=true dh_virtualenv \
-		--python '/usr/bin/python2.7' \
-		--use-system-packages \
-		--no-package ordereddict \
-		--no-package paramiko \
-		--no-package WebOb \
-		--extra-pip-arg '--no-compile' \
-		--pypi-url https://pypi.python.org/simple
-	find $(CURDIR)/debian/zuul/usr/share/python/zuul -type f -name '*.pyc' -delete
-	# python2.7 is populated in postinst and refreshed by dh-virtualenv trigger
-	rm $(CURDIR)/debian/zuul/usr/share/python/zuul/bin/python2.7
-
-override_dh_auto_test:
-ifeq (,$(filter nocheck,$(DEB_BUILD_OPTIONS)))
-	nosetests
-endif
+#!/usr/bin/make -f
+
+# Uncomment this to turn on verbose mode.
+export DH_VERBOSE=1
+
+UPSTREAM_GIT = git://github.com/openstack-infra/zuul.git
+
+include /usr/share/openstack-pkg-tools/pkgos.make
+
+# Make pbr to use the version from the Debian changelog
+export OSLO_PACKAGE_VERSION=$(DEBVERS)
+
+%:
+	dh $@ --with python-virtualenv
+
+override_dh_auto_build:
+	dh_auto_build
+	# Ubuntu Precise is missing blockdiag/pillow. Skip doc
+	#PYTHONPATH=. http_proxy='.' https_proxy='.' $(MAKE) -C doc html man BUILDDIR=$(CURDIR)/build/docs
+
+override_dh_auto_clean:
+	dh_auto_clean
+	rm -rf *.egg*
+
+override_dh_installinit:
+	dh_installinit --name=zuul
+	dh_installinit --name=zuul-merger
+
+override_dh_virtualenv:
+	# Needs dh_virtualenv 0.9+ for --use-system-packages
+	# The .pyc are generated in postinst
+	PIP_NO_COMPILE=1 PYTHONDONTWRITEBYTECODE=true VIRTUALENV_VERBOSE=true dh_virtualenv \
+		--python '/usr/bin/python2.7' \
+		--use-system-packages \
+		--no-package ordereddict \
+		--no-package paramiko \
+		--no-package WebOb \
+		--extra-pip-arg '--no-compile' \
+		--pypi-url https://pypi.python.org/simple
+	find $(CURDIR)/debian/zuul/usr/share/python/zuul -type f -name '*.pyc' -delete
+	# python2.7 is populated in postinst and refreshed by dh-virtualenv trigger
+	rm $(CURDIR)/debian/zuul/usr/share/python/zuul/bin/python2.7
+
+override_dh_auto_test:
+ifeq (,$(filter nocheck,$(DEB_BUILD_OPTIONS)))
+	nosetests
+endif
diff --git a/debian/source/format b/debian/source/format
index 163aaf8..db1d718 100644
--- a/debian/source/format
+++ b/debian/source/format
@@ -1 +1 @@
-3.0 (quilt)
+3.0 (quilt)
diff --git a/debian/source/lintian-overrides b/debian/source/lintian-overrides
index 5361481..6425a4f 100644
--- a/debian/source/lintian-overrides
+++ b/debian/source/lintian-overrides
@@ -1,5 +1,5 @@
-# dh-virtualenv is not recognized by lintian
-zuul source: python-depends-but-no-python-helper
-
-# Wikimedia distribution is 'precise-wikimedia'
-zuul source: bad-distribution-in-changes-file
+# dh-virtualenv is not recognized by lintian
+zuul source: python-depends-but-no-python-helper
+
+# Wikimedia distribution is 'precise-wikimedia'
+zuul source: bad-distribution-in-changes-file
diff --git a/debian/source/options b/debian/source/options
index d3fdcc3..852f77c 100644
--- a/debian/source/options
+++ b/debian/source/options
@@ -1 +1 @@
-extend-diff-ignore = ".gitreview"
+extend-diff-ignore = ".gitreview"
diff --git a/debian/watch b/debian/watch
index 247b90d..4007e3c 100644
--- a/debian/watch
+++ b/debian/watch
@@ -1,2 +1,2 @@
-version=3
-http://tarballs.openstack.org/zuul/zuul-([\d\.]+)\.tar\.gz debian
+version=3
+http://tarballs.openstack.org/zuul/zuul-([\d\.]+)\.tar\.gz debian
diff --git a/debian/zuul-merger.default b/debian/zuul-merger.default
index 32e3fe0..f4ec7af 100644
--- a/debian/zuul-merger.default
+++ b/debian/zuul-merger.default
@@ -1,7 +1,7 @@
-# START_DAEMON:
-#   Set to 1 to start the zuul-merger daemon
-START_DAEMON=0
-
-# DAEMON_ARGS:
-#   Arguments passed to the Zuul merger
-#DAEMON_ARGS='-c /etc/zuul/zuul-merger.conf'
+# START_DAEMON:
+#   Set to 1 to start the zuul-merger daemon
+START_DAEMON=0
+
+# DAEMON_ARGS:
+#   Arguments passed to the Zuul merger
+#DAEMON_ARGS='-c /etc/zuul/zuul-merger.conf'
diff --git a/debian/zuul-merger.init b/debian/zuul-merger.init
index dad6e41..42139de 100644
--- a/debian/zuul-merger.init
+++ b/debian/zuul-merger.init
@@ -1,145 +1,145 @@
-#! /bin/sh
-### BEGIN INIT INFO
-# Provides:          zuul-merger
-# Required-Start:    $remote_fs $syslog
-# Required-Stop:     $remote_fs $syslog
-# Default-Start:     2 3 4 5
-# Default-Stop:      0 1 6
-# Short-Description: Zuul Merger
-# Description:       Trunk gating system
-### END INIT INFO
-
-# Do NOT "set -e"
-
-# PATH should only include /usr/* if it runs after the mountnfs.sh script
-PATH=/sbin:/usr/sbin:/bin:/usr/bin
-DESC="Zuul Merger"
-NAME=zuul-merger
-DAEMON=/usr/bin/$NAME
-PIDFILE=/var/run/$NAME/$NAME.pid
-SCRIPTNAME=/etc/init.d/$NAME
-USER=zuul
-
-# Exit if the package is not installed
-[ -x "$DAEMON" ] || exit 0
-
-START_DAEMON=0
-DAEMON_ARGS=''
-
-# Load the VERBOSE setting and other rcS variables
-. /lib/init/vars.sh
-
-# Define LSB log_* functions.
-# Depend on lsb-base (>= 3.0-6) to ensure that this file is present.
-. /lib/lsb/init-functions
-
-# Read configuration variable file if it is present
-if [ -r /etc/default/$NAME ] ; then
-	. /etc/default/$NAME
-fi
-
-if ! [ "${START_DAEMON}" = "1" ] ; then
-	log_daemon_msg "$DESC: /etc/default/$NAME is not set to START_DAEMON=1: exiting"
-	log_end_msg 1
-	exit 0
-fi
-
-#
-# Function that starts the daemon/service
-#
-do_start()
-{
-	# Return
-	#   0 if daemon has been started
-	#   1 if daemon was already running
-	#   2 if daemon could not be started
-
-	mkdir -p /var/run/$NAME
-	chown $USER /var/run/$NAME
-	ulimit -n 8192
-	start-stop-daemon --start --quiet --pidfile $PIDFILE -c $USER --exec $DAEMON --test > /dev/null \
-		|| return 1
-	start-stop-daemon --start --quiet --pidfile $PIDFILE -c $USER --exec $DAEMON -- \
-		$DAEMON_ARGS \
-		|| return 2
-	# Add code here, if necessary, that waits for the process to be ready
-	# to handle requests from services started subsequently which depend
-	# on this one.  As a last resort, sleep for some time.
-}
-
-#
-# Function that stops the daemon/service
-#
-do_stop()
-{
-	# Return
-	#   0 if daemon has been stopped
-	#   1 if daemon was already stopped
-	#   2 if daemon could not be stopped
-	#   other if a failure occurred
-	start-stop-daemon --stop --signal 9 --pidfile $PIDFILE
-	RETVAL="$?"
-	[ "$RETVAL" = 2 ] && return 2
-	rm -f /var/run/$NAME/*
-	return "$RETVAL"
-}
-
-#
-# Function that sends a SIGHUP to the daemon/service
-#
-do_reload() {
-	#
-	# If the daemon can reload its configuration without
-	# restarting (for example, when it is sent a SIGHUP),
-	# then implement that here.
-	#
-	start-stop-daemon --stop --signal 1 --quiet --pidfile $PIDFILE --name $NAME
-	return 0
-}
-
-case "$1" in
-  start)
-	[ "$VERBOSE" != no ] && log_daemon_msg "Starting $DESC" "$NAME"
-	do_start
-	case "$?" in
-		0|1) [ "$VERBOSE" != no ] && log_end_msg 0 ;;
-		2) [ "$VERBOSE" != no ] && log_end_msg 1 ;;
-	esac
-	;;
-  stop)
-	[ "$VERBOSE" != no ] && log_daemon_msg "Stopping $DESC" "$NAME"
-	do_stop
-	case "$?" in
-		0|1) [ "$VERBOSE" != no ] && log_end_msg 0 ;;
-		2) [ "$VERBOSE" != no ] && log_end_msg 1 ;;
-	esac
-	;;
-  status)
-	status_of_proc "$DAEMON" "$NAME" && exit 0 || exit $?
-	;;
-  reload|force-reload)
-	#
-	# If do_reload() is not implemented then leave this commented out
-	# and leave 'force-reload' as an alias for 'restart'.
-	#
-	log_daemon_msg "Reloading $DESC" "$NAME"
-	do_reload
-	log_end_msg $?
-	;;
-  restart)
-	#
-	# If the "reload" option is implemented then remove the
-	# 'force-reload' alias
-	#
-	log_daemon_msg "Restarting $DESC" "$NAME"
-	do_stop
-	do_start
-	;;
-  *)
-	#echo "Usage: $SCRIPTNAME {start|stop|restart|reload|force-reload}" >&2
-	echo "Usage: $SCRIPTNAME {start|stop|status|restart|force-reload}" >&2
-	exit 3
-	;;
-esac
-
-:
+#! /bin/sh
+### BEGIN INIT INFO
+# Provides:          zuul-merger
+# Required-Start:    $remote_fs $syslog
+# Required-Stop:     $remote_fs $syslog
+# Default-Start:     2 3 4 5
+# Default-Stop:      0 1 6
+# Short-Description: Zuul Merger
+# Description:       Trunk gating system
+### END INIT INFO
+
+# Do NOT "set -e"
+
+# PATH should only include /usr/* if it runs after the mountnfs.sh script
+PATH=/sbin:/usr/sbin:/bin:/usr/bin
+DESC="Zuul Merger"
+NAME=zuul-merger
+DAEMON=/usr/bin/$NAME
+PIDFILE=/var/run/$NAME/$NAME.pid
+SCRIPTNAME=/etc/init.d/$NAME
+USER=zuul
+
+# Exit if the package is not installed
+[ -x "$DAEMON" ] || exit 0
+
+START_DAEMON=0
+DAEMON_ARGS=''
+
+# Load the VERBOSE setting and other rcS variables
+. /lib/init/vars.sh
+
+# Define LSB log_* functions.
+# Depend on lsb-base (>= 3.0-6) to ensure that this file is present.
+. /lib/lsb/init-functions
+
+# Read configuration variable file if it is present
+if [ -r /etc/default/$NAME ] ; then
+	. /etc/default/$NAME
+fi
+
+if ! [ "${START_DAEMON}" = "1" ] ; then
+	log_daemon_msg "$DESC: /etc/default/$NAME is not set to START_DAEMON=1: exiting"
+	log_end_msg 1
+	exit 0
+fi
+
+#
+# Function that starts the daemon/service
+#
+do_start()
+{
+	# Return
+	#   0 if daemon has been started
+	#   1 if daemon was already running
+	#   2 if daemon could not be started
+
+	mkdir -p /var/run/$NAME
+	chown $USER /var/run/$NAME
+	ulimit -n 8192
+	start-stop-daemon --start --quiet --pidfile $PIDFILE -c $USER --exec $DAEMON --test > /dev/null \
+		|| return 1
+	start-stop-daemon --start --quiet --pidfile $PIDFILE -c $USER --exec $DAEMON -- \
+		$DAEMON_ARGS \
+		|| return 2
+	# Add code here, if necessary, that waits for the process to be ready
+	# to handle requests from services started subsequently which depend
+	# on this one.  As a last resort, sleep for some time.
+}
+
+#
+# Function that stops the daemon/service
+#
+do_stop()
+{
+	# Return
+	#   0 if daemon has been stopped
+	#   1 if daemon was already stopped
+	#   2 if daemon could not be stopped
+	#   other if a failure occurred
+	start-stop-daemon --stop --signal 9 --pidfile $PIDFILE
+	RETVAL="$?"
+	[ "$RETVAL" = 2 ] && return 2
+	rm -f /var/run/$NAME/*
+	return "$RETVAL"
+}
+
+#
+# Function that sends a SIGHUP to the daemon/service
+#
+do_reload() {
+	#
+	# If the daemon can reload its configuration without
+	# restarting (for example, when it is sent a SIGHUP),
+	# then implement that here.
+	#
+	start-stop-daemon --stop --signal 1 --quiet --pidfile $PIDFILE --name $NAME
+	return 0
+}
+
+case "$1" in
+  start)
+	[ "$VERBOSE" != no ] && log_daemon_msg "Starting $DESC" "$NAME"
+	do_start
+	case "$?" in
+		0|1) [ "$VERBOSE" != no ] && log_end_msg 0 ;;
+		2) [ "$VERBOSE" != no ] && log_end_msg 1 ;;
+	esac
+	;;
+  stop)
+	[ "$VERBOSE" != no ] && log_daemon_msg "Stopping $DESC" "$NAME"
+	do_stop
+	case "$?" in
+		0|1) [ "$VERBOSE" != no ] && log_end_msg 0 ;;
+		2) [ "$VERBOSE" != no ] && log_end_msg 1 ;;
+	esac
+	;;
+  status)
+	status_of_proc "$DAEMON" "$NAME" && exit 0 || exit $?
+	;;
+  reload|force-reload)
+	#
+	# If do_reload() is not implemented then leave this commented out
+	# and leave 'force-reload' as an alias for 'restart'.
+	#
+	log_daemon_msg "Reloading $DESC" "$NAME"
+	do_reload
+	log_end_msg $?
+	;;
+  restart)
+	#
+	# If the "reload" option is implemented then remove the
+	# 'force-reload' alias
+	#
+	log_daemon_msg "Restarting $DESC" "$NAME"
+	do_stop
+	do_start
+	;;
+  *)
+	#echo "Usage: $SCRIPTNAME {start|stop|restart|reload|force-reload}" >&2
+	echo "Usage: $SCRIPTNAME {start|stop|status|restart|force-reload}" >&2
+	exit 3
+	;;
+esac
+
+:
diff --git a/debian/zuul.default b/debian/zuul.default
index 197fd5b..8e7d537 100644
--- a/debian/zuul.default
+++ b/debian/zuul.default
@@ -1,15 +1,15 @@
-# START_DAEMON:
-#   Set to 1 to start the zuul daemon
-START_DAEMON=0
-
-# STATSD_HOST:
-#   hostname or IP to send statsd packets to.
-#export STATSD_HOST=localhost
-
-# STATSD_PORT:
-#   udp port to send statsd to.
-#export STATSD_PORT=8125
-
-# DAEMON_ARGS:
-#   Arguments passed to the Zuul server
-#DAEMON_ARGS='-c /etc/zuul/zuul-server.conf'
+# START_DAEMON:
+#   Set to 1 to start the zuul daemon
+START_DAEMON=0
+
+# STATSD_HOST:
+#   hostname or IP to send statsd packets to.
+#export STATSD_HOST=localhost
+
+# STATSD_PORT:
+#   udp port to send statsd to.
+#export STATSD_PORT=8125
+
+# DAEMON_ARGS:
+#   Arguments passed to the Zuul server
+#DAEMON_ARGS='-c /etc/zuul/zuul-server.conf'
diff --git a/debian/zuul.init b/debian/zuul.init
index 9c96a23..bb774f7 100644
--- a/debian/zuul.init
+++ b/debian/zuul.init
@@ -1,169 +1,169 @@
-#! /bin/sh
-### BEGIN INIT INFO
-# Provides:          zuul
-# Required-Start:    $remote_fs $syslog
-# Required-Stop:     $remote_fs $syslog
-# Default-Start:     2 3 4 5
-# Default-Stop:      0 1 6
-# Short-Description: Zuul
-# Description:       Trunk gating system
-### END INIT INFO
-
-# Do NOT "set -e"
-
-# PATH should only include /usr/* if it runs after the mountnfs.sh script
-PATH=/sbin:/usr/sbin:/bin:/usr/bin
-DESC="Zuul Server"
-NAME=zuul
-DAEMON=/usr/bin/zuul-server
-PIDFILE=/var/run/$NAME/$NAME.pid
-SCRIPTNAME=/etc/init.d/$NAME
-USER=zuul
-
-# Exit if the package is not installed
-[ -x "$DAEMON" ] || exit 0
-
-START_DAEMON=0
-DAEMON_ARGS=''
-
-# Load the VERBOSE setting and other rcS variables
-. /lib/init/vars.sh
-
-# Define LSB log_* functions.
-# Depend on lsb-base (>= 3.0-6) to ensure that this file is present.
-. /lib/lsb/init-functions
-
-# Read configuration variable file if it is present
-if [ -r /etc/default/$NAME ] ; then
-	. /etc/default/$NAME
-fi
-
-if ! [ "${START_DAEMON}" = "1" ] ; then
-	log_daemon_msg "$DESC: /etc/default/$NAME is not set to START_DAEMON=1: exiting"
-	log_end_msg 1
-	exit 0
-fi
-
-#
-# Function that starts the daemon/service
-#
-do_start()
-{
-	# Return
-	#   0 if daemon has been started
-	#   1 if daemon was already running
-	#   2 if daemon could not be started
-
-	mkdir -p /var/run/$NAME
-	chown $USER /var/run/$NAME
-	start-stop-daemon --start --quiet --pidfile $PIDFILE -c $USER --exec $DAEMON --test > /dev/null \
-		|| return 1
-	start-stop-daemon --start --quiet --pidfile $PIDFILE -c $USER --exec $DAEMON -- \
-		$DAEMON_ARGS \
-		|| return 2
-	# Add code here, if necessary, that waits for the process to be ready
-	# to handle requests from services started subsequently which depend
-	# on this one.  As a last resort, sleep for some time.
-}
-
-#
-# Function that stops the daemon/service
-#
-do_stop()
-{
-	# Return
-	#   0 if daemon has been stopped
-	#   1 if daemon was already stopped
-	#   2 if daemon could not be stopped
-	#   other if a failure occurred
-	start-stop-daemon --stop --signal 9 --pidfile $PIDFILE
-	RETVAL="$?"
-	[ "$RETVAL" = 2 ] && return 2
-	rm -f /var/run/$NAME/*
-	return "$RETVAL"
-}
-
-#
-# Function that stops the daemon/service
-#
-do_graceful_stop()
-{
-	PID=`cat $PIDFILE`
-	kill -USR1 $PID
-
-	# wait until really stopped
-	if [ -n "${PID:-}" ]; then
-		i=0
-		while kill -0 "${PID:-}" 2> /dev/null;  do
-			if [ $i -eq '0' ]; then
-				echo -n " ... waiting for jobs to complete "
-			else
-				echo -n "."
-			fi
-			i=$(($i+1))
-			sleep 1
-		done
-	fi
-
-	rm -f /var/run/$NAME/*
-}
-
-#
-# Function that sends a SIGHUP to the daemon/service
-#
-do_reload() {
-	#
-	# If the daemon can reload its configuration without
-	# restarting (for example, when it is sent a SIGHUP),
-	# then implement that here.
-	#
-	start-stop-daemon --stop --signal 1 --quiet --pidfile $PIDFILE --name zuul-server
-	return 0
-}
-
-case "$1" in
-  start)
-	[ "$VERBOSE" != no ] && log_daemon_msg "Starting $DESC" "$NAME"
-	do_start
-	case "$?" in
-		0|1) [ "$VERBOSE" != no ] && log_end_msg 0 ;;
-		2) [ "$VERBOSE" != no ] && log_end_msg 1 ;;
-	esac
-	;;
-  stop)
-	[ "$VERBOSE" != no ] && log_daemon_msg "Stopping $DESC" "$NAME"
-	do_stop
-	case "$?" in
-		0|1) [ "$VERBOSE" != no ] && log_end_msg 0 ;;
-		2) [ "$VERBOSE" != no ] && log_end_msg 1 ;;
-	esac
-	;;
-  status)
-	status_of_proc "$DAEMON" "$NAME" && exit 0 || exit $?
-	;;
-  reload|force-reload)
-	#
-	# If do_reload() is not implemented then leave this commented out
-	# and leave 'force-reload' as an alias for 'restart'.
-	#
-	log_daemon_msg "Reloading $DESC" "$NAME"
-	do_reload
-	log_end_msg $?
-	;;
-  restart)
-	#
-	# If the "reload" option is implemented then remove the
-	# 'force-reload' alias
-	#
-	log_daemon_msg "Restarting $DESC" "$NAME"
-	do_graceful_stop
-	do_start
-	;;
-  *)
-	#echo "Usage: $SCRIPTNAME {start|stop|restart|reload|force-reload}" >&2
-	echo "Usage: $SCRIPTNAME {start|stop|status|restart|force-reload}" >&2
-	exit 3
-	;;
-esac
-
-:
+#! /bin/sh
+### BEGIN INIT INFO
+# Provides:          zuul
+# Required-Start:    $remote_fs $syslog
+# Required-Stop:     $remote_fs $syslog
+# Default-Start:     2 3 4 5
+# Default-Stop:      0 1 6
+# Short-Description: Zuul
+# Description:       Trunk gating system
+### END INIT INFO
+
+# Do NOT "set -e"
+
+# PATH should only include /usr/* if it runs after the mountnfs.sh script
+PATH=/sbin:/usr/sbin:/bin:/usr/bin
+DESC="Zuul Server"
+NAME=zuul
+DAEMON=/usr/bin/zuul-server
+PIDFILE=/var/run/$NAME/$NAME.pid
+SCRIPTNAME=/etc/init.d/$NAME
+USER=zuul
+
+# Exit if the package is not installed
+[ -x "$DAEMON" ] || exit 0
+
+START_DAEMON=0
+DAEMON_ARGS=''
+
+# Load the VERBOSE setting and other rcS variables
+. /lib/init/vars.sh
+
+# Define LSB log_* functions.
+# Depend on lsb-base (>= 3.0-6) to ensure that this file is present.
+. /lib/lsb/init-functions
+
+# Read configuration variable file if it is present
+if [ -r /etc/default/$NAME ] ; then
+	. /etc/default/$NAME
+fi
+
+if ! [ "${START_DAEMON}" = "1" ] ; then
+	log_daemon_msg "$DESC: /etc/default/$NAME is not set to START_DAEMON=1: exiting"
+	log_end_msg 1
+	exit 0
+fi
+
+#
+# Function that starts the daemon/service
+#
+do_start()
+{
+	# Return
+	#   0 if daemon has been started
+	#   1 if daemon was already running
+	#   2 if daemon could not be started
+
+	mkdir -p /var/run/$NAME
+	chown $USER /var/run/$NAME
+	start-stop-daemon --start --quiet --pidfile $PIDFILE -c $USER --exec $DAEMON --test > /dev/null \
+		|| return 1
+	start-stop-daemon --start --quiet --pidfile $PIDFILE -c $USER --exec $DAEMON -- \
+		$DAEMON_ARGS \
+		|| return 2
+	# Add code here, if necessary, that waits for the process to be ready
+	# to handle requests from services started subsequently which depend
+	# on this one.  As a last resort, sleep for some time.
+}
+
+#
+# Function that stops the daemon/service
+#
+do_stop()
+{
+	# Return
+	#   0 if daemon has been stopped
+	#   1 if daemon was already stopped
+	#   2 if daemon could not be stopped
+	#   other if a failure occurred
+	start-stop-daemon --stop --signal 9 --pidfile $PIDFILE
+	RETVAL="$?"
+	[ "$RETVAL" = 2 ] && return 2
+	rm -f /var/run/$NAME/*
+	return "$RETVAL"
+}
+
+#
+# Function that stops the daemon/service
+#
+do_graceful_stop()
+{
+	PID=`cat $PIDFILE`
+	kill -USR1 $PID
+
+	# wait until really stopped
+	if [ -n "${PID:-}" ]; then
+		i=0
+		while kill -0 "${PID:-}" 2> /dev/null;  do
+			if [ $i -eq '0' ]; then
+				echo -n " ... waiting for jobs to complete "
+			else
+				echo -n "."
+			fi
+			i=$(($i+1))
+			sleep 1
+		done
+	fi
+
+	rm -f /var/run/$NAME/*
+}
+
+#
+# Function that sends a SIGHUP to the daemon/service
+#
+do_reload() {
+	#
+	# If the daemon can reload its configuration without
+	# restarting (for example, when it is sent a SIGHUP),
+	# then implement that here.
+	#
+	start-stop-daemon --stop --signal 1 --quiet --pidfile $PIDFILE --name zuul-server
+	return 0
+}
+
+case "$1" in
+  start)
+	[ "$VERBOSE" != no ] && log_daemon_msg "Starting $DESC" "$NAME"
+	do_start
+	case "$?" in
+		0|1) [ "$VERBOSE" != no ] && log_end_msg 0 ;;
+		2) [ "$VERBOSE" != no ] && log_end_msg 1 ;;
+	esac
+	;;
+  stop)
+	[ "$VERBOSE" != no ] && log_daemon_msg "Stopping $DESC" "$NAME"
+	do_stop
+	case "$?" in
+		0|1) [ "$VERBOSE" != no ] && log_end_msg 0 ;;
+		2) [ "$VERBOSE" != no ] && log_end_msg 1 ;;
+	esac
+	;;
+  status)
+	status_of_proc "$DAEMON" "$NAME" && exit 0 || exit $?
+	;;
+  reload|force-reload)
+	#
+	# If do_reload() is not implemented then leave this commented out
+	# and leave 'force-reload' as an alias for 'restart'.
+	#
+	log_daemon_msg "Reloading $DESC" "$NAME"
+	do_reload
+	log_end_msg $?
+	;;
+  restart)
+	#
+	# If the "reload" option is implemented then remove the
+	# 'force-reload' alias
+	#
+	log_daemon_msg "Restarting $DESC" "$NAME"
+	do_graceful_stop
+	do_start
+	;;
+  *)
+	#echo "Usage: $SCRIPTNAME {start|stop|restart|reload|force-reload}" >&2
+	echo "Usage: $SCRIPTNAME {start|stop|status|restart|force-reload}" >&2
+	exit 3
+	;;
+esac
+
+:
diff --git a/debian/zuul.lintian-overrides b/debian/zuul.lintian-overrides
index 95b8a33..63e18c3 100644
--- a/debian/zuul.lintian-overrides
+++ b/debian/zuul.lintian-overrides
@@ -1,22 +1,22 @@
-# virtualenv uses the python provided by virtualenv
-# there is a trigger to update it though
-zuul binary: wrong-path-for-interpreter *
-
-# Scripts shebang are rewritten by virtualenv and point to
-# /usr/share/python/zuul/bin/python2.7 which is a copy of the system python2.7
-zuul binary: python-script-but-no-python-dep usr/share/python/zuul/bin/*
-
-# We need to keep the python2.7 binary copy to have it adjust its is sys.path
-# and be able to find modules provided by the Zuul virtualenv.
-#
-# From https://www.python.org/dev/peps/pep-0405/#copies-versus-symlinks:
-#
-# Virtualenv must copy the binary in order to provide isolation, as Python
-# dereferences a symlinked executable before searching for sys.prefix .
-zuul binary: arch-dependent-file-in-usr-share usr/share/python/zuul/bin/python2.7
-
-# No sphinx doc possible with Precise
-zuul binary: binary-without-manpage usr/bin/zuul
-zuul binary: binary-without-manpage usr/bin/zuul-cloner
-zuul binary: binary-without-manpage usr/bin/zuul-merger
-zuul binary: binary-without-manpage usr/bin/zuul-server
+# virtualenv uses the python provided by virtualenv
+# there is a trigger to update it though
+zuul binary: wrong-path-for-interpreter *
+
+# Scripts shebang are rewritten by virtualenv and point to
+# /usr/share/python/zuul/bin/python2.7 which is a copy of the system python2.7
+zuul binary: python-script-but-no-python-dep usr/share/python/zuul/bin/*
+
+# We need to keep the python2.7 binary copy to have it adjust its is sys.path
+# and be able to find modules provided by the Zuul virtualenv.
+#
+# From https://www.python.org/dev/peps/pep-0405/#copies-versus-symlinks:
+#
+# Virtualenv must copy the binary in order to provide isolation, as Python
+# dereferences a symlinked executable before searching for sys.prefix .
+zuul binary: arch-dependent-file-in-usr-share usr/share/python/zuul/bin/python2.7
+
+# No sphinx doc possible with Precise
+zuul binary: binary-without-manpage usr/bin/zuul
+zuul binary: binary-without-manpage usr/bin/zuul-cloner
+zuul binary: binary-without-manpage usr/bin/zuul-merger
+zuul binary: binary-without-manpage usr/bin/zuul-server
diff --git a/debian/zuul.triggers b/debian/zuul.triggers
index 643f7a2..b4a0b73 100644
--- a/debian/zuul.triggers
+++ b/debian/zuul.triggers
@@ -1,7 +1,7 @@
-# Register interest in Python interpreter changes (Python 2 for now); and
-# don't make the Python package dependent on the virtualenv package
-# processing (noawait)
-interest-noawait /usr/bin/python2.7
-
-# Also provide a symbolic trigger for all dh-virtualenv packages
-interest dh-virtualenv-interpreter-update
+# Register interest in Python interpreter changes (Python 2 for now); and
+# don't make the Python package dependent on the virtualenv package
+# processing (noawait)
+interest-noawait /usr/bin/python2.7
+
+# Also provide a symbolic trigger for all dh-virtualenv packages
+interest dh-virtualenv-interpreter-update
diff --git a/doc/Makefile b/doc/Makefile
index 5957660..2edfc0e 100644
--- a/doc/Makefile
+++ b/doc/Makefile
@@ -1,153 +1,153 @@
-# Makefile for Sphinx documentation
-#
-
-# You can set these variables from the command line.
-SPHINXOPTS    = -W
-SPHINXBUILD   = sphinx-build
-PAPER         =
-BUILDDIR      = build
-
-# Internal variables.
-PAPEROPT_a4     = -D latex_paper_size=a4
-PAPEROPT_letter = -D latex_paper_size=letter
-ALLSPHINXOPTS   = -d $(BUILDDIR)/doctrees $(PAPEROPT_$(PAPER)) $(SPHINXOPTS) source
-# the i18n builder cannot share the environment and doctrees with the others
-I18NSPHINXOPTS  = $(PAPEROPT_$(PAPER)) $(SPHINXOPTS) source
-
-.PHONY: help clean html dirhtml singlehtml pickle json htmlhelp qthelp devhelp epub latex latexpdf text man changes linkcheck doctest gettext
-
-help:
-	@echo "Please use \`make <target>' where <target> is one of"
-	@echo "  html       to make standalone HTML files"
-	@echo "  dirhtml    to make HTML files named index.html in directories"
-	@echo "  singlehtml to make a single large HTML file"
-	@echo "  pickle     to make pickle files"
-	@echo "  json       to make JSON files"
-	@echo "  htmlhelp   to make HTML files and a HTML help project"
-	@echo "  qthelp     to make HTML files and a qthelp project"
-	@echo "  devhelp    to make HTML files and a Devhelp project"
-	@echo "  epub       to make an epub"
-	@echo "  latex      to make LaTeX files, you can set PAPER=a4 or PAPER=letter"
-	@echo "  latexpdf   to make LaTeX files and run them through pdflatex"
-	@echo "  text       to make text files"
-	@echo "  man        to make manual pages"
-	@echo "  texinfo    to make Texinfo files"
-	@echo "  info       to make Texinfo files and run them through makeinfo"
-	@echo "  gettext    to make PO message catalogs"
-	@echo "  changes    to make an overview of all changed/added/deprecated items"
-	@echo "  linkcheck  to check all external links for integrity"
-	@echo "  doctest    to run all doctests embedded in the documentation (if enabled)"
-
-clean:
-	-rm -rf $(BUILDDIR)/*
-
-html:
-	$(SPHINXBUILD) -b html $(ALLSPHINXOPTS) $(BUILDDIR)/html
-	@echo
-	@echo "Build finished. The HTML pages are in $(BUILDDIR)/html."
-
-dirhtml:
-	$(SPHINXBUILD) -b dirhtml $(ALLSPHINXOPTS) $(BUILDDIR)/dirhtml
-	@echo
-	@echo "Build finished. The HTML pages are in $(BUILDDIR)/dirhtml."
-
-singlehtml:
-	$(SPHINXBUILD) -b singlehtml $(ALLSPHINXOPTS) $(BUILDDIR)/singlehtml
-	@echo
-	@echo "Build finished. The HTML page is in $(BUILDDIR)/singlehtml."
-
-pickle:
-	$(SPHINXBUILD) -b pickle $(ALLSPHINXOPTS) $(BUILDDIR)/pickle
-	@echo
-	@echo "Build finished; now you can process the pickle files."
-
-json:
-	$(SPHINXBUILD) -b json $(ALLSPHINXOPTS) $(BUILDDIR)/json
-	@echo
-	@echo "Build finished; now you can process the JSON files."
-
-htmlhelp:
-	$(SPHINXBUILD) -b htmlhelp $(ALLSPHINXOPTS) $(BUILDDIR)/htmlhelp
-	@echo
-	@echo "Build finished; now you can run HTML Help Workshop with the" \
-	      ".hhp project file in $(BUILDDIR)/htmlhelp."
-
-qthelp:
-	$(SPHINXBUILD) -b qthelp $(ALLSPHINXOPTS) $(BUILDDIR)/qthelp
-	@echo
-	@echo "Build finished; now you can run "qcollectiongenerator" with the" \
-	      ".qhcp project file in $(BUILDDIR)/qthelp, like this:"
-	@echo "# qcollectiongenerator $(BUILDDIR)/qthelp/Zuul.qhcp"
-	@echo "To view the help file:"
-	@echo "# assistant -collectionFile $(BUILDDIR)/qthelp/Zuul.qhc"
-
-devhelp:
-	$(SPHINXBUILD) -b devhelp $(ALLSPHINXOPTS) $(BUILDDIR)/devhelp
-	@echo
-	@echo "Build finished."
-	@echo "To view the help file:"
-	@echo "# mkdir -p $$HOME/.local/share/devhelp/Zuul"
-	@echo "# ln -s $(BUILDDIR)/devhelp $$HOME/.local/share/devhelp/Zuul"
-	@echo "# devhelp"
-
-epub:
-	$(SPHINXBUILD) -b epub $(ALLSPHINXOPTS) $(BUILDDIR)/epub
-	@echo
-	@echo "Build finished. The epub file is in $(BUILDDIR)/epub."
-
-latex:
-	$(SPHINXBUILD) -b latex $(ALLSPHINXOPTS) $(BUILDDIR)/latex
-	@echo
-	@echo "Build finished; the LaTeX files are in $(BUILDDIR)/latex."
-	@echo "Run \`make' in that directory to run these through (pdf)latex" \
-	      "(use \`make latexpdf' here to do that automatically)."
-
-latexpdf:
-	$(SPHINXBUILD) -b latex $(ALLSPHINXOPTS) $(BUILDDIR)/latex
-	@echo "Running LaTeX files through pdflatex..."
-	$(MAKE) -C $(BUILDDIR)/latex all-pdf
-	@echo "pdflatex finished; the PDF files are in $(BUILDDIR)/latex."
-
-text:
-	$(SPHINXBUILD) -b text $(ALLSPHINXOPTS) $(BUILDDIR)/text
-	@echo
-	@echo "Build finished. The text files are in $(BUILDDIR)/text."
-
-man:
-	$(SPHINXBUILD) -b man $(ALLSPHINXOPTS) $(BUILDDIR)/man
-	@echo
-	@echo "Build finished. The manual pages are in $(BUILDDIR)/man."
-
-texinfo:
-	$(SPHINXBUILD) -b texinfo $(ALLSPHINXOPTS) $(BUILDDIR)/texinfo
-	@echo
-	@echo "Build finished. The Texinfo files are in $(BUILDDIR)/texinfo."
-	@echo "Run \`make' in that directory to run these through makeinfo" \
-	      "(use \`make info' here to do that automatically)."
-
-info:
-	$(SPHINXBUILD) -b texinfo $(ALLSPHINXOPTS) $(BUILDDIR)/texinfo
-	@echo "Running Texinfo files through makeinfo..."
-	make -C $(BUILDDIR)/texinfo info
-	@echo "makeinfo finished; the Info files are in $(BUILDDIR)/texinfo."
-
-gettext:
-	$(SPHINXBUILD) -b gettext $(I18NSPHINXOPTS) $(BUILDDIR)/locale
-	@echo
-	@echo "Build finished. The message catalogs are in $(BUILDDIR)/locale."
-
-changes:
-	$(SPHINXBUILD) -b changes $(ALLSPHINXOPTS) $(BUILDDIR)/changes
-	@echo
-	@echo "The overview file is in $(BUILDDIR)/changes."
-
-linkcheck:
-	$(SPHINXBUILD) -b linkcheck $(ALLSPHINXOPTS) $(BUILDDIR)/linkcheck
-	@echo
-	@echo "Link check complete; look for any errors in the above output " \
-	      "or in $(BUILDDIR)/linkcheck/output.txt."
-
-doctest:
-	$(SPHINXBUILD) -b doctest $(ALLSPHINXOPTS) $(BUILDDIR)/doctest
-	@echo "Testing of doctests in the sources finished, look at the " \
-	      "results in $(BUILDDIR)/doctest/output.txt."
+# Makefile for Sphinx documentation
+#
+
+# You can set these variables from the command line.
+SPHINXOPTS    = -W
+SPHINXBUILD   = sphinx-build
+PAPER         =
+BUILDDIR      = build
+
+# Internal variables.
+PAPEROPT_a4     = -D latex_paper_size=a4
+PAPEROPT_letter = -D latex_paper_size=letter
+ALLSPHINXOPTS   = -d $(BUILDDIR)/doctrees $(PAPEROPT_$(PAPER)) $(SPHINXOPTS) source
+# the i18n builder cannot share the environment and doctrees with the others
+I18NSPHINXOPTS  = $(PAPEROPT_$(PAPER)) $(SPHINXOPTS) source
+
+.PHONY: help clean html dirhtml singlehtml pickle json htmlhelp qthelp devhelp epub latex latexpdf text man changes linkcheck doctest gettext
+
+help:
+	@echo "Please use \`make <target>' where <target> is one of"
+	@echo "  html       to make standalone HTML files"
+	@echo "  dirhtml    to make HTML files named index.html in directories"
+	@echo "  singlehtml to make a single large HTML file"
+	@echo "  pickle     to make pickle files"
+	@echo "  json       to make JSON files"
+	@echo "  htmlhelp   to make HTML files and a HTML help project"
+	@echo "  qthelp     to make HTML files and a qthelp project"
+	@echo "  devhelp    to make HTML files and a Devhelp project"
+	@echo "  epub       to make an epub"
+	@echo "  latex      to make LaTeX files, you can set PAPER=a4 or PAPER=letter"
+	@echo "  latexpdf   to make LaTeX files and run them through pdflatex"
+	@echo "  text       to make text files"
+	@echo "  man        to make manual pages"
+	@echo "  texinfo    to make Texinfo files"
+	@echo "  info       to make Texinfo files and run them through makeinfo"
+	@echo "  gettext    to make PO message catalogs"
+	@echo "  changes    to make an overview of all changed/added/deprecated items"
+	@echo "  linkcheck  to check all external links for integrity"
+	@echo "  doctest    to run all doctests embedded in the documentation (if enabled)"
+
+clean:
+	-rm -rf $(BUILDDIR)/*
+
+html:
+	$(SPHINXBUILD) -b html $(ALLSPHINXOPTS) $(BUILDDIR)/html
+	@echo
+	@echo "Build finished. The HTML pages are in $(BUILDDIR)/html."
+
+dirhtml:
+	$(SPHINXBUILD) -b dirhtml $(ALLSPHINXOPTS) $(BUILDDIR)/dirhtml
+	@echo
+	@echo "Build finished. The HTML pages are in $(BUILDDIR)/dirhtml."
+
+singlehtml:
+	$(SPHINXBUILD) -b singlehtml $(ALLSPHINXOPTS) $(BUILDDIR)/singlehtml
+	@echo
+	@echo "Build finished. The HTML page is in $(BUILDDIR)/singlehtml."
+
+pickle:
+	$(SPHINXBUILD) -b pickle $(ALLSPHINXOPTS) $(BUILDDIR)/pickle
+	@echo
+	@echo "Build finished; now you can process the pickle files."
+
+json:
+	$(SPHINXBUILD) -b json $(ALLSPHINXOPTS) $(BUILDDIR)/json
+	@echo
+	@echo "Build finished; now you can process the JSON files."
+
+htmlhelp:
+	$(SPHINXBUILD) -b htmlhelp $(ALLSPHINXOPTS) $(BUILDDIR)/htmlhelp
+	@echo
+	@echo "Build finished; now you can run HTML Help Workshop with the" \
+	      ".hhp project file in $(BUILDDIR)/htmlhelp."
+
+qthelp:
+	$(SPHINXBUILD) -b qthelp $(ALLSPHINXOPTS) $(BUILDDIR)/qthelp
+	@echo
+	@echo "Build finished; now you can run "qcollectiongenerator" with the" \
+	      ".qhcp project file in $(BUILDDIR)/qthelp, like this:"
+	@echo "# qcollectiongenerator $(BUILDDIR)/qthelp/Zuul.qhcp"
+	@echo "To view the help file:"
+	@echo "# assistant -collectionFile $(BUILDDIR)/qthelp/Zuul.qhc"
+
+devhelp:
+	$(SPHINXBUILD) -b devhelp $(ALLSPHINXOPTS) $(BUILDDIR)/devhelp
+	@echo
+	@echo "Build finished."
+	@echo "To view the help file:"
+	@echo "# mkdir -p $$HOME/.local/share/devhelp/Zuul"
+	@echo "# ln -s $(BUILDDIR)/devhelp $$HOME/.local/share/devhelp/Zuul"
+	@echo "# devhelp"
+
+epub:
+	$(SPHINXBUILD) -b epub $(ALLSPHINXOPTS) $(BUILDDIR)/epub
+	@echo
+	@echo "Build finished. The epub file is in $(BUILDDIR)/epub."
+
+latex:
+	$(SPHINXBUILD) -b latex $(ALLSPHINXOPTS) $(BUILDDIR)/latex
+	@echo
+	@echo "Build finished; the LaTeX files are in $(BUILDDIR)/latex."
+	@echo "Run \`make' in that directory to run these through (pdf)latex" \
+	      "(use \`make latexpdf' here to do that automatically)."
+
+latexpdf:
+	$(SPHINXBUILD) -b latex $(ALLSPHINXOPTS) $(BUILDDIR)/latex
+	@echo "Running LaTeX files through pdflatex..."
+	$(MAKE) -C $(BUILDDIR)/latex all-pdf
+	@echo "pdflatex finished; the PDF files are in $(BUILDDIR)/latex."
+
+text:
+	$(SPHINXBUILD) -b text $(ALLSPHINXOPTS) $(BUILDDIR)/text
+	@echo
+	@echo "Build finished. The text files are in $(BUILDDIR)/text."
+
+man:
+	$(SPHINXBUILD) -b man $(ALLSPHINXOPTS) $(BUILDDIR)/man
+	@echo
+	@echo "Build finished. The manual pages are in $(BUILDDIR)/man."
+
+texinfo:
+	$(SPHINXBUILD) -b texinfo $(ALLSPHINXOPTS) $(BUILDDIR)/texinfo
+	@echo
+	@echo "Build finished. The Texinfo files are in $(BUILDDIR)/texinfo."
+	@echo "Run \`make' in that directory to run these through makeinfo" \
+	      "(use \`make info' here to do that automatically)."
+
+info:
+	$(SPHINXBUILD) -b texinfo $(ALLSPHINXOPTS) $(BUILDDIR)/texinfo
+	@echo "Running Texinfo files through makeinfo..."
+	make -C $(BUILDDIR)/texinfo info
+	@echo "makeinfo finished; the Info files are in $(BUILDDIR)/texinfo."
+
+gettext:
+	$(SPHINXBUILD) -b gettext $(I18NSPHINXOPTS) $(BUILDDIR)/locale
+	@echo
+	@echo "Build finished. The message catalogs are in $(BUILDDIR)/locale."
+
+changes:
+	$(SPHINXBUILD) -b changes $(ALLSPHINXOPTS) $(BUILDDIR)/changes
+	@echo
+	@echo "The overview file is in $(BUILDDIR)/changes."
+
+linkcheck:
+	$(SPHINXBUILD) -b linkcheck $(ALLSPHINXOPTS) $(BUILDDIR)/linkcheck
+	@echo
+	@echo "Link check complete; look for any errors in the above output " \
+	      "or in $(BUILDDIR)/linkcheck/output.txt."
+
+doctest:
+	$(SPHINXBUILD) -b doctest $(ALLSPHINXOPTS) $(BUILDDIR)/doctest
+	@echo "Testing of doctests in the sources finished, look at the " \
+	      "results in $(BUILDDIR)/doctest/output.txt."
diff --git a/doc/source/client.rst b/doc/source/client.rst
index 5fe2252..908a9d0 100644
--- a/doc/source/client.rst
+++ b/doc/source/client.rst
@@ -1,51 +1,51 @@
-:title: Zuul Client
-
-Zuul Client
-===========
-
-Zuul includes a simple command line client that may be used by
-administrators to affect Zuul's behavior while running.  It must be
-run on a host that has access to the Gearman server (e.g., locally on
-the Zuul host).
-
-Configuration
--------------
-
-The client uses the same zuul.conf file as the server, and will look
-for it in the same locations if not specified on the command line.
-
-Usage
------
-The general options that apply to all subcommands are:
-
-.. program-output:: zuul --help
-
-The following subcommands are supported:
-
-Enqueue
-^^^^^^^
-.. program-output:: zuul enqueue --help
-
-Example::
-
-  zuul enqueue --trigger gerrit --pipeline check --project example_project --change 12345,1
-
-Note that the format of change id is <number>,<patchset>.
-
-Promote
-^^^^^^^
-.. program-output:: zuul promote --help
-
-Example::
-
-  zuul promote --pipeline check --changes 12345,1 13336,3
-
-Note that the format of changes id is <number>,<patchset>.
-
-Show
-^^^^
-.. program-output:: zuul show --help
-
-Example::
-
-  zuul show running-jobs
+:title: Zuul Client
+
+Zuul Client
+===========
+
+Zuul includes a simple command line client that may be used by
+administrators to affect Zuul's behavior while running.  It must be
+run on a host that has access to the Gearman server (e.g., locally on
+the Zuul host).
+
+Configuration
+-------------
+
+The client uses the same zuul.conf file as the server, and will look
+for it in the same locations if not specified on the command line.
+
+Usage
+-----
+The general options that apply to all subcommands are:
+
+.. program-output:: zuul --help
+
+The following subcommands are supported:
+
+Enqueue
+^^^^^^^
+.. program-output:: zuul enqueue --help
+
+Example::
+
+  zuul enqueue --trigger gerrit --pipeline check --project example_project --change 12345,1
+
+Note that the format of change id is <number>,<patchset>.
+
+Promote
+^^^^^^^
+.. program-output:: zuul promote --help
+
+Example::
+
+  zuul promote --pipeline check --changes 12345,1 13336,3
+
+Note that the format of changes id is <number>,<patchset>.
+
+Show
+^^^^
+.. program-output:: zuul show --help
+
+Example::
+
+  zuul show running-jobs
diff --git a/doc/source/cloner.rst b/doc/source/cloner.rst
index c0ca990..97e9bdb 100644
--- a/doc/source/cloner.rst
+++ b/doc/source/cloner.rst
@@ -1,115 +1,115 @@
-:title: Zuul Cloner
-
-Zuul Cloner
-===========
-
-Zuul includes a simple command line client that may be used to clone
-repositories with Zuul references applied.
-
-Configuration
--------------
-
-Clone map
-'''''''''
-
-By default, Zuul cloner will clone the project under ``basepath`` which
-would create sub directories whenever a project name contains slashes.  Since
-you might want to finely tweak the final destination, a clone map lets you
-change the destination on a per project basis.  The configuration is done using
-a YAML file passed with ``-m``.
-
-With a project hierarchy such as::
-
- project
- thirdparty/plugins/plugin1
-
-You might want to get ``project`` straight in the base path, the clone map would be::
-
-  clonemap:
-   - name: 'project'
-     dest: '.'
-
-Then to strip out ``thirdparty`` such that the plugins land under the
-``/plugins`` directory of the basepath, you can use regex and capturing
-groups::
-
-  clonemap:
-   - name: 'project'
-     dest: '.'
-   - name: 'thirdparty/(plugins/.*)'
-     dest: '\1'
-
-The resulting workspace will contains::
-
-  project -> ./
-  thirdparty/plugins/plugin1  -> ./plugins/plugin1
-
-
-Zuul parameters
-'''''''''''''''
-
-The Zuul cloner reuses Zuul parameters such as ZUUL_BRANCH, ZUUL_REF or
-ZUUL_PROJECT.  It will attempt to load them from the environment variables or
-you can pass them as parameters (in which case it will override the
-environment variable if it is set).  The matching command line parameters use
-the ``zuul`` prefix hence ZUUL_REF can be passed to the cloner using
-``--zuul-ref``.
-
-Usage
------
-The general options that apply are:
-
-.. program-output:: zuul-cloner --help
-
-
-Ref lookup order
-''''''''''''''''
-
-The Zuul cloner will attempt to lookup references in this order:
-
- 1) Zuul reference for the indicated branch
- 2) Zuul reference for the master branch
- 3) The tip of the indicated branch
- 4) The tip of the master branch
-
-The "indicated branch" is one of the following:
-
- A) The project-specific override branch (from project_branches arg)
- B) The user specified branch (from the branch arg)
- C) ZUUL_BRANCH (from the zuul_branch arg)
-
-Clone order
------------
-
-When cloning repositories, the destination folder should not exist or
-``git clone`` will complain. This happens whenever cloning a sub project
-before its parent project. For example::
-
- zuul-cloner project/plugins/plugin1 project
-
-Will create the directory ``project`` when cloning the plugin. The
-cloner processes the clones in the order supplied, so you should swap the
-projects::
-
-  zuul-cloner project project/plugins/plugin1
-
-Cached repositories
--------------------
-
-The ``--cache-dir`` option can be used to reduce network traffic by
-cloning from a local repository which may not be up to date.
-
-If the ``--cache-dir`` option is supplied, zuul-cloner will start by
-cloning any projects it processes from those found in that directory.
-The URL of origin remote of the resulting clone will be reset to use
-the ``git_base_url`` and then the remote will be updated so that the
-repository has all the information in the upstream repository.
-
-The default for ``--cache-dir`` is taken from the environment variable
-``ZUUL_CACHE_DIR``. A value provided explicitly on the command line
-overrides the environment variable setting.
-
-The ``--cache-no-hardlinks`` option can be used to force git to
-always copy git objects from the cache directory. By default, if
-the cache directory is on the same disk as the workspace, git-clone
-will hardlink git objects to speed up the process and save space.
+:title: Zuul Cloner
+
+Zuul Cloner
+===========
+
+Zuul includes a simple command line client that may be used to clone
+repositories with Zuul references applied.
+
+Configuration
+-------------
+
+Clone map
+'''''''''
+
+By default, Zuul cloner will clone the project under ``basepath`` which
+would create sub directories whenever a project name contains slashes.  Since
+you might want to finely tweak the final destination, a clone map lets you
+change the destination on a per project basis.  The configuration is done using
+a YAML file passed with ``-m``.
+
+With a project hierarchy such as::
+
+ project
+ thirdparty/plugins/plugin1
+
+You might want to get ``project`` straight in the base path, the clone map would be::
+
+  clonemap:
+   - name: 'project'
+     dest: '.'
+
+Then to strip out ``thirdparty`` such that the plugins land under the
+``/plugins`` directory of the basepath, you can use regex and capturing
+groups::
+
+  clonemap:
+   - name: 'project'
+     dest: '.'
+   - name: 'thirdparty/(plugins/.*)'
+     dest: '\1'
+
+The resulting workspace will contains::
+
+  project -> ./
+  thirdparty/plugins/plugin1  -> ./plugins/plugin1
+
+
+Zuul parameters
+'''''''''''''''
+
+The Zuul cloner reuses Zuul parameters such as ZUUL_BRANCH, ZUUL_REF or
+ZUUL_PROJECT.  It will attempt to load them from the environment variables or
+you can pass them as parameters (in which case it will override the
+environment variable if it is set).  The matching command line parameters use
+the ``zuul`` prefix hence ZUUL_REF can be passed to the cloner using
+``--zuul-ref``.
+
+Usage
+-----
+The general options that apply are:
+
+.. program-output:: zuul-cloner --help
+
+
+Ref lookup order
+''''''''''''''''
+
+The Zuul cloner will attempt to lookup references in this order:
+
+ 1) Zuul reference for the indicated branch
+ 2) Zuul reference for the master branch
+ 3) The tip of the indicated branch
+ 4) The tip of the master branch
+
+The "indicated branch" is one of the following:
+
+ A) The project-specific override branch (from project_branches arg)
+ B) The user specified branch (from the branch arg)
+ C) ZUUL_BRANCH (from the zuul_branch arg)
+
+Clone order
+-----------
+
+When cloning repositories, the destination folder should not exist or
+``git clone`` will complain. This happens whenever cloning a sub project
+before its parent project. For example::
+
+ zuul-cloner project/plugins/plugin1 project
+
+Will create the directory ``project`` when cloning the plugin. The
+cloner processes the clones in the order supplied, so you should swap the
+projects::
+
+  zuul-cloner project project/plugins/plugin1
+
+Cached repositories
+-------------------
+
+The ``--cache-dir`` option can be used to reduce network traffic by
+cloning from a local repository which may not be up to date.
+
+If the ``--cache-dir`` option is supplied, zuul-cloner will start by
+cloning any projects it processes from those found in that directory.
+The URL of origin remote of the resulting clone will be reset to use
+the ``git_base_url`` and then the remote will be updated so that the
+repository has all the information in the upstream repository.
+
+The default for ``--cache-dir`` is taken from the environment variable
+``ZUUL_CACHE_DIR``. A value provided explicitly on the command line
+overrides the environment variable setting.
+
+The ``--cache-no-hardlinks`` option can be used to force git to
+always copy git objects from the cache directory. By default, if
+the cache directory is on the same disk as the workspace, git-clone
+will hardlink git objects to speed up the process and save space.
diff --git a/doc/source/conf.py b/doc/source/conf.py
index 9e0d2c7..d7648b2 100644
--- a/doc/source/conf.py
+++ b/doc/source/conf.py
@@ -1,235 +1,235 @@
-# -*- coding: utf-8 -*-
-#
-# Zuul documentation build configuration file, created by
-# sphinx-quickstart on Fri Jun  8 14:44:26 2012.
-#
-# This file is execfile()d with the current directory set to its containing dir.
-#
-# Note that not all possible configuration values are present in this
-# autogenerated file.
-#
-# All configuration values have a default; values that are commented out
-# serve to show the default.
-
-import sys, os
-
-# If extensions (or modules to document with autodoc) are in another directory,
-# add these directories to sys.path here. If the directory is relative to the
-# documentation root, use os.path.abspath to make it absolute, like shown here.
-sys.path.insert(0, os.path.abspath('../..'))
-
-# -- General configuration -----------------------------------------------------
-
-# If your documentation needs a minimal Sphinx version, state it here.
-#needs_sphinx = '1.0'
-
-# Add any Sphinx extension module names here, as strings. They can be extensions
-# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
-extensions = [ 'sphinxcontrib.blockdiag', 'sphinxcontrib.programoutput' ]
-#extensions = ['sphinx.ext.intersphinx']
-#intersphinx_mapping = {'python': ('http://docs.python.org/2.7', None)}
-
-# Add any paths that contain templates here, relative to this directory.
-templates_path = ['_templates']
-
-# The suffix of source filenames.
-source_suffix = '.rst'
-
-# The encoding of source files.
-#source_encoding = 'utf-8-sig'
-
-# The master toctree document.
-master_doc = 'index'
-
-# General information about the project.
-project = u'Zuul'
-copyright = u'2012, OpenStack'
-
-# The language for content autogenerated by Sphinx. Refer to documentation
-# for a list of supported languages.
-#language = None
-
-# There are two options for replacing |today|: either, you set today to some
-# non-false value, then it is used:
-#today = ''
-# Else, today_fmt is used as the format for a strftime call.
-#today_fmt = '%B %d, %Y'
-
-# List of patterns, relative to source directory, that match files and
-# directories to ignore when looking for source files.
-exclude_patterns = []
-
-# The reST default role (used for this markup: `text`) to use for all documents.
-#default_role = None
-
-# If true, '()' will be appended to :func: etc. cross-reference text.
-#add_function_parentheses = True
-
-# If true, the current module name will be prepended to all description
-# unit titles (such as .. function::).
-#add_module_names = True
-
-# If true, sectionauthor and moduleauthor directives will be shown in the
-# output. They are ignored by default.
-#show_authors = False
-
-# The name of the Pygments (syntax highlighting) style to use.
-pygments_style = 'sphinx'
-
-# A list of ignored prefixes for module index sorting.
-#modindex_common_prefix = []
-
-
-# -- Options for HTML output ---------------------------------------------------
-
-# The theme to use for HTML and HTML Help pages.  See the documentation for
-# a list of builtin themes.
-html_theme = 'default'
-
-# Theme options are theme-specific and customize the look and feel of a theme
-# further.  For a list of options available for each theme, see the
-# documentation.
-#html_theme_options = {}
-
-# Add any paths that contain custom themes here, relative to this directory.
-#html_theme_path = []
-
-# The name for this set of Sphinx documents.  If None, it defaults to
-# "<project> v<release> documentation".
-#html_title = None
-
-# A shorter title for the navigation bar.  Default is the same as html_title.
-#html_short_title = None
-
-# The name of an image file (relative to this directory) to place at the top
-# of the sidebar.
-#html_logo = None
-
-# The name of an image file (within the static path) to use as favicon of the
-# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
-# pixels large.
-#html_favicon = None
-
-# Add any paths that contain custom static files (such as style sheets) here,
-# relative to this directory. They are copied after the builtin static files,
-# so a file named "default.css" will overwrite the builtin "default.css".
-#html_static_path = ['_static']
-
-# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
-# using the given strftime format.
-#html_last_updated_fmt = '%b %d, %Y'
-
-# If true, SmartyPants will be used to convert quotes and dashes to
-# typographically correct entities.
-#html_use_smartypants = True
-
-# Custom sidebar templates, maps document names to template names.
-#html_sidebars = {}
-
-# Additional templates that should be rendered to pages, maps page names to
-# template names.
-#html_additional_pages = {}
-
-# If false, no module index is generated.
-#html_domain_indices = True
-
-# If false, no index is generated.
-#html_use_index = True
-
-# If true, the index is split into individual pages for each letter.
-#html_split_index = False
-
-# If true, links to the reST sources are added to the pages.
-#html_show_sourcelink = True
-
-# If true, "Created using Sphinx" is shown in the HTML footer. Default is True.
-#html_show_sphinx = True
-
-# If true, "(C) Copyright ..." is shown in the HTML footer. Default is True.
-#html_show_copyright = True
-
-# If true, an OpenSearch description file will be output, and all pages will
-# contain a <link> tag referring to it.  The value of this option must be the
-# base URL from which the finished HTML is served.
-#html_use_opensearch = ''
-
-# This is the file name suffix for HTML files (e.g. ".xhtml").
-#html_file_suffix = None
-
-# Output file base name for HTML help builder.
-htmlhelp_basename = 'Zuuldoc'
-
-
-# -- Options for LaTeX output --------------------------------------------------
-
-latex_elements = {
-# The paper size ('letterpaper' or 'a4paper').
-#'papersize': 'letterpaper',
-
-# The font size ('10pt', '11pt' or '12pt').
-#'pointsize': '10pt',
-
-# Additional stuff for the LaTeX preamble.
-#'preamble': '',
-}
-
-# Grouping the document tree into LaTeX files. List of tuples
-# (source start file, target name, title, author, documentclass [howto/manual]).
-latex_documents = [
-  ('index', 'Zuul.tex', u'Zuul Documentation',
-   u'OpenStack', 'manual'),
-]
-
-# The name of an image file (relative to this directory) to place at the top of
-# the title page.
-#latex_logo = None
-
-# For "manual" documents, if this is true, then toplevel headings are parts,
-# not chapters.
-#latex_use_parts = False
-
-# If true, show page references after internal links.
-#latex_show_pagerefs = False
-
-# If true, show URL addresses after external links.
-#latex_show_urls = False
-
-# Documents to append as an appendix to all manuals.
-#latex_appendices = []
-
-# If false, no module index is generated.
-#latex_domain_indices = True
-
-
-# -- Options for manual page output --------------------------------------------
-
-# One entry per manual page. List of tuples
-# (source start file, name, description, authors, manual section).
-man_pages = [
-    ('index', 'zuul-server', u'Zuul Documentation',
-     [u'OpenStack'], 1)
-]
-
-# If true, show URL addresses after external links.
-#man_show_urls = False
-
-
-# -- Options for Texinfo output ------------------------------------------------
-
-# Grouping the document tree into Texinfo files. List of tuples
-# (source start file, target name, title, author,
-#  dir menu entry, description, category)
-texinfo_documents = [
-  ('index', 'Zuul', u'Zuul Documentation',
-   u'OpenStack', 'Zuul', 'One line description of project.',
-   'Miscellaneous'),
-]
-
-# Documents to append as an appendix to all manuals.
-#texinfo_appendices = []
-
-# If false, no module index is generated.
-#texinfo_domain_indices = True
-
-# How to display URL addresses: 'footnote', 'no', or 'inline'.
-#texinfo_show_urls = 'footnote'
+# -*- coding: utf-8 -*-
+#
+# Zuul documentation build configuration file, created by
+# sphinx-quickstart on Fri Jun  8 14:44:26 2012.
+#
+# This file is execfile()d with the current directory set to its containing dir.
+#
+# Note that not all possible configuration values are present in this
+# autogenerated file.
+#
+# All configuration values have a default; values that are commented out
+# serve to show the default.
+
+import sys, os
+
+# If extensions (or modules to document with autodoc) are in another directory,
+# add these directories to sys.path here. If the directory is relative to the
+# documentation root, use os.path.abspath to make it absolute, like shown here.
+sys.path.insert(0, os.path.abspath('../..'))
+
+# -- General configuration -----------------------------------------------------
+
+# If your documentation needs a minimal Sphinx version, state it here.
+#needs_sphinx = '1.0'
+
+# Add any Sphinx extension module names here, as strings. They can be extensions
+# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
+extensions = [ 'sphinxcontrib.blockdiag', 'sphinxcontrib.programoutput' ]
+#extensions = ['sphinx.ext.intersphinx']
+#intersphinx_mapping = {'python': ('http://docs.python.org/2.7', None)}
+
+# Add any paths that contain templates here, relative to this directory.
+templates_path = ['_templates']
+
+# The suffix of source filenames.
+source_suffix = '.rst'
+
+# The encoding of source files.
+#source_encoding = 'utf-8-sig'
+
+# The master toctree document.
+master_doc = 'index'
+
+# General information about the project.
+project = u'Zuul'
+copyright = u'2012, OpenStack'
+
+# The language for content autogenerated by Sphinx. Refer to documentation
+# for a list of supported languages.
+#language = None
+
+# There are two options for replacing |today|: either, you set today to some
+# non-false value, then it is used:
+#today = ''
+# Else, today_fmt is used as the format for a strftime call.
+#today_fmt = '%B %d, %Y'
+
+# List of patterns, relative to source directory, that match files and
+# directories to ignore when looking for source files.
+exclude_patterns = []
+
+# The reST default role (used for this markup: `text`) to use for all documents.
+#default_role = None
+
+# If true, '()' will be appended to :func: etc. cross-reference text.
+#add_function_parentheses = True
+
+# If true, the current module name will be prepended to all description
+# unit titles (such as .. function::).
+#add_module_names = True
+
+# If true, sectionauthor and moduleauthor directives will be shown in the
+# output. They are ignored by default.
+#show_authors = False
+
+# The name of the Pygments (syntax highlighting) style to use.
+pygments_style = 'sphinx'
+
+# A list of ignored prefixes for module index sorting.
+#modindex_common_prefix = []
+
+
+# -- Options for HTML output ---------------------------------------------------
+
+# The theme to use for HTML and HTML Help pages.  See the documentation for
+# a list of builtin themes.
+html_theme = 'default'
+
+# Theme options are theme-specific and customize the look and feel of a theme
+# further.  For a list of options available for each theme, see the
+# documentation.
+#html_theme_options = {}
+
+# Add any paths that contain custom themes here, relative to this directory.
+#html_theme_path = []
+
+# The name for this set of Sphinx documents.  If None, it defaults to
+# "<project> v<release> documentation".
+#html_title = None
+
+# A shorter title for the navigation bar.  Default is the same as html_title.
+#html_short_title = None
+
+# The name of an image file (relative to this directory) to place at the top
+# of the sidebar.
+#html_logo = None
+
+# The name of an image file (within the static path) to use as favicon of the
+# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
+# pixels large.
+#html_favicon = None
+
+# Add any paths that contain custom static files (such as style sheets) here,
+# relative to this directory. They are copied after the builtin static files,
+# so a file named "default.css" will overwrite the builtin "default.css".
+#html_static_path = ['_static']
+
+# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
+# using the given strftime format.
+#html_last_updated_fmt = '%b %d, %Y'
+
+# If true, SmartyPants will be used to convert quotes and dashes to
+# typographically correct entities.
+#html_use_smartypants = True
+
+# Custom sidebar templates, maps document names to template names.
+#html_sidebars = {}
+
+# Additional templates that should be rendered to pages, maps page names to
+# template names.
+#html_additional_pages = {}
+
+# If false, no module index is generated.
+#html_domain_indices = True
+
+# If false, no index is generated.
+#html_use_index = True
+
+# If true, the index is split into individual pages for each letter.
+#html_split_index = False
+
+# If true, links to the reST sources are added to the pages.
+#html_show_sourcelink = True
+
+# If true, "Created using Sphinx" is shown in the HTML footer. Default is True.
+#html_show_sphinx = True
+
+# If true, "(C) Copyright ..." is shown in the HTML footer. Default is True.
+#html_show_copyright = True
+
+# If true, an OpenSearch description file will be output, and all pages will
+# contain a <link> tag referring to it.  The value of this option must be the
+# base URL from which the finished HTML is served.
+#html_use_opensearch = ''
+
+# This is the file name suffix for HTML files (e.g. ".xhtml").
+#html_file_suffix = None
+
+# Output file base name for HTML help builder.
+htmlhelp_basename = 'Zuuldoc'
+
+
+# -- Options for LaTeX output --------------------------------------------------
+
+latex_elements = {
+# The paper size ('letterpaper' or 'a4paper').
+#'papersize': 'letterpaper',
+
+# The font size ('10pt', '11pt' or '12pt').
+#'pointsize': '10pt',
+
+# Additional stuff for the LaTeX preamble.
+#'preamble': '',
+}
+
+# Grouping the document tree into LaTeX files. List of tuples
+# (source start file, target name, title, author, documentclass [howto/manual]).
+latex_documents = [
+  ('index', 'Zuul.tex', u'Zuul Documentation',
+   u'OpenStack', 'manual'),
+]
+
+# The name of an image file (relative to this directory) to place at the top of
+# the title page.
+#latex_logo = None
+
+# For "manual" documents, if this is true, then toplevel headings are parts,
+# not chapters.
+#latex_use_parts = False
+
+# If true, show page references after internal links.
+#latex_show_pagerefs = False
+
+# If true, show URL addresses after external links.
+#latex_show_urls = False
+
+# Documents to append as an appendix to all manuals.
+#latex_appendices = []
+
+# If false, no module index is generated.
+#latex_domain_indices = True
+
+
+# -- Options for manual page output --------------------------------------------
+
+# One entry per manual page. List of tuples
+# (source start file, name, description, authors, manual section).
+man_pages = [
+    ('index', 'zuul-server', u'Zuul Documentation',
+     [u'OpenStack'], 1)
+]
+
+# If true, show URL addresses after external links.
+#man_show_urls = False
+
+
+# -- Options for Texinfo output ------------------------------------------------
+
+# Grouping the document tree into Texinfo files. List of tuples
+# (source start file, target name, title, author,
+#  dir menu entry, description, category)
+texinfo_documents = [
+  ('index', 'Zuul', u'Zuul Documentation',
+   u'OpenStack', 'Zuul', 'One line description of project.',
+   'Miscellaneous'),
+]
+
+# Documents to append as an appendix to all manuals.
+#texinfo_appendices = []
+
+# If false, no module index is generated.
+#texinfo_domain_indices = True
+
+# How to display URL addresses: 'footnote', 'no', or 'inline'.
+#texinfo_show_urls = 'footnote'
diff --git a/doc/source/connections.rst b/doc/source/connections.rst
index c1753da..cf9b053 100644
--- a/doc/source/connections.rst
+++ b/doc/source/connections.rst
@@ -1,91 +1,91 @@
-:title: Connections
-
-.. _connections:
-
-Connections
-===========
-
-zuul coordinates talking to multiple different systems via the concept
-of connections. A connection is listed in the :ref:`zuulconf` file and is
-then referred to from the :ref:`layoutyaml`. This makes it possible to
-receive events from gerrit via one connection and post results from another
-connection that may report back as a different user.
-
-Gerrit
-------
-
-Create a connection with gerrit.
-
-**driver=gerrit**
-
-**server**
-  FQDN of Gerrit server.
-  ``server=review.example.com``
-
-**port**
-  Optional: Gerrit server port.
-  ``port=29418``
-
-**baseurl**
-  Optional: path to Gerrit web interface. Defaults to ``https://<value
-  of server>/``. ``baseurl=https://review.example.com/review_site/``
-
-**user**
-  User name to use when logging into above server via ssh.
-  ``user=zuul``
-
-**sshkey**
-  Path to SSH key to use when logging into above server.
-  ``sshkey=/home/zuul/.ssh/id_rsa``
-
-**event_delay** (optional)
-
-  When querying a change immediately after a patchset upload, Gerrit may
-  return incorrect data about dependent changes. In order to avoid this,
-  the events are not delivered to Zuul until a constant number of
-  seconds has passed.
-
-  Note that if we receive several events in succession, we will only
-  need to delay for the first event.
-
-  Default: ``10`` (seconds)
-
-
-Gerrit Configuration
-~~~~~~~~~~~~~~~~~~~~
-
-Zuul will need access to a Gerrit user.
-
-Create an SSH keypair for Zuul to use if there isn't one already, and
-create a Gerrit user with that key::
-
-  cat ~/id_rsa.pub | ssh -p29418 gerrit.example.com gerrit create-account --ssh-key - --full-name Jenkins jenkins
-
-Give that user whatever permissions will be needed on the projects you
-want Zuul to gate.  For instance, you may want to grant ``Verified
-+/-1`` and ``Submit`` to the user.  Additional categories or values may
-be added to Gerrit.  Zuul is very flexible and can take advantage of
-those.
-
-SMTP
-----
-
-**driver=smtp**
-
-**server**
-  SMTP server hostname or address to use.
-  ``server=localhost``
-
-**port**
-  Optional: SMTP server port.
-  ``port=25``
-
-**default_from**
-  Who the email should appear to be sent from when emailing the report.
-  This can be overridden by individual pipelines.
-  ``default_from=zuul@example.com``
-
-**default_to**
-  Who the report should be emailed to by default.
-  This can be overridden by individual pipelines.
-  ``default_to=you@example.com``
+:title: Connections
+
+.. _connections:
+
+Connections
+===========
+
+zuul coordinates talking to multiple different systems via the concept
+of connections. A connection is listed in the :ref:`zuulconf` file and is
+then referred to from the :ref:`layoutyaml`. This makes it possible to
+receive events from gerrit via one connection and post results from another
+connection that may report back as a different user.
+
+Gerrit
+------
+
+Create a connection with gerrit.
+
+**driver=gerrit**
+
+**server**
+  FQDN of Gerrit server.
+  ``server=review.example.com``
+
+**port**
+  Optional: Gerrit server port.
+  ``port=29418``
+
+**baseurl**
+  Optional: path to Gerrit web interface. Defaults to ``https://<value
+  of server>/``. ``baseurl=https://review.example.com/review_site/``
+
+**user**
+  User name to use when logging into above server via ssh.
+  ``user=zuul``
+
+**sshkey**
+  Path to SSH key to use when logging into above server.
+  ``sshkey=/home/zuul/.ssh/id_rsa``
+
+**event_delay** (optional)
+
+  When querying a change immediately after a patchset upload, Gerrit may
+  return incorrect data about dependent changes. In order to avoid this,
+  the events are not delivered to Zuul until a constant number of
+  seconds has passed.
+
+  Note that if we receive several events in succession, we will only
+  need to delay for the first event.
+
+  Default: ``10`` (seconds)
+
+
+Gerrit Configuration
+~~~~~~~~~~~~~~~~~~~~
+
+Zuul will need access to a Gerrit user.
+
+Create an SSH keypair for Zuul to use if there isn't one already, and
+create a Gerrit user with that key::
+
+  cat ~/id_rsa.pub | ssh -p29418 gerrit.example.com gerrit create-account --ssh-key - --full-name Jenkins jenkins
+
+Give that user whatever permissions will be needed on the projects you
+want Zuul to gate.  For instance, you may want to grant ``Verified
++/-1`` and ``Submit`` to the user.  Additional categories or values may
+be added to Gerrit.  Zuul is very flexible and can take advantage of
+those.
+
+SMTP
+----
+
+**driver=smtp**
+
+**server**
+  SMTP server hostname or address to use.
+  ``server=localhost``
+
+**port**
+  Optional: SMTP server port.
+  ``port=25``
+
+**default_from**
+  Who the email should appear to be sent from when emailing the report.
+  This can be overridden by individual pipelines.
+  ``default_from=zuul@example.com``
+
+**default_to**
+  Who the report should be emailed to by default.
+  This can be overridden by individual pipelines.
+  ``default_to=you@example.com``
diff --git a/doc/source/gating.rst b/doc/source/gating.rst
index c10ba83..06eb430 100644
--- a/doc/source/gating.rst
+++ b/doc/source/gating.rst
@@ -1,471 +1,471 @@
-:title: Project Gating
-
-Project Gating
-==============
-
-Traditionally, many software development projects merge changes from
-developers into the repository, and then identify regressions
-resulting from those changes (perhaps by running a test suite with a
-continuous integration system such as Jenkins), followed by more
-patches to fix those bugs.  When the mainline of development is
-broken, it can be very frustrating for developers and can cause lost
-productivity, particularly so when the number of contributors or
-contributions is large.
-
-The process of gating attempts to prevent changes that introduce
-regressions from being merged.  This keeps the mainline of development
-open and working for all developers, and only when a change is
-confirmed to work without disruption is it merged.
-
-Many projects practice an informal method of gating where developers
-with mainline commit access ensure that a test suite runs before
-merging a change.  With more developers, more changes, and more
-comprehensive test suites, that process does not scale very well, and
-is not the best use of a developer's time.  Zuul can help automate
-this process, with a particular emphasis on ensuring large numbers of
-changes are tested correctly.
-
-Zuul was designed to handle the workflow of the OpenStack project, but
-can be used with any project.
-
-Testing in parallel
--------------------
-
-A particular focus of Zuul is ensuring correctly ordered testing of
-changes in parallel.  A gating system should always test each change
-applied to the tip of the branch exactly as it is going to be merged.
-A simple way to do that would be to test one change at a time, and
-merge it only if it passes tests.  That works very well, but if
-changes take a long time to test, developers may have to wait a long
-time for their changes to make it into the repository.  With some
-projects, it may take hours to test changes, and it is easy for
-developers to create changes at a rate faster than they can be tested
-and merged.
-
-Zuul's DependentPipelineManager allows for parallel execution of test
-jobs for gating while ensuring changes are tested correctly, exactly
-as if they had been tested one at a time.  It does this by performing
-speculative execution of test jobs; it assumes that all jobs will
-succeed and tests them in parallel accordingly.  If they do succeed,
-they can all be merged.  However, if one fails, then changes that were
-expecting it to succeed are re-tested without the failed change.  In
-the best case, as many changes as execution contexts are available may
-be tested in parallel and merged at once.  In the worst case, changes
-are tested one at a time (as each subsequent change fails, changes
-behind it start again).  In practice, the OpenStack project observes
-something closer to the best case.
-
-For example, if a core developer approves five changes in rapid
-succession::
-
-  A, B, C, D, E
-
-Zuul queues those changes in the order they were approved, and notes
-that each subsequent change depends on the one ahead of it merging:
-
-.. blockdiag::
-
-  blockdiag foo {
-    node_width = 40;
-    span_width = 40;
-    A <- B <- C <- D <- E;
-  }
-
-Zuul then starts immediately testing all of the changes in parallel.
-But in the case of changes that depend on others, it instructs the
-test system to include the changes ahead of it, with the assumption
-they pass.  That means jobs testing change *B* include change *A* as
-well::
-
-  Jobs for A: merge change A, then test
-  Jobs for B: merge changes A and B, then test
-  Jobs for C: merge changes A, B and C, then test
-  Jobs for D: merge changes A, B, C and D, then test
-  Jobs for E: merge changes A, B, C, D and E, then test
-
-Hence jobs triggered to tests A will only test A and ignore B, C, D:
-
-.. blockdiag::
-
-  blockdiag foo {
-    node_width = 40;
-    span_width = 40;
-    master -> A -> B -> C -> D -> E;
-    group jobs_for_A {
-        label = "Merged changes for A";
-        master -> A;
-    }
-    group ignored_to_test_A {
-        label = "Ignored changes";
-        color = "lightgray";
-        B -> C -> D -> E;
-    }
-  }
-
-The jobs for E would include the whole dependency chain: A, B, C, D, and E.
-E will be tested assuming A, B, C, and D passed:
-
-.. blockdiag::
-
-  blockdiag foo {
-    node_width = 40;
-    span_width = 40;
-    group jobs_for_E {
-        label = "Merged changes for E";
-        master -> A -> B -> C -> D -> E;
-    }
-  }
-
-If changes *A* and *B* pass tests (green), and *C*, *D*, and *E* fail (red):
-
-.. blockdiag::
-
-  blockdiag foo {
-    node_width = 40;
-    span_width = 40;
-
-    A [color = lightgreen];
-    B [color = lightgreen];
-    C [color = pink];
-    D [color = pink];
-    E [color = pink];
-
-    master <- A <- B <- C <- D <- E;
-  }
-
-Zuul will merge change *A* followed by change *B*, leaving this queue:
-
-.. blockdiag::
-
-  blockdiag foo {
-    node_width = 40;
-    span_width = 40;
-
-    C [color = pink];
-    D [color = pink];
-    E [color = pink];
-
-    C <- D <- E;
-  }
-
-Since *D* was dependent on *C*, it is not clear whether *D*'s failure is the
-result of a defect in *D* or *C*:
-
-.. blockdiag::
-
-  blockdiag foo {
-    node_width = 40;
-    span_width = 40;
-
-    C [color = pink];
-    D [label = "D\n?"];
-    E [label = "E\n?"];
-
-    C <- D <- E;
-  }
-
-Since *C* failed, Zuul will report its failure and drop *C* from the queue,
-keeping D and E:
-
-.. blockdiag::
-
-  blockdiag foo {
-    node_width = 40;
-    span_width = 40;
-
-    D [label = "D\n?"];
-    E [label = "E\n?"];
-
-    D <- E;
-  }
-
-This queue is the same as if two new changes had just arrived, so Zuul
-starts the process again testing *D* against the tip of the branch, and
-*E* against *D*:
-
-.. blockdiag::
-
-  blockdiag foo {
-    node_width = 40;
-    span_width = 40;
-    master -> D -> E;
-    group jobs_for_D {
-        label = "Merged changes for D";
-        master -> D;
-    }
-    group ignored_to_test_D {
-        label = "Skip";
-        color = "lightgray";
-        E;
-    }
-  }
-
-.. blockdiag::
-
-  blockdiag foo {
-    node_width = 40;
-    span_width = 40;
-    group jobs_for_E {
-        label = "Merged changes for E";
-        master -> D -> E;
-    }
-  }
-
-
-Cross Project Testing
----------------------
-
-When your projects are closely coupled together, you want to make sure
-changes entering the gate are going to be tested with the version of
-other projects currently enqueued in the gate (since they will
-eventually be merged and might introduce breaking features).
-
-Such relationships can be defined in Zuul configuration by registering
-a job in a DependentPipeline of several projects. Whenever a change
-enters such a pipeline, it will create references for the other
-projects as well.  As an example, given a main project ``acme`` and a
-plugin ``plugin`` you can define a job ``acme-tests`` which should be
-run for both projects:
-
-.. code-block:: yaml
-
-  pipelines:
-    - name: gate
-      manager: DependentPipelineManager
-
-  projects::
-    - name: acme
-      gate:
-       - acme-tests
-    - name: plugin
-      gate:
-       - acme-tests  # Register job again
-
-Whenever a change enters the ``gate`` pipeline queue, Zuul creates a reference
-for it.  For each subsequent change, an additional reference is created for the
-changes ahead in the queue.  As a result, you will always be able to fetch the
-future state of your project dependencies for each change in the queue.
-
-Based on the pipeline and project definitions above, three changes are
-inserted in the ``gate`` pipeline with the associated references:
-
-  ========  ======= ====== =========
-  Change    Project Branch Zuul Ref.
-  ========  ======= ====== =========
-  Change 1  acme    master master/Z1
-  Change 2  plugin  stable stable/Z2
-  Change 3  plugin  master master/Z3
-  ========  ======= ====== =========
-
-Since the changes enter a DependentPipelineManager pipeline, Zuul creates
-additional references:
-
-  ====== ======= ========= =============================
-  Change Project Zuul Ref. Description
-  ====== ======= ========= =============================
-  1      acme    master/Z1 acme master + change 1
-  ------ ------- --------- -----------------------------
-  2      acme    master/Z2 acme master + change 1
-  2      plugin  stable/Z2 plugin stable + change 2
-  ------ ------- --------- -----------------------------
-  3      acme    master/Z3 acme master + change 1
-  3      plugin  stable/Z3 plugin stable + change 2
-  3      plugin  master/Z3 plugin master + change 3
-  ====== ======= ========= =============================
-
-In order to test change 3, you would clone both repositories and simply
-fetch the Z3 reference for each combination of project/branch you are
-interested in testing. For example, you could fetch ``acme`` with
-master/Z3 and ``plugin`` with master/Z3 and thus have ``acme`` with
-change 1 applied as the expected state for when Change 3 would merge.
-When your job fetches several repositories without changes ahead in the
-queue, they may not have a Z reference in which case you can just check
-out the branch.
-
-
-Cross Repository Dependencies
------------------------------
-
-Zuul permits users to specify dependencies across repositories.  Using
-a special header in Git commit messages, Users may specify that a
-change depends on another change in any repository known to Zuul.
-
-Zuul's cross-repository dependencies (CRD) behave like a directed
-acyclic graph (DAG), like git itself, to indicate a one-way dependency
-relationship between changes in different git repositories.  Change A
-may depend on B, but B may not depend on A.
-
-To use them, include ``Depends-On: <gerrit-change-id>`` in the footer of
-a commit message.  Use the full Change-ID ('I' + 40 characters).
-
-
-Dependent Pipeline
-~~~~~~~~~~~~~~~~~~
-
-When Zuul sees CRD changes, it serializes them in the usual manner when
-enqueuing them into a pipeline.  This means that if change A depends on
-B, then when they are added to a dependent pipeline, B will appear first
-and A will follow:
-
-.. blockdiag::
-  :align: center
-
-  blockdiag crd {
-    orientation = portrait
-    span_width = 30
-    class greendot [
-        label = "",
-        shape = circle,
-        color = green,
-        width = 20, height = 20
-    ]
-
-    A_status [ class = greendot ]
-    B_status [ class = greendot ]
-    B_status -- A_status
-
-    'Change B\nChange-Id: Iabc' <- 'Change A\nDepends-On: Iabc'
-  }
-
-If tests for B fail, both B and A will be removed from the pipeline, and
-it will not be possible for A to merge until B does.
-
-
-.. note::
-
-   If changes with CRD do not share a change queue then Zuul is unable
-   to enqueue them together, and the first will be required to merge
-   before the second is enqueued.
-
-Independent Pipeline
-~~~~~~~~~~~~~~~~~~~~
-
-When changes are enqueued into an independent pipeline, all of the
-related dependencies (both normal git-dependencies that come from parent
-commits as well as CRD changes) appear in a dependency graph, as in a
-dependent pipeline. This means that even in an independent pipeline,
-your change will be tested with its dependencies.  So changes that were
-previously unable to be fully tested until a related change landed in a
-different repository may now be tested together from the start.
-
-All of the changes are still independent (so you will note that the
-whole pipeline does not share a graph as in a dependent pipeline), but
-for each change tested, all of its dependencies are visually connected
-to it, and they are used to construct the git references that Zuul uses
-when testing.
-
-When looking at this graph on the status page, you will note that the
-dependencies show up as grey dots, while the actual change tested shows
-up as red or green (depending on the jobs results):
-
-.. blockdiag::
-  :align: center
-
-  blockdiag crdgrey {
-    orientation = portrait
-    span_width = 30
-    class dot [
-        label = "",
-        shape = circle,
-        width = 20, height = 20
-    ]
-
-    A_status [class = "dot", color = green]
-    B_status [class = "dot", color = grey]
-    B_status -- A_status
-
-    "Change B" <- "Change A\nDepends-On: B"
-  }
-
-This is to indicate that the grey changes are only there to establish
-dependencies.  Even if one of the dependencies is also being tested, it
-will show up as a grey dot when used as a dependency, but separately and
-additionally will appear as its own red or green dot for its test.
-
-
-Multiple Changes
-~~~~~~~~~~~~~~~~
-
-A Gerrit change ID may refer to multiple changes (on multiple branches
-of the same project, or even multiple projects).  In these cases, Zuul
-will treat all of the changes with that change ID as dependencies.  So
-if you say that change in project A Depends-On a change ID that has
-changes in two branches of project B, then when testing the change to
-project A, both project B changes will be applied, and when deciding
-whether the project A change can merge, both changes must merge ahead
-of it.
-
-.. blockdiag::
-  :align: center
-
-  blockdiag crdmultirepos {
-    orientation = portrait
-    span_width = 30
-    class greendot [
-        label = "",
-        shape = circle,
-        color = green,
-        width = 20, height = 20
-    ]
-
-    B_stable_status [ class = "greendot" ]
-    B_master_status [ class = "greendot" ]
-    A_status [ class = "greendot" ]
-    B_stable_status -- B_master_status -- A_status
-
-    A [ label = "Repo A\nDepends-On: I123" ]
-    group {
-        orientation = portrait
-        label = "Dependencies"
-        color = "lightgray"
-
-        B_stable [ label = "Repo B\nChange-Id: I123\nBranch: stable" ]
-        B_master [ label = "Repo B\nChange-Id: I123\nBranch: master" ]
-    }
-    B_master <- A
-    B_stable <- A
-
-  }
-
-A change may depend on more than one Gerrit change ID as well.  So it
-is possible for a change in project A to depend on a change in project
-B and a change in project C.  Simply add more ``Depends-On:`` lines to
-the commit message footer.
-
-.. blockdiag::
-  :align: center
-
-  blockdiag crdmultichanges {
-    orientation = portrait
-    span_width = 30
-    class greendot [
-        label = "",
-        shape = circle,
-        color = green,
-        width = 20, height = 20
-    ]
-
-    C_status [ class = "greendot" ]
-    B_status [ class = "greendot" ]
-    A_status [ class = "greendot" ]
-    C_status -- B_status -- A_status
-
-    A [ label = "Repo A\nDepends-On: I123\nDepends-On: Iabc" ]
-    group {
-        orientation = portrait
-        label = "Dependencies"
-        color = "lightgray"
-
-        B [ label = "Repo B\nChange-Id: I123" ]
-        C [ label = "Repo C\nChange-Id: Iabc" ]
-    }
-    B, C <- A
-  }
-
-Cycles
-~~~~~~
-
-If a cycle is created by use of CRD, Zuul will abort its work very
-early.  There will be no message in Gerrit and no changes that are part
-of the cycle will be enqueued into any pipeline.  This is to protect
-Zuul from infinite loops.
+:title: Project Gating
+
+Project Gating
+==============
+
+Traditionally, many software development projects merge changes from
+developers into the repository, and then identify regressions
+resulting from those changes (perhaps by running a test suite with a
+continuous integration system such as Jenkins), followed by more
+patches to fix those bugs.  When the mainline of development is
+broken, it can be very frustrating for developers and can cause lost
+productivity, particularly so when the number of contributors or
+contributions is large.
+
+The process of gating attempts to prevent changes that introduce
+regressions from being merged.  This keeps the mainline of development
+open and working for all developers, and only when a change is
+confirmed to work without disruption is it merged.
+
+Many projects practice an informal method of gating where developers
+with mainline commit access ensure that a test suite runs before
+merging a change.  With more developers, more changes, and more
+comprehensive test suites, that process does not scale very well, and
+is not the best use of a developer's time.  Zuul can help automate
+this process, with a particular emphasis on ensuring large numbers of
+changes are tested correctly.
+
+Zuul was designed to handle the workflow of the OpenStack project, but
+can be used with any project.
+
+Testing in parallel
+-------------------
+
+A particular focus of Zuul is ensuring correctly ordered testing of
+changes in parallel.  A gating system should always test each change
+applied to the tip of the branch exactly as it is going to be merged.
+A simple way to do that would be to test one change at a time, and
+merge it only if it passes tests.  That works very well, but if
+changes take a long time to test, developers may have to wait a long
+time for their changes to make it into the repository.  With some
+projects, it may take hours to test changes, and it is easy for
+developers to create changes at a rate faster than they can be tested
+and merged.
+
+Zuul's DependentPipelineManager allows for parallel execution of test
+jobs for gating while ensuring changes are tested correctly, exactly
+as if they had been tested one at a time.  It does this by performing
+speculative execution of test jobs; it assumes that all jobs will
+succeed and tests them in parallel accordingly.  If they do succeed,
+they can all be merged.  However, if one fails, then changes that were
+expecting it to succeed are re-tested without the failed change.  In
+the best case, as many changes as execution contexts are available may
+be tested in parallel and merged at once.  In the worst case, changes
+are tested one at a time (as each subsequent change fails, changes
+behind it start again).  In practice, the OpenStack project observes
+something closer to the best case.
+
+For example, if a core developer approves five changes in rapid
+succession::
+
+  A, B, C, D, E
+
+Zuul queues those changes in the order they were approved, and notes
+that each subsequent change depends on the one ahead of it merging:
+
+.. blockdiag::
+
+  blockdiag foo {
+    node_width = 40;
+    span_width = 40;
+    A <- B <- C <- D <- E;
+  }
+
+Zuul then starts immediately testing all of the changes in parallel.
+But in the case of changes that depend on others, it instructs the
+test system to include the changes ahead of it, with the assumption
+they pass.  That means jobs testing change *B* include change *A* as
+well::
+
+  Jobs for A: merge change A, then test
+  Jobs for B: merge changes A and B, then test
+  Jobs for C: merge changes A, B and C, then test
+  Jobs for D: merge changes A, B, C and D, then test
+  Jobs for E: merge changes A, B, C, D and E, then test
+
+Hence jobs triggered to tests A will only test A and ignore B, C, D:
+
+.. blockdiag::
+
+  blockdiag foo {
+    node_width = 40;
+    span_width = 40;
+    master -> A -> B -> C -> D -> E;
+    group jobs_for_A {
+        label = "Merged changes for A";
+        master -> A;
+    }
+    group ignored_to_test_A {
+        label = "Ignored changes";
+        color = "lightgray";
+        B -> C -> D -> E;
+    }
+  }
+
+The jobs for E would include the whole dependency chain: A, B, C, D, and E.
+E will be tested assuming A, B, C, and D passed:
+
+.. blockdiag::
+
+  blockdiag foo {
+    node_width = 40;
+    span_width = 40;
+    group jobs_for_E {
+        label = "Merged changes for E";
+        master -> A -> B -> C -> D -> E;
+    }
+  }
+
+If changes *A* and *B* pass tests (green), and *C*, *D*, and *E* fail (red):
+
+.. blockdiag::
+
+  blockdiag foo {
+    node_width = 40;
+    span_width = 40;
+
+    A [color = lightgreen];
+    B [color = lightgreen];
+    C [color = pink];
+    D [color = pink];
+    E [color = pink];
+
+    master <- A <- B <- C <- D <- E;
+  }
+
+Zuul will merge change *A* followed by change *B*, leaving this queue:
+
+.. blockdiag::
+
+  blockdiag foo {
+    node_width = 40;
+    span_width = 40;
+
+    C [color = pink];
+    D [color = pink];
+    E [color = pink];
+
+    C <- D <- E;
+  }
+
+Since *D* was dependent on *C*, it is not clear whether *D*'s failure is the
+result of a defect in *D* or *C*:
+
+.. blockdiag::
+
+  blockdiag foo {
+    node_width = 40;
+    span_width = 40;
+
+    C [color = pink];
+    D [label = "D\n?"];
+    E [label = "E\n?"];
+
+    C <- D <- E;
+  }
+
+Since *C* failed, Zuul will report its failure and drop *C* from the queue,
+keeping D and E:
+
+.. blockdiag::
+
+  blockdiag foo {
+    node_width = 40;
+    span_width = 40;
+
+    D [label = "D\n?"];
+    E [label = "E\n?"];
+
+    D <- E;
+  }
+
+This queue is the same as if two new changes had just arrived, so Zuul
+starts the process again testing *D* against the tip of the branch, and
+*E* against *D*:
+
+.. blockdiag::
+
+  blockdiag foo {
+    node_width = 40;
+    span_width = 40;
+    master -> D -> E;
+    group jobs_for_D {
+        label = "Merged changes for D";
+        master -> D;
+    }
+    group ignored_to_test_D {
+        label = "Skip";
+        color = "lightgray";
+        E;
+    }
+  }
+
+.. blockdiag::
+
+  blockdiag foo {
+    node_width = 40;
+    span_width = 40;
+    group jobs_for_E {
+        label = "Merged changes for E";
+        master -> D -> E;
+    }
+  }
+
+
+Cross Project Testing
+---------------------
+
+When your projects are closely coupled together, you want to make sure
+changes entering the gate are going to be tested with the version of
+other projects currently enqueued in the gate (since they will
+eventually be merged and might introduce breaking features).
+
+Such relationships can be defined in Zuul configuration by registering
+a job in a DependentPipeline of several projects. Whenever a change
+enters such a pipeline, it will create references for the other
+projects as well.  As an example, given a main project ``acme`` and a
+plugin ``plugin`` you can define a job ``acme-tests`` which should be
+run for both projects:
+
+.. code-block:: yaml
+
+  pipelines:
+    - name: gate
+      manager: DependentPipelineManager
+
+  projects::
+    - name: acme
+      gate:
+       - acme-tests
+    - name: plugin
+      gate:
+       - acme-tests  # Register job again
+
+Whenever a change enters the ``gate`` pipeline queue, Zuul creates a reference
+for it.  For each subsequent change, an additional reference is created for the
+changes ahead in the queue.  As a result, you will always be able to fetch the
+future state of your project dependencies for each change in the queue.
+
+Based on the pipeline and project definitions above, three changes are
+inserted in the ``gate`` pipeline with the associated references:
+
+  ========  ======= ====== =========
+  Change    Project Branch Zuul Ref.
+  ========  ======= ====== =========
+  Change 1  acme    master master/Z1
+  Change 2  plugin  stable stable/Z2
+  Change 3  plugin  master master/Z3
+  ========  ======= ====== =========
+
+Since the changes enter a DependentPipelineManager pipeline, Zuul creates
+additional references:
+
+  ====== ======= ========= =============================
+  Change Project Zuul Ref. Description
+  ====== ======= ========= =============================
+  1      acme    master/Z1 acme master + change 1
+  ------ ------- --------- -----------------------------
+  2      acme    master/Z2 acme master + change 1
+  2      plugin  stable/Z2 plugin stable + change 2
+  ------ ------- --------- -----------------------------
+  3      acme    master/Z3 acme master + change 1
+  3      plugin  stable/Z3 plugin stable + change 2
+  3      plugin  master/Z3 plugin master + change 3
+  ====== ======= ========= =============================
+
+In order to test change 3, you would clone both repositories and simply
+fetch the Z3 reference for each combination of project/branch you are
+interested in testing. For example, you could fetch ``acme`` with
+master/Z3 and ``plugin`` with master/Z3 and thus have ``acme`` with
+change 1 applied as the expected state for when Change 3 would merge.
+When your job fetches several repositories without changes ahead in the
+queue, they may not have a Z reference in which case you can just check
+out the branch.
+
+
+Cross Repository Dependencies
+-----------------------------
+
+Zuul permits users to specify dependencies across repositories.  Using
+a special header in Git commit messages, Users may specify that a
+change depends on another change in any repository known to Zuul.
+
+Zuul's cross-repository dependencies (CRD) behave like a directed
+acyclic graph (DAG), like git itself, to indicate a one-way dependency
+relationship between changes in different git repositories.  Change A
+may depend on B, but B may not depend on A.
+
+To use them, include ``Depends-On: <gerrit-change-id>`` in the footer of
+a commit message.  Use the full Change-ID ('I' + 40 characters).
+
+
+Dependent Pipeline
+~~~~~~~~~~~~~~~~~~
+
+When Zuul sees CRD changes, it serializes them in the usual manner when
+enqueuing them into a pipeline.  This means that if change A depends on
+B, then when they are added to a dependent pipeline, B will appear first
+and A will follow:
+
+.. blockdiag::
+  :align: center
+
+  blockdiag crd {
+    orientation = portrait
+    span_width = 30
+    class greendot [
+        label = "",
+        shape = circle,
+        color = green,
+        width = 20, height = 20
+    ]
+
+    A_status [ class = greendot ]
+    B_status [ class = greendot ]
+    B_status -- A_status
+
+    'Change B\nChange-Id: Iabc' <- 'Change A\nDepends-On: Iabc'
+  }
+
+If tests for B fail, both B and A will be removed from the pipeline, and
+it will not be possible for A to merge until B does.
+
+
+.. note::
+
+   If changes with CRD do not share a change queue then Zuul is unable
+   to enqueue them together, and the first will be required to merge
+   before the second is enqueued.
+
+Independent Pipeline
+~~~~~~~~~~~~~~~~~~~~
+
+When changes are enqueued into an independent pipeline, all of the
+related dependencies (both normal git-dependencies that come from parent
+commits as well as CRD changes) appear in a dependency graph, as in a
+dependent pipeline. This means that even in an independent pipeline,
+your change will be tested with its dependencies.  So changes that were
+previously unable to be fully tested until a related change landed in a
+different repository may now be tested together from the start.
+
+All of the changes are still independent (so you will note that the
+whole pipeline does not share a graph as in a dependent pipeline), but
+for each change tested, all of its dependencies are visually connected
+to it, and they are used to construct the git references that Zuul uses
+when testing.
+
+When looking at this graph on the status page, you will note that the
+dependencies show up as grey dots, while the actual change tested shows
+up as red or green (depending on the jobs results):
+
+.. blockdiag::
+  :align: center
+
+  blockdiag crdgrey {
+    orientation = portrait
+    span_width = 30
+    class dot [
+        label = "",
+        shape = circle,
+        width = 20, height = 20
+    ]
+
+    A_status [class = "dot", color = green]
+    B_status [class = "dot", color = grey]
+    B_status -- A_status
+
+    "Change B" <- "Change A\nDepends-On: B"
+  }
+
+This is to indicate that the grey changes are only there to establish
+dependencies.  Even if one of the dependencies is also being tested, it
+will show up as a grey dot when used as a dependency, but separately and
+additionally will appear as its own red or green dot for its test.
+
+
+Multiple Changes
+~~~~~~~~~~~~~~~~
+
+A Gerrit change ID may refer to multiple changes (on multiple branches
+of the same project, or even multiple projects).  In these cases, Zuul
+will treat all of the changes with that change ID as dependencies.  So
+if you say that change in project A Depends-On a change ID that has
+changes in two branches of project B, then when testing the change to
+project A, both project B changes will be applied, and when deciding
+whether the project A change can merge, both changes must merge ahead
+of it.
+
+.. blockdiag::
+  :align: center
+
+  blockdiag crdmultirepos {
+    orientation = portrait
+    span_width = 30
+    class greendot [
+        label = "",
+        shape = circle,
+        color = green,
+        width = 20, height = 20
+    ]
+
+    B_stable_status [ class = "greendot" ]
+    B_master_status [ class = "greendot" ]
+    A_status [ class = "greendot" ]
+    B_stable_status -- B_master_status -- A_status
+
+    A [ label = "Repo A\nDepends-On: I123" ]
+    group {
+        orientation = portrait
+        label = "Dependencies"
+        color = "lightgray"
+
+        B_stable [ label = "Repo B\nChange-Id: I123\nBranch: stable" ]
+        B_master [ label = "Repo B\nChange-Id: I123\nBranch: master" ]
+    }
+    B_master <- A
+    B_stable <- A
+
+  }
+
+A change may depend on more than one Gerrit change ID as well.  So it
+is possible for a change in project A to depend on a change in project
+B and a change in project C.  Simply add more ``Depends-On:`` lines to
+the commit message footer.
+
+.. blockdiag::
+  :align: center
+
+  blockdiag crdmultichanges {
+    orientation = portrait
+    span_width = 30
+    class greendot [
+        label = "",
+        shape = circle,
+        color = green,
+        width = 20, height = 20
+    ]
+
+    C_status [ class = "greendot" ]
+    B_status [ class = "greendot" ]
+    A_status [ class = "greendot" ]
+    C_status -- B_status -- A_status
+
+    A [ label = "Repo A\nDepends-On: I123\nDepends-On: Iabc" ]
+    group {
+        orientation = portrait
+        label = "Dependencies"
+        color = "lightgray"
+
+        B [ label = "Repo B\nChange-Id: I123" ]
+        C [ label = "Repo C\nChange-Id: Iabc" ]
+    }
+    B, C <- A
+  }
+
+Cycles
+~~~~~~
+
+If a cycle is created by use of CRD, Zuul will abort its work very
+early.  There will be no message in Gerrit and no changes that are part
+of the cycle will be enqueued into any pipeline.  This is to protect
+Zuul from infinite loops.
diff --git a/doc/source/index.rst b/doc/source/index.rst
index 61f9e4f..3fe6c09 100644
--- a/doc/source/index.rst
+++ b/doc/source/index.rst
@@ -1,33 +1,33 @@
-Zuul - A Project Gating System
-==============================
-
-Zuul is a program that is used to gate the source code repository of a
-project so that changes are only merged if they pass tests.
-
-The main component of Zuul is the scheduler.  It receives events
-related to proposed changes, triggers tests based on those events, and
-reports back.
-
-Contents:
-
-.. toctree::
-   :maxdepth: 2
-
-   gating
-   connections
-   triggers
-   reporters
-   zuul
-   merger
-   cloner
-   launchers
-   statsd
-   client
-
-Indices and tables
-==================
-
-* :ref:`genindex`
-* :ref:`modindex`
-* :ref:`search`
-
+Zuul - A Project Gating System
+==============================
+
+Zuul is a program that is used to gate the source code repository of a
+project so that changes are only merged if they pass tests.
+
+The main component of Zuul is the scheduler.  It receives events
+related to proposed changes, triggers tests based on those events, and
+reports back.
+
+Contents:
+
+.. toctree::
+   :maxdepth: 2
+
+   gating
+   connections
+   triggers
+   reporters
+   zuul
+   merger
+   cloner
+   launchers
+   statsd
+   client
+
+Indices and tables
+==================
+
+* :ref:`genindex`
+* :ref:`modindex`
+* :ref:`search`
+
diff --git a/doc/source/launchers.rst b/doc/source/launchers.rst
index c61cea8..c9e080a 100644
--- a/doc/source/launchers.rst
+++ b/doc/source/launchers.rst
@@ -1,385 +1,385 @@
-:title: Launchers
-
-.. _Gearman: http://gearman.org/
-
-.. _`Gearman Plugin`:
-   https://wiki.jenkins-ci.org/display/JENKINS/Gearman+Plugin
-
-.. _`Turbo-Hipster`:
-   http://git.openstack.org/cgit/stackforge/turbo-hipster/
-
-.. _`Turbo-Hipster Documentation`:
-   http://turbo-hipster.rtfd.org/
-
-.. _FormPost: http://docs.openstack.org/developer/swift/misc.html#module-swift.common.middleware.formpost
-
-.. _launchers:
-
-Launchers
-=========
-
-Zuul has a modular architecture for launching jobs.  Currently, the
-only supported module interfaces with Gearman_.  This design allows
-any system to run jobs for Zuul simply by interfacing with a Gearman
-server.  The recommended way of integrating a new job-runner with Zuul
-is via this method.
-
-If Gearman is unsuitable, Zuul may be extended with a new launcher
-module.  Zuul makes very few assumptions about the interface to a
-launcher -- if it can trigger jobs, cancel them, and receive success
-or failure reports, it should be able to be used with Zuul.  Patches
-to this effect are welcome.
-
-Zuul Parameters
----------------
-
-Zuul will pass some parameters with every job it launches.  These are
-for workers to be able to get the repositories into the state they are
-intended to be tested in.  Builds can be triggered either by an action
-on a change or by a reference update.  Both events share a common set
-of parameters and more specific parameters as follows:
-
-Common parameters
-~~~~~~~~~~~~~~~~~
-
-**ZUUL_UUID**
-  Zuul provided key to link builds with Gerrit events.
-**ZUUL_REF**
-  Zuul provided ref that includes commit(s) to build.
-**ZUUL_COMMIT**
-  The commit SHA1 at the head of ZUUL_REF.
-**ZUUL_PROJECT**
-  The project that triggered this build.
-**ZUUL_PIPELINE**
-  The Zuul pipeline that is building this job.
-**ZUUL_URL**
-  The URL for the zuul server as configured in zuul.conf.
-  A test runner may use this URL as the basis for fetching
-  git commits.
-**BASE_LOG_PATH**
-  zuul suggests a path to store and address logs. This is deterministic
-  and hence useful for where you want workers to upload to a specific
-  destination or need them to have a specific final URL you can link to
-  in advanced. For changes it is:
-  "last-two-digits-of-change/change-number/patchset-number".
-  For reference updates it is: "first-two-digits-of-newrev/newrev"
-**LOG_PATH**
-  zuul also suggests a unique path for logs to the worker. This is
-  "BASE_LOG_PATH/pipeline-name/job-name/uuid"
-**ZUUL_VOTING**
-  Whether Zuul considers this job voting or not.  Note that if Zuul is
-  reconfigured during the run, the voting status of a job may change
-  and this value will be out of date.  Values are '1' if voting, '0'
-  otherwise.
-
-Change related parameters
-~~~~~~~~~~~~~~~~~~~~~~~~~
-
-The following additional parameters will only be provided for builds
-associated with changes (i.e., in response to patchset-created or
-comment-added events):
-
-**ZUUL_BRANCH**
-  The target branch for the change that triggered this build.
-**ZUUL_CHANGE**
-  The Gerrit change ID for the change that triggered this build.
-**ZUUL_CHANGES**
-  A caret character separated list of the changes upon which this build
-  is dependent upon in the form of a colon character separated list
-  consisting of project name, target branch, and revision ref.
-**ZUUL_CHANGE_IDS**
-  All of the Gerrit change IDs that are included in this build (useful
-  when the DependentPipelineManager combines changes for testing).
-**ZUUL_PATCHSET**
-  The Gerrit patchset number for the change that triggered this build.
-
-Reference updated parameters
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-The following additional parameters will only be provided for
-post-merge (ref-updated) builds:
-
-**ZUUL_OLDREV**
-  The SHA1 of the old revision at this ref (recall the ref name is
-  in ZUUL_REF).
-**ZUUL_NEWREV**
-  The SHA1 of the new revision at this ref (recall the ref name is
-  in ZUUL_REF).
-**ZUUL_SHORT_OLDREV**
-  The shortened (7 character) SHA1 of the old revision.
-**ZUUL_SHORT_NEWREV**
-  The shortened (7 character) SHA1 of the new revision.
-
-Unset revisions default to 00000000000000000000000000000000.
-
-Examples:
-
-When a reference is created::
-
-    ZUUL_OLDREV=00000000000000000000000000000000
-    ZUUL_NEWREV=123456789abcdef123456789abcdef12
-    ZUUL_SHORT_OLDREV=0000000
-    ZUUL_SHORT_NEWREV=1234567
-
-When a reference is deleted::
-
-    ZUUL_OLDREV=123456789abcdef123456789abcdef12
-    ZUUL_NEWREV=00000000000000000000000000000000
-    ZUUL_SHORT_OLDREV=1234567
-    ZUUL_SHORT_NEWREV=0000000
-
-And finally a reference being altered::
-
-    ZUUL_OLDREV=123456789abcdef123456789abcdef12
-    ZUUL_NEWREV=abcdef123456789abcdef123456789ab
-    ZUUL_SHORT_OLDREV=1234567
-    ZUUL_SHORT_NEWREV=abcdef1
-
-Your jobs can check whether the parameters are ``000000`` to act
-differently on each kind of event.
-
-Swift parameters
-~~~~~~~~~~~~~~~~
-
-If swift information has been configured for the job zuul will also
-provide signed credentials for the builder to upload results and
-assets into containers using the `FormPost`_ middleware.
-
-Each zuul container/instruction set will contain each of the following
-parameters where $NAME is the ``name`` defined in the layout.
-
-*SWIFT_$NAME_URL*
-  The swift destination URL. This will be the entire URL including
-  the AUTH, container and path prefix (folder).
-*SWIFT_$NAME_HMAC_BODY*
-  The information signed in the HMAC body. The body is as follows::
-
-    PATH TO OBJECT PREFIX (excluding domain)
-    BLANK LINE (zuul implements no form redirect)
-    MAX FILE SIZE
-    MAX FILE COUNT
-    SIGNATURE EXPIRY
-
-*SWIFT_$NAME_SIGNATURE*
-  The HMAC body signed with the configured key.
-*SWIFT_$NAME_LOGSERVER_PREFIX*
-  The URL to prepend to the object path when returning the results
-  from a build.
-
-Gearman
--------
-
-Gearman_ is a general-purpose protocol for distributing jobs to any
-number of workers.  Zuul works with Gearman by sending specific
-information with job requests to Gearman, and expects certain
-information to be returned on completion.  This protocol is described
-in `Zuul-Gearman Protocol`_.
-
-In order for Zuul to run any jobs, you will need a running Gearman
-server.  Zuul includes a Gearman server, and it is recommended that it
-be used as it supports the following features needed by Zuul:
-
-* Canceling jobs in the queue (admin protocol command "cancel job").
-* Strict FIFO queue operation (gearmand's round-robin mode may be
-  sufficient, but is untested).
-
-To enable the built-in server, see the ``gearman_server`` section of
-``zuul.conf``.  Be sure that the host allows connections from Zuul and
-any workers (e.g., Jenkins masters) on TCP port 4730, and nowhere else
-(as the Gearman protocol does not include any provision for
-authentication).
-
-Gearman Jenkins Plugin
-~~~~~~~~~~~~~~~~~~~~~~
-
-The `Gearman Jenkins Plugin`_ makes it easy to use Jenkins with Zuul
-by providing an interface between Jenkins and Gearman.  In this
-configuration, Zuul asks Gearman to run jobs, and Gearman can then
-distribute those jobs to any number of Jenkins systems (including
-multiple Jenkins masters).
-
-The `Gearman Plugin`_ can be installed in Jenkins in order to
-facilitate Jenkins running jobs for Zuul.  Install the plugin and
-configure it with the hostname or IP address of your Gearman server
-and the port on which it is listening (4730 by default).  It will
-automatically register all known Jenkins jobs as functions that Zuul
-can invoke via Gearman.
-
-Any number of masters can be configured in this way, and Gearman will
-distribute jobs to all of them as appropriate.
-
-No special Jenkins job configuration is needed to support triggering
-by Zuul.
-
-The Gearman Plugin will ensure the `Zuul Parameters`_ are supplied as
-Jenkins build parameters, so they will be available for use in the job
-configuration as well as to the running job as environment variables.
-
-Jenkins git plugin configuration
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-In order to test the correct build, configure the Jenkins Git SCM
-plugin as follows::
-
-  Source Code Management:
-    Git
-      Repositories:
-        Repository URL:  <your Gerrit or Zuul repository URL>
-          Advanced:
-            Refspec: ${ZUUL_REF}
-      Branches to build:
-        Branch Specifier: ${ZUUL_COMMIT}
-            Advanced:
-            Clean after checkout: True
-
-That should be sufficient for a job that only builds a single project.
-If you have multiple interrelated projects (i.e., they share a Zuul
-Change Queue) that are built together, you may be able to configure
-the Git plugin to prepare them, or you may chose to use a shell script
-instead.  As an example, the OpenStack project uses the following
-script to prepare the workspace for its integration testing:
-
-  https://git.openstack.org/cgit/openstack-infra/devstack-gate/tree/devstack-vm-gate-wrap.sh
-
-Turbo Hipster Worker
-~~~~~~~~~~~~~~~~~~~~
-
-As an alternative to Jenkins, `Turbo-Hipster`_ is a small python
-project designed specifically as a zuul job worker which can be
-registered with gearman as a job runner. Please see the
-`Turbo-Hipster Documentation`_ for details on how to set it up.
-
-Zuul-Gearman Protocol
-~~~~~~~~~~~~~~~~~~~~~
-
-This section is only relevant if you intend to implement a new kind of
-worker that runs jobs for Zuul via Gearman.  If you just want to use
-Jenkins, see `Gearman Jenkins Plugin`_ instead.
-
-The Zuul protocol as used with Gearman is as follows:
-
-Starting Builds
-^^^^^^^^^^^^^^^
-
-To start a build, Zuul invokes a Gearman function with the following
-format:
-
-  build:FUNCTION_NAME
-
-where **FUNCTION_NAME** is the name of the job that should be run.  If
-the job should run on a specific node (or class of node), Zuul will
-instead invoke:
-
-  build:FUNCTION_NAME:NODE_NAME
-
-where **NODE_NAME** is the name or class of node on which the job
-should be run.  This can be specified by setting the ZUUL_NODE
-parameter in a parameter-function (see :ref:`includes` section in
-:ref:`zuulconf`).
-
-Zuul sends the ZUUL_* parameters described in `Zuul Parameters`_
-encoded in JSON format as the argument included with the
-SUBMIT_JOB_UNIQ request to Gearman.  A unique ID (equal to the
-ZUUL_UUID parameter) is also supplied to Gearman, and is accessible as
-an added Gearman parameter with GRAB_JOB_UNIQ.
-
-When a Gearman worker starts running a job for Zuul, it should
-immediately send a WORK_DATA packet with the following information
-encoded in JSON format:
-
-**name**
-  The name of the job.
-
-**number**
-  The build number (unique to this job).
-
-**manager**
-  A unique identifier associated with the Gearman worker that can
-  abort this build.  See `Stopping Builds`_ for more information.
-
-**url** (optional)
-  The URL with the status or results of the build.  Will be used in
-  the status page and the final report.
-
-To help with debugging builds a worker may send back some optional
-metadata:
-
-**worker_name** (optional)
-  The name of the worker.
-
-**worker_hostname** (optional)
-  The hostname of the worker.
-
-**worker_ips** (optional)
-  A list of IPs for the worker.
-
-**worker_fqdn** (optional)
-  The FQDN of the worker.
-
-**worker_program** (optional)
-  The program name of the worker. For example Jenkins or turbo-hipster.
-
-**worker_version** (optional)
-  The version of the software running the job.
-
-**worker_extra** (optional)
-  A dictionary of any extra metadata you may want to pass along.
-
-It should then immediately send a WORK_STATUS packet with a value of 0
-percent complete.  It may then optionally send subsequent WORK_STATUS
-packets with updated completion values.
-
-When the build is complete, it should send a final WORK_DATA packet
-with the following in JSON format:
-
-**result**
-  Either the string 'SUCCESS' if the job succeeded, or any other value
-  that describes the result if the job failed.
-
-Finally, it should send either a WORK_FAIL or WORK_COMPLETE packet as
-appropriate.  A WORK_EXCEPTION packet will be interpreted as a
-WORK_FAIL, but the exception will be logged in Zuul's error log.
-
-Stopping Builds
-^^^^^^^^^^^^^^^
-
-If Zuul needs to abort a build already in progress, it will invoke the
-following function through Gearman:
-
-  stop:MANAGER_NAME
-
-Where **MANAGER_NAME** is the name of the manager worker supplied in
-the initial WORK_DATA packet when the job started.  This is used to
-direct the stop: function invocation to the correct Gearman worker
-that is capable of stopping that particular job.  The argument to the
-function should be the following encoded in JSON format:
-
-**name**
-  The job name of the build to stop.
-
-**number**
-  The build number of the build to stop.
-
-The original job is expected to complete with a WORK_DATA and
-WORK_FAIL packet as described in `Starting Builds`_.
-
-Build Descriptions
-^^^^^^^^^^^^^^^^^^
-
-In order to update the job running system with a description of the
-current state of all related builds, the job runner may optionally
-implement the following Gearman function:
-
-  set_description:MANAGER_NAME
-
-Where **MANAGER_NAME** is used as described in `Stopping Builds`_.
-The argument to the function is the following encoded in JSON format:
-
-**name**
-  The job name of the build to describe.
-
-**number**
-  The build number of the build to describe.
-
-**html_description**
-  The description of the build in HTML format.
+:title: Launchers
+
+.. _Gearman: http://gearman.org/
+
+.. _`Gearman Plugin`:
+   https://wiki.jenkins-ci.org/display/JENKINS/Gearman+Plugin
+
+.. _`Turbo-Hipster`:
+   http://git.openstack.org/cgit/stackforge/turbo-hipster/
+
+.. _`Turbo-Hipster Documentation`:
+   http://turbo-hipster.rtfd.org/
+
+.. _FormPost: http://docs.openstack.org/developer/swift/misc.html#module-swift.common.middleware.formpost
+
+.. _launchers:
+
+Launchers
+=========
+
+Zuul has a modular architecture for launching jobs.  Currently, the
+only supported module interfaces with Gearman_.  This design allows
+any system to run jobs for Zuul simply by interfacing with a Gearman
+server.  The recommended way of integrating a new job-runner with Zuul
+is via this method.
+
+If Gearman is unsuitable, Zuul may be extended with a new launcher
+module.  Zuul makes very few assumptions about the interface to a
+launcher -- if it can trigger jobs, cancel them, and receive success
+or failure reports, it should be able to be used with Zuul.  Patches
+to this effect are welcome.
+
+Zuul Parameters
+---------------
+
+Zuul will pass some parameters with every job it launches.  These are
+for workers to be able to get the repositories into the state they are
+intended to be tested in.  Builds can be triggered either by an action
+on a change or by a reference update.  Both events share a common set
+of parameters and more specific parameters as follows:
+
+Common parameters
+~~~~~~~~~~~~~~~~~
+
+**ZUUL_UUID**
+  Zuul provided key to link builds with Gerrit events.
+**ZUUL_REF**
+  Zuul provided ref that includes commit(s) to build.
+**ZUUL_COMMIT**
+  The commit SHA1 at the head of ZUUL_REF.
+**ZUUL_PROJECT**
+  The project that triggered this build.
+**ZUUL_PIPELINE**
+  The Zuul pipeline that is building this job.
+**ZUUL_URL**
+  The URL for the zuul server as configured in zuul.conf.
+  A test runner may use this URL as the basis for fetching
+  git commits.
+**BASE_LOG_PATH**
+  zuul suggests a path to store and address logs. This is deterministic
+  and hence useful for where you want workers to upload to a specific
+  destination or need them to have a specific final URL you can link to
+  in advanced. For changes it is:
+  "last-two-digits-of-change/change-number/patchset-number".
+  For reference updates it is: "first-two-digits-of-newrev/newrev"
+**LOG_PATH**
+  zuul also suggests a unique path for logs to the worker. This is
+  "BASE_LOG_PATH/pipeline-name/job-name/uuid"
+**ZUUL_VOTING**
+  Whether Zuul considers this job voting or not.  Note that if Zuul is
+  reconfigured during the run, the voting status of a job may change
+  and this value will be out of date.  Values are '1' if voting, '0'
+  otherwise.
+
+Change related parameters
+~~~~~~~~~~~~~~~~~~~~~~~~~
+
+The following additional parameters will only be provided for builds
+associated with changes (i.e., in response to patchset-created or
+comment-added events):
+
+**ZUUL_BRANCH**
+  The target branch for the change that triggered this build.
+**ZUUL_CHANGE**
+  The Gerrit change ID for the change that triggered this build.
+**ZUUL_CHANGES**
+  A caret character separated list of the changes upon which this build
+  is dependent upon in the form of a colon character separated list
+  consisting of project name, target branch, and revision ref.
+**ZUUL_CHANGE_IDS**
+  All of the Gerrit change IDs that are included in this build (useful
+  when the DependentPipelineManager combines changes for testing).
+**ZUUL_PATCHSET**
+  The Gerrit patchset number for the change that triggered this build.
+
+Reference updated parameters
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+The following additional parameters will only be provided for
+post-merge (ref-updated) builds:
+
+**ZUUL_OLDREV**
+  The SHA1 of the old revision at this ref (recall the ref name is
+  in ZUUL_REF).
+**ZUUL_NEWREV**
+  The SHA1 of the new revision at this ref (recall the ref name is
+  in ZUUL_REF).
+**ZUUL_SHORT_OLDREV**
+  The shortened (7 character) SHA1 of the old revision.
+**ZUUL_SHORT_NEWREV**
+  The shortened (7 character) SHA1 of the new revision.
+
+Unset revisions default to 00000000000000000000000000000000.
+
+Examples:
+
+When a reference is created::
+
+    ZUUL_OLDREV=00000000000000000000000000000000
+    ZUUL_NEWREV=123456789abcdef123456789abcdef12
+    ZUUL_SHORT_OLDREV=0000000
+    ZUUL_SHORT_NEWREV=1234567
+
+When a reference is deleted::
+
+    ZUUL_OLDREV=123456789abcdef123456789abcdef12
+    ZUUL_NEWREV=00000000000000000000000000000000
+    ZUUL_SHORT_OLDREV=1234567
+    ZUUL_SHORT_NEWREV=0000000
+
+And finally a reference being altered::
+
+    ZUUL_OLDREV=123456789abcdef123456789abcdef12
+    ZUUL_NEWREV=abcdef123456789abcdef123456789ab
+    ZUUL_SHORT_OLDREV=1234567
+    ZUUL_SHORT_NEWREV=abcdef1
+
+Your jobs can check whether the parameters are ``000000`` to act
+differently on each kind of event.
+
+Swift parameters
+~~~~~~~~~~~~~~~~
+
+If swift information has been configured for the job zuul will also
+provide signed credentials for the builder to upload results and
+assets into containers using the `FormPost`_ middleware.
+
+Each zuul container/instruction set will contain each of the following
+parameters where $NAME is the ``name`` defined in the layout.
+
+*SWIFT_$NAME_URL*
+  The swift destination URL. This will be the entire URL including
+  the AUTH, container and path prefix (folder).
+*SWIFT_$NAME_HMAC_BODY*
+  The information signed in the HMAC body. The body is as follows::
+
+    PATH TO OBJECT PREFIX (excluding domain)
+    BLANK LINE (zuul implements no form redirect)
+    MAX FILE SIZE
+    MAX FILE COUNT
+    SIGNATURE EXPIRY
+
+*SWIFT_$NAME_SIGNATURE*
+  The HMAC body signed with the configured key.
+*SWIFT_$NAME_LOGSERVER_PREFIX*
+  The URL to prepend to the object path when returning the results
+  from a build.
+
+Gearman
+-------
+
+Gearman_ is a general-purpose protocol for distributing jobs to any
+number of workers.  Zuul works with Gearman by sending specific
+information with job requests to Gearman, and expects certain
+information to be returned on completion.  This protocol is described
+in `Zuul-Gearman Protocol`_.
+
+In order for Zuul to run any jobs, you will need a running Gearman
+server.  Zuul includes a Gearman server, and it is recommended that it
+be used as it supports the following features needed by Zuul:
+
+* Canceling jobs in the queue (admin protocol command "cancel job").
+* Strict FIFO queue operation (gearmand's round-robin mode may be
+  sufficient, but is untested).
+
+To enable the built-in server, see the ``gearman_server`` section of
+``zuul.conf``.  Be sure that the host allows connections from Zuul and
+any workers (e.g., Jenkins masters) on TCP port 4730, and nowhere else
+(as the Gearman protocol does not include any provision for
+authentication).
+
+Gearman Jenkins Plugin
+~~~~~~~~~~~~~~~~~~~~~~
+
+The `Gearman Jenkins Plugin`_ makes it easy to use Jenkins with Zuul
+by providing an interface between Jenkins and Gearman.  In this
+configuration, Zuul asks Gearman to run jobs, and Gearman can then
+distribute those jobs to any number of Jenkins systems (including
+multiple Jenkins masters).
+
+The `Gearman Plugin`_ can be installed in Jenkins in order to
+facilitate Jenkins running jobs for Zuul.  Install the plugin and
+configure it with the hostname or IP address of your Gearman server
+and the port on which it is listening (4730 by default).  It will
+automatically register all known Jenkins jobs as functions that Zuul
+can invoke via Gearman.
+
+Any number of masters can be configured in this way, and Gearman will
+distribute jobs to all of them as appropriate.
+
+No special Jenkins job configuration is needed to support triggering
+by Zuul.
+
+The Gearman Plugin will ensure the `Zuul Parameters`_ are supplied as
+Jenkins build parameters, so they will be available for use in the job
+configuration as well as to the running job as environment variables.
+
+Jenkins git plugin configuration
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+In order to test the correct build, configure the Jenkins Git SCM
+plugin as follows::
+
+  Source Code Management:
+    Git
+      Repositories:
+        Repository URL:  <your Gerrit or Zuul repository URL>
+          Advanced:
+            Refspec: ${ZUUL_REF}
+      Branches to build:
+        Branch Specifier: ${ZUUL_COMMIT}
+            Advanced:
+            Clean after checkout: True
+
+That should be sufficient for a job that only builds a single project.
+If you have multiple interrelated projects (i.e., they share a Zuul
+Change Queue) that are built together, you may be able to configure
+the Git plugin to prepare them, or you may chose to use a shell script
+instead.  As an example, the OpenStack project uses the following
+script to prepare the workspace for its integration testing:
+
+  https://git.openstack.org/cgit/openstack-infra/devstack-gate/tree/devstack-vm-gate-wrap.sh
+
+Turbo Hipster Worker
+~~~~~~~~~~~~~~~~~~~~
+
+As an alternative to Jenkins, `Turbo-Hipster`_ is a small python
+project designed specifically as a zuul job worker which can be
+registered with gearman as a job runner. Please see the
+`Turbo-Hipster Documentation`_ for details on how to set it up.
+
+Zuul-Gearman Protocol
+~~~~~~~~~~~~~~~~~~~~~
+
+This section is only relevant if you intend to implement a new kind of
+worker that runs jobs for Zuul via Gearman.  If you just want to use
+Jenkins, see `Gearman Jenkins Plugin`_ instead.
+
+The Zuul protocol as used with Gearman is as follows:
+
+Starting Builds
+^^^^^^^^^^^^^^^
+
+To start a build, Zuul invokes a Gearman function with the following
+format:
+
+  build:FUNCTION_NAME
+
+where **FUNCTION_NAME** is the name of the job that should be run.  If
+the job should run on a specific node (or class of node), Zuul will
+instead invoke:
+
+  build:FUNCTION_NAME:NODE_NAME
+
+where **NODE_NAME** is the name or class of node on which the job
+should be run.  This can be specified by setting the ZUUL_NODE
+parameter in a parameter-function (see :ref:`includes` section in
+:ref:`zuulconf`).
+
+Zuul sends the ZUUL_* parameters described in `Zuul Parameters`_
+encoded in JSON format as the argument included with the
+SUBMIT_JOB_UNIQ request to Gearman.  A unique ID (equal to the
+ZUUL_UUID parameter) is also supplied to Gearman, and is accessible as
+an added Gearman parameter with GRAB_JOB_UNIQ.
+
+When a Gearman worker starts running a job for Zuul, it should
+immediately send a WORK_DATA packet with the following information
+encoded in JSON format:
+
+**name**
+  The name of the job.
+
+**number**
+  The build number (unique to this job).
+
+**manager**
+  A unique identifier associated with the Gearman worker that can
+  abort this build.  See `Stopping Builds`_ for more information.
+
+**url** (optional)
+  The URL with the status or results of the build.  Will be used in
+  the status page and the final report.
+
+To help with debugging builds a worker may send back some optional
+metadata:
+
+**worker_name** (optional)
+  The name of the worker.
+
+**worker_hostname** (optional)
+  The hostname of the worker.
+
+**worker_ips** (optional)
+  A list of IPs for the worker.
+
+**worker_fqdn** (optional)
+  The FQDN of the worker.
+
+**worker_program** (optional)
+  The program name of the worker. For example Jenkins or turbo-hipster.
+
+**worker_version** (optional)
+  The version of the software running the job.
+
+**worker_extra** (optional)
+  A dictionary of any extra metadata you may want to pass along.
+
+It should then immediately send a WORK_STATUS packet with a value of 0
+percent complete.  It may then optionally send subsequent WORK_STATUS
+packets with updated completion values.
+
+When the build is complete, it should send a final WORK_DATA packet
+with the following in JSON format:
+
+**result**
+  Either the string 'SUCCESS' if the job succeeded, or any other value
+  that describes the result if the job failed.
+
+Finally, it should send either a WORK_FAIL or WORK_COMPLETE packet as
+appropriate.  A WORK_EXCEPTION packet will be interpreted as a
+WORK_FAIL, but the exception will be logged in Zuul's error log.
+
+Stopping Builds
+^^^^^^^^^^^^^^^
+
+If Zuul needs to abort a build already in progress, it will invoke the
+following function through Gearman:
+
+  stop:MANAGER_NAME
+
+Where **MANAGER_NAME** is the name of the manager worker supplied in
+the initial WORK_DATA packet when the job started.  This is used to
+direct the stop: function invocation to the correct Gearman worker
+that is capable of stopping that particular job.  The argument to the
+function should be the following encoded in JSON format:
+
+**name**
+  The job name of the build to stop.
+
+**number**
+  The build number of the build to stop.
+
+The original job is expected to complete with a WORK_DATA and
+WORK_FAIL packet as described in `Starting Builds`_.
+
+Build Descriptions
+^^^^^^^^^^^^^^^^^^
+
+In order to update the job running system with a description of the
+current state of all related builds, the job runner may optionally
+implement the following Gearman function:
+
+  set_description:MANAGER_NAME
+
+Where **MANAGER_NAME** is used as described in `Stopping Builds`_.
+The argument to the function is the following encoded in JSON format:
+
+**name**
+  The job name of the build to describe.
+
+**number**
+  The build number of the build to describe.
+
+**html_description**
+  The description of the build in HTML format.
diff --git a/doc/source/merger.rst b/doc/source/merger.rst
index e01bc8c..bdb1c3e 100644
--- a/doc/source/merger.rst
+++ b/doc/source/merger.rst
@@ -1,60 +1,60 @@
-:title: Merger
-
-Merger
-======
-
-The Zuul Merger is a separate component which communicates with the
-main Zuul server via Gearman.  Its purpose is to speculatively merge
-the changes for Zuul in preparation for testing.  The resulting git
-commits also must be served to the test workers, and the server(s)
-running the Zuul Merger are expected to do this as well.  Because both
-of these tasks are resource intensive, any number of Zuul Mergers can
-be run in parallel on distinct hosts.
-
-Configuration
-~~~~~~~~~~~~~
-
-The Zuul Merger can read the same zuul.conf file as the main Zuul
-server and requires the ``gearman``, ``gerrit``, ``merger``, and
-``zuul`` sections (indicated fields only).  Be sure the zuul_url is
-set appropriately on each host that runs a zuul-merger.
-
-Zuul References
-~~~~~~~~~~~~~~~
-
-As the DependentPipelineManager may combine several changes together
-for testing when performing speculative execution, determining exactly
-how the workspace should be set up when running a Job can be complex.
-To alleviate this problem, Zuul performs merges itself, merging or
-cherry-picking changes as required and identifies the result with a
-Git reference of the form ``refs/zuul/<branch>/Z<random sha1>``.
-Preparing the workspace is then a simple matter of fetching that ref
-and checking it out.  The parameters that provide this information are
-described in :ref:`launchers`.
-
-These references need to be made available via a Git repository that
-is available to workers (such as Jenkins).  This is accomplished by
-serving Zuul's Git repositories directly.
-
-Serving Zuul Git Repos
-~~~~~~~~~~~~~~~~~~~~~~
-
-Zuul maintains its own copies of any needed Git repositories in the
-directory specified by ``git_dir`` in the ``merger`` section of
-zuul.conf (by default, /var/lib/zuul/git).  To directly serve Zuul's
-Git repositories in order to provide Zuul refs for workers, you can
-configure Apache to do so using the following directives::
-
-  SetEnv GIT_PROJECT_ROOT /var/lib/zuul/git
-  SetEnv GIT_HTTP_EXPORT_ALL
-
-  AliasMatch ^/p/(.*/objects/[0-9a-f]{2}/[0-9a-f]{38})$ /var/lib/zuul/git/$1
-  AliasMatch ^/p/(.*/objects/pack/pack-[0-9a-f]{40}.(pack|idx))$ /var/lib/zuul/git/$1
-  ScriptAlias /p/ /usr/lib/git-core/git-http-backend/
-
-Note that Zuul's Git repositories are not bare, which means they have
-a working tree, and are not suitable for public consumption (for
-instance, a clone will produce a repository in an unpredictable state
-depending on what the state of Zuul's repository is when the clone
-happens).  They are, however, suitable for automated systems that
-respond to Zuul triggers.
+:title: Merger
+
+Merger
+======
+
+The Zuul Merger is a separate component which communicates with the
+main Zuul server via Gearman.  Its purpose is to speculatively merge
+the changes for Zuul in preparation for testing.  The resulting git
+commits also must be served to the test workers, and the server(s)
+running the Zuul Merger are expected to do this as well.  Because both
+of these tasks are resource intensive, any number of Zuul Mergers can
+be run in parallel on distinct hosts.
+
+Configuration
+~~~~~~~~~~~~~
+
+The Zuul Merger can read the same zuul.conf file as the main Zuul
+server and requires the ``gearman``, ``gerrit``, ``merger``, and
+``zuul`` sections (indicated fields only).  Be sure the zuul_url is
+set appropriately on each host that runs a zuul-merger.
+
+Zuul References
+~~~~~~~~~~~~~~~
+
+As the DependentPipelineManager may combine several changes together
+for testing when performing speculative execution, determining exactly
+how the workspace should be set up when running a Job can be complex.
+To alleviate this problem, Zuul performs merges itself, merging or
+cherry-picking changes as required and identifies the result with a
+Git reference of the form ``refs/zuul/<branch>/Z<random sha1>``.
+Preparing the workspace is then a simple matter of fetching that ref
+and checking it out.  The parameters that provide this information are
+described in :ref:`launchers`.
+
+These references need to be made available via a Git repository that
+is available to workers (such as Jenkins).  This is accomplished by
+serving Zuul's Git repositories directly.
+
+Serving Zuul Git Repos
+~~~~~~~~~~~~~~~~~~~~~~
+
+Zuul maintains its own copies of any needed Git repositories in the
+directory specified by ``git_dir`` in the ``merger`` section of
+zuul.conf (by default, /var/lib/zuul/git).  To directly serve Zuul's
+Git repositories in order to provide Zuul refs for workers, you can
+configure Apache to do so using the following directives::
+
+  SetEnv GIT_PROJECT_ROOT /var/lib/zuul/git
+  SetEnv GIT_HTTP_EXPORT_ALL
+
+  AliasMatch ^/p/(.*/objects/[0-9a-f]{2}/[0-9a-f]{38})$ /var/lib/zuul/git/$1
+  AliasMatch ^/p/(.*/objects/pack/pack-[0-9a-f]{40}.(pack|idx))$ /var/lib/zuul/git/$1
+  ScriptAlias /p/ /usr/lib/git-core/git-http-backend/
+
+Note that Zuul's Git repositories are not bare, which means they have
+a working tree, and are not suitable for public consumption (for
+instance, a clone will produce a repository in an unpredictable state
+depending on what the state of Zuul's repository is when the clone
+happens).  They are, however, suitable for automated systems that
+respond to Zuul triggers.
diff --git a/doc/source/reporters.rst b/doc/source/reporters.rst
index 97bed4a..ca842d7 100644
--- a/doc/source/reporters.rst
+++ b/doc/source/reporters.rst
@@ -1,62 +1,62 @@
-:title: Reporters
-
-Reporters
-=========
-
-Zuul can communicate results and progress back to configurable
-protocols. For example, after succeeding in a build a pipeline can be
-configured to post a positive review back to Gerrit.
-
-There are three stages when a report can be handled. That is on:
-Start, Success or Failure. Each stage can have multiple reports.
-For example, you can set verified on Gerrit and send an email.
-
-Gerrit
-------
-
-Zuul works with standard versions of Gerrit by invoking the
-``gerrit`` command over an SSH connection.  It reports back to
-Gerrit using SSH.
-
-The dictionary passed to the Gerrit reporter is used for ``gerrit
-review`` arguments, with the boolean value of ``true`` simply
-indicating that the argument should be present without following it
-with a value. For example, ``verified: 1`` becomes ``gerrit review
---verified 1`` and ``submit: true`` becomes ``gerrit review
---submit``.
-
-A :ref:`connection` that uses the gerrit driver must be supplied to the
-trigger.
-
-SMTP
-----
-
-A simple email reporter is also available.
-
-A :ref:`connection` that uses the smtp driver must be supplied to the
-trigger.
-
-SMTP Configuration
-~~~~~~~~~~~~~~~~~~
-
-zuul.conf contains the SMTP server and default to/from as described
-in :ref:`zuulconf`.
-
-Each pipeline can overwrite the ``subject`` or the ``to`` or ``from`` address by
-providing alternatives as arguments to the reporter. For example, ::
-
-  pipelines:
-    - name: post-merge
-      manager: IndependentPipelineManager
-      source: my_gerrit
-      trigger:
-        my_gerrit:
-          - event: change-merged
-      success:
-        outgoing_smtp:
-          to: you@example.com
-      failure:
-        internal_smtp:
-          to: you@example.com
-          from: alternative@example.com
-          subject: Change {change} failed
+:title: Reporters
+
+Reporters
+=========
+
+Zuul can communicate results and progress back to configurable
+protocols. For example, after succeeding in a build a pipeline can be
+configured to post a positive review back to Gerrit.
+
+There are three stages when a report can be handled. That is on:
+Start, Success or Failure. Each stage can have multiple reports.
+For example, you can set verified on Gerrit and send an email.
+
+Gerrit
+------
+
+Zuul works with standard versions of Gerrit by invoking the
+``gerrit`` command over an SSH connection.  It reports back to
+Gerrit using SSH.
+
+The dictionary passed to the Gerrit reporter is used for ``gerrit
+review`` arguments, with the boolean value of ``true`` simply
+indicating that the argument should be present without following it
+with a value. For example, ``verified: 1`` becomes ``gerrit review
+--verified 1`` and ``submit: true`` becomes ``gerrit review
+--submit``.
+
+A :ref:`connection` that uses the gerrit driver must be supplied to the
+trigger.
+
+SMTP
+----
+
+A simple email reporter is also available.
+
+A :ref:`connection` that uses the smtp driver must be supplied to the
+trigger.
+
+SMTP Configuration
+~~~~~~~~~~~~~~~~~~
+
+zuul.conf contains the SMTP server and default to/from as described
+in :ref:`zuulconf`.
+
+Each pipeline can overwrite the ``subject`` or the ``to`` or ``from`` address by
+providing alternatives as arguments to the reporter. For example, ::
+
+  pipelines:
+    - name: post-merge
+      manager: IndependentPipelineManager
+      source: my_gerrit
+      trigger:
+        my_gerrit:
+          - event: change-merged
+      success:
+        outgoing_smtp:
+          to: you@example.com
+      failure:
+        internal_smtp:
+          to: you@example.com
+          from: alternative@example.com
+          subject: Change {change} failed
diff --git a/doc/source/statsd.rst b/doc/source/statsd.rst
index f789d61..8c75d6e 100644
--- a/doc/source/statsd.rst
+++ b/doc/source/statsd.rst
@@ -1,89 +1,89 @@
-:title: Statsd reporting
-
-Statsd reporting
-================
-
-Zuul comes with support for the statsd protocol, when enabled and configured
-(see below), the Zuul scheduler will emit raw metrics to a statsd receiver
-which let you in turn generate nice graphics. An example is OpenStack Zuul
-status page: http://status.openstack.org/zuul/
-
-Configuration
--------------
-
-Statsd support uses the statsd python module. Note that Zuul will start without
-the statsd python module, so an existing Zuul installation may be missing it.
-
-The configuration is done via environment variables STATSD_HOST and
-STATSD_PORT. They are interpreted by the statsd module directly and there is no
-such parameter in zuul.conf yet. Your init script will have to initialize both
-of them before launching Zuul.
-
-Your init script most probably loads a configuration file named
-``/etc/default/zuul`` which would contain the environment variables::
-
-  $ cat /etc/default/zuul
-  STATSD_HOST=10.0.0.1
-  STATSD_PORT=8125
-
-Metrics
--------
-
-The metrics are emitted by the Zuul scheduler (`zuul/scheduler.py`):
-
-**gerrit.events.<type> (counters)**
-  Gerrit emits different kind of message over its `stream-events` interface. As
-  a convenience, Zuul emits metrics to statsd which save you from having to use
-  a different daemon to measure Gerrit events.
-  The Gerrit events have different types defined by Gerrit itself, Zuul will
-  relay any type of event reusing the name defined by Gerrit. Some of the
-  events emitted are:
-
-    * patchset-created
-    * draft-published
-    * change-abandonned
-    * change-restored
-    * change-merged
-    * merge-failed
-    * comment-added
-    * ref-updated
-    * reviewer-added
-
-  Refer to your Gerrit installation documentation for an exhaustive list of
-  Gerrit event types.
-
-**zuul.pipeline.**
-  Holds metrics specific to jobs. The hierarchy is:
-
-    #. **<pipeline name>** as defined in your `layout.yaml` file (ex: `gate`,
-                         `test`, `publish`). It contains:
-
-      #. **all_jobs** counter of jobs triggered by the pipeline.
-      #. **current_changes** A gauge for the number of Gerrit changes being
-               processed by this pipeline.
-      #. **job** subtree detailing per jobs statistics:
-
-        #. **<jobname>** The triggered job name.
-        #. **<build result>** Result as defined in your triggering system. For
-                 Jenkins that would be SUCCESS, FAILURE, UNSTABLE, LOST.  The
-                 metrics holds both an increasing counter and a timing
-                 reporting the duration of the build. Whenever the result is a
-                 SUCCESS or FAILURE, Zuul will additionally report the duration
-                 of the build as a timing event.
-
-      #. **resident_time** timing representing how long the Change has been
-               known by Zuul (which includes build time and Zuul overhead).
-      #. **total_changes** counter of the number of change proceeding since
-               Zuul started.
-
-  Additionally, the `zuul.pipeline.<pipeline name>` hierarchy contains
-  `current_changes` and `resident_time` metrics for each projects. The slash
-  separator used in Gerrit name being replaced by dots.
-
-  As an example, given a job named `myjob` triggered by the `gate` pipeline
-  which took 40 seconds to build, the Zuul scheduler will emit the following
-  statsd events:
-
-    * `zuul.pipeline.gate.job.myjob.SUCCESS` +1
-    * `zuul.pipeline.gate.job.myjob`  40 seconds
-    * `zuul.pipeline.gate.all_jobs` +1
+:title: Statsd reporting
+
+Statsd reporting
+================
+
+Zuul comes with support for the statsd protocol, when enabled and configured
+(see below), the Zuul scheduler will emit raw metrics to a statsd receiver
+which let you in turn generate nice graphics. An example is OpenStack Zuul
+status page: http://status.openstack.org/zuul/
+
+Configuration
+-------------
+
+Statsd support uses the statsd python module. Note that Zuul will start without
+the statsd python module, so an existing Zuul installation may be missing it.
+
+The configuration is done via environment variables STATSD_HOST and
+STATSD_PORT. They are interpreted by the statsd module directly and there is no
+such parameter in zuul.conf yet. Your init script will have to initialize both
+of them before launching Zuul.
+
+Your init script most probably loads a configuration file named
+``/etc/default/zuul`` which would contain the environment variables::
+
+  $ cat /etc/default/zuul
+  STATSD_HOST=10.0.0.1
+  STATSD_PORT=8125
+
+Metrics
+-------
+
+The metrics are emitted by the Zuul scheduler (`zuul/scheduler.py`):
+
+**gerrit.events.<type> (counters)**
+  Gerrit emits different kind of message over its `stream-events` interface. As
+  a convenience, Zuul emits metrics to statsd which save you from having to use
+  a different daemon to measure Gerrit events.
+  The Gerrit events have different types defined by Gerrit itself, Zuul will
+  relay any type of event reusing the name defined by Gerrit. Some of the
+  events emitted are:
+
+    * patchset-created
+    * draft-published
+    * change-abandonned
+    * change-restored
+    * change-merged
+    * merge-failed
+    * comment-added
+    * ref-updated
+    * reviewer-added
+
+  Refer to your Gerrit installation documentation for an exhaustive list of
+  Gerrit event types.
+
+**zuul.pipeline.**
+  Holds metrics specific to jobs. The hierarchy is:
+
+    #. **<pipeline name>** as defined in your `layout.yaml` file (ex: `gate`,
+                         `test`, `publish`). It contains:
+
+      #. **all_jobs** counter of jobs triggered by the pipeline.
+      #. **current_changes** A gauge for the number of Gerrit changes being
+               processed by this pipeline.
+      #. **job** subtree detailing per jobs statistics:
+
+        #. **<jobname>** The triggered job name.
+        #. **<build result>** Result as defined in your triggering system. For
+                 Jenkins that would be SUCCESS, FAILURE, UNSTABLE, LOST.  The
+                 metrics holds both an increasing counter and a timing
+                 reporting the duration of the build. Whenever the result is a
+                 SUCCESS or FAILURE, Zuul will additionally report the duration
+                 of the build as a timing event.
+
+      #. **resident_time** timing representing how long the Change has been
+               known by Zuul (which includes build time and Zuul overhead).
+      #. **total_changes** counter of the number of change proceeding since
+               Zuul started.
+
+  Additionally, the `zuul.pipeline.<pipeline name>` hierarchy contains
+  `current_changes` and `resident_time` metrics for each projects. The slash
+  separator used in Gerrit name being replaced by dots.
+
+  As an example, given a job named `myjob` triggered by the `gate` pipeline
+  which took 40 seconds to build, the Zuul scheduler will emit the following
+  statsd events:
+
+    * `zuul.pipeline.gate.job.myjob.SUCCESS` +1
+    * `zuul.pipeline.gate.job.myjob`  40 seconds
+    * `zuul.pipeline.gate.all_jobs` +1
diff --git a/doc/source/triggers.rst b/doc/source/triggers.rst
index 263f280..0b557c3 100644
--- a/doc/source/triggers.rst
+++ b/doc/source/triggers.rst
@@ -1,157 +1,157 @@
-:title: Triggers
-
-Triggers
-========
-
-The process of merging a change starts with proposing a change to be
-merged.  Primarily, Zuul supports Gerrit as a triggering system.
-Zuul's design is modular, so alternate triggering and reporting
-systems can be supported.
-
-Gerrit
-------
-
-Zuul works with standard versions of Gerrit by invoking the ``gerrit
-stream-events`` command over an SSH connection.  It also reports back
-to Gerrit using SSH.
-
-If using Gerrit 2.7 or later, make sure the user is a member of a group
-that is granted the ``Stream Events`` permission, otherwise it will not
-be able to invoke the ``gerrit stream-events`` command over SSH.
-
-A connection name with the gerrit driver can take multiple events with
-the following options.
-
-  **event**
-  The event name from gerrit.  Examples: ``patchset-created``,
-  ``comment-added``, ``ref-updated``.  This field is treated as a
-  regular expression.
-
-  **branch**
-  The branch associated with the event.  Example: ``master``.  This
-  field is treated as a regular expression, and multiple branches may
-  be listed.
-
-  **ref**
-  On ref-updated events, the branch parameter is not used, instead the
-  ref is provided.  Currently Gerrit has the somewhat idiosyncratic
-  behavior of specifying bare refs for branch names (e.g., ``master``),
-  but full ref names for other kinds of refs (e.g., ``refs/tags/foo``).
-  Zuul matches what you put here exactly against what Gerrit
-  provides.  This field is treated as a regular expression, and
-  multiple refs may be listed.
-
-  **ignore-deletes**
-  When a branch is deleted, a ref-updated event is emitted with a newrev
-  of all zeros specified. The ``ignore-deletes`` field is a boolean value
-  that describes whether or not these newrevs trigger ref-updated events.
-  The default is True, which will not trigger ref-updated events.
-
-  **approval**
-  This is only used for ``comment-added`` events.  It only matches if
-  the event has a matching approval associated with it.  Example:
-  ``code-review: 2`` matches a ``+2`` vote on the code review category.
-  Multiple approvals may be listed.
-
-  **email**
-  This is used for any event.  It takes a regex applied on the performer
-  email, i.e. Gerrit account email address.  If you want to specify
-  several email filters, you must use a YAML list.  Make sure to use non
-  greedy matchers and to escapes dots!
-  Example: ``email: ^.*?@example\.org$``.
-
-  **email_filter** (deprecated)
-  A deprecated alternate spelling of *email*.  Only one of *email* or
-  *email_filter* should be used.
-
-  **username**
-  This is used for any event.  It takes a regex applied on the performer
-  username, i.e. Gerrit account name.  If you want to specify several
-  username filters, you must use a YAML list.  Make sure to use non greedy
-  matchers and to escapes dots!
-  Example: ``username: ^jenkins$``.
-
-  **username_filter** (deprecated)
-  A deprecated alternate spelling of *username*.  Only one of *username* or
-  *username_filter* should be used.
-
-  **comment**
-  This is only used for ``comment-added`` events.  It accepts a list of
-  regexes that are searched for in the comment string. If any of these
-  regexes matches a portion of the comment string the trigger is
-  matched. ``comment: retrigger`` will match when comments
-  containing 'retrigger' somewhere in the comment text are added to a
-  change.
-
-  **comment_filter** (deprecated)
-  A deprecated alternate spelling of *comment*.  Only one of *comment* or
-  *comment_filter* should be used.
-
-  *require-approval*
-  This may be used for any event.  It requires that a certain kind
-  of approval be present for the current patchset of the change (the
-  approval could be added by the event in question).  It follows the
-  same syntax as the :ref:`"approval" pipeline requirement
-  <pipeline-require-approval>`. For each specified criteria there must
-  exist a matching approval.
-
-  *reject-approval*
-  This takes a list of approvals in the same format as
-  *require-approval* but will fail to enter the pipeline if there is
-  a matching approval.
-
-
-Timer
------
-
-A simple timer trigger is available as well.  It supports triggering
-jobs in a pipeline based on cron-style time instructions.
-
-Timers don't require a special connection or driver. Instead they can
-be used by listing **timer** as the trigger.
-
-This trigger will run based on a cron-style time specification.
-It will enqueue an event into its pipeline for every project
-defined in the configuration.  Any job associated with the
-pipeline will run in response to that event.
-
-  **time**
-  The time specification in cron syntax.  Only the 5 part syntax is
-  supported, not the symbolic names.  Example: ``0 0 * * *`` runs
-  at midnight.
-
-Zuul
-----
-
-The Zuul trigger generates events based on internal actions in Zuul.
-Multiple events may be listed.
-
-Zuul events don't require a special connection or driver. Instead they
-can be used by listing **zuul** as the trigger.
-
-  **event**
-  The event name.  Currently supported:
-
-    *project-change-merged* when Zuul merges a change to a project,
-    it generates this event for every open change in the project.
-
-    *parent-change-enqueued* when Zuul enqueues a change into any
-    pipeline, it generates this event for every child of that
-    change.
-
-  **pipeline**
-  Only available for ``parent-change-enqueued`` events.  This is the
-  name of the pipeline in which the parent change was enqueued.
-
-  *require-approval*
-  This may be used for any event.  It requires that a certain kind
-  of approval be present for the current patchset of the change (the
-  approval could be added by the event in question).  It follows the
-  same syntax as the :ref:`"approval" pipeline requirement
-  <pipeline-require-approval>`. For each specified criteria there must
-  exist a matching approval.
-
-  *reject-approval*
-  This takes a list of approvals in the same format as
-  *require-approval* but will fail to enter the pipeline if there is
+:title: Triggers
+
+Triggers
+========
+
+The process of merging a change starts with proposing a change to be
+merged.  Primarily, Zuul supports Gerrit as a triggering system.
+Zuul's design is modular, so alternate triggering and reporting
+systems can be supported.
+
+Gerrit
+------
+
+Zuul works with standard versions of Gerrit by invoking the ``gerrit
+stream-events`` command over an SSH connection.  It also reports back
+to Gerrit using SSH.
+
+If using Gerrit 2.7 or later, make sure the user is a member of a group
+that is granted the ``Stream Events`` permission, otherwise it will not
+be able to invoke the ``gerrit stream-events`` command over SSH.
+
+A connection name with the gerrit driver can take multiple events with
+the following options.
+
+  **event**
+  The event name from gerrit.  Examples: ``patchset-created``,
+  ``comment-added``, ``ref-updated``.  This field is treated as a
+  regular expression.
+
+  **branch**
+  The branch associated with the event.  Example: ``master``.  This
+  field is treated as a regular expression, and multiple branches may
+  be listed.
+
+  **ref**
+  On ref-updated events, the branch parameter is not used, instead the
+  ref is provided.  Currently Gerrit has the somewhat idiosyncratic
+  behavior of specifying bare refs for branch names (e.g., ``master``),
+  but full ref names for other kinds of refs (e.g., ``refs/tags/foo``).
+  Zuul matches what you put here exactly against what Gerrit
+  provides.  This field is treated as a regular expression, and
+  multiple refs may be listed.
+
+  **ignore-deletes**
+  When a branch is deleted, a ref-updated event is emitted with a newrev
+  of all zeros specified. The ``ignore-deletes`` field is a boolean value
+  that describes whether or not these newrevs trigger ref-updated events.
+  The default is True, which will not trigger ref-updated events.
+
+  **approval**
+  This is only used for ``comment-added`` events.  It only matches if
+  the event has a matching approval associated with it.  Example:
+  ``code-review: 2`` matches a ``+2`` vote on the code review category.
+  Multiple approvals may be listed.
+
+  **email**
+  This is used for any event.  It takes a regex applied on the performer
+  email, i.e. Gerrit account email address.  If you want to specify
+  several email filters, you must use a YAML list.  Make sure to use non
+  greedy matchers and to escapes dots!
+  Example: ``email: ^.*?@example\.org$``.
+
+  **email_filter** (deprecated)
+  A deprecated alternate spelling of *email*.  Only one of *email* or
+  *email_filter* should be used.
+
+  **username**
+  This is used for any event.  It takes a regex applied on the performer
+  username, i.e. Gerrit account name.  If you want to specify several
+  username filters, you must use a YAML list.  Make sure to use non greedy
+  matchers and to escapes dots!
+  Example: ``username: ^jenkins$``.
+
+  **username_filter** (deprecated)
+  A deprecated alternate spelling of *username*.  Only one of *username* or
+  *username_filter* should be used.
+
+  **comment**
+  This is only used for ``comment-added`` events.  It accepts a list of
+  regexes that are searched for in the comment string. If any of these
+  regexes matches a portion of the comment string the trigger is
+  matched. ``comment: retrigger`` will match when comments
+  containing 'retrigger' somewhere in the comment text are added to a
+  change.
+
+  **comment_filter** (deprecated)
+  A deprecated alternate spelling of *comment*.  Only one of *comment* or
+  *comment_filter* should be used.
+
+  *require-approval*
+  This may be used for any event.  It requires that a certain kind
+  of approval be present for the current patchset of the change (the
+  approval could be added by the event in question).  It follows the
+  same syntax as the :ref:`"approval" pipeline requirement
+  <pipeline-require-approval>`. For each specified criteria there must
+  exist a matching approval.
+
+  *reject-approval*
+  This takes a list of approvals in the same format as
+  *require-approval* but will fail to enter the pipeline if there is
+  a matching approval.
+
+
+Timer
+-----
+
+A simple timer trigger is available as well.  It supports triggering
+jobs in a pipeline based on cron-style time instructions.
+
+Timers don't require a special connection or driver. Instead they can
+be used by listing **timer** as the trigger.
+
+This trigger will run based on a cron-style time specification.
+It will enqueue an event into its pipeline for every project
+defined in the configuration.  Any job associated with the
+pipeline will run in response to that event.
+
+  **time**
+  The time specification in cron syntax.  Only the 5 part syntax is
+  supported, not the symbolic names.  Example: ``0 0 * * *`` runs
+  at midnight.
+
+Zuul
+----
+
+The Zuul trigger generates events based on internal actions in Zuul.
+Multiple events may be listed.
+
+Zuul events don't require a special connection or driver. Instead they
+can be used by listing **zuul** as the trigger.
+
+  **event**
+  The event name.  Currently supported:
+
+    *project-change-merged* when Zuul merges a change to a project,
+    it generates this event for every open change in the project.
+
+    *parent-change-enqueued* when Zuul enqueues a change into any
+    pipeline, it generates this event for every child of that
+    change.
+
+  **pipeline**
+  Only available for ``parent-change-enqueued`` events.  This is the
+  name of the pipeline in which the parent change was enqueued.
+
+  *require-approval*
+  This may be used for any event.  It requires that a certain kind
+  of approval be present for the current patchset of the change (the
+  approval could be added by the event in question).  It follows the
+  same syntax as the :ref:`"approval" pipeline requirement
+  <pipeline-require-approval>`. For each specified criteria there must
+  exist a matching approval.
+
+  *reject-approval*
+  This takes a list of approvals in the same format as
+  *require-approval* but will fail to enter the pipeline if there is
   a matching approval.
\ No newline at end of file
diff --git a/doc/source/zuul.rst b/doc/source/zuul.rst
index 98e4bb8..7b7cbc7 100644
--- a/doc/source/zuul.rst
+++ b/doc/source/zuul.rst
@@ -1,1029 +1,1029 @@
-:title: Zuul
-
-Zuul
-====
-
-Configuration
--------------
-
-Zuul has three configuration files:
-
-**zuul.conf**
-  Connection information for Gerrit and Gearman, locations of the
-  other config files.
-**layout.yaml**
-  Project and pipeline configuration -- what Zuul does.
-**logging.conf**
-    Python logging config.
-
-Examples of each of the three files can be found in the etc/ directory
-of the source distribution.
-
-.. _zuulconf:
-
-zuul.conf
-~~~~~~~~~
-
-Zuul will look for ``/etc/zuul/zuul.conf`` or ``~/zuul.conf`` to
-bootstrap its configuration.  Alternately, you may specify ``-c
-/path/to/zuul.conf`` on the command line.
-
-Gerrit and Gearman connection information are each described in a
-section of zuul.conf.  The location of the other two configuration
-files (as well as the location of the PID file when running Zuul as a
-server) are specified in a third section.
-
-The three sections of this config and their options are documented below.
-You can also find an example zuul.conf file in the git
-`repository
-<https://git.openstack.org/cgit/openstack-infra/zuul/tree/etc/zuul.conf-sample>`_
-
-gearman
-"""""""
-
-**server**
-  Hostname or IP address of the Gearman server.
-  ``server=gearman.example.com``
-
-**port**
-  Port on which the Gearman server is listening.
-  ``port=4730``
-
-gearman_server
-""""""""""""""
-
-**start**
-  Whether to start the internal Gearman server (default: False).
-  ``start=true``
-
-**listen_address**
-  IP address or domain name on which to listen (default: all addresses).
-  ``listen_address=127.0.0.1``
-
-**log_config**
-  Path to log config file for internal Gearman server.
-  ``log_config=/etc/zuul/gearman-logging.yaml``
-
-zuul
-""""
-
-.. _layout_config:
-
-**layout_config**
-  Path to layout config file.  Used by zuul-server only.
-  ``layout_config=/etc/zuul/layout.yaml``
-
-**log_config**
-  Path to log config file.  Used by zuul-server only.
-  ``log_config=/etc/zuul/logging.yaml``
-
-**pidfile**
-  Path to PID lock file.  Used by zuul-server only.
-  ``pidfile=/var/run/zuul/zuul.pid``
-
-**state_dir**
-  Path to directory that Zuul should save state to.  Used by all Zuul
-  commands.
-  ``state_dir=/var/lib/zuul``
-
-**report_times**
-  Boolean value (``true`` or ``false``) that determines if Zuul should
-  include elapsed times for each job in the textual report.  Used by
-  zuul-server only.
-  ``report_times=true``
-
-**status_url**
-  URL that will be posted in Zuul comments made to Gerrit changes when
-  starting jobs for a change.  Used by zuul-server only.
-  ``status_url=https://zuul.example.com/status``
-
-**status_expiry**
-  Zuul will cache the status.json file for this many seconds. This is an
-  optional value and ``1`` is used by default.
-  ``status_expiry=1``
-
-**url_pattern**
-  If you are storing build logs external to the system that originally
-  ran jobs and wish to link to those logs when Zuul makes comments on
-  Gerrit changes for completed jobs this setting configures what the
-  URLs for those links should be.  Used by zuul-server only.
-  ``http://logs.example.com/{change.number}/{change.patchset}/{pipeline.name}/{job.name}/{build.number}``
-
-**job_name_in_report**
-  Boolean value (``true`` or ``false``) that indicates whether the
-  job name should be included in the report (normally only the URL
-  is included).  Defaults to ``false``.  Used by zuul-server only.
-  ``job_name_in_report=true``
-
-merger
-""""""
-
-**git_dir**
-  Directory that Zuul should clone local git repositories to.
-  ``git_dir=/var/lib/zuul/git``
-
-**git_user_email**
-  Optional: Value to pass to `git config user.email`.
-  ``git_user_email=zuul@example.com``
-
-**git_user_name**
-  Optional: Value to pass to `git config user.name`.
-  ``git_user_name=zuul``
-
-**zuul_url**
-  URL of this merger's git repos, accessible to test workers.  Usually
-  "http://zuul.example.com/p" or "http://zuul-merger01.example.com/p"
-  depending on whether the merger is co-located with the Zuul server.
-
-**log_config**
-  Path to log config file for the merger process.
-  ``log_config=/etc/zuul/logging.yaml``
-
-**pidfile**
-  Path to PID lock file for the merger process.
-  ``pidfile=/var/run/zuul-merger/merger.pid``
-
-.. _swift:
-
-swift
-"""""
-
-To send (optional) swift upload instructions this section must be
-present. Multiple destinations can be defined in the :ref:`jobs` section
-of the layout.
-
-If you are sending the temp-url-key or fetching the x-storage-url, you
-will need the python-swiftclient module installed.
-
-**X-Account-Meta-Temp-Url-Key** (optional)
-  This is the key used to sign the HMAC message. If you do not set a
-  key Zuul will generate one automatically.
-
-**Send-Temp-Url-Key** (optional)
-  Zuul can send the X-Account-Meta-Temp-Url-Key to swift for you if
-  you have set up the appropriate credentials in ``authurl`` below.
-  This isn't necessary if you know and have set your
-  X-Account-Meta-Temp-Url-Key.
-  If set, Zuul requires the python-swiftclient module.
-  ``default: true``
-
-**X-Storage-Url** (optional)
-  The storage URL is the destination to upload files into. If you do
-  not set this the ``authurl`` credentials are used to fetch the url
-  from swift and Zuul will requires the python-swiftclient module.
-
-**authurl** (optional)
-  The (keystone) Auth URL for swift.
-  ``For example, https://identity.api.rackspacecloud.com/v2.0/``
-  This is required if you have Send-Temp-Url-Key set to ``True`` or
-  if you have not supplied the X-Storage-Url.
-
-Any of the `swiftclient connection parameters`_ can also be defined
-here by the same name. Including the os_options by their key name (
-``for example tenant_id``)
-
-.. _swiftclient connection parameters: http://docs.openstack.org/developer/python-swiftclient/swiftclient.html#module-swiftclient.client
-
-**region_name** (optional)
-  The region name holding the swift container
-  ``For example, SYD``
-
-Each destination defined by the :ref:`jobs` will have the following
-default values that it may overwrite.
-
-**default_container** (optional)
-  Container name to place the log into
-  ``For example, logs``
-
-**default_expiry** (optional)
-  How long the signed destination should be available for
-  ``default: 7200 (2hrs)``
-
-**default_max_file_size** (optional)
-  The maximum size of an individual file
-  ``default: 104857600 (100MB)``
-
-**default_max_file_count** (optional)
-  The maximum number of separate files to allow
-  ``default: 10``
-
-**default_logserver_prefix**
-  Provide a URL to the CDN or logserver app so that a worker knows
-  what URL to return. The worker should return the logserver_prefix
-  url and the object path.
-  ``For example: http://logs.example.org/server.app?obj=``
-
-.. _connection:
-
-connection ArbitraryName
-""""""""""""""""""""""""
-
-A connection can be listed with any arbitrary name. The required
-parameters are specified in the :ref:`connections` documentation
-depending on what driver you are using.
-
-.. _layoutyaml:
-
-layout.yaml
-~~~~~~~~~~~
-
-This is the main configuration file for Zuul, where all of the pipelines
-and projects are defined, what tests should be run, and what actions
-Zuul should perform.  There are three sections: pipelines, jobs, and
-projects.
-
-.. _includes:
-
-Includes
-""""""""
-
-Custom functions to be used in Zuul's configuration may be provided
-using the ``includes`` directive.  It accepts a list of files to
-include, and currently supports one type of inclusion, a python file::
-
-  includes:
-    - python-file: local_functions.py
-
-**python-file**
-  The path to a python file (either an absolute path or relative to the
-  directory name of :ref:`layout_config <layout_config>`).  The
-  file will be loaded and objects that it defines will be placed in a
-  special environment which can be referenced in the Zuul configuration.
-  Currently only the parameter-function attribute of a Job uses this
-  feature.
-
-Pipelines
-"""""""""
-
-Zuul can have any number of independent pipelines.  Whenever a matching
-Gerrit event is found for a pipeline, that event is added to the
-pipeline, and the jobs specified for that pipeline are run.  When all
-jobs specified for the pipeline that were triggered by an event are
-completed, Zuul reports back to Gerrit the results.
-
-There are no pre-defined pipelines in Zuul, rather you can define
-whatever pipelines you need in the layout file.  This is a very flexible
-system that can accommodate many kinds of workflows.
-
-Here is a quick example of a pipeline definition followed by an
-explanation of each of the parameters::
-
-  - name: check
-    manager: IndependentPipelineManager
-    source: my_gerrit
-    trigger:
-      my_gerrit:
-        - event: patchset-created
-    success:
-      my_gerrit:
-        verified: 1
-    failure:
-      my_gerrit
-        verified: -1
-
-**name**
-  This is used later in the project definition to indicate what jobs
-  should be run for events in the pipeline.
-
-**description**
-  This is an optional field that may be used to provide a textual
-  description of the pipeline.
-
-**source**
-  A required field that specifies a connection that provides access to
-  the change objects that this pipeline operates on. The name of the
-  connection as per the zuul.conf should be specified. The driver used
-  for the connection named will be the source. Currently only ``gerrit``
-  drivers are supported.
-
-**success-message**
-  An optional field that supplies the introductory text in message
-  reported back to Gerrit when all the voting builds are successful.
-  Defaults to "Build successful."
-
-**failure-message**
-  An optional field that supplies the introductory text in message
-  reported back to Gerrit when at least one voting build fails.
-  Defaults to "Build failed."
-
-**merge-failure-message**
-  An optional field that supplies the introductory text in message
-  reported back to Gerrit when a change fails to merge with the
-  current state of the repository.
-  Defaults to "Merge failed."
-
-**footer-message**
-  An optional field to supply additional information after test results.
-  Useful for adding information about the CI system such as debugging
-  and contact details.
-
-**manager**
-  There are currently two schemes for managing pipelines:
-
-  *IndependentPipelineManager*
-    Every event in this pipeline should be treated as independent of
-    other events in the pipeline.  This is appropriate when the order of
-    events in the pipeline doesn't matter because the results of the
-    actions this pipeline performs can not affect other events in the
-    pipeline.  For example, when a change is first uploaded for review,
-    you may want to run tests on that change to provide early feedback
-    to reviewers.  At the end of the tests, the change is not going to
-    be merged, so it is safe to run these tests in parallel without
-    regard to any other changes in the pipeline.  They are independent.
-
-    Another type of pipeline that is independent is a post-merge
-    pipeline. In that case, the changes have already merged, so the
-    results can not affect any other events in the pipeline.
-
-  *DependentPipelineManager*
-    The dependent pipeline manager is designed for gating.  It ensures
-    that every change is tested exactly as it is going to be merged
-    into the repository.  An ideal gating system would test one change
-    at a time, applied to the tip of the repository, and only if that
-    change passed tests would it be merged.  Then the next change in
-    line would be tested the same way.  In order to achieve parallel
-    testing of changes, the dependent pipeline manager performs
-    speculative execution on changes.  It orders changes based on
-    their entry into the pipeline.  It begins testing all changes in
-    parallel, assuming that each change ahead in the pipeline will pass
-    its tests.  If they all succeed, all the changes can be tested and
-    merged in parallel.  If a change near the front of the pipeline
-    fails its tests, each change behind it ignores whatever tests have
-    been completed and are tested again without the change in front.
-    This way gate tests may run in parallel but still be tested
-    correctly, exactly as they will appear in the repository when
-    merged.
-
-    One important characteristic of the DependentPipelineManager is that
-    it analyzes the jobs that are triggered by different projects, and
-    if those projects have jobs in common, it treats those projects as
-    related, and they share a single virtual queue of changes.  Thus,
-    if there is a job that performs integration testing on two
-    projects, those two projects will automatically share a virtual
-    change queue.  If a third project does not invoke that job, it
-    will be part of a separate virtual change queue, and changes to
-    it will not depend on changes to the first two jobs.
-
-    For more detail on the theory and operation of Zuul's
-    DependentPipelineManager, see: :doc:`gating`.
-
-**trigger**
-  At least one trigger source must be supplied for each pipeline.
-  Triggers are not exclusive -- matching events may be placed in
-  multiple pipelines, and they will behave independently in each of
-  the pipelines they match.
-
-  Triggers are loaded from their connection name. The driver type of
-  the connection will dictate which options are available.
-  See :doc:`triggers`.
-
-**require**
-  If this section is present, it established pre-requisites for any
-  kind of item entering the Pipeline.  Regardless of how the item is
-  to be enqueued (via any trigger or automatic dependency resolution),
-  the conditions specified here must be met or the item will not be
-  enqueued.
-
-.. _pipeline-require-approval:
-
-  **approval**
-  This requires that a certain kind of approval be present for the
-  current patchset of the change (the approval could be added by the
-  event in question).  It takes several sub-parameters, all of which
-  are optional and are combined together so that there must be an
-  approval matching all specified requirements.
-
-    *username*
-    If present, an approval from this username is required.
-
-    *email*
-    If present, an approval with this email address is required.  It
-    is treated as a regular expression as above.
-
-    *email-filter* (deprecated)
-    A deprecated alternate spelling of *email*.  Only one of *email* or
-    *email_filter* should be used.
-
-    *older-than*
-    If present, the approval must be older than this amount of time
-    to match.  Provide a time interval as a number with a suffix of
-    "w" (weeks), "d" (days), "h" (hours), "m" (minutes), "s"
-    (seconds).  Example ``48h`` or ``2d``.
-
-    *newer-than*
-    If present, the approval must be newer than this amount of time
-    to match.  Same format as "older-than".
-
-    Any other field is interpreted as a review category and value
-    pair.  For example ``verified: 1`` would require that the approval
-    be for a +1 vote in the "Verified" column.  The value may either
-    be a single value or a list: ``verified: [1, 2]`` would match
-    either a +1 or +2 vote.
-
-  **open**
-  A boolean value (``true`` or ``false``) that indicates whether the change
-  must be open or closed in order to be enqueued.
-
-  **current-patchset**
-  A boolean value (``true`` or ``false``) that indicates whether the change
-  must be the current patchset in order to be enqueued.
-
-  **status**
-  A string value that corresponds with the status of the change
-  reported by the trigger.  For example, when using the Gerrit
-  trigger, status values such as ``NEW`` or ``MERGED`` may be useful.
-
-**reject**
-  If this section is present, it establishes pre-requisites that can
-  block an item from being enqueued. It can be considered a negative
-  version of **require**.
-
-  **approval**
-  This takes a list of approvals. If an approval matches the provided
-  criteria the change can not be entered into the pipeline. It follows
-  the same syntax as the :ref:`"require approval" pipeline above
-  <pipeline-require-approval>`.
-
-  Example to reject a change with any negative vote::
-
-    reject:
-      approval:
-        - code-review: [-1, -2]
-
-**dequeue-on-new-patchset**
-  Normally, if a new patchset is uploaded to a change that is in a
-  pipeline, the existing entry in the pipeline will be removed (with
-  jobs canceled and any dependent changes that can no longer merge as
-  well.  To suppress this behavior (and allow jobs to continue
-  running), set this to ``false``.  Default: ``true``.
-
-**ignore-dependencies**
-  In any kind of pipeline (dependent or independent), Zuul will
-  attempt to enqueue all dependencies ahead of the current change so
-  that they are tested together (independent pipelines report the
-  results of each change regardless of the results of changes ahead).
-  To ignore dependencies completely in an independent pipeline, set
-  this to ``true``.  This option is ignored by dependent pipelines.
-  The default is: ``false``.
-
-**success**
-  Describes where Zuul should report to if all the jobs complete
-  successfully.
-  This section is optional; if it is omitted, Zuul will run jobs and
-  do nothing on success; it will not even report a message to Gerrit.
-  If the section is present, the listed reporter plugins will be
-  asked to report on the jobs.
-  The reporters are listed by their connection name. The options
-  available depend on the driver for the supplied connection.
-  See :doc:`reporters` for more details.
-
-**failure**
-  Uses the same syntax as **success**, but describes what Zuul should
-  do if at least one job fails.
-
-**merge-failure**
-  Uses the same syntax as **success**, but describes what Zuul should
-  do if it is unable to merge in the patchset. If no merge-failure
-  reporters are listed then the ``failure`` reporters will be used to
-  notify of unsuccessful merges.
-
-**start**
-  Uses the same syntax as **success**, but describes what Zuul should
-  do when a change is added to the pipeline manager.  This can be used,
-  for example, to reset the value of the Verified review category.
-
-**disabled**
-  Uses the same syntax as **success**, but describes what Zuul should
-  do when a pipeline is disabled.
-  See ``disable-after-consecutive-failures``.
-
-**disable-after-consecutive-failures**
-  If set, a pipeline can enter a ''disabled'' state if too many changes
-  in a row fail. When this value is exceeded the pipeline will stop
-  reporting to any of the ``success``, ``failure`` or ``merge-failure``
-  reporters and instead only report to the ``disabled`` reporters.
-  (No ``start`` reports are made when a pipeline is disabled).
-
-**precedence**
-  Indicates how the build scheduler should prioritize jobs for
-  different pipelines.  Each pipeline may have one precedence, jobs
-  for pipelines with a higher precedence will be run before ones with
-  lower.  The value should be one of ``high``, ``normal``, or ``low``.
-  Default: ``normal``.
-
-**window**
-  DependentPipelineManagers only. Zuul can rate limit
-  DependentPipelineManagers in a manner similar to TCP flow control.
-  Jobs are only started for changes in the queue if they sit in the
-  actionable window for the pipeline. The initial length of this window
-  is configurable with this value. The value given should be a positive
-  integer value. A value of ``0`` disables rate limiting on the
-  DependentPipelineManager.
-  Default: ``20``.
-
-**window-floor**
-  DependentPipelineManagers only. This is the minimum value for the
-  window described above. Should be a positive non zero integer value.
-  Default: ``3``.
-
-**window-increase-type**
-  DependentPipelineManagers only. This value describes how the window
-  should grow when changes are successfully merged by zuul. A value of
-  ``linear`` indicates that ``window-increase-factor`` should be added
-  to the previous window value. A value of ``exponential`` indicates
-  that ``window-increase-factor`` should be multiplied against the
-  previous window value and the result will become the window size.
-  Default: ``linear``.
-
-**window-increase-factor**
-  DependentPipelineManagers only. The value to be added or multiplied
-  against the previous window value to determine the new window after
-  successful change merges.
-  Default: ``1``.
-
-**window-decrease-type**
-  DependentPipelineManagers only. This value describes how the window
-  should shrink when changes are not able to be merged by Zuul. A value
-  of ``linear`` indicates that ``window-decrease-factor`` should be
-  subtracted from the previous window value. A value of ``exponential``
-  indicates that ``window-decrease-factor`` should be divided against
-  the previous window value and the result will become the window size.
-  Default: ``exponential``.
-
-**window-decrease-factor**
-  DependentPipelineManagers only. The value to be subtracted or divided
-  against the previous window value to determine the new window after
-  unsuccessful change merges.
-  Default: ``2``.
-
-Some example pipeline configurations are included in the sample layout
-file.  The first is called a *check* pipeline::
-
-  - name: check
-    manager: IndependentPipelineManager
-    trigger:
-      my_gerrit:
-        - event: patchset-created
-    success:
-      my_gerrit:
-        verified: 1
-    failure:
-      my_gerrit:
-        verified: -1
-
-This will trigger jobs each time a new patchset (or change) is
-uploaded to Gerrit, and report +/-1 values to Gerrit in the
-``verified`` review category. ::
-
-  - name: gate
-    manager: DependentPipelineManager
-    trigger:
-      my_gerrit:
-        - event: comment-added
-          approval:
-            - approved: 1
-    success:
-      my_gerrit:
-        verified: 2
-        submit: true
-    failure:
-      my_gerrit:
-        verified: -2
-
-This will trigger jobs whenever a reviewer leaves a vote of ``1`` in the
-``approved`` review category in Gerrit (a non-standard category).
-Changes will be tested in such a way as to guarantee that they will be
-merged exactly as tested, though that will happen in parallel by
-creating a virtual queue of dependent changes and performing
-speculative execution of jobs. ::
-
-  - name: post
-    manager: IndependentPipelineManager
-    trigger:
-      my_gerrit:
-        - event: ref-updated
-          ref: ^(?!refs/).*$
-
-This will trigger jobs whenever a change is merged to a named branch
-(e.g., ``master``).  No output will be reported to Gerrit.  This is
-useful for side effects such as creating per-commit tarballs. ::
-
-  - name: silent
-    manager: IndependentPipelineManager
-    trigger:
-      my_gerrit:
-        - event: patchset-created
-
-This also triggers jobs when changes are uploaded to Gerrit, but no
-results are reported to Gerrit.  This is useful for jobs that are in
-development and not yet ready to be presented to developers. ::
-
-  pipelines:
-    - name: post-merge
-      manager: IndependentPipelineManager
-      trigger:
-        my_gerrit:
-          - event: change-merged
-      success:
-        my_gerrit:
-          force-message: True
-      failure:
-        my_gerrit:
-          force-message: True
-
-The ``change-merged`` events happen when a change has been merged in the git
-repository. The change is thus closed and Gerrit will not accept modifications
-to the review scoring such as ``code-review`` or ``verified``. By using the
-``force-message: True`` parameter, Zuul will pass ``--force-message`` to the
-``gerrit review`` command, thus making sure the message is actually
-sent back to Gerrit regardless of approval scores.
-That kind of pipeline is nice to run regression or performance tests.
-
-.. note::
-  The ``change-merged`` event does not include the commit sha1 which can be
-  hazardous, it would let you report back to Gerrit though.  If you were to
-  build a tarball for a specific commit, you should consider instead using
-  the ``ref-updated`` event which does include the commit sha1 (but lacks the
-  Gerrit change number).
-
-
-.. _jobs:
-
-Jobs
-""""
-
-The jobs section is optional, and can be used to set attributes of
-jobs that are independent of their association with a project.  For
-example, if a job should return a customized message on failure, that
-may be specified here.  Otherwise, Zuul does not need to be told about
-each job as it builds a list from the project specification.
-
-**name**
-  The name of the job.  This field is treated as a regular expression
-  and will be applied to each job that matches.
-
-**queue-name (optional)**
-  Zuul will automatically combine projects that share a job into
-  shared change queues for dependent pipeline managers.  In order to
-  report statistics about these queues, it is convenient for them to
-  have names.  Zuul can automatically name change queues, however
-  these can grow quite long and are prone to changing as projects in
-  the queue change.  If you assign a queue-name to a job, Zuul will
-  use that as the name for the shared change queue that contains that
-  job instead of the automatically generated one.  It is an error for
-  a shared change queue to have more than one job with a queue-name if
-  they are not the same.
-
-**failure-message (optional)**
-  The message that should be reported to Gerrit if the job fails.
-
-**success-message (optional)**
-  The message that should be reported to Gerrit if the job fails.
-
-**failure-pattern (optional)**
-  The URL that should be reported to Gerrit if the job fails.
-  Defaults to the build URL or the url_pattern configured in
-  zuul.conf.  May be supplied as a string pattern with substitutions
-  as described in url_pattern in :ref:`zuulconf`.
-
-**success-pattern (optional)**
-  The URL that should be reported to Gerrit if the job succeeds.
-  Defaults to the build URL or the url_pattern configured in
-  zuul.conf.  May be supplied as a string pattern with substitutions
-  as described in url_pattern in :ref:`zuulconf`.
-
-**hold-following-changes (optional)**
-  This is a boolean that indicates that changes that follow this
-  change in a dependent change pipeline should wait until this job
-  succeeds before launching.  If this is applied to a very short job
-  that can predict whether longer jobs will fail early, this can be
-  used to reduce the number of jobs that Zuul will launch and
-  ultimately have to cancel.  In that case, a small amount of
-  parallelization of jobs is traded for more efficient use of testing
-  resources.  On the other hand, to apply this to a long running job
-  would largely defeat the parallelization of dependent change testing
-  that is the main feature of Zuul.  Default: ``false``.
-
-**mutex (optional)**
-  This is a string that names a mutex that should be observed by this
-  job.  Only one build of any job that references the same named mutex
-  will be enqueued at a time.  This applies across all pipelines.
-
-**branch (optional)**
-  This job should only be run on matching branches.  This field is
-  treated as a regular expression and multiple branches may be
-  listed.
-
-**files (optional)**
-  This job should only be run if at least one of the files involved in
-  the change (added, deleted, or modified) matches at least one of the
-  file patterns listed here.  This field is treated as a regular
-  expression and multiple expressions may be listed.
-
-**skip-if (optional)**
-
-  This job should not be run if all the patterns specified by the
-  optional fields listed below match on their targets.  When multiple
-  sets of parameters are provided, this job will be skipped if any set
-  matches.  For example: ::
-
-    jobs:
-      - name: check-tempest-dsvm-neutron
-        skip-if:
-          - project: ^openstack/neutron$
-            branch: ^stable/juno$
-            all-files-match-any:
-              - ^neutron/tests/.*$
-              - ^tools/.*$
-          - all-files-match-any:
-              - ^doc/.*$
-              - ^.*\.rst$
-
-  With this configuration, the job would be skipped for a neutron
-  patchset for the stable/juno branch provided that every file in the
-  change matched at least one of the specified file regexes.  The job
-  will also be skipped for any patchset that modified only the doc
-  tree or rst files.
-
-  *project* (optional)
-    The regular expression to match against the project of the change.
-
-  *branch* (optional)
-    The regular expression to match against the branch or ref of the
-    change.
-
-  *all-files-match-any* (optional)
-    A list of regular expressions intended to match the files involved
-    in the change.  This parameter will be considered matching a
-    change only if all files in a change match at least one of these
-    expressions.
-
-    The pattern for '/COMMIT_MSG' is always matched on and does not
-    have to be included.
-
-**voting (optional)**
-  Boolean value (``true`` or ``false``) that indicates whatever
-  a job is voting or not.  Default: ``true``.
-
-**tags (optional)**
-  A list of arbitrary strings which will be associated with the job.
-  Can be used by the parameter-function to alter behavior based on
-  their presence on a job.  If the job name is a regular expression,
-  tags will accumulate on jobs that match.
-
-**parameter-function (optional)**
-  Specifies a function that should be applied to the parameters before
-  the job is launched.  The function should be defined in a python file
-  included with the :ref:`includes` directive.  The function
-  should have the following signature:
-
-  .. function:: parameters(item, job, parameters)
-
-     Manipulate the parameters passed to a job before a build is
-     launched.  The ``parameters`` dictionary will already contain the
-     standard Zuul job parameters, and is expected to be modified
-     in-place.
-
-     :param item: the current queue item
-     :type item: zuul.model.QueueItem
-     :param job: the job about to be run
-     :type job: zuul.model.Job
-     :param parameters: parameters to be passed to the job
-     :type parameters: dict
-
-  If the parameter **ZUUL_NODE** is set by this function, then it will
-  be used to specify on what node (or class of node) the job should be
-  run.
-
-**swift**
-  If :ref:`swift` is configured then each job can define a destination
-  container for the builder to place logs and/or assets into. Multiple
-  containers can be listed for each job by providing a unique ``name``.
-
-  *name*
-    Set an identifying name for the container. This is used in the
-    parameter key sent to the builder. For example if it ``logs`` then
-    one of the parameters sent will be ``SWIFT_logs_CONTAINER``
-    (case-sensitive).
-
-  Each of the defaults defined in :ref:`swift` can be overwritten as:
-
-  *container* (optional)
-    Container name to place the log into
-    ``For example, logs``
-
-  *expiry* (optional)
-    How long the signed destination should be available for
-
-  *max-file-size** (optional)
-    The maximum size of an individual file
-
-  *max_file_size** (optional, deprecated)
-    A deprecated alternate spelling of *max-file-size*.
-
-  *max-file-count* (optional)
-    The maximum number of separate files to allow
-
-  *max_file_count* (optional, deprecated)
-    A deprecated alternate spelling of *max-file-count*.
-
-  *logserver-prefix*
-    Provide a URL to the CDN or logserver app so that a worker knows
-    what URL to return.
-    ``For example: http://logs.example.org/server.app?obj=``
-    The worker should return the logserver-prefix url and the object
-    path as the URL in the results data packet.
-
-  *logserver_prefix* (deprecated)
-    A deprecated alternate spelling of *logserver-prefix*.
-
-Here is an example of setting the failure message for jobs that check
-whether a change merges cleanly::
-
-  - name: ^.*-merge$
-    failure-message: This change or one of its cross-repo dependencies
-    was unable to be automatically merged with the current state of
-    its repository. Please rebase the change and upload a new
-    patchset.
-
-Projects
-""""""""
-
-The projects section indicates what jobs should be run in each pipeline
-for events associated with each project.  It contains a list of
-projects.  Here is an example::
-
-  - name: example/project
-    check:
-      - project-merge:
-        - project-unittest
-        - project-pep8
-        - project-pyflakes
-    gate:
-      - project-merge:
-        - project-unittest
-        - project-pep8
-        - project-pyflakes
-    post:
-      - project-publish
-
-**name**
-  The name of the project (as known by Gerrit).
-
-**merge-mode (optional)**
-  An optional value that indicates what strategy should be used to
-  merge changes to this project.  Supported values are:
-
-  ** merge-resolve **
-  Equivalent to 'git merge -s resolve'.  This corresponds closely to
-  what Gerrit performs (using JGit) for a project if the "Merge if
-  necessary" merge mode is selected and "Automatically resolve
-  conflicts" is checked.  This is the default.
-
-  ** merge **
-  Equivalent to 'git merge'.
-
-  ** cherry-pick **
-  Equivalent to 'git cherry-pick'.
-
-This is followed by a section for each of the pipelines defined above.
-Pipelines may be omitted if no jobs should run for this project in a
-given pipeline.  Within the pipeline section, the jobs that should be
-executed are listed.  If a job is entered as a dictionary key, then
-jobs contained within that key are only executed if the key job
-succeeds.  In the above example, project-unittest, project-pep8, and
-project-pyflakes are only executed if project-merge succeeds.  This
-can help avoid running unnecessary jobs.
-
-The special job named ``noop`` is internal to Zuul and will always
-return ``SUCCESS`` immediately.  This can be useful if you require
-that all changes be processed by a pipeline but a project has no jobs
-that can be run on it.
-
-.. seealso:: The OpenStack Zuul configuration for a comprehensive example: https://git.openstack.org/cgit/openstack-infra/project-config/tree/zuul/layout.yaml
-
-Project Templates
-"""""""""""""""""
-
-Whenever you have lot of similar projects (such as plugins for a project) you
-will most probably want to use the same pipeline configurations.  The
-project templates let you define pipelines and job name templates to trigger.
-One can then just apply the template on its project which make it easier to
-update several similar projects. As an example::
-
-  project-templates:
-    # Name of the template
-    - name: plugin-triggering
-      # Definition of pipelines just like for a `project`
-      check:
-       - '{jobprefix}-merge':
-         - '{jobprefix}-pep8'
-         - '{jobprefix}-pyflakes'
-      gate:
-       - '{jobprefix}-merge':
-         - '{jobprefix}-unittest'
-         - '{jobprefix}-pep8'
-         - '{jobprefix}-pyflakes'
-
-In your projects definition, you will then apply the template using the template
-key::
-
-  projects:
-   - name: plugin/foobar
-     template:
-      - name: plugin-triggering
-        jobprefix: plugin-foobar
-
-You can pass several parameters to a template. A ``parameter`` value
-will be used for expansion of ``{parameter}`` in the template
-strings. The parameter ``name`` will be automatically provided and
-will contain the short name of the project, that is the portion of the
-project name after the last ``/`` character.
-
-Multiple templates can be combined in a project, and the jobs from all
-of those templates will be added to the project.  Individual jobs may
-also be added::
-
-  projects:
-   - name: plugin/foobar
-     template:
-      - name: plugin-triggering
-        jobprefix: plugin-foobar
-      - name: plugin-extras
-        jobprefix: plugin-foobar
-     check:
-      - foobar-extra-special-job
-
-Individual jobs may optionally be added to pipelines (e.g. check,
-gate, et cetera) for a project, in addition to those provided by
-templates.
-
-The order of the jobs listed in the project (which only affects the
-order of jobs listed on the report) will be the jobs from each
-template in the order listed, followed by any jobs individually listed
-for the project.
-
-Note that if multiple templates are used for a project and one
-template specifies a job that is also specified in another template,
-or specified in the project itself, the configuration defined by
-either the last template or the project itself will take priority.
-
-logging.conf
-~~~~~~~~~~~~
-This file is optional.  If provided, it should be a standard
-:mod:`logging.config` module configuration file.  If not present, Zuul will
-output all log messages of DEBUG level or higher to the console.
-
-Starting Zuul
--------------
-
-To start Zuul, run **zuul-server**::
-
-  usage: zuul-server [-h] [-c CONFIG] [-l LAYOUT] [-d] [-t] [--version]
-
-  Project gating system.
-
-  optional arguments:
-    -h, --help  show this help message and exit
-    -c CONFIG   specify the config file
-    -l LAYOUT   specify the layout file
-    -d          do not run as a daemon
-    -t          validate layout file syntax
-    --version   show zuul version
-
-You may want to use the ``-d`` argument while you are initially setting
-up Zuul so you can detect any configuration errors quickly.  Under
-normal operation, omit ``-d`` and let Zuul run as a daemon.
-
-If you send signal 1 (SIGHUP) to the zuul-server process, Zuul will
-stop executing new jobs, wait until all executing jobs are finished,
-reload its layout.yaml, and resume. Changes to any connections or
-the PID  file will be ignored until Zuul is restarted.
-
-If you send a SIGUSR1 to the zuul-server process, Zuul will stop
-executing new jobs, wait until all executing jobs are finished,
-then exit. While waiting to exit Zuul will queue Gerrit events and
-save these events prior to exiting. When Zuul starts again it will
-read these saved events and act on them.
-
-If you need to abort Zuul and intend to manually requeue changes for
-jobs which were running in its pipelines, prior to terminating you can
-use the zuul-changes.py tool script to simplify the process. For
-example, this would give you a list of zuul-enqueue commands to requeue
-changes for the gate and check pipelines respectively::
-
-  ./tools/zuul-changes.py http://zuul.openstack.org/ gate
-  ./tools/zuul-changes.py http://zuul.openstack.org/ check
-
-If you send a SIGUSR2 to the zuul-server process, or the forked process
-that runs the Gearman daemon, Zuul will dump a stack trace for each
-running thread into its debug log. It is written under the log bucket
-``zuul.stack_dump``.  This is useful for tracking down deadlock or
-otherwise slow threads.
-
-When `yappi <https://code.google.com/p/yappi/>`_ (Yet Another Python
-Profiler) is available, additional functions' and threads' stats are
-emitted as well. The first SIGUSR2 will enable yappi, on the second
-SIGUSR2 it dumps the information collected, resets all yappi state and
-stops profiling. This is to minimize the impact of yappi on a running
-system.
+:title: Zuul
+
+Zuul
+====
+
+Configuration
+-------------
+
+Zuul has three configuration files:
+
+**zuul.conf**
+  Connection information for Gerrit and Gearman, locations of the
+  other config files.
+**layout.yaml**
+  Project and pipeline configuration -- what Zuul does.
+**logging.conf**
+    Python logging config.
+
+Examples of each of the three files can be found in the etc/ directory
+of the source distribution.
+
+.. _zuulconf:
+
+zuul.conf
+~~~~~~~~~
+
+Zuul will look for ``/etc/zuul/zuul.conf`` or ``~/zuul.conf`` to
+bootstrap its configuration.  Alternately, you may specify ``-c
+/path/to/zuul.conf`` on the command line.
+
+Gerrit and Gearman connection information are each described in a
+section of zuul.conf.  The location of the other two configuration
+files (as well as the location of the PID file when running Zuul as a
+server) are specified in a third section.
+
+The three sections of this config and their options are documented below.
+You can also find an example zuul.conf file in the git
+`repository
+<https://git.openstack.org/cgit/openstack-infra/zuul/tree/etc/zuul.conf-sample>`_
+
+gearman
+"""""""
+
+**server**
+  Hostname or IP address of the Gearman server.
+  ``server=gearman.example.com``
+
+**port**
+  Port on which the Gearman server is listening.
+  ``port=4730``
+
+gearman_server
+""""""""""""""
+
+**start**
+  Whether to start the internal Gearman server (default: False).
+  ``start=true``
+
+**listen_address**
+  IP address or domain name on which to listen (default: all addresses).
+  ``listen_address=127.0.0.1``
+
+**log_config**
+  Path to log config file for internal Gearman server.
+  ``log_config=/etc/zuul/gearman-logging.yaml``
+
+zuul
+""""
+
+.. _layout_config:
+
+**layout_config**
+  Path to layout config file.  Used by zuul-server only.
+  ``layout_config=/etc/zuul/layout.yaml``
+
+**log_config**
+  Path to log config file.  Used by zuul-server only.
+  ``log_config=/etc/zuul/logging.yaml``
+
+**pidfile**
+  Path to PID lock file.  Used by zuul-server only.
+  ``pidfile=/var/run/zuul/zuul.pid``
+
+**state_dir**
+  Path to directory that Zuul should save state to.  Used by all Zuul
+  commands.
+  ``state_dir=/var/lib/zuul``
+
+**report_times**
+  Boolean value (``true`` or ``false``) that determines if Zuul should
+  include elapsed times for each job in the textual report.  Used by
+  zuul-server only.
+  ``report_times=true``
+
+**status_url**
+  URL that will be posted in Zuul comments made to Gerrit changes when
+  starting jobs for a change.  Used by zuul-server only.
+  ``status_url=https://zuul.example.com/status``
+
+**status_expiry**
+  Zuul will cache the status.json file for this many seconds. This is an
+  optional value and ``1`` is used by default.
+  ``status_expiry=1``
+
+**url_pattern**
+  If you are storing build logs external to the system that originally
+  ran jobs and wish to link to those logs when Zuul makes comments on
+  Gerrit changes for completed jobs this setting configures what the
+  URLs for those links should be.  Used by zuul-server only.
+  ``http://logs.example.com/{change.number}/{change.patchset}/{pipeline.name}/{job.name}/{build.number}``
+
+**job_name_in_report**
+  Boolean value (``true`` or ``false``) that indicates whether the
+  job name should be included in the report (normally only the URL
+  is included).  Defaults to ``false``.  Used by zuul-server only.
+  ``job_name_in_report=true``
+
+merger
+""""""
+
+**git_dir**
+  Directory that Zuul should clone local git repositories to.
+  ``git_dir=/var/lib/zuul/git``
+
+**git_user_email**
+  Optional: Value to pass to `git config user.email`.
+  ``git_user_email=zuul@example.com``
+
+**git_user_name**
+  Optional: Value to pass to `git config user.name`.
+  ``git_user_name=zuul``
+
+**zuul_url**
+  URL of this merger's git repos, accessible to test workers.  Usually
+  "http://zuul.example.com/p" or "http://zuul-merger01.example.com/p"
+  depending on whether the merger is co-located with the Zuul server.
+
+**log_config**
+  Path to log config file for the merger process.
+  ``log_config=/etc/zuul/logging.yaml``
+
+**pidfile**
+  Path to PID lock file for the merger process.
+  ``pidfile=/var/run/zuul-merger/merger.pid``
+
+.. _swift:
+
+swift
+"""""
+
+To send (optional) swift upload instructions this section must be
+present. Multiple destinations can be defined in the :ref:`jobs` section
+of the layout.
+
+If you are sending the temp-url-key or fetching the x-storage-url, you
+will need the python-swiftclient module installed.
+
+**X-Account-Meta-Temp-Url-Key** (optional)
+  This is the key used to sign the HMAC message. If you do not set a
+  key Zuul will generate one automatically.
+
+**Send-Temp-Url-Key** (optional)
+  Zuul can send the X-Account-Meta-Temp-Url-Key to swift for you if
+  you have set up the appropriate credentials in ``authurl`` below.
+  This isn't necessary if you know and have set your
+  X-Account-Meta-Temp-Url-Key.
+  If set, Zuul requires the python-swiftclient module.
+  ``default: true``
+
+**X-Storage-Url** (optional)
+  The storage URL is the destination to upload files into. If you do
+  not set this the ``authurl`` credentials are used to fetch the url
+  from swift and Zuul will requires the python-swiftclient module.
+
+**authurl** (optional)
+  The (keystone) Auth URL for swift.
+  ``For example, https://identity.api.rackspacecloud.com/v2.0/``
+  This is required if you have Send-Temp-Url-Key set to ``True`` or
+  if you have not supplied the X-Storage-Url.
+
+Any of the `swiftclient connection parameters`_ can also be defined
+here by the same name. Including the os_options by their key name (
+``for example tenant_id``)
+
+.. _swiftclient connection parameters: http://docs.openstack.org/developer/python-swiftclient/swiftclient.html#module-swiftclient.client
+
+**region_name** (optional)
+  The region name holding the swift container
+  ``For example, SYD``
+
+Each destination defined by the :ref:`jobs` will have the following
+default values that it may overwrite.
+
+**default_container** (optional)
+  Container name to place the log into
+  ``For example, logs``
+
+**default_expiry** (optional)
+  How long the signed destination should be available for
+  ``default: 7200 (2hrs)``
+
+**default_max_file_size** (optional)
+  The maximum size of an individual file
+  ``default: 104857600 (100MB)``
+
+**default_max_file_count** (optional)
+  The maximum number of separate files to allow
+  ``default: 10``
+
+**default_logserver_prefix**
+  Provide a URL to the CDN or logserver app so that a worker knows
+  what URL to return. The worker should return the logserver_prefix
+  url and the object path.
+  ``For example: http://logs.example.org/server.app?obj=``
+
+.. _connection:
+
+connection ArbitraryName
+""""""""""""""""""""""""
+
+A connection can be listed with any arbitrary name. The required
+parameters are specified in the :ref:`connections` documentation
+depending on what driver you are using.
+
+.. _layoutyaml:
+
+layout.yaml
+~~~~~~~~~~~
+
+This is the main configuration file for Zuul, where all of the pipelines
+and projects are defined, what tests should be run, and what actions
+Zuul should perform.  There are three sections: pipelines, jobs, and
+projects.
+
+.. _includes:
+
+Includes
+""""""""
+
+Custom functions to be used in Zuul's configuration may be provided
+using the ``includes`` directive.  It accepts a list of files to
+include, and currently supports one type of inclusion, a python file::
+
+  includes:
+    - python-file: local_functions.py
+
+**python-file**
+  The path to a python file (either an absolute path or relative to the
+  directory name of :ref:`layout_config <layout_config>`).  The
+  file will be loaded and objects that it defines will be placed in a
+  special environment which can be referenced in the Zuul configuration.
+  Currently only the parameter-function attribute of a Job uses this
+  feature.
+
+Pipelines
+"""""""""
+
+Zuul can have any number of independent pipelines.  Whenever a matching
+Gerrit event is found for a pipeline, that event is added to the
+pipeline, and the jobs specified for that pipeline are run.  When all
+jobs specified for the pipeline that were triggered by an event are
+completed, Zuul reports back to Gerrit the results.
+
+There are no pre-defined pipelines in Zuul, rather you can define
+whatever pipelines you need in the layout file.  This is a very flexible
+system that can accommodate many kinds of workflows.
+
+Here is a quick example of a pipeline definition followed by an
+explanation of each of the parameters::
+
+  - name: check
+    manager: IndependentPipelineManager
+    source: my_gerrit
+    trigger:
+      my_gerrit:
+        - event: patchset-created
+    success:
+      my_gerrit:
+        verified: 1
+    failure:
+      my_gerrit
+        verified: -1
+
+**name**
+  This is used later in the project definition to indicate what jobs
+  should be run for events in the pipeline.
+
+**description**
+  This is an optional field that may be used to provide a textual
+  description of the pipeline.
+
+**source**
+  A required field that specifies a connection that provides access to
+  the change objects that this pipeline operates on. The name of the
+  connection as per the zuul.conf should be specified. The driver used
+  for the connection named will be the source. Currently only ``gerrit``
+  drivers are supported.
+
+**success-message**
+  An optional field that supplies the introductory text in message
+  reported back to Gerrit when all the voting builds are successful.
+  Defaults to "Build successful."
+
+**failure-message**
+  An optional field that supplies the introductory text in message
+  reported back to Gerrit when at least one voting build fails.
+  Defaults to "Build failed."
+
+**merge-failure-message**
+  An optional field that supplies the introductory text in message
+  reported back to Gerrit when a change fails to merge with the
+  current state of the repository.
+  Defaults to "Merge failed."
+
+**footer-message**
+  An optional field to supply additional information after test results.
+  Useful for adding information about the CI system such as debugging
+  and contact details.
+
+**manager**
+  There are currently two schemes for managing pipelines:
+
+  *IndependentPipelineManager*
+    Every event in this pipeline should be treated as independent of
+    other events in the pipeline.  This is appropriate when the order of
+    events in the pipeline doesn't matter because the results of the
+    actions this pipeline performs can not affect other events in the
+    pipeline.  For example, when a change is first uploaded for review,
+    you may want to run tests on that change to provide early feedback
+    to reviewers.  At the end of the tests, the change is not going to
+    be merged, so it is safe to run these tests in parallel without
+    regard to any other changes in the pipeline.  They are independent.
+
+    Another type of pipeline that is independent is a post-merge
+    pipeline. In that case, the changes have already merged, so the
+    results can not affect any other events in the pipeline.
+
+  *DependentPipelineManager*
+    The dependent pipeline manager is designed for gating.  It ensures
+    that every change is tested exactly as it is going to be merged
+    into the repository.  An ideal gating system would test one change
+    at a time, applied to the tip of the repository, and only if that
+    change passed tests would it be merged.  Then the next change in
+    line would be tested the same way.  In order to achieve parallel
+    testing of changes, the dependent pipeline manager performs
+    speculative execution on changes.  It orders changes based on
+    their entry into the pipeline.  It begins testing all changes in
+    parallel, assuming that each change ahead in the pipeline will pass
+    its tests.  If they all succeed, all the changes can be tested and
+    merged in parallel.  If a change near the front of the pipeline
+    fails its tests, each change behind it ignores whatever tests have
+    been completed and are tested again without the change in front.
+    This way gate tests may run in parallel but still be tested
+    correctly, exactly as they will appear in the repository when
+    merged.
+
+    One important characteristic of the DependentPipelineManager is that
+    it analyzes the jobs that are triggered by different projects, and
+    if those projects have jobs in common, it treats those projects as
+    related, and they share a single virtual queue of changes.  Thus,
+    if there is a job that performs integration testing on two
+    projects, those two projects will automatically share a virtual
+    change queue.  If a third project does not invoke that job, it
+    will be part of a separate virtual change queue, and changes to
+    it will not depend on changes to the first two jobs.
+
+    For more detail on the theory and operation of Zuul's
+    DependentPipelineManager, see: :doc:`gating`.
+
+**trigger**
+  At least one trigger source must be supplied for each pipeline.
+  Triggers are not exclusive -- matching events may be placed in
+  multiple pipelines, and they will behave independently in each of
+  the pipelines they match.
+
+  Triggers are loaded from their connection name. The driver type of
+  the connection will dictate which options are available.
+  See :doc:`triggers`.
+
+**require**
+  If this section is present, it established pre-requisites for any
+  kind of item entering the Pipeline.  Regardless of how the item is
+  to be enqueued (via any trigger or automatic dependency resolution),
+  the conditions specified here must be met or the item will not be
+  enqueued.
+
+.. _pipeline-require-approval:
+
+  **approval**
+  This requires that a certain kind of approval be present for the
+  current patchset of the change (the approval could be added by the
+  event in question).  It takes several sub-parameters, all of which
+  are optional and are combined together so that there must be an
+  approval matching all specified requirements.
+
+    *username*
+    If present, an approval from this username is required.
+
+    *email*
+    If present, an approval with this email address is required.  It
+    is treated as a regular expression as above.
+
+    *email-filter* (deprecated)
+    A deprecated alternate spelling of *email*.  Only one of *email* or
+    *email_filter* should be used.
+
+    *older-than*
+    If present, the approval must be older than this amount of time
+    to match.  Provide a time interval as a number with a suffix of
+    "w" (weeks), "d" (days), "h" (hours), "m" (minutes), "s"
+    (seconds).  Example ``48h`` or ``2d``.
+
+    *newer-than*
+    If present, the approval must be newer than this amount of time
+    to match.  Same format as "older-than".
+
+    Any other field is interpreted as a review category and value
+    pair.  For example ``verified: 1`` would require that the approval
+    be for a +1 vote in the "Verified" column.  The value may either
+    be a single value or a list: ``verified: [1, 2]`` would match
+    either a +1 or +2 vote.
+
+  **open**
+  A boolean value (``true`` or ``false``) that indicates whether the change
+  must be open or closed in order to be enqueued.
+
+  **current-patchset**
+  A boolean value (``true`` or ``false``) that indicates whether the change
+  must be the current patchset in order to be enqueued.
+
+  **status**
+  A string value that corresponds with the status of the change
+  reported by the trigger.  For example, when using the Gerrit
+  trigger, status values such as ``NEW`` or ``MERGED`` may be useful.
+
+**reject**
+  If this section is present, it establishes pre-requisites that can
+  block an item from being enqueued. It can be considered a negative
+  version of **require**.
+
+  **approval**
+  This takes a list of approvals. If an approval matches the provided
+  criteria the change can not be entered into the pipeline. It follows
+  the same syntax as the :ref:`"require approval" pipeline above
+  <pipeline-require-approval>`.
+
+  Example to reject a change with any negative vote::
+
+    reject:
+      approval:
+        - code-review: [-1, -2]
+
+**dequeue-on-new-patchset**
+  Normally, if a new patchset is uploaded to a change that is in a
+  pipeline, the existing entry in the pipeline will be removed (with
+  jobs canceled and any dependent changes that can no longer merge as
+  well.  To suppress this behavior (and allow jobs to continue
+  running), set this to ``false``.  Default: ``true``.
+
+**ignore-dependencies**
+  In any kind of pipeline (dependent or independent), Zuul will
+  attempt to enqueue all dependencies ahead of the current change so
+  that they are tested together (independent pipelines report the
+  results of each change regardless of the results of changes ahead).
+  To ignore dependencies completely in an independent pipeline, set
+  this to ``true``.  This option is ignored by dependent pipelines.
+  The default is: ``false``.
+
+**success**
+  Describes where Zuul should report to if all the jobs complete
+  successfully.
+  This section is optional; if it is omitted, Zuul will run jobs and
+  do nothing on success; it will not even report a message to Gerrit.
+  If the section is present, the listed reporter plugins will be
+  asked to report on the jobs.
+  The reporters are listed by their connection name. The options
+  available depend on the driver for the supplied connection.
+  See :doc:`reporters` for more details.
+
+**failure**
+  Uses the same syntax as **success**, but describes what Zuul should
+  do if at least one job fails.
+
+**merge-failure**
+  Uses the same syntax as **success**, but describes what Zuul should
+  do if it is unable to merge in the patchset. If no merge-failure
+  reporters are listed then the ``failure`` reporters will be used to
+  notify of unsuccessful merges.
+
+**start**
+  Uses the same syntax as **success**, but describes what Zuul should
+  do when a change is added to the pipeline manager.  This can be used,
+  for example, to reset the value of the Verified review category.
+
+**disabled**
+  Uses the same syntax as **success**, but describes what Zuul should
+  do when a pipeline is disabled.
+  See ``disable-after-consecutive-failures``.
+
+**disable-after-consecutive-failures**
+  If set, a pipeline can enter a ''disabled'' state if too many changes
+  in a row fail. When this value is exceeded the pipeline will stop
+  reporting to any of the ``success``, ``failure`` or ``merge-failure``
+  reporters and instead only report to the ``disabled`` reporters.
+  (No ``start`` reports are made when a pipeline is disabled).
+
+**precedence**
+  Indicates how the build scheduler should prioritize jobs for
+  different pipelines.  Each pipeline may have one precedence, jobs
+  for pipelines with a higher precedence will be run before ones with
+  lower.  The value should be one of ``high``, ``normal``, or ``low``.
+  Default: ``normal``.
+
+**window**
+  DependentPipelineManagers only. Zuul can rate limit
+  DependentPipelineManagers in a manner similar to TCP flow control.
+  Jobs are only started for changes in the queue if they sit in the
+  actionable window for the pipeline. The initial length of this window
+  is configurable with this value. The value given should be a positive
+  integer value. A value of ``0`` disables rate limiting on the
+  DependentPipelineManager.
+  Default: ``20``.
+
+**window-floor**
+  DependentPipelineManagers only. This is the minimum value for the
+  window described above. Should be a positive non zero integer value.
+  Default: ``3``.
+
+**window-increase-type**
+  DependentPipelineManagers only. This value describes how the window
+  should grow when changes are successfully merged by zuul. A value of
+  ``linear`` indicates that ``window-increase-factor`` should be added
+  to the previous window value. A value of ``exponential`` indicates
+  that ``window-increase-factor`` should be multiplied against the
+  previous window value and the result will become the window size.
+  Default: ``linear``.
+
+**window-increase-factor**
+  DependentPipelineManagers only. The value to be added or multiplied
+  against the previous window value to determine the new window after
+  successful change merges.
+  Default: ``1``.
+
+**window-decrease-type**
+  DependentPipelineManagers only. This value describes how the window
+  should shrink when changes are not able to be merged by Zuul. A value
+  of ``linear`` indicates that ``window-decrease-factor`` should be
+  subtracted from the previous window value. A value of ``exponential``
+  indicates that ``window-decrease-factor`` should be divided against
+  the previous window value and the result will become the window size.
+  Default: ``exponential``.
+
+**window-decrease-factor**
+  DependentPipelineManagers only. The value to be subtracted or divided
+  against the previous window value to determine the new window after
+  unsuccessful change merges.
+  Default: ``2``.
+
+Some example pipeline configurations are included in the sample layout
+file.  The first is called a *check* pipeline::
+
+  - name: check
+    manager: IndependentPipelineManager
+    trigger:
+      my_gerrit:
+        - event: patchset-created
+    success:
+      my_gerrit:
+        verified: 1
+    failure:
+      my_gerrit:
+        verified: -1
+
+This will trigger jobs each time a new patchset (or change) is
+uploaded to Gerrit, and report +/-1 values to Gerrit in the
+``verified`` review category. ::
+
+  - name: gate
+    manager: DependentPipelineManager
+    trigger:
+      my_gerrit:
+        - event: comment-added
+          approval:
+            - approved: 1
+    success:
+      my_gerrit:
+        verified: 2
+        submit: true
+    failure:
+      my_gerrit:
+        verified: -2
+
+This will trigger jobs whenever a reviewer leaves a vote of ``1`` in the
+``approved`` review category in Gerrit (a non-standard category).
+Changes will be tested in such a way as to guarantee that they will be
+merged exactly as tested, though that will happen in parallel by
+creating a virtual queue of dependent changes and performing
+speculative execution of jobs. ::
+
+  - name: post
+    manager: IndependentPipelineManager
+    trigger:
+      my_gerrit:
+        - event: ref-updated
+          ref: ^(?!refs/).*$
+
+This will trigger jobs whenever a change is merged to a named branch
+(e.g., ``master``).  No output will be reported to Gerrit.  This is
+useful for side effects such as creating per-commit tarballs. ::
+
+  - name: silent
+    manager: IndependentPipelineManager
+    trigger:
+      my_gerrit:
+        - event: patchset-created
+
+This also triggers jobs when changes are uploaded to Gerrit, but no
+results are reported to Gerrit.  This is useful for jobs that are in
+development and not yet ready to be presented to developers. ::
+
+  pipelines:
+    - name: post-merge
+      manager: IndependentPipelineManager
+      trigger:
+        my_gerrit:
+          - event: change-merged
+      success:
+        my_gerrit:
+          force-message: True
+      failure:
+        my_gerrit:
+          force-message: True
+
+The ``change-merged`` events happen when a change has been merged in the git
+repository. The change is thus closed and Gerrit will not accept modifications
+to the review scoring such as ``code-review`` or ``verified``. By using the
+``force-message: True`` parameter, Zuul will pass ``--force-message`` to the
+``gerrit review`` command, thus making sure the message is actually
+sent back to Gerrit regardless of approval scores.
+That kind of pipeline is nice to run regression or performance tests.
+
+.. note::
+  The ``change-merged`` event does not include the commit sha1 which can be
+  hazardous, it would let you report back to Gerrit though.  If you were to
+  build a tarball for a specific commit, you should consider instead using
+  the ``ref-updated`` event which does include the commit sha1 (but lacks the
+  Gerrit change number).
+
+
+.. _jobs:
+
+Jobs
+""""
+
+The jobs section is optional, and can be used to set attributes of
+jobs that are independent of their association with a project.  For
+example, if a job should return a customized message on failure, that
+may be specified here.  Otherwise, Zuul does not need to be told about
+each job as it builds a list from the project specification.
+
+**name**
+  The name of the job.  This field is treated as a regular expression
+  and will be applied to each job that matches.
+
+**queue-name (optional)**
+  Zuul will automatically combine projects that share a job into
+  shared change queues for dependent pipeline managers.  In order to
+  report statistics about these queues, it is convenient for them to
+  have names.  Zuul can automatically name change queues, however
+  these can grow quite long and are prone to changing as projects in
+  the queue change.  If you assign a queue-name to a job, Zuul will
+  use that as the name for the shared change queue that contains that
+  job instead of the automatically generated one.  It is an error for
+  a shared change queue to have more than one job with a queue-name if
+  they are not the same.
+
+**failure-message (optional)**
+  The message that should be reported to Gerrit if the job fails.
+
+**success-message (optional)**
+  The message that should be reported to Gerrit if the job fails.
+
+**failure-pattern (optional)**
+  The URL that should be reported to Gerrit if the job fails.
+  Defaults to the build URL or the url_pattern configured in
+  zuul.conf.  May be supplied as a string pattern with substitutions
+  as described in url_pattern in :ref:`zuulconf`.
+
+**success-pattern (optional)**
+  The URL that should be reported to Gerrit if the job succeeds.
+  Defaults to the build URL or the url_pattern configured in
+  zuul.conf.  May be supplied as a string pattern with substitutions
+  as described in url_pattern in :ref:`zuulconf`.
+
+**hold-following-changes (optional)**
+  This is a boolean that indicates that changes that follow this
+  change in a dependent change pipeline should wait until this job
+  succeeds before launching.  If this is applied to a very short job
+  that can predict whether longer jobs will fail early, this can be
+  used to reduce the number of jobs that Zuul will launch and
+  ultimately have to cancel.  In that case, a small amount of
+  parallelization of jobs is traded for more efficient use of testing
+  resources.  On the other hand, to apply this to a long running job
+  would largely defeat the parallelization of dependent change testing
+  that is the main feature of Zuul.  Default: ``false``.
+
+**mutex (optional)**
+  This is a string that names a mutex that should be observed by this
+  job.  Only one build of any job that references the same named mutex
+  will be enqueued at a time.  This applies across all pipelines.
+
+**branch (optional)**
+  This job should only be run on matching branches.  This field is
+  treated as a regular expression and multiple branches may be
+  listed.
+
+**files (optional)**
+  This job should only be run if at least one of the files involved in
+  the change (added, deleted, or modified) matches at least one of the
+  file patterns listed here.  This field is treated as a regular
+  expression and multiple expressions may be listed.
+
+**skip-if (optional)**
+
+  This job should not be run if all the patterns specified by the
+  optional fields listed below match on their targets.  When multiple
+  sets of parameters are provided, this job will be skipped if any set
+  matches.  For example: ::
+
+    jobs:
+      - name: check-tempest-dsvm-neutron
+        skip-if:
+          - project: ^openstack/neutron$
+            branch: ^stable/juno$
+            all-files-match-any:
+              - ^neutron/tests/.*$
+              - ^tools/.*$
+          - all-files-match-any:
+              - ^doc/.*$
+              - ^.*\.rst$
+
+  With this configuration, the job would be skipped for a neutron
+  patchset for the stable/juno branch provided that every file in the
+  change matched at least one of the specified file regexes.  The job
+  will also be skipped for any patchset that modified only the doc
+  tree or rst files.
+
+  *project* (optional)
+    The regular expression to match against the project of the change.
+
+  *branch* (optional)
+    The regular expression to match against the branch or ref of the
+    change.
+
+  *all-files-match-any* (optional)
+    A list of regular expressions intended to match the files involved
+    in the change.  This parameter will be considered matching a
+    change only if all files in a change match at least one of these
+    expressions.
+
+    The pattern for '/COMMIT_MSG' is always matched on and does not
+    have to be included.
+
+**voting (optional)**
+  Boolean value (``true`` or ``false``) that indicates whatever
+  a job is voting or not.  Default: ``true``.
+
+**tags (optional)**
+  A list of arbitrary strings which will be associated with the job.
+  Can be used by the parameter-function to alter behavior based on
+  their presence on a job.  If the job name is a regular expression,
+  tags will accumulate on jobs that match.
+
+**parameter-function (optional)**
+  Specifies a function that should be applied to the parameters before
+  the job is launched.  The function should be defined in a python file
+  included with the :ref:`includes` directive.  The function
+  should have the following signature:
+
+  .. function:: parameters(item, job, parameters)
+
+     Manipulate the parameters passed to a job before a build is
+     launched.  The ``parameters`` dictionary will already contain the
+     standard Zuul job parameters, and is expected to be modified
+     in-place.
+
+     :param item: the current queue item
+     :type item: zuul.model.QueueItem
+     :param job: the job about to be run
+     :type job: zuul.model.Job
+     :param parameters: parameters to be passed to the job
+     :type parameters: dict
+
+  If the parameter **ZUUL_NODE** is set by this function, then it will
+  be used to specify on what node (or class of node) the job should be
+  run.
+
+**swift**
+  If :ref:`swift` is configured then each job can define a destination
+  container for the builder to place logs and/or assets into. Multiple
+  containers can be listed for each job by providing a unique ``name``.
+
+  *name*
+    Set an identifying name for the container. This is used in the
+    parameter key sent to the builder. For example if it ``logs`` then
+    one of the parameters sent will be ``SWIFT_logs_CONTAINER``
+    (case-sensitive).
+
+  Each of the defaults defined in :ref:`swift` can be overwritten as:
+
+  *container* (optional)
+    Container name to place the log into
+    ``For example, logs``
+
+  *expiry* (optional)
+    How long the signed destination should be available for
+
+  *max-file-size** (optional)
+    The maximum size of an individual file
+
+  *max_file_size** (optional, deprecated)
+    A deprecated alternate spelling of *max-file-size*.
+
+  *max-file-count* (optional)
+    The maximum number of separate files to allow
+
+  *max_file_count* (optional, deprecated)
+    A deprecated alternate spelling of *max-file-count*.
+
+  *logserver-prefix*
+    Provide a URL to the CDN or logserver app so that a worker knows
+    what URL to return.
+    ``For example: http://logs.example.org/server.app?obj=``
+    The worker should return the logserver-prefix url and the object
+    path as the URL in the results data packet.
+
+  *logserver_prefix* (deprecated)
+    A deprecated alternate spelling of *logserver-prefix*.
+
+Here is an example of setting the failure message for jobs that check
+whether a change merges cleanly::
+
+  - name: ^.*-merge$
+    failure-message: This change or one of its cross-repo dependencies
+    was unable to be automatically merged with the current state of
+    its repository. Please rebase the change and upload a new
+    patchset.
+
+Projects
+""""""""
+
+The projects section indicates what jobs should be run in each pipeline
+for events associated with each project.  It contains a list of
+projects.  Here is an example::
+
+  - name: example/project
+    check:
+      - project-merge:
+        - project-unittest
+        - project-pep8
+        - project-pyflakes
+    gate:
+      - project-merge:
+        - project-unittest
+        - project-pep8
+        - project-pyflakes
+    post:
+      - project-publish
+
+**name**
+  The name of the project (as known by Gerrit).
+
+**merge-mode (optional)**
+  An optional value that indicates what strategy should be used to
+  merge changes to this project.  Supported values are:
+
+  ** merge-resolve **
+  Equivalent to 'git merge -s resolve'.  This corresponds closely to
+  what Gerrit performs (using JGit) for a project if the "Merge if
+  necessary" merge mode is selected and "Automatically resolve
+  conflicts" is checked.  This is the default.
+
+  ** merge **
+  Equivalent to 'git merge'.
+
+  ** cherry-pick **
+  Equivalent to 'git cherry-pick'.
+
+This is followed by a section for each of the pipelines defined above.
+Pipelines may be omitted if no jobs should run for this project in a
+given pipeline.  Within the pipeline section, the jobs that should be
+executed are listed.  If a job is entered as a dictionary key, then
+jobs contained within that key are only executed if the key job
+succeeds.  In the above example, project-unittest, project-pep8, and
+project-pyflakes are only executed if project-merge succeeds.  This
+can help avoid running unnecessary jobs.
+
+The special job named ``noop`` is internal to Zuul and will always
+return ``SUCCESS`` immediately.  This can be useful if you require
+that all changes be processed by a pipeline but a project has no jobs
+that can be run on it.
+
+.. seealso:: The OpenStack Zuul configuration for a comprehensive example: https://git.openstack.org/cgit/openstack-infra/project-config/tree/zuul/layout.yaml
+
+Project Templates
+"""""""""""""""""
+
+Whenever you have lot of similar projects (such as plugins for a project) you
+will most probably want to use the same pipeline configurations.  The
+project templates let you define pipelines and job name templates to trigger.
+One can then just apply the template on its project which make it easier to
+update several similar projects. As an example::
+
+  project-templates:
+    # Name of the template
+    - name: plugin-triggering
+      # Definition of pipelines just like for a `project`
+      check:
+       - '{jobprefix}-merge':
+         - '{jobprefix}-pep8'
+         - '{jobprefix}-pyflakes'
+      gate:
+       - '{jobprefix}-merge':
+         - '{jobprefix}-unittest'
+         - '{jobprefix}-pep8'
+         - '{jobprefix}-pyflakes'
+
+In your projects definition, you will then apply the template using the template
+key::
+
+  projects:
+   - name: plugin/foobar
+     template:
+      - name: plugin-triggering
+        jobprefix: plugin-foobar
+
+You can pass several parameters to a template. A ``parameter`` value
+will be used for expansion of ``{parameter}`` in the template
+strings. The parameter ``name`` will be automatically provided and
+will contain the short name of the project, that is the portion of the
+project name after the last ``/`` character.
+
+Multiple templates can be combined in a project, and the jobs from all
+of those templates will be added to the project.  Individual jobs may
+also be added::
+
+  projects:
+   - name: plugin/foobar
+     template:
+      - name: plugin-triggering
+        jobprefix: plugin-foobar
+      - name: plugin-extras
+        jobprefix: plugin-foobar
+     check:
+      - foobar-extra-special-job
+
+Individual jobs may optionally be added to pipelines (e.g. check,
+gate, et cetera) for a project, in addition to those provided by
+templates.
+
+The order of the jobs listed in the project (which only affects the
+order of jobs listed on the report) will be the jobs from each
+template in the order listed, followed by any jobs individually listed
+for the project.
+
+Note that if multiple templates are used for a project and one
+template specifies a job that is also specified in another template,
+or specified in the project itself, the configuration defined by
+either the last template or the project itself will take priority.
+
+logging.conf
+~~~~~~~~~~~~
+This file is optional.  If provided, it should be a standard
+:mod:`logging.config` module configuration file.  If not present, Zuul will
+output all log messages of DEBUG level or higher to the console.
+
+Starting Zuul
+-------------
+
+To start Zuul, run **zuul-server**::
+
+  usage: zuul-server [-h] [-c CONFIG] [-l LAYOUT] [-d] [-t] [--version]
+
+  Project gating system.
+
+  optional arguments:
+    -h, --help  show this help message and exit
+    -c CONFIG   specify the config file
+    -l LAYOUT   specify the layout file
+    -d          do not run as a daemon
+    -t          validate layout file syntax
+    --version   show zuul version
+
+You may want to use the ``-d`` argument while you are initially setting
+up Zuul so you can detect any configuration errors quickly.  Under
+normal operation, omit ``-d`` and let Zuul run as a daemon.
+
+If you send signal 1 (SIGHUP) to the zuul-server process, Zuul will
+stop executing new jobs, wait until all executing jobs are finished,
+reload its layout.yaml, and resume. Changes to any connections or
+the PID  file will be ignored until Zuul is restarted.
+
+If you send a SIGUSR1 to the zuul-server process, Zuul will stop
+executing new jobs, wait until all executing jobs are finished,
+then exit. While waiting to exit Zuul will queue Gerrit events and
+save these events prior to exiting. When Zuul starts again it will
+read these saved events and act on them.
+
+If you need to abort Zuul and intend to manually requeue changes for
+jobs which were running in its pipelines, prior to terminating you can
+use the zuul-changes.py tool script to simplify the process. For
+example, this would give you a list of zuul-enqueue commands to requeue
+changes for the gate and check pipelines respectively::
+
+  ./tools/zuul-changes.py http://zuul.openstack.org/ gate
+  ./tools/zuul-changes.py http://zuul.openstack.org/ check
+
+If you send a SIGUSR2 to the zuul-server process, or the forked process
+that runs the Gearman daemon, Zuul will dump a stack trace for each
+running thread into its debug log. It is written under the log bucket
+``zuul.stack_dump``.  This is useful for tracking down deadlock or
+otherwise slow threads.
+
+When `yappi <https://code.google.com/p/yappi/>`_ (Yet Another Python
+Profiler) is available, additional functions' and threads' stats are
+emitted as well. The first SIGUSR2 will enable yappi, on the second
+SIGUSR2 it dumps the information collected, resets all yappi state and
+stops profiling. This is to minimize the impact of yappi on a running
+system.
diff --git a/etc/clonemap.yaml-sample b/etc/clonemap.yaml-sample
index 9695d9d..69498e0 100644
--- a/etc/clonemap.yaml-sample
+++ b/etc/clonemap.yaml-sample
@@ -1,16 +1,16 @@
-# vim: ft=yaml
-#
-# Example clone map for Zuul cloner
-#
-# By default it would clone projects under the directory specified by its
-# option --basepath, but you can override this behavior by definining per
-# project destinations.
-clonemap:
-
- # Clone project 'mediawiki/core' directly in {basepath}
- - name: 'mediawiki/core'
-   dest: '.'
-
- # Clone projects below mediawiki/extensions to {basepath}/extensions/
- - name: 'mediawiki/extensions/(.*)'
-   dest: 'extensions/\1'
+# vim: ft=yaml
+#
+# Example clone map for Zuul cloner
+#
+# By default it would clone projects under the directory specified by its
+# option --basepath, but you can override this behavior by definining per
+# project destinations.
+clonemap:
+
+ # Clone project 'mediawiki/core' directly in {basepath}
+ - name: 'mediawiki/core'
+   dest: '.'
+
+ # Clone projects below mediawiki/extensions to {basepath}/extensions/
+ - name: 'mediawiki/extensions/(.*)'
+   dest: 'extensions/\1'
diff --git a/etc/layout.yaml-sample b/etc/layout.yaml-sample
index 53f6ba1..6953652 100644
--- a/etc/layout.yaml-sample
+++ b/etc/layout.yaml-sample
@@ -1,67 +1,67 @@
-pipelines:
-  - name: check
-    manager: IndependentPipelineManager
-    trigger:
-      gerrit:
-        - event: patchset-created
-    success:
-      gerrit:
-        verified: 1
-    failure:
-     gerrit:
-        verified: -1
-
-  - name: tests
-    manager: IndependentPipelineManager
-    trigger:
-      gerrit:
-        - event: patchset-created
-          email_filter: ^.*@example.org$
-    success:
-      gerrit:
-        verified: 1
-    failure:
-      gerrit:
-        verified: -1
-
-  - name: post
-    manager: IndependentPipelineManager
-    trigger:
-      gerrit:
-        - event: ref-updated
-          ref: ^(?!refs/).*$
-          ignore-deletes: False
-
-  - name: gate
-    manager: DependentPipelineManager
-    trigger:
-      gerrit:
-        - event: comment-added
-          approval:
-            - approved: 1
-    start:
-      gerrit:
-        verified: 0
-    success:
-      gerrit:
-        verified: 1
-    failure:
-      gerrit:
-        verified: -1
-
-jobs:
-  - name: ^.*-merge$
-    failure-message: Unable to merge change, please rebase and try again.
-
-projects:
-  - name: example/project
-    check:
-      - project-merge
-    tests:
-      - project-merge:
-        - project-test
-    gate:
-      - project-merge:
-        - project-test
-    post:
-      - project-publish
+pipelines:
+  - name: check
+    manager: IndependentPipelineManager
+    trigger:
+      gerrit:
+        - event: patchset-created
+    success:
+      gerrit:
+        verified: 1
+    failure:
+     gerrit:
+        verified: -1
+
+  - name: tests
+    manager: IndependentPipelineManager
+    trigger:
+      gerrit:
+        - event: patchset-created
+          email_filter: ^.*@example.org$
+    success:
+      gerrit:
+        verified: 1
+    failure:
+      gerrit:
+        verified: -1
+
+  - name: post
+    manager: IndependentPipelineManager
+    trigger:
+      gerrit:
+        - event: ref-updated
+          ref: ^(?!refs/).*$
+          ignore-deletes: False
+
+  - name: gate
+    manager: DependentPipelineManager
+    trigger:
+      gerrit:
+        - event: comment-added
+          approval:
+            - approved: 1
+    start:
+      gerrit:
+        verified: 0
+    success:
+      gerrit:
+        verified: 1
+    failure:
+      gerrit:
+        verified: -1
+
+jobs:
+  - name: ^.*-merge$
+    failure-message: Unable to merge change, please rebase and try again.
+
+projects:
+  - name: example/project
+    check:
+      - project-merge
+    tests:
+      - project-merge:
+        - project-test
+    gate:
+      - project-merge:
+        - project-test
+    post:
+      - project-publish
diff --git a/etc/logging.conf-sample b/etc/logging.conf-sample
index 8b76da2..6bd46cb 100644
--- a/etc/logging.conf-sample
+++ b/etc/logging.conf-sample
@@ -1,44 +1,44 @@
-[loggers]
-keys=root,zuul,gerrit
-
-[handlers]
-keys=console,debug,normal
-
-[formatters]
-keys=simple
-
-[logger_root]
-level=WARNING
-handlers=console
-
-[logger_zuul]
-level=DEBUG
-handlers=debug,normal
-qualname=zuul
-
-[logger_gerrit]
-level=DEBUG
-handlers=debug,normal
-qualname=gerrit
-
-[handler_console]
-level=WARNING
-class=StreamHandler
-formatter=simple
-args=(sys.stdout,)
-
-[handler_debug]
-level=DEBUG
-class=logging.handlers.TimedRotatingFileHandler
-formatter=simple
-args=('/var/log/zuul/debug.log', 'midnight', 1, 30,)
-
-[handler_normal]
-level=INFO
-class=logging.handlers.TimedRotatingFileHandler
-formatter=simple
-args=('/var/log/zuul/zuul.log', 'midnight', 1, 30,)
-
-[formatter_simple]
-format=%(asctime)s %(levelname)s %(name)s: %(message)s
-datefmt=
+[loggers]
+keys=root,zuul,gerrit
+
+[handlers]
+keys=console,debug,normal
+
+[formatters]
+keys=simple
+
+[logger_root]
+level=WARNING
+handlers=console
+
+[logger_zuul]
+level=DEBUG
+handlers=debug,normal
+qualname=zuul
+
+[logger_gerrit]
+level=DEBUG
+handlers=debug,normal
+qualname=gerrit
+
+[handler_console]
+level=WARNING
+class=StreamHandler
+formatter=simple
+args=(sys.stdout,)
+
+[handler_debug]
+level=DEBUG
+class=logging.handlers.TimedRotatingFileHandler
+formatter=simple
+args=('/var/log/zuul/debug.log', 'midnight', 1, 30,)
+
+[handler_normal]
+level=INFO
+class=logging.handlers.TimedRotatingFileHandler
+formatter=simple
+args=('/var/log/zuul/zuul.log', 'midnight', 1, 30,)
+
+[formatter_simple]
+format=%(asctime)s %(levelname)s %(name)s: %(message)s
+datefmt=
diff --git a/etc/status/.gitignore b/etc/status/.gitignore
index 218f297..9db4d6f 100644
--- a/etc/status/.gitignore
+++ b/etc/status/.gitignore
@@ -1 +1 @@
-public_html/lib
+public_html/lib
diff --git a/etc/status/.jshintignore b/etc/status/.jshintignore
index 218f297..9db4d6f 100644
--- a/etc/status/.jshintignore
+++ b/etc/status/.jshintignore
@@ -1 +1 @@
-public_html/lib
+public_html/lib
diff --git a/etc/status/.jshintrc b/etc/status/.jshintrc
index 15bd571..3ec2fce 100644
--- a/etc/status/.jshintrc
+++ b/etc/status/.jshintrc
@@ -1,21 +1,21 @@
-{
-    "bitwise": true,
-    "eqeqeq": true,
-    "forin": true,
-    "latedef": true,
-    "newcap": true,
-    "noarg": true,
-    "noempty": true,
-    "nonew": true,
-    "undef": true,
-    "unused": true,
-
-    "strict": false,
-    "laxbreak": true,
-    "browser": true,
-
-    "predef": [
-        "jQuery",
-        "zuul"
-    ]
-}
+{
+    "bitwise": true,
+    "eqeqeq": true,
+    "forin": true,
+    "latedef": true,
+    "newcap": true,
+    "noarg": true,
+    "noempty": true,
+    "nonew": true,
+    "undef": true,
+    "unused": true,
+
+    "strict": false,
+    "laxbreak": true,
+    "browser": true,
+
+    "predef": [
+        "jQuery",
+        "zuul"
+    ]
+}
diff --git a/etc/status/README.rst b/etc/status/README.rst
index 762b49c..dde5ccb 100644
--- a/etc/status/README.rst
+++ b/etc/status/README.rst
@@ -1,27 +1,27 @@
-Zuul Status
-====
-
-Zuul Status is a web portal for a Zuul server.
-
-Set up
-------------
-
-The markup generated by the javascript is fairly generic so it should be easy
-to drop into an existing portal. All it needs is
-``<div id="id="zuul-container"></div>``.
-
-Having said that, the markup is optimised for Twitter Bootstrap, though it in
-no way depends on Boostrap and any element using a bootstrap class has a
-``zuul-`` prefixed class alongside it.
-
-The script depends on jQuery (tested with version 1.8 and 1.9).
-
-The script optimises updates by stopping when the page is not visible.
-This is done by listerning to ``show`` and ``hide`` events emitted by the
-Page Visibility plugin for jQuery. If you don't want to load this plugin you
-can undo undo this optimisation by removing the code at the bottom of
-``index.html``
-
-To automatically fetch the latest versions of jQuery, the Page Visibility
-plugin and Twitter Boostrap, run the ``fetch-dependencies.sh`` script.
-The default ``index.html`` references these.
+Zuul Status
+====
+
+Zuul Status is a web portal for a Zuul server.
+
+Set up
+------------
+
+The markup generated by the javascript is fairly generic so it should be easy
+to drop into an existing portal. All it needs is
+``<div id="id="zuul-container"></div>``.
+
+Having said that, the markup is optimised for Twitter Bootstrap, though it in
+no way depends on Boostrap and any element using a bootstrap class has a
+``zuul-`` prefixed class alongside it.
+
+The script depends on jQuery (tested with version 1.8 and 1.9).
+
+The script optimises updates by stopping when the page is not visible.
+This is done by listerning to ``show`` and ``hide`` events emitted by the
+Page Visibility plugin for jQuery. If you don't want to load this plugin you
+can undo undo this optimisation by removing the code at the bottom of
+``index.html``
+
+To automatically fetch the latest versions of jQuery, the Page Visibility
+plugin and Twitter Boostrap, run the ``fetch-dependencies.sh`` script.
+The default ``index.html`` references these.
diff --git a/etc/status/fetch-dependencies.sh b/etc/status/fetch-dependencies.sh
index ccaf74c..5e5da9c 100755
--- a/etc/status/fetch-dependencies.sh
+++ b/etc/status/fetch-dependencies.sh
@@ -1,23 +1,23 @@
-#!/bin/bash
-BASE_DIR=$(cd $(dirname $0); pwd)
-DEST_DIR=$BASE_DIR/public_html/lib
-mkdir -p $DEST_DIR
-echo "Destination: $DEST_DIR"
-
-echo "Fetching jquery.min.js..."
-curl -L --silent http://code.jquery.com/jquery.min.js > $DEST_DIR/jquery.min.js
-
-echo "Fetching jquery-visibility.min.js..."
-curl -L --silent https://raw.githubusercontent.com/mathiasbynens/jquery-visibility/master/jquery-visibility.js > $DEST_DIR/jquery-visibility.js
-
-echo "Fetching jquery.graphite.js..."
-curl -L --silent https://github.com/prestontimmons/graphitejs/archive/master.zip > jquery-graphite.zip
-unzip -q -o jquery-graphite.zip -d $DEST_DIR/
-mv $DEST_DIR/graphitejs-master/jquery.graphite.js $DEST_DIR/
-rm -R jquery-graphite.zip $DEST_DIR/graphitejs-master
-
-echo "Fetching bootstrap..."
-curl -L --silent https://github.com/twbs/bootstrap/releases/download/v3.1.1/bootstrap-3.1.1-dist.zip > bootstrap.zip
-unzip -q -o bootstrap.zip -d $DEST_DIR/
-mv $DEST_DIR/bootstrap-3.1.1-dist $DEST_DIR/bootstrap
-rm bootstrap.zip
+#!/bin/bash
+BASE_DIR=$(cd $(dirname $0); pwd)
+DEST_DIR=$BASE_DIR/public_html/lib
+mkdir -p $DEST_DIR
+echo "Destination: $DEST_DIR"
+
+echo "Fetching jquery.min.js..."
+curl -L --silent http://code.jquery.com/jquery.min.js > $DEST_DIR/jquery.min.js
+
+echo "Fetching jquery-visibility.min.js..."
+curl -L --silent https://raw.githubusercontent.com/mathiasbynens/jquery-visibility/master/jquery-visibility.js > $DEST_DIR/jquery-visibility.js
+
+echo "Fetching jquery.graphite.js..."
+curl -L --silent https://github.com/prestontimmons/graphitejs/archive/master.zip > jquery-graphite.zip
+unzip -q -o jquery-graphite.zip -d $DEST_DIR/
+mv $DEST_DIR/graphitejs-master/jquery.graphite.js $DEST_DIR/
+rm -R jquery-graphite.zip $DEST_DIR/graphitejs-master
+
+echo "Fetching bootstrap..."
+curl -L --silent https://github.com/twbs/bootstrap/releases/download/v3.1.1/bootstrap-3.1.1-dist.zip > bootstrap.zip
+unzip -q -o bootstrap.zip -d $DEST_DIR/
+mv $DEST_DIR/bootstrap-3.1.1-dist $DEST_DIR/bootstrap
+rm bootstrap.zip
diff --git a/etc/status/public_html/index.html b/etc/status/public_html/index.html
index 97025a6..8533ca5 100644
--- a/etc/status/public_html/index.html
+++ b/etc/status/public_html/index.html
@@ -1,37 +1,37 @@
-<!--
-Copyright 2013 OpenStack Foundation
-Copyright 2013 Timo Tijhof
-Copyright 2013 Wikimedia Foundation
-
-Licensed under the Apache License, Version 2.0 (the "License"); you may
-not use this file except in compliance with the License. You may obtain
-a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
-Unless required by applicable law or agreed to in writing, software
-distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-License for the specific language governing permissions and limitations
-under the License.
--->
-<!DOCTYPE html>
-<html dir="ltr" lang="en">
-<head>
-    <title>Zuul Status</title>
-    <link rel="stylesheet" href="lib/bootstrap/css/bootstrap.min.css">
-    <link rel="stylesheet" href="styles/zuul.css" />
-</head>
-<body>
-    <div id="zuul_container"></div>
-    <script src="lib/jquery.min.js"></script>
-    <script src="lib/jquery-visibility.js"></script>
-    <script src="lib/jquery.graphite.js"></script>
-    <script src="jquery.zuul.js"></script>
-    <script src="zuul.app.js"></script>
-    <script>
-        zuul_build_dom(jQuery, '#zuul_container');
-        zuul_start(jQuery);
-    </script>
-</body>
-</html>
+<!--
+Copyright 2013 OpenStack Foundation
+Copyright 2013 Timo Tijhof
+Copyright 2013 Wikimedia Foundation
+
+Licensed under the Apache License, Version 2.0 (the "License"); you may
+not use this file except in compliance with the License. You may obtain
+a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+License for the specific language governing permissions and limitations
+under the License.
+-->
+<!DOCTYPE html>
+<html dir="ltr" lang="en">
+<head>
+    <title>Zuul Status</title>
+    <link rel="stylesheet" href="lib/bootstrap/css/bootstrap.min.css">
+    <link rel="stylesheet" href="styles/zuul.css" />
+</head>
+<body>
+    <div id="zuul_container"></div>
+    <script src="lib/jquery.min.js"></script>
+    <script src="lib/jquery-visibility.js"></script>
+    <script src="lib/jquery.graphite.js"></script>
+    <script src="jquery.zuul.js"></script>
+    <script src="zuul.app.js"></script>
+    <script>
+        zuul_build_dom(jQuery, '#zuul_container');
+        zuul_start(jQuery);
+    </script>
+</body>
+</html>
diff --git a/etc/status/public_html/jquery.zuul.js b/etc/status/public_html/jquery.zuul.js
index 9df44ce..82304fa 100644
--- a/etc/status/public_html/jquery.zuul.js
+++ b/etc/status/public_html/jquery.zuul.js
@@ -1,910 +1,910 @@
-// jquery plugin for Zuul status page
-//
-// Copyright 2012 OpenStack Foundation
-// Copyright 2013 Timo Tijhof
-// Copyright 2013 Wikimedia Foundation
-// Copyright 2014 Rackspace Australia
-//
-// Licensed under the Apache License, Version 2.0 (the "License"); you may
-// not use this file except in compliance with the License. You may obtain
-// a copy of the License at
-//
-//      http://www.apache.org/licenses/LICENSE-2.0
-//
-// Unless required by applicable law or agreed to in writing, software
-// distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-// WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-// License for the specific language governing permissions and limitations
-// under the License.
-
-(function ($) {
-    'use strict';
-
-    function set_cookie(name, value) {
-        document.cookie = name + '=' + value + '; path=/';
-    }
-
-    function read_cookie(name, default_value) {
-        var nameEQ = name + '=';
-        var ca = document.cookie.split(';');
-        for(var i=0;i < ca.length;i++) {
-            var c = ca[i];
-            while (c.charAt(0) === ' ') {
-                c = c.substring(1, c.length);
-            }
-            if (c.indexOf(nameEQ) === 0) {
-                return c.substring(nameEQ.length, c.length);
-            }
-        }
-        return default_value;
-    }
-
-    $.zuul = function(options) {
-        options = $.extend({
-            'enabled': true,
-            'graphite_url': '',
-            'source': 'status.json',
-            'msg_id': '#zuul_msg',
-            'pipelines_id': '#zuul_pipelines',
-            'queue_events_num': '#zuul_queue_events_num',
-            'queue_results_num': '#zuul_queue_results_num',
-        }, options);
-
-        var collapsed_exceptions = [];
-        var current_filter = read_cookie('zuul_filter_string', '');
-        var $jq;
-
-        var xhr,
-            zuul_graph_update_count = 0,
-            zuul_sparkline_urls = {};
-
-        function get_sparkline_url(pipeline_name) {
-            if (options.graphite_url !== '') {
-                if (!(pipeline_name in zuul_sparkline_urls)) {
-                    zuul_sparkline_urls[pipeline_name] = $.fn.graphite
-                        .geturl({
-                        url: options.graphite_url,
-                        from: "-8hours",
-                        width: 100,
-                        height: 26,
-                        margin: 0,
-                        hideLegend: true,
-                        hideAxes: true,
-                        hideGrid: true,
-                        target: [
-                            "color(stats.gauges.zuul.pipeline." + pipeline_name
-                                + ".current_changes, '6b8182')"
-                        ]
-                    });
-                }
-                return zuul_sparkline_urls[pipeline_name];
-            }
-            return false;
-        }
-
-        var format = {
-            job: function(job) {
-                var $job_line = $('<span />');
-
-                if (job.url !== null) {
-                    $job_line.append(
-                        $('<a />')
-                            .addClass('zuul-job-name')
-                            .attr('href', job.url)
-                            .text(job.name)
-                    );
-                }
-                else {
-                    $job_line.append(
-                        $('<span />')
-                            .addClass('zuul-job-name')
-                            .text(job.name)
-                    );
-                }
-
-                $job_line.append(this.job_status(job));
-
-                if (job.voting === false) {
-                    $job_line.append(
-                        $(' <small />')
-                            .addClass('zuul-non-voting-desc')
-                            .text(' (non-voting)')
-                    );
-                }
-
-                $job_line.append($('<div style="clear: both"></div>'));
-                return $job_line;
-            },
-
-            job_status: function(job) {
-                var result = job.result ? job.result.toLowerCase() : null;
-                if (result === null) {
-                    result = job.url ? 'in progress' : 'queued';
-                }
-
-                if (result === 'in progress') {
-                    return this.job_progress_bar(job.elapsed_time,
-                                                        job.remaining_time);
-                }
-                else {
-                    return this.status_label(result);
-                }
-            },
-
-            status_label: function(result) {
-                var $status = $('<span />');
-                $status.addClass('zuul-job-result label');
-
-                switch (result) {
-                    case 'success':
-                        $status.addClass('label-success');
-                        break;
-                    case 'failure':
-                        $status.addClass('label-danger');
-                        break;
-                    case 'unstable':
-                        $status.addClass('label-warning');
-                        break;
-                    case 'skipped':
-                        $status.addClass('label-info');
-                        break;
-                    case 'in progress':
-                    case 'queued':
-                    case 'lost':
-                        $status.addClass('label-default');
-                        break;
-                }
-                $status.text(result);
-                return $status;
-            },
-
-            job_progress_bar: function(elapsed_time, remaining_time) {
-                var progress_percent = 100 * (elapsed_time / (elapsed_time +
-                                                              remaining_time));
-                var $bar_inner = $('<div />')
-                    .addClass('progress-bar')
-                    .attr('role', 'progressbar')
-                    .attr('aria-valuenow', 'progressbar')
-                    .attr('aria-valuemin', progress_percent)
-                    .attr('aria-valuemin', '0')
-                    .attr('aria-valuemax', '100')
-                    .css('width', progress_percent + '%');
-
-                var $bar_outter = $('<div />')
-                    .addClass('progress zuul-job-result')
-                    .append($bar_inner);
-
-                return $bar_outter;
-            },
-
-            enqueue_time: function(ms) {
-                // Special format case for enqueue time to add style
-                var hours = 60 * 60 * 1000;
-                var now = Date.now();
-                var delta = now - ms;
-                var status = 'text-success';
-                var text = this.time(delta, true);
-                if (delta > (4 * hours)) {
-                    status = 'text-danger';
-                } else if (delta > (2 * hours)) {
-                    status = 'text-warning';
-                }
-                return '<span class="' + status + '">' + text + '</span>';
-            },
-
-            time: function(ms, words) {
-                if (typeof(words) === 'undefined') {
-                    words = false;
-                }
-                var seconds = (+ms)/1000;
-                var minutes = Math.floor(seconds/60);
-                var hours = Math.floor(minutes/60);
-                seconds = Math.floor(seconds % 60);
-                minutes = Math.floor(minutes % 60);
-                var r = '';
-                if (words) {
-                    if (hours) {
-                        r += hours;
-                        r += ' hr ';
-                    }
-                    r += minutes + ' min';
-                } else {
-                    if (hours < 10) {
-                        r += '0';
-                    }
-                    r += hours + ':';
-                    if (minutes < 10) {
-                        r += '0';
-                    }
-                    r += minutes + ':';
-                    if (seconds < 10) {
-                        r += '0';
-                    }
-                    r += seconds;
-                }
-                return r;
-            },
-
-            change_total_progress_bar: function(change) {
-                var job_percent = Math.floor(100 / change.jobs.length);
-                var $bar_outter = $('<div />')
-                    .addClass('progress zuul-change-total-result');
-
-                $.each(change.jobs, function (i, job) {
-                    var result = job.result ? job.result.toLowerCase() : null;
-                    if (result === null) {
-                        result = job.url ? 'in progress' : 'queued';
-                    }
-
-                    if (result !== 'queued') {
-                        var $bar_inner = $('<div />')
-                            .addClass('progress-bar');
-
-                        switch (result) {
-                            case 'success':
-                                $bar_inner.addClass('progress-bar-success');
-                                break;
-                            case 'lost':
-                            case 'failure':
-                                $bar_inner.addClass('progress-bar-danger');
-                                break;
-                            case 'unstable':
-                                $bar_inner.addClass('progress-bar-warning');
-                                break;
-                            case 'in progress':
-                            case 'queued':
-                                break;
-                        }
-                        $bar_inner.attr('title', job.name)
-                            .css('width', job_percent + '%');
-                        $bar_outter.append($bar_inner);
-                    }
-                });
-                return $bar_outter;
-            },
-
-            change_header: function(change) {
-                var change_id = change.id || 'NA';
-                if (change_id.length === 40) {
-                    change_id = change_id.substr(0, 7);
-                }
-
-                var $change_link = $('<small />');
-                if (change.url !== null) {
-                    if (/^[0-9a-f]{40}$/.test(change.id)) {
-                        var change_id_short = change.id.slice(0, 7);
-                        $change_link.append(
-                            $('<a />').attr('href', change.url).append(
-                                $('<abbr />')
-                                    .attr('title', change.id)
-                                    .text(change_id_short)
-                            )
-                        );
-                    }
-                    else {
-                        $change_link.append(
-                            $('<a />').attr('href', change.url).text(change.id)
-                        );
-                    }
-                }
-                else {
-                    $change_link.text(change_id);
-                }
-
-                var $change_progress_row_left = $('<div />')
-                    .addClass('col-xs-4')
-                    .append($change_link);
-                var $change_progress_row_right = $('<div />')
-                    .addClass('col-xs-8')
-                    .append(this.change_total_progress_bar(change));
-
-                var $change_progress_row = $('<div />')
-                    .addClass('row')
-                    .append($change_progress_row_left)
-                    .append($change_progress_row_right);
-
-                var $project_span = $('<span />')
-                    .addClass('change_project')
-                    .text(change.project);
-
-                var $left = $('<div />')
-                    .addClass('col-xs-8')
-                    .append($project_span, $change_progress_row);
-
-                var remaining_time = this.time(
-                        change.remaining_time, true);
-                var enqueue_time = this.enqueue_time(
-                        change.enqueue_time);
-                var $remaining_time = $('<small />').addClass('time')
-                    .attr('title', 'Remaining Time').html(remaining_time);
-                var $enqueue_time = $('<small />').addClass('time')
-                    .attr('title', 'Elapsed Time').html(enqueue_time);
-
-                var $right = $('<div />');
-                if (change.live === true) {
-                    $right.addClass('col-xs-4 text-right')
-                        .append($remaining_time, $('<br />'), $enqueue_time);
-                }
-
-                var $header = $('<div />')
-                    .addClass('row')
-                    .append($left, $right);
-                return $header;
-            },
-
-            change_list: function(jobs) {
-                var format = this;
-                var $list = $('<ul />')
-                    .addClass('list-group zuul-patchset-body');
-
-                $.each(jobs, function (i, job) {
-                    var $item = $('<li />')
-                        .addClass('list-group-item')
-                        .addClass('zuul-change-job')
-                        .append(format.job(job));
-                    $list.append($item);
-                });
-
-                return $list;
-            },
-
-            change_panel: function (change) {
-                var $header = $('<div />')
-                    .addClass('panel-heading zuul-patchset-header')
-                    .append(this.change_header(change));
-
-                var panel_id = change.id ? change.id.replace(',', '_')
-                                         : change.project.replace('/', '_') +
-                                           '-' + change.enqueue_time;
-                var $panel = $('<div />')
-                    .attr('id', panel_id)
-                    .addClass('panel panel-default zuul-change')
-                    .append($header)
-                    .append(this.change_list(change.jobs));
-
-                $header.click(this.toggle_patchset);
-                return $panel;
-            },
-
-            change_status_icon: function(change) {
-                var icon_name = 'green.png';
-                var icon_title = 'Succeeding';
-
-                if (change.active !== true) {
-                    // Grey icon
-                    icon_name = 'grey.png';
-                    icon_title = 'Waiting until closer to head of queue to' +
-                        ' start jobs';
-                }
-                else if (change.live !== true) {
-                    // Grey icon
-                    icon_name = 'grey.png';
-                    icon_title = 'Dependent change required for testing';
-                }
-                else if (change.failing_reasons &&
-                         change.failing_reasons.length > 0) {
-                    var reason = change.failing_reasons.join(', ');
-                    icon_title = 'Failing because ' + reason;
-                    if (reason.match(/merge conflict/)) {
-                        // Black icon
-                        icon_name = 'black.png';
-                    }
-                    else {
-                        // Red icon
-                        icon_name = 'red.png';
-                    }
-                }
-
-                var $icon = $('<img />')
-                    .attr('src', 'images/' + icon_name)
-                    .attr('title', icon_title)
-                    .css('margin-top', '-6px');
-
-                return $icon;
-            },
-
-            change_with_status_tree: function(change, change_queue) {
-                var $change_row = $('<tr />');
-
-                for (var i = 0; i < change_queue._tree_columns; i++) {
-                    var $tree_cell  = $('<td />')
-                        .css('height', '100%')
-                        .css('padding', '0 0 10px 0')
-                        .css('margin', '0')
-                        .css('width', '16px')
-                        .css('min-width', '16px')
-                        .css('overflow', 'hidden')
-                        .css('vertical-align', 'top');
-
-                    if (i < change._tree.length && change._tree[i] !== null) {
-                        $tree_cell.css('background-image',
-                                       'url(\'images/line.png\')')
-                            .css('background-repeat', 'repeat-y');
-                    }
-
-                    if (i === change._tree_index) {
-                        $tree_cell.append(
-                            this.change_status_icon(change));
-                    }
-                    if (change._tree_branches.indexOf(i) !== -1) {
-                        var $image = $('<img />')
-                            .css('vertical-align', 'baseline');
-                        if (change._tree_branches.indexOf(i) ===
-                            change._tree_branches.length - 1) {
-                            // Angle line
-                            $image.attr('src', 'images/line-angle.png');
-                        }
-                        else {
-                            // T line
-                            $image.attr('src', 'images/line-t.png');
-                        }
-                        $tree_cell.append($image);
-                    }
-                    $change_row.append($tree_cell);
-                }
-
-                var change_width = 360 - 16*change_queue._tree_columns;
-                var $change_column = $('<td />')
-                    .css('width', change_width + 'px')
-                    .addClass('zuul-change-cell')
-                    .append(this.change_panel(change));
-
-                $change_row.append($change_column);
-
-                var $change_table = $('<table />')
-                    .addClass('zuul-change-box')
-                    .css('-moz-box-sizing', 'content-box')
-                    .css('box-sizing', 'content-box')
-                    .append($change_row);
-
-                return $change_table;
-            },
-
-            pipeline_sparkline: function(pipeline_name) {
-                if (options.graphite_url !== '') {
-                    var $sparkline = $('<img />')
-                        .addClass('pull-right')
-                        .attr('src', get_sparkline_url(pipeline_name));
-                    return $sparkline;
-                }
-                return false;
-            },
-
-            pipeline_header: function(pipeline, count) {
-                // Format the pipeline name, sparkline and description
-                var $header_div = $('<div />')
-                    .addClass('zuul-pipeline-header');
-
-                var $heading = $('<h3 />')
-                    .css('vertical-align', 'middle')
-                    .text(pipeline.name)
-                    .append(
-                        $('<span />')
-                            .addClass('badge pull-right')
-                            .css('vertical-align', 'middle')
-                            .css('margin-top', '0.5em')
-                            .text(count)
-                    )
-                    .append(this.pipeline_sparkline(pipeline.name));
-
-                $header_div.append($heading);
-
-                if (typeof pipeline.description === 'string') {
-                    var descr = $('<small />')
-                    $.each( pipeline.description.split(/\r?\n\r?\n/), function(index, descr_part){
-                        descr.append($('<p />').text(descr_part));
-                    });
-                    $header_div.append(
-                        $('<p />').append(descr)
-                    );
-                }
-                return $header_div;
-            },
-
-            pipeline: function (pipeline, count) {
-                var format = this;
-                var $html = $('<div />')
-                    .addClass('zuul-pipeline col-md-4')
-                    .append(this.pipeline_header(pipeline, count));
-
-                $.each(pipeline.change_queues,
-                       function (queue_i, change_queue) {
-                    $.each(change_queue.heads, function (head_i, changes) {
-                        if (pipeline.change_queues.length > 1 &&
-                            head_i === 0) {
-                            var name = change_queue.name;
-                            var short_name = name;
-                            if (short_name.length > 32) {
-                                short_name = short_name.substr(0, 32) + '...';
-                            }
-                            $html.append(
-                                $('<p />')
-                                    .text('Queue: ')
-                                    .append(
-                                        $('<abbr />')
-                                            .attr('title', name)
-                                            .text(short_name)
-                                    )
-                            );
-                        }
-
-                        $.each(changes, function (change_i, change) {
-                            var $change_box =
-                                format.change_with_status_tree(
-                                    change, change_queue);
-                            $html.append($change_box);
-                            format.display_patchset($change_box);
-                        });
-                    });
-                });
-                return $html;
-            },
-
-            toggle_patchset: function(e) {
-                // Toggle showing/hiding the patchset when the header is
-                // clicked.
-
-                // Grab the patchset panel
-                var $panel = $(e.target).parents('.zuul-change');
-                var $body = $panel.children('.zuul-patchset-body');
-                $body.toggle(200);
-                var collapsed_index = collapsed_exceptions.indexOf(
-                    $panel.attr('id'));
-                if (collapsed_index === -1 ) {
-                    // Currently not an exception, add it to list
-                    collapsed_exceptions.push($panel.attr('id'));
-                }
-                else {
-                    // Currently an except, remove from exceptions
-                    collapsed_exceptions.splice(collapsed_index, 1);
-                }
-            },
-
-            display_patchset: function($change_box, animate) {
-                // Determine if to show or hide the patchset and/or the results
-                // when loaded
-
-                // See if we should hide the body/results
-                var $panel = $change_box.find('.zuul-change');
-                var panel_change = $panel.attr('id');
-                var $body = $panel.children('.zuul-patchset-body');
-                var expand_by_default = $('#expand_by_default')
-                    .prop('checked');
-
-                var collapsed_index = collapsed_exceptions
-                    .indexOf(panel_change);
-
-                if (expand_by_default && collapsed_index === -1 ||
-                    !expand_by_default && collapsed_index !== -1) {
-                    // Expand by default, or is an exception
-                    $body.show(animate);
-                }
-                else {
-                    $body.hide(animate);
-                }
-
-                // Check if we should hide the whole panel
-                var panel_project = $panel.find('.change_project').text()
-                    .toLowerCase();
-
-
-                var panel_pipeline = $change_box
-                    .parents('.zuul-pipeline')
-                    .find('.zuul-pipeline-header > h3')
-                    .html()
-                    .toLowerCase();
-
-                if (current_filter !== '') {
-                    var show_panel = false;
-                    var filter = current_filter.trim().split(/[\s,]+/);
-                    $.each(filter, function(index, f_val) {
-                        if (f_val !== '') {
-                            f_val = f_val.toLowerCase();
-                            if (panel_project.indexOf(f_val) !== -1 ||
-                                panel_pipeline.indexOf(f_val) !== -1 ||
-                                panel_change.indexOf(f_val) !== -1) {
-                                show_panel = true;
-                            }
-                        }
-                    });
-                    if (show_panel === true) {
-                        $change_box.show(animate);
-                    }
-                    else {
-                        $change_box.hide(animate);
-                    }
-                }
-                else {
-                    $change_box.show(animate);
-                }
-            },
-        };
-
-        var app = {
-            schedule: function (app) {
-                app = app || this;
-                if (!options.enabled) {
-                    setTimeout(function() {app.schedule(app);}, 5000);
-                    return;
-                }
-                app.update().complete(function () {
-                    setTimeout(function() {app.schedule(app);}, 5000);
-                });
-
-                /* Only update graphs every minute */
-                if (zuul_graph_update_count > 11) {
-                    zuul_graph_update_count = 0;
-                    zuul.update_sparklines();
-                }
-            },
-
-            /** @return {jQuery.Promise} */
-            update: function () {
-                // Cancel the previous update if it hasn't completed yet.
-                if (xhr) {
-                    xhr.abort();
-                }
-
-                this.emit('update-start');
-                var app = this;
-
-                var $msg = $(options.msg_id);
-                xhr = $.getJSON(options.source)
-                    .done(function (data) {
-                        if ('message' in data) {
-                            $msg.removeClass('alert-danger')
-                                .addClass('alert-info')
-                                .text(data.message)
-                                .show();
-                        } else {
-                            $msg.empty()
-                                .hide();
-                        }
-
-                        if ('zuul_version' in data) {
-                            $('#zuul-version-span').text(data.zuul_version);
-                        }
-                        if ('last_reconfigured' in data) {
-                            var last_reconfigured =
-                                new Date(data.last_reconfigured);
-                            $('#last-reconfigured-span').text(
-                                last_reconfigured.toString());
-                        }
-
-                        var $pipelines = $(options.pipelines_id);
-                        $pipelines.html('');
-                        $.each(data.pipelines, function (i, pipeline) {
-                            var count = app.create_tree(pipeline);
-                            $pipelines.append(
-                                format.pipeline(pipeline, count));
-                        });
-
-                        $(options.queue_events_num).text(
-                            data.trigger_event_queue ?
-                                data.trigger_event_queue.length : '0'
-                        );
-                        $(options.queue_results_num).text(
-                            data.result_event_queue ?
-                                data.result_event_queue.length : '0'
-                        );
-                    })
-                    .fail(function (jqXHR, statusText, errMsg) {
-                        if (statusText === 'abort') {
-                            return;
-                        }
-                        $msg.text(options.source + ': ' + errMsg)
-                            .addClass('alert-danger')
-                            .removeClass('zuul-msg-wrap-off')
-                            .show();
-                    })
-                    .complete(function () {
-                        xhr = undefined;
-                        app.emit('update-end');
-                    });
-
-                return xhr;
-            },
-
-            update_sparklines: function() {
-                $.each(zuul_sparkline_urls, function(name, url) {
-                    var newimg = new Image();
-                    var parts = url.split('#');
-                    newimg.src = parts[0] + '#' + new Date().getTime();
-                    $(newimg).load(function () {
-                        zuul_sparkline_urls[name] = newimg.src;
-                    });
-                });
-            },
-
-            emit: function () {
-                $jq.trigger.apply($jq, arguments);
-                return this;
-            },
-            on: function () {
-                $jq.on.apply($jq, arguments);
-                return this;
-            },
-            one: function () {
-                $jq.one.apply($jq, arguments);
-                return this;
-            },
-
-            control_form: function() {
-                // Build the filter form filling anything from cookies
-
-                var $control_form = $('<form />')
-                    .attr('role', 'form')
-                    .addClass('form-inline')
-                    .submit(this.handle_filter_change);
-
-                $control_form
-                    .append(this.filter_form_group())
-                    .append(this.expand_form_group());
-
-                return $control_form;
-            },
-
-            filter_form_group: function() {
-                // Update the filter form with a clear button if required
-
-                var $label = $('<label />')
-                    .addClass('control-label')
-                    .attr('for', 'filter_string')
-                    .text('Filters')
-                    .css('padding-right', '0.5em');
-
-                var $input = $('<input />')
-                    .attr('type', 'text')
-                    .attr('id', 'filter_string')
-                    .addClass('form-control')
-                    .attr('title',
-                          'project(s), pipeline(s) or review(s) comma ' +
-                          'separated')
-                    .attr('value', current_filter);
-
-                $input.change(this.handle_filter_change);
-
-                var $clear_icon = $('<span />')
-                    .addClass('form-control-feedback')
-                    .addClass('glyphicon glyphicon-remove-circle')
-                    .attr('id', 'filter_form_clear_box')
-                    .attr('title', 'clear filter')
-                    .css('cursor', 'pointer');
-
-                $clear_icon.click(function() {
-                    $('#filter_string').val('').change();
-                });
-
-                if (current_filter === '') {
-                    $clear_icon.hide();
-                }
-
-                var $form_group = $('<div />')
-                    .addClass('form-group has-feedback')
-                    .append($label, $input, $clear_icon);
-                return $form_group;
-            },
-
-            expand_form_group: function() {
-                var expand_by_default = (
-                    read_cookie('zuul_expand_by_default', false) === 'true');
-
-                var $checkbox = $('<input />')
-                    .attr('type', 'checkbox')
-                    .attr('id', 'expand_by_default')
-                    .prop('checked', expand_by_default)
-                    .change(this.handle_expand_by_default);
-
-                var $label = $('<label />')
-                    .css('padding-left', '1em')
-                    .html('Expand by default: ')
-                    .append($checkbox);
-
-                var $form_group = $('<div />')
-                    .addClass('checkbox')
-                    .append($label);
-                return $form_group;
-            },
-
-            handle_filter_change: function() {
-                // Update the filter and save it to a cookie
-                current_filter = $('#filter_string').val();
-                set_cookie('zuul_filter_string', current_filter);
-                if (current_filter === '') {
-                    $('#filter_form_clear_box').hide();
-                }
-                else {
-                    $('#filter_form_clear_box').show();
-                }
-
-                $('.zuul-change-box').each(function(index, obj) {
-                    var $change_box = $(obj);
-                    format.display_patchset($change_box, 200);
-                });
-                return false;
-            },
-
-            handle_expand_by_default: function(e) {
-                // Handle toggling expand by default
-                set_cookie('zuul_expand_by_default', e.target.checked);
-                collapsed_exceptions = [];
-                $('.zuul-change-box').each(function(index, obj) {
-                    var $change_box = $(obj);
-                    format.display_patchset($change_box, 200);
-                });
-            },
-
-            create_tree: function(pipeline) {
-                var count = 0;
-                var pipeline_max_tree_columns = 1;
-                $.each(pipeline.change_queues, function(change_queue_i,
-                                                           change_queue) {
-                    var tree = [];
-                    var max_tree_columns = 1;
-                    var changes = [];
-                    var last_tree_length = 0;
-                    $.each(change_queue.heads, function(head_i, head) {
-                        $.each(head, function(change_i, change) {
-                            changes[change.id] = change;
-                            change._tree_position = change_i;
-                        });
-                    });
-                    $.each(change_queue.heads, function(head_i, head) {
-                        $.each(head, function(change_i, change) {
-                            if (change.live === true) {
-                                count += 1;
-                            }
-                            var idx = tree.indexOf(change.id);
-                            if (idx > -1) {
-                                change._tree_index = idx;
-                                // remove...
-                                tree[idx] = null;
-                                while (tree[tree.length - 1] === null) {
-                                    tree.pop();
-                                }
-                            } else {
-                                change._tree_index = 0;
-                            }
-                            change._tree_branches = [];
-                            change._tree = [];
-                            if (typeof(change.items_behind) === 'undefined') {
-                                change.items_behind = [];
-                            }
-                            change.items_behind.sort(function(a, b) {
-                                return (changes[b]._tree_position -
-                                        changes[a]._tree_position);
-                            });
-                            $.each(change.items_behind, function(i, id) {
-                                tree.push(id);
-                                if (tree.length>last_tree_length &&
-                                    last_tree_length > 0) {
-                                    change._tree_branches.push(
-                                        tree.length - 1);
-                                }
-                            });
-                            if (tree.length > max_tree_columns) {
-                                max_tree_columns = tree.length;
-                            }
-                            if (tree.length > pipeline_max_tree_columns) {
-                                pipeline_max_tree_columns = tree.length;
-                            }
-                            change._tree = tree.slice(0);  // make a copy
-                            last_tree_length = tree.length;
-                        });
-                    });
-                    change_queue._tree_columns = max_tree_columns;
-                });
-                pipeline._tree_columns = pipeline_max_tree_columns;
-                return count;
-            },
-        };
-
-        $jq = $(app);
-        return {
-            options: options,
-            format: format,
-            app: app,
-            jq: $jq
-        };
-    };
-}(jQuery));
+// jquery plugin for Zuul status page
+//
+// Copyright 2012 OpenStack Foundation
+// Copyright 2013 Timo Tijhof
+// Copyright 2013 Wikimedia Foundation
+// Copyright 2014 Rackspace Australia
+//
+// Licensed under the Apache License, Version 2.0 (the "License"); you may
+// not use this file except in compliance with the License. You may obtain
+// a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+// WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+// License for the specific language governing permissions and limitations
+// under the License.
+
+(function ($) {
+    'use strict';
+
+    function set_cookie(name, value) {
+        document.cookie = name + '=' + value + '; path=/';
+    }
+
+    function read_cookie(name, default_value) {
+        var nameEQ = name + '=';
+        var ca = document.cookie.split(';');
+        for(var i=0;i < ca.length;i++) {
+            var c = ca[i];
+            while (c.charAt(0) === ' ') {
+                c = c.substring(1, c.length);
+            }
+            if (c.indexOf(nameEQ) === 0) {
+                return c.substring(nameEQ.length, c.length);
+            }
+        }
+        return default_value;
+    }
+
+    $.zuul = function(options) {
+        options = $.extend({
+            'enabled': true,
+            'graphite_url': '',
+            'source': 'status.json',
+            'msg_id': '#zuul_msg',
+            'pipelines_id': '#zuul_pipelines',
+            'queue_events_num': '#zuul_queue_events_num',
+            'queue_results_num': '#zuul_queue_results_num',
+        }, options);
+
+        var collapsed_exceptions = [];
+        var current_filter = read_cookie('zuul_filter_string', '');
+        var $jq;
+
+        var xhr,
+            zuul_graph_update_count = 0,
+            zuul_sparkline_urls = {};
+
+        function get_sparkline_url(pipeline_name) {
+            if (options.graphite_url !== '') {
+                if (!(pipeline_name in zuul_sparkline_urls)) {
+                    zuul_sparkline_urls[pipeline_name] = $.fn.graphite
+                        .geturl({
+                        url: options.graphite_url,
+                        from: "-8hours",
+                        width: 100,
+                        height: 26,
+                        margin: 0,
+                        hideLegend: true,
+                        hideAxes: true,
+                        hideGrid: true,
+                        target: [
+                            "color(stats.gauges.zuul.pipeline." + pipeline_name
+                                + ".current_changes, '6b8182')"
+                        ]
+                    });
+                }
+                return zuul_sparkline_urls[pipeline_name];
+            }
+            return false;
+        }
+
+        var format = {
+            job: function(job) {
+                var $job_line = $('<span />');
+
+                if (job.url !== null) {
+                    $job_line.append(
+                        $('<a />')
+                            .addClass('zuul-job-name')
+                            .attr('href', job.url)
+                            .text(job.name)
+                    );
+                }
+                else {
+                    $job_line.append(
+                        $('<span />')
+                            .addClass('zuul-job-name')
+                            .text(job.name)
+                    );
+                }
+
+                $job_line.append(this.job_status(job));
+
+                if (job.voting === false) {
+                    $job_line.append(
+                        $(' <small />')
+                            .addClass('zuul-non-voting-desc')
+                            .text(' (non-voting)')
+                    );
+                }
+
+                $job_line.append($('<div style="clear: both"></div>'));
+                return $job_line;
+            },
+
+            job_status: function(job) {
+                var result = job.result ? job.result.toLowerCase() : null;
+                if (result === null) {
+                    result = job.url ? 'in progress' : 'queued';
+                }
+
+                if (result === 'in progress') {
+                    return this.job_progress_bar(job.elapsed_time,
+                                                        job.remaining_time);
+                }
+                else {
+                    return this.status_label(result);
+                }
+            },
+
+            status_label: function(result) {
+                var $status = $('<span />');
+                $status.addClass('zuul-job-result label');
+
+                switch (result) {
+                    case 'success':
+                        $status.addClass('label-success');
+                        break;
+                    case 'failure':
+                        $status.addClass('label-danger');
+                        break;
+                    case 'unstable':
+                        $status.addClass('label-warning');
+                        break;
+                    case 'skipped':
+                        $status.addClass('label-info');
+                        break;
+                    case 'in progress':
+                    case 'queued':
+                    case 'lost':
+                        $status.addClass('label-default');
+                        break;
+                }
+                $status.text(result);
+                return $status;
+            },
+
+            job_progress_bar: function(elapsed_time, remaining_time) {
+                var progress_percent = 100 * (elapsed_time / (elapsed_time +
+                                                              remaining_time));
+                var $bar_inner = $('<div />')
+                    .addClass('progress-bar')
+                    .attr('role', 'progressbar')
+                    .attr('aria-valuenow', 'progressbar')
+                    .attr('aria-valuemin', progress_percent)
+                    .attr('aria-valuemin', '0')
+                    .attr('aria-valuemax', '100')
+                    .css('width', progress_percent + '%');
+
+                var $bar_outter = $('<div />')
+                    .addClass('progress zuul-job-result')
+                    .append($bar_inner);
+
+                return $bar_outter;
+            },
+
+            enqueue_time: function(ms) {
+                // Special format case for enqueue time to add style
+                var hours = 60 * 60 * 1000;
+                var now = Date.now();
+                var delta = now - ms;
+                var status = 'text-success';
+                var text = this.time(delta, true);
+                if (delta > (4 * hours)) {
+                    status = 'text-danger';
+                } else if (delta > (2 * hours)) {
+                    status = 'text-warning';
+                }
+                return '<span class="' + status + '">' + text + '</span>';
+            },
+
+            time: function(ms, words) {
+                if (typeof(words) === 'undefined') {
+                    words = false;
+                }
+                var seconds = (+ms)/1000;
+                var minutes = Math.floor(seconds/60);
+                var hours = Math.floor(minutes/60);
+                seconds = Math.floor(seconds % 60);
+                minutes = Math.floor(minutes % 60);
+                var r = '';
+                if (words) {
+                    if (hours) {
+                        r += hours;
+                        r += ' hr ';
+                    }
+                    r += minutes + ' min';
+                } else {
+                    if (hours < 10) {
+                        r += '0';
+                    }
+                    r += hours + ':';
+                    if (minutes < 10) {
+                        r += '0';
+                    }
+                    r += minutes + ':';
+                    if (seconds < 10) {
+                        r += '0';
+                    }
+                    r += seconds;
+                }
+                return r;
+            },
+
+            change_total_progress_bar: function(change) {
+                var job_percent = Math.floor(100 / change.jobs.length);
+                var $bar_outter = $('<div />')
+                    .addClass('progress zuul-change-total-result');
+
+                $.each(change.jobs, function (i, job) {
+                    var result = job.result ? job.result.toLowerCase() : null;
+                    if (result === null) {
+                        result = job.url ? 'in progress' : 'queued';
+                    }
+
+                    if (result !== 'queued') {
+                        var $bar_inner = $('<div />')
+                            .addClass('progress-bar');
+
+                        switch (result) {
+                            case 'success':
+                                $bar_inner.addClass('progress-bar-success');
+                                break;
+                            case 'lost':
+                            case 'failure':
+                                $bar_inner.addClass('progress-bar-danger');
+                                break;
+                            case 'unstable':
+                                $bar_inner.addClass('progress-bar-warning');
+                                break;
+                            case 'in progress':
+                            case 'queued':
+                                break;
+                        }
+                        $bar_inner.attr('title', job.name)
+                            .css('width', job_percent + '%');
+                        $bar_outter.append($bar_inner);
+                    }
+                });
+                return $bar_outter;
+            },
+
+            change_header: function(change) {
+                var change_id = change.id || 'NA';
+                if (change_id.length === 40) {
+                    change_id = change_id.substr(0, 7);
+                }
+
+                var $change_link = $('<small />');
+                if (change.url !== null) {
+                    if (/^[0-9a-f]{40}$/.test(change.id)) {
+                        var change_id_short = change.id.slice(0, 7);
+                        $change_link.append(
+                            $('<a />').attr('href', change.url).append(
+                                $('<abbr />')
+                                    .attr('title', change.id)
+                                    .text(change_id_short)
+                            )
+                        );
+                    }
+                    else {
+                        $change_link.append(
+                            $('<a />').attr('href', change.url).text(change.id)
+                        );
+                    }
+                }
+                else {
+                    $change_link.text(change_id);
+                }
+
+                var $change_progress_row_left = $('<div />')
+                    .addClass('col-xs-4')
+                    .append($change_link);
+                var $change_progress_row_right = $('<div />')
+                    .addClass('col-xs-8')
+                    .append(this.change_total_progress_bar(change));
+
+                var $change_progress_row = $('<div />')
+                    .addClass('row')
+                    .append($change_progress_row_left)
+                    .append($change_progress_row_right);
+
+                var $project_span = $('<span />')
+                    .addClass('change_project')
+                    .text(change.project);
+
+                var $left = $('<div />')
+                    .addClass('col-xs-8')
+                    .append($project_span, $change_progress_row);
+
+                var remaining_time = this.time(
+                        change.remaining_time, true);
+                var enqueue_time = this.enqueue_time(
+                        change.enqueue_time);
+                var $remaining_time = $('<small />').addClass('time')
+                    .attr('title', 'Remaining Time').html(remaining_time);
+                var $enqueue_time = $('<small />').addClass('time')
+                    .attr('title', 'Elapsed Time').html(enqueue_time);
+
+                var $right = $('<div />');
+                if (change.live === true) {
+                    $right.addClass('col-xs-4 text-right')
+                        .append($remaining_time, $('<br />'), $enqueue_time);
+                }
+
+                var $header = $('<div />')
+                    .addClass('row')
+                    .append($left, $right);
+                return $header;
+            },
+
+            change_list: function(jobs) {
+                var format = this;
+                var $list = $('<ul />')
+                    .addClass('list-group zuul-patchset-body');
+
+                $.each(jobs, function (i, job) {
+                    var $item = $('<li />')
+                        .addClass('list-group-item')
+                        .addClass('zuul-change-job')
+                        .append(format.job(job));
+                    $list.append($item);
+                });
+
+                return $list;
+            },
+
+            change_panel: function (change) {
+                var $header = $('<div />')
+                    .addClass('panel-heading zuul-patchset-header')
+                    .append(this.change_header(change));
+
+                var panel_id = change.id ? change.id.replace(',', '_')
+                                         : change.project.replace('/', '_') +
+                                           '-' + change.enqueue_time;
+                var $panel = $('<div />')
+                    .attr('id', panel_id)
+                    .addClass('panel panel-default zuul-change')
+                    .append($header)
+                    .append(this.change_list(change.jobs));
+
+                $header.click(this.toggle_patchset);
+                return $panel;
+            },
+
+            change_status_icon: function(change) {
+                var icon_name = 'green.png';
+                var icon_title = 'Succeeding';
+
+                if (change.active !== true) {
+                    // Grey icon
+                    icon_name = 'grey.png';
+                    icon_title = 'Waiting until closer to head of queue to' +
+                        ' start jobs';
+                }
+                else if (change.live !== true) {
+                    // Grey icon
+                    icon_name = 'grey.png';
+                    icon_title = 'Dependent change required for testing';
+                }
+                else if (change.failing_reasons &&
+                         change.failing_reasons.length > 0) {
+                    var reason = change.failing_reasons.join(', ');
+                    icon_title = 'Failing because ' + reason;
+                    if (reason.match(/merge conflict/)) {
+                        // Black icon
+                        icon_name = 'black.png';
+                    }
+                    else {
+                        // Red icon
+                        icon_name = 'red.png';
+                    }
+                }
+
+                var $icon = $('<img />')
+                    .attr('src', 'images/' + icon_name)
+                    .attr('title', icon_title)
+                    .css('margin-top', '-6px');
+
+                return $icon;
+            },
+
+            change_with_status_tree: function(change, change_queue) {
+                var $change_row = $('<tr />');
+
+                for (var i = 0; i < change_queue._tree_columns; i++) {
+                    var $tree_cell  = $('<td />')
+                        .css('height', '100%')
+                        .css('padding', '0 0 10px 0')
+                        .css('margin', '0')
+                        .css('width', '16px')
+                        .css('min-width', '16px')
+                        .css('overflow', 'hidden')
+                        .css('vertical-align', 'top');
+
+                    if (i < change._tree.length && change._tree[i] !== null) {
+                        $tree_cell.css('background-image',
+                                       'url(\'images/line.png\')')
+                            .css('background-repeat', 'repeat-y');
+                    }
+
+                    if (i === change._tree_index) {
+                        $tree_cell.append(
+                            this.change_status_icon(change));
+                    }
+                    if (change._tree_branches.indexOf(i) !== -1) {
+                        var $image = $('<img />')
+                            .css('vertical-align', 'baseline');
+                        if (change._tree_branches.indexOf(i) ===
+                            change._tree_branches.length - 1) {
+                            // Angle line
+                            $image.attr('src', 'images/line-angle.png');
+                        }
+                        else {
+                            // T line
+                            $image.attr('src', 'images/line-t.png');
+                        }
+                        $tree_cell.append($image);
+                    }
+                    $change_row.append($tree_cell);
+                }
+
+                var change_width = 360 - 16*change_queue._tree_columns;
+                var $change_column = $('<td />')
+                    .css('width', change_width + 'px')
+                    .addClass('zuul-change-cell')
+                    .append(this.change_panel(change));
+
+                $change_row.append($change_column);
+
+                var $change_table = $('<table />')
+                    .addClass('zuul-change-box')
+                    .css('-moz-box-sizing', 'content-box')
+                    .css('box-sizing', 'content-box')
+                    .append($change_row);
+
+                return $change_table;
+            },
+
+            pipeline_sparkline: function(pipeline_name) {
+                if (options.graphite_url !== '') {
+                    var $sparkline = $('<img />')
+                        .addClass('pull-right')
+                        .attr('src', get_sparkline_url(pipeline_name));
+                    return $sparkline;
+                }
+                return false;
+            },
+
+            pipeline_header: function(pipeline, count) {
+                // Format the pipeline name, sparkline and description
+                var $header_div = $('<div />')
+                    .addClass('zuul-pipeline-header');
+
+                var $heading = $('<h3 />')
+                    .css('vertical-align', 'middle')
+                    .text(pipeline.name)
+                    .append(
+                        $('<span />')
+                            .addClass('badge pull-right')
+                            .css('vertical-align', 'middle')
+                            .css('margin-top', '0.5em')
+                            .text(count)
+                    )
+                    .append(this.pipeline_sparkline(pipeline.name));
+
+                $header_div.append($heading);
+
+                if (typeof pipeline.description === 'string') {
+                    var descr = $('<small />')
+                    $.each( pipeline.description.split(/\r?\n\r?\n/), function(index, descr_part){
+                        descr.append($('<p />').text(descr_part));
+                    });
+                    $header_div.append(
+                        $('<p />').append(descr)
+                    );
+                }
+                return $header_div;
+            },
+
+            pipeline: function (pipeline, count) {
+                var format = this;
+                var $html = $('<div />')
+                    .addClass('zuul-pipeline col-md-4')
+                    .append(this.pipeline_header(pipeline, count));
+
+                $.each(pipeline.change_queues,
+                       function (queue_i, change_queue) {
+                    $.each(change_queue.heads, function (head_i, changes) {
+                        if (pipeline.change_queues.length > 1 &&
+                            head_i === 0) {
+                            var name = change_queue.name;
+                            var short_name = name;
+                            if (short_name.length > 32) {
+                                short_name = short_name.substr(0, 32) + '...';
+                            }
+                            $html.append(
+                                $('<p />')
+                                    .text('Queue: ')
+                                    .append(
+                                        $('<abbr />')
+                                            .attr('title', name)
+                                            .text(short_name)
+                                    )
+                            );
+                        }
+
+                        $.each(changes, function (change_i, change) {
+                            var $change_box =
+                                format.change_with_status_tree(
+                                    change, change_queue);
+                            $html.append($change_box);
+                            format.display_patchset($change_box);
+                        });
+                    });
+                });
+                return $html;
+            },
+
+            toggle_patchset: function(e) {
+                // Toggle showing/hiding the patchset when the header is
+                // clicked.
+
+                // Grab the patchset panel
+                var $panel = $(e.target).parents('.zuul-change');
+                var $body = $panel.children('.zuul-patchset-body');
+                $body.toggle(200);
+                var collapsed_index = collapsed_exceptions.indexOf(
+                    $panel.attr('id'));
+                if (collapsed_index === -1 ) {
+                    // Currently not an exception, add it to list
+                    collapsed_exceptions.push($panel.attr('id'));
+                }
+                else {
+                    // Currently an except, remove from exceptions
+                    collapsed_exceptions.splice(collapsed_index, 1);
+                }
+            },
+
+            display_patchset: function($change_box, animate) {
+                // Determine if to show or hide the patchset and/or the results
+                // when loaded
+
+                // See if we should hide the body/results
+                var $panel = $change_box.find('.zuul-change');
+                var panel_change = $panel.attr('id');
+                var $body = $panel.children('.zuul-patchset-body');
+                var expand_by_default = $('#expand_by_default')
+                    .prop('checked');
+
+                var collapsed_index = collapsed_exceptions
+                    .indexOf(panel_change);
+
+                if (expand_by_default && collapsed_index === -1 ||
+                    !expand_by_default && collapsed_index !== -1) {
+                    // Expand by default, or is an exception
+                    $body.show(animate);
+                }
+                else {
+                    $body.hide(animate);
+                }
+
+                // Check if we should hide the whole panel
+                var panel_project = $panel.find('.change_project').text()
+                    .toLowerCase();
+
+
+                var panel_pipeline = $change_box
+                    .parents('.zuul-pipeline')
+                    .find('.zuul-pipeline-header > h3')
+                    .html()
+                    .toLowerCase();
+
+                if (current_filter !== '') {
+                    var show_panel = false;
+                    var filter = current_filter.trim().split(/[\s,]+/);
+                    $.each(filter, function(index, f_val) {
+                        if (f_val !== '') {
+                            f_val = f_val.toLowerCase();
+                            if (panel_project.indexOf(f_val) !== -1 ||
+                                panel_pipeline.indexOf(f_val) !== -1 ||
+                                panel_change.indexOf(f_val) !== -1) {
+                                show_panel = true;
+                            }
+                        }
+                    });
+                    if (show_panel === true) {
+                        $change_box.show(animate);
+                    }
+                    else {
+                        $change_box.hide(animate);
+                    }
+                }
+                else {
+                    $change_box.show(animate);
+                }
+            },
+        };
+
+        var app = {
+            schedule: function (app) {
+                app = app || this;
+                if (!options.enabled) {
+                    setTimeout(function() {app.schedule(app);}, 5000);
+                    return;
+                }
+                app.update().complete(function () {
+                    setTimeout(function() {app.schedule(app);}, 5000);
+                });
+
+                /* Only update graphs every minute */
+                if (zuul_graph_update_count > 11) {
+                    zuul_graph_update_count = 0;
+                    zuul.update_sparklines();
+                }
+            },
+
+            /** @return {jQuery.Promise} */
+            update: function () {
+                // Cancel the previous update if it hasn't completed yet.
+                if (xhr) {
+                    xhr.abort();
+                }
+
+                this.emit('update-start');
+                var app = this;
+
+                var $msg = $(options.msg_id);
+                xhr = $.getJSON(options.source)
+                    .done(function (data) {
+                        if ('message' in data) {
+                            $msg.removeClass('alert-danger')
+                                .addClass('alert-info')
+                                .text(data.message)
+                                .show();
+                        } else {
+                            $msg.empty()
+                                .hide();
+                        }
+
+                        if ('zuul_version' in data) {
+                            $('#zuul-version-span').text(data.zuul_version);
+                        }
+                        if ('last_reconfigured' in data) {
+                            var last_reconfigured =
+                                new Date(data.last_reconfigured);
+                            $('#last-reconfigured-span').text(
+                                last_reconfigured.toString());
+                        }
+
+                        var $pipelines = $(options.pipelines_id);
+                        $pipelines.html('');
+                        $.each(data.pipelines, function (i, pipeline) {
+                            var count = app.create_tree(pipeline);
+                            $pipelines.append(
+                                format.pipeline(pipeline, count));
+                        });
+
+                        $(options.queue_events_num).text(
+                            data.trigger_event_queue ?
+                                data.trigger_event_queue.length : '0'
+                        );
+                        $(options.queue_results_num).text(
+                            data.result_event_queue ?
+                                data.result_event_queue.length : '0'
+                        );
+                    })
+                    .fail(function (jqXHR, statusText, errMsg) {
+                        if (statusText === 'abort') {
+                            return;
+                        }
+                        $msg.text(options.source + ': ' + errMsg)
+                            .addClass('alert-danger')
+                            .removeClass('zuul-msg-wrap-off')
+                            .show();
+                    })
+                    .complete(function () {
+                        xhr = undefined;
+                        app.emit('update-end');
+                    });
+
+                return xhr;
+            },
+
+            update_sparklines: function() {
+                $.each(zuul_sparkline_urls, function(name, url) {
+                    var newimg = new Image();
+                    var parts = url.split('#');
+                    newimg.src = parts[0] + '#' + new Date().getTime();
+                    $(newimg).load(function () {
+                        zuul_sparkline_urls[name] = newimg.src;
+                    });
+                });
+            },
+
+            emit: function () {
+                $jq.trigger.apply($jq, arguments);
+                return this;
+            },
+            on: function () {
+                $jq.on.apply($jq, arguments);
+                return this;
+            },
+            one: function () {
+                $jq.one.apply($jq, arguments);
+                return this;
+            },
+
+            control_form: function() {
+                // Build the filter form filling anything from cookies
+
+                var $control_form = $('<form />')
+                    .attr('role', 'form')
+                    .addClass('form-inline')
+                    .submit(this.handle_filter_change);
+
+                $control_form
+                    .append(this.filter_form_group())
+                    .append(this.expand_form_group());
+
+                return $control_form;
+            },
+
+            filter_form_group: function() {
+                // Update the filter form with a clear button if required
+
+                var $label = $('<label />')
+                    .addClass('control-label')
+                    .attr('for', 'filter_string')
+                    .text('Filters')
+                    .css('padding-right', '0.5em');
+
+                var $input = $('<input />')
+                    .attr('type', 'text')
+                    .attr('id', 'filter_string')
+                    .addClass('form-control')
+                    .attr('title',
+                          'project(s), pipeline(s) or review(s) comma ' +
+                          'separated')
+                    .attr('value', current_filter);
+
+                $input.change(this.handle_filter_change);
+
+                var $clear_icon = $('<span />')
+                    .addClass('form-control-feedback')
+                    .addClass('glyphicon glyphicon-remove-circle')
+                    .attr('id', 'filter_form_clear_box')
+                    .attr('title', 'clear filter')
+                    .css('cursor', 'pointer');
+
+                $clear_icon.click(function() {
+                    $('#filter_string').val('').change();
+                });
+
+                if (current_filter === '') {
+                    $clear_icon.hide();
+                }
+
+                var $form_group = $('<div />')
+                    .addClass('form-group has-feedback')
+                    .append($label, $input, $clear_icon);
+                return $form_group;
+            },
+
+            expand_form_group: function() {
+                var expand_by_default = (
+                    read_cookie('zuul_expand_by_default', false) === 'true');
+
+                var $checkbox = $('<input />')
+                    .attr('type', 'checkbox')
+                    .attr('id', 'expand_by_default')
+                    .prop('checked', expand_by_default)
+                    .change(this.handle_expand_by_default);
+
+                var $label = $('<label />')
+                    .css('padding-left', '1em')
+                    .html('Expand by default: ')
+                    .append($checkbox);
+
+                var $form_group = $('<div />')
+                    .addClass('checkbox')
+                    .append($label);
+                return $form_group;
+            },
+
+            handle_filter_change: function() {
+                // Update the filter and save it to a cookie
+                current_filter = $('#filter_string').val();
+                set_cookie('zuul_filter_string', current_filter);
+                if (current_filter === '') {
+                    $('#filter_form_clear_box').hide();
+                }
+                else {
+                    $('#filter_form_clear_box').show();
+                }
+
+                $('.zuul-change-box').each(function(index, obj) {
+                    var $change_box = $(obj);
+                    format.display_patchset($change_box, 200);
+                });
+                return false;
+            },
+
+            handle_expand_by_default: function(e) {
+                // Handle toggling expand by default
+                set_cookie('zuul_expand_by_default', e.target.checked);
+                collapsed_exceptions = [];
+                $('.zuul-change-box').each(function(index, obj) {
+                    var $change_box = $(obj);
+                    format.display_patchset($change_box, 200);
+                });
+            },
+
+            create_tree: function(pipeline) {
+                var count = 0;
+                var pipeline_max_tree_columns = 1;
+                $.each(pipeline.change_queues, function(change_queue_i,
+                                                           change_queue) {
+                    var tree = [];
+                    var max_tree_columns = 1;
+                    var changes = [];
+                    var last_tree_length = 0;
+                    $.each(change_queue.heads, function(head_i, head) {
+                        $.each(head, function(change_i, change) {
+                            changes[change.id] = change;
+                            change._tree_position = change_i;
+                        });
+                    });
+                    $.each(change_queue.heads, function(head_i, head) {
+                        $.each(head, function(change_i, change) {
+                            if (change.live === true) {
+                                count += 1;
+                            }
+                            var idx = tree.indexOf(change.id);
+                            if (idx > -1) {
+                                change._tree_index = idx;
+                                // remove...
+                                tree[idx] = null;
+                                while (tree[tree.length - 1] === null) {
+                                    tree.pop();
+                                }
+                            } else {
+                                change._tree_index = 0;
+                            }
+                            change._tree_branches = [];
+                            change._tree = [];
+                            if (typeof(change.items_behind) === 'undefined') {
+                                change.items_behind = [];
+                            }
+                            change.items_behind.sort(function(a, b) {
+                                return (changes[b]._tree_position -
+                                        changes[a]._tree_position);
+                            });
+                            $.each(change.items_behind, function(i, id) {
+                                tree.push(id);
+                                if (tree.length>last_tree_length &&
+                                    last_tree_length > 0) {
+                                    change._tree_branches.push(
+                                        tree.length - 1);
+                                }
+                            });
+                            if (tree.length > max_tree_columns) {
+                                max_tree_columns = tree.length;
+                            }
+                            if (tree.length > pipeline_max_tree_columns) {
+                                pipeline_max_tree_columns = tree.length;
+                            }
+                            change._tree = tree.slice(0);  // make a copy
+                            last_tree_length = tree.length;
+                        });
+                    });
+                    change_queue._tree_columns = max_tree_columns;
+                });
+                pipeline._tree_columns = pipeline_max_tree_columns;
+                return count;
+            },
+        };
+
+        $jq = $(app);
+        return {
+            options: options,
+            format: format,
+            app: app,
+            jq: $jq
+        };
+    };
+}(jQuery));
diff --git a/etc/status/public_html/status-basic.json-sample b/etc/status/public_html/status-basic.json-sample
index 63745a3..4737a7c 100644
--- a/etc/status/public_html/status-basic.json-sample
+++ b/etc/status/public_html/status-basic.json-sample
@@ -1,178 +1,178 @@
-{
-    "last_reconfigured": 1389381756000,
-    "message": "Example info message",
-    "pipelines": [
-        {
-            "name": "test",
-            "description": "Lint and unit tests",
-            "change_queues": [
-                {
-                    "name": "some-jobs@worker301.ci-example.org",
-                    "heads": [
-                        [
-                            {
-                                "id": "10101,1",
-                                "url": "#!/review.example.org/r/10101",
-                                "project": "openstack/infra/zuul",
-                                "jobs": [
-                                    {
-                                        "name": "zuul-merge",
-                                        "url": "#!/jenkins.example.org/job/zuul-merge/201",
-                                        "result": "SUCCESS",
-                                        "voting": true
-                                    },
-                                    {
-                                        "name": "zuul-lint",
-                                        "url": "#!/jenkins.example.org/job/zuul-lint/201",
-                                        "result": "SUCCESS",
-                                        "voting": true
-                                    },
-                                    {
-                                        "name": "zuul-test",
-                                        "url": "#!/jenkins.example.org/job/zuul-test/201",
-                                        "result": "SUCCESS",
-                                        "voting": true
-                                    }
-                                ]
-                            }
-                        ],
-                        [
-                            {
-                                "id": "10103,1",
-                                "url": "#!/review.example.org/r/10103",
-                                "project": "google/gerrit",
-                                "jobs": [
-                                    {
-                                        "name": "gerrit-merge",
-                                        "url": "#!/jenkins.example.org/job/gerrit-merge/203",
-                                        "result": "SUCCESS",
-                                        "voting": false
-                                    }
-                                ]
-                            }
-                        ]
-                    ]
-                },
-                {
-                    "name": "other-jobs@worker301.ci-example.org",
-                    "heads": [
-                        [
-                            {
-                                "id": "10102,1",
-                                "url": "#!/review.example.org/r/10102",
-                                "project": "google/gerrit",
-                                "jobs": [
-                                    {
-                                        "name": "gerrit-merge",
-                                        "url": "#!/jenkins.example.org/job/gerrit-merge/202",
-                                        "result": "UNSTABLE",
-                                        "voting": false
-                                    }
-                                ]
-                            }
-                        ],
-                        [
-                            {
-                                "id": "10104,1",
-                                "url": "#!/review.example.org/r/10104",
-                                "project": "openstack/infra/zuul",
-                                "jobs": [
-                                    {
-                                        "name": "zuul-merge",
-                                        "url": "#!/jenkins.example.org/job/zuul-merge/204",
-                                        "result": "SUCCESS",
-                                        "voting": true
-                                    },
-                                    {
-                                        "name": "zuul-lint",
-                                        "url": "#!/jenkins.example.org/job/zuul-lint/204",
-                                        "result": "FAILURE",
-                                        "voting": true
-                                    },
-                                    {
-                                        "name": "zuul-test",
-                                        "url": "#!/jenkins.example.org/job/zuul-test/204",
-                                        "result": null,
-                                        "voting": true
-                                    }
-                                ]
-                            }
-                        ]
-                    ]
-                }
-            ]
-        },
-        {
-            "name": "gate-and-submit",
-            "change_queues": []
-        },
-        {
-            "name": "postmerge",
-            "change_queues": [
-                {
-                    "name": "some-jobs@worker301.ci-example.org",
-                    "heads": [
-                        [
-                            {
-                                "id": "7f1d65cb0f663c907698f915da01c008c7ef4748",
-                                "url": "#!/review.example.org/r/10100",
-                                "project": "openstack/infra/zuul",
-                                "jobs": [
-                                    {
-                                        "name": "zuul-lint",
-                                        "url": "#!/jenkins.example.org/job/zuul-lint/200",
-                                        "result": "SUCCESS",
-                                        "voting": true
-                                    },
-                                    {
-                                        "name": "zuul-test",
-                                        "url": "#!/jenkins.example.org/job/zuul-test/200",
-                                        "result": "FAILURE",
-                                        "voting": true
-                                    },
-                                    {
-                                        "name": "zuul-regression-python2",
-                                        "url": "#!/jenkins.example.org/job/zuul-regression-python2/200",
-                                        "result": "SUCCESS",
-                                        "voting": true
-                                    },
-                                    {
-                                        "name": "zuul-regression-python3",
-                                        "url": "#!/jenkins.example.org/job/zuul-regression-python3/200",
-                                        "result": "FAILURE",
-                                        "voting": true
-                                    },
-                                    {
-                                        "name": "zuul-performance-python2",
-                                        "url": null,
-                                        "result": null,
-                                        "voting": true
-                                    },
-                                    {
-                                        "name": "zuul-performance-python3",
-                                        "url": null,
-                                        "result": null,
-                                        "voting": true
-                                    },
-                                    {
-                                        "name": "zuul-docs-publish",
-                                        "url": null,
-                                        "result": null,
-                                        "voting": true
-                                    }
-                                ]
-                            }
-                        ]
-                    ]
-                }
-            ]
-        }
-    ],
-    "trigger_event_queue": {
-        "length": 0
-    },
-    "result_event_queue": {
-        "length": 0
-    },
-    "zuul_version": "2.0.0.19"
-}
+{
+    "last_reconfigured": 1389381756000,
+    "message": "Example info message",
+    "pipelines": [
+        {
+            "name": "test",
+            "description": "Lint and unit tests",
+            "change_queues": [
+                {
+                    "name": "some-jobs@worker301.ci-example.org",
+                    "heads": [
+                        [
+                            {
+                                "id": "10101,1",
+                                "url": "#!/review.example.org/r/10101",
+                                "project": "openstack/infra/zuul",
+                                "jobs": [
+                                    {
+                                        "name": "zuul-merge",
+                                        "url": "#!/jenkins.example.org/job/zuul-merge/201",
+                                        "result": "SUCCESS",
+                                        "voting": true
+                                    },
+                                    {
+                                        "name": "zuul-lint",
+                                        "url": "#!/jenkins.example.org/job/zuul-lint/201",
+                                        "result": "SUCCESS",
+                                        "voting": true
+                                    },
+                                    {
+                                        "name": "zuul-test",
+                                        "url": "#!/jenkins.example.org/job/zuul-test/201",
+                                        "result": "SUCCESS",
+                                        "voting": true
+                                    }
+                                ]
+                            }
+                        ],
+                        [
+                            {
+                                "id": "10103,1",
+                                "url": "#!/review.example.org/r/10103",
+                                "project": "google/gerrit",
+                                "jobs": [
+                                    {
+                                        "name": "gerrit-merge",
+                                        "url": "#!/jenkins.example.org/job/gerrit-merge/203",
+                                        "result": "SUCCESS",
+                                        "voting": false
+                                    }
+                                ]
+                            }
+                        ]
+                    ]
+                },
+                {
+                    "name": "other-jobs@worker301.ci-example.org",
+                    "heads": [
+                        [
+                            {
+                                "id": "10102,1",
+                                "url": "#!/review.example.org/r/10102",
+                                "project": "google/gerrit",
+                                "jobs": [
+                                    {
+                                        "name": "gerrit-merge",
+                                        "url": "#!/jenkins.example.org/job/gerrit-merge/202",
+                                        "result": "UNSTABLE",
+                                        "voting": false
+                                    }
+                                ]
+                            }
+                        ],
+                        [
+                            {
+                                "id": "10104,1",
+                                "url": "#!/review.example.org/r/10104",
+                                "project": "openstack/infra/zuul",
+                                "jobs": [
+                                    {
+                                        "name": "zuul-merge",
+                                        "url": "#!/jenkins.example.org/job/zuul-merge/204",
+                                        "result": "SUCCESS",
+                                        "voting": true
+                                    },
+                                    {
+                                        "name": "zuul-lint",
+                                        "url": "#!/jenkins.example.org/job/zuul-lint/204",
+                                        "result": "FAILURE",
+                                        "voting": true
+                                    },
+                                    {
+                                        "name": "zuul-test",
+                                        "url": "#!/jenkins.example.org/job/zuul-test/204",
+                                        "result": null,
+                                        "voting": true
+                                    }
+                                ]
+                            }
+                        ]
+                    ]
+                }
+            ]
+        },
+        {
+            "name": "gate-and-submit",
+            "change_queues": []
+        },
+        {
+            "name": "postmerge",
+            "change_queues": [
+                {
+                    "name": "some-jobs@worker301.ci-example.org",
+                    "heads": [
+                        [
+                            {
+                                "id": "7f1d65cb0f663c907698f915da01c008c7ef4748",
+                                "url": "#!/review.example.org/r/10100",
+                                "project": "openstack/infra/zuul",
+                                "jobs": [
+                                    {
+                                        "name": "zuul-lint",
+                                        "url": "#!/jenkins.example.org/job/zuul-lint/200",
+                                        "result": "SUCCESS",
+                                        "voting": true
+                                    },
+                                    {
+                                        "name": "zuul-test",
+                                        "url": "#!/jenkins.example.org/job/zuul-test/200",
+                                        "result": "FAILURE",
+                                        "voting": true
+                                    },
+                                    {
+                                        "name": "zuul-regression-python2",
+                                        "url": "#!/jenkins.example.org/job/zuul-regression-python2/200",
+                                        "result": "SUCCESS",
+                                        "voting": true
+                                    },
+                                    {
+                                        "name": "zuul-regression-python3",
+                                        "url": "#!/jenkins.example.org/job/zuul-regression-python3/200",
+                                        "result": "FAILURE",
+                                        "voting": true
+                                    },
+                                    {
+                                        "name": "zuul-performance-python2",
+                                        "url": null,
+                                        "result": null,
+                                        "voting": true
+                                    },
+                                    {
+                                        "name": "zuul-performance-python3",
+                                        "url": null,
+                                        "result": null,
+                                        "voting": true
+                                    },
+                                    {
+                                        "name": "zuul-docs-publish",
+                                        "url": null,
+                                        "result": null,
+                                        "voting": true
+                                    }
+                                ]
+                            }
+                        ]
+                    ]
+                }
+            ]
+        }
+    ],
+    "trigger_event_queue": {
+        "length": 0
+    },
+    "result_event_queue": {
+        "length": 0
+    },
+    "zuul_version": "2.0.0.19"
+}
diff --git a/etc/status/public_html/status-openstack.json-sample b/etc/status/public_html/status-openstack.json-sample
index 41c231a..ab992c3 100644
--- a/etc/status/public_html/status-openstack.json-sample
+++ b/etc/status/public_html/status-openstack.json-sample
@@ -1,312 +1,312 @@
-{
-    "last_reconfigured": 1389381756000,
-    "pipelines": [
-        {
-            "name": "check",
-            "description": "Newly uploaded patchsets enter this pipeline to receive an initial +/-1 Verified vote from Jenkins.",
-            "change_queues": [
-                {
-                    "heads": [],
-                    "name": "stackforge/tripleo-image-elements"
-                }
-            ]
-        },
-        {
-            "description": "Changes that have been approved by core developers are enqueued in order in this pipeline, and .",
-            "change_queues": [
-                {
-                    "heads": [],
-                    "name": "openstack-detackforge/reddwarf-integration"
-                },
-                {
-                    "heads": [],
-                    "name": "stackforge/moniker"
-                },
-                {
-                    "heads": [
-                        [
-                            {
-                                "url": "https://review.openstack.org/26292",
-                                "project": "openstack/quantum",
-                                "jobs": [
-                                    {
-                                        "url": "https://jenkins.openstack.org/job/gate-quantum-docs/5501/consoleFull",
-                                        "voting": true,
-                                        "result": "SUCCESS",
-                                        "name": "gate-quantum-docs"
-                                    },
-                                    {
-                                        "url": "https://jenkins.openstack.org/job/gate-quantum-pep8/6254/consoleFull",
-                                        "voting": true,
-                                        "result": "SUCCESS",
-                                        "name": "gate-quantum-pep8"
-                                    },
-                                    {
-                                        "url": "https://jenkins.openstack.org/job/gate-quantum-python26/5876/",
-                                        "voting": true,
-                                        "result": null,
-                                        "name": "gate-quantum-python26"
-                                    },
-                                    {
-                                        "url": "https://jenkins.openstack.org/job/gate-quantum-python27/5887/",
-                                        "voting": true,
-                                        "result": null,
-                                        "name": "gate-quantum-python27"
-                                    },
-                                    {
-                                        "url": "https://jenkins.openstack.org/job/gate-tempest-devstack-vm-quantum/17548/",
-                                        "voting": true,
-                                        "result": null,
-                                        "name": "gate-tempest-devstack-vm-quantum"
-                                    }
-                                ],
-                                "id": "26292,1"
-                            }
-                        ]
-                    ],
-                    "name": "openstack-dev/devstack, openstack-infra/devstack-gate, openstack/cinder, openstack/glance, openstack/horizon, openstack/keystone, openstack/nova, openstack/python-cinderclient, openstack/python-glanceclient, openstack/python-keystoneclient, openstack/python-novaclient, openstack/python-quantumclient, openstack/quantum, openstack/swift, openstack/tempest, z/tempest"
-                },
-                {
-                    "heads": [],
-                    "name": "openstack/ceilometer"
-                },
-                {
-                    "heads": [],
-                    "name": "stackforge/puppet-openstack"
-                },
-                {
-                    "heads": [],
-                    "name": "stackforge/puppet-cinder"
-                },
-                {
-                    "heads": [],
-                    "name": "stackforge/marconi"
-                },
-                {
-                    "heads": [],
-                    "name": "openstack-infra/config"
-                },
-                {
-                    "heads": [],
-                    "name": "stackforge/tripleo-image-elements"
-                },
-                {
-                    "heads": [],
-                    "name": "stackforge/kwapi"
-                },
-                {
-                    "heads": [],
-                    "name": "stackforge/python-reddwarfclient"
-                },
-                {
-                    "heads": [],
-                    "name": "stackforge/python-savannaclient"
-                },
-                {
-                    "heads": [],
-                    "name": "stackforge/python-monikerclient"
-                },
-                {
-                    "heads": [],
-                    "name": "stackforge/packstack"
-                },
-                {
-                    "heads": [],
-                    "name": "openstack/oslo.config"
-                },
-                {
-                    "heads": [],
-                    "name": "openstack-infra/jenkins-job-builder"
-                },
-                {
-                    "heads": [],
-                    "name": "stackforge/puppet-horizon"
-                },
-                {
-                    "heads": [],
-                    "name": "openstack/heat-cfntools"
-                },
-                {
-                    "heads": [],
-                    "name": "openstack/oslo-incubator"
-                },
-                {
-                    "heads": [],
-                    "name": "stackforge/os-config-applier"
-                },
-                {
-                    "heads": [],
-                    "name": "openstack/requirements"
-                },
-                {
-                    "heads": [],
-                    "name": "stackforge/puppet-glance"
-                },
-                {
-                    "heads": [],
-                    "name": "openstack-infra/gearman-plugin"
-                },
-                {
-                    "heads": [],
-                    "name": "stackforge/puppet-keystone"
-                },
-                {
-                    "heads": [],
-                    "name": "stackforge/puppet-nova"
-                },
-                {
-                    "heads": [],
-                    "name": "stackforge/climate"
-                },
-                {
-                    "heads": [],
-                    "name": "openstack/python-swiftclient"
-                },
-                {
-                    "heads": [],
-                    "name": "openstack/python-ceilometerclient"
-                },
-                {
-                    "heads": [],
-                    "name": "openstack-infra/git-review"
-                },
-                {
-                    "heads": [],
-                    "name": "stackforge/bufunfa"
-                },
-                {
-                    "heads": [],
-                    "name": "stackforge/puppet-swift"
-                },
-                {
-                    "heads": [],
-                    "name": "openstack-infra/statusbot"
-                },
-                {
-                    "heads": [],
-                    "name": "openstack/openstack-planet"
-                },
-                {
-                    "heads": [],
-                    "name": "openstack/python-openstackclient"
-                },
-                {
-                    "heads": [],
-                    "name": "stackforge/diskimage-builder"
-                },
-                {
-                    "heads": [],
-                    "name": "openstack-infra/gerritlib"
-                },
-                {
-                    "heads": [],
-                    "name": "openstack-infra/zuul"
-                },
-                {
-                    "heads": [],
-                    "name": "stackforge/reddwarf"
-                },
-                {
-                    "heads": [],
-                    "name": "openstack-dev/hacking"
-                },
-                {
-                    "heads": [],
-                    "name": "openstack/python-heatclient"
-                },
-                {
-                    "heads": [],
-                    "name": "stackforge/python-libraclient"
-                },
-                {
-                    "heads": [],
-                    "name": "openstack-infra/reviewday"
-                },
-                {
-                    "heads": [],
-                    "name": "openstack-infra/jeepyb"
-                },
-                {
-                    "heads": [],
-                    "name": "openstack/heat"
-                },
-                {
-                    "heads": [],
-                    "name": "stackforge/libra"
-                },
-                {
-                    "heads": [],
-                    "name": "openstack-infra/gerrit"
-                },
-                {
-                    "heads": [],
-                    "name": "stackforge/healthnmon"
-                },
-                {
-                    "heads": [],
-                    "name": "openstack-infra/gerritbot"
-                },
-                {
-                    "heads": [],
-                    "name": "openstack-dev/pbr"
-                },
-                {
-                    "heads": [],
-                    "name": "stackforge/savanna"
-                },
-                {
-                    "heads": [],
-                    "name": "openstack/openstack-manuals"
-                }
-            ],
-            "name": "gate"
-        },
-        {
-            "description": "This pipeline runs jobs that operate after each change is merged.",
-            "change_queues": [
-                {
-                    "heads": [],
-                    "name": "openstack-dev/hacking, openstack-dev/openstack-qa, openstack-dev/pbr, openstack-infra/config, openstack-infra/gearman-plugin, openstack-infra/gerrit, openstack-infra/gerritbot, openstack-infra/git-review, openstack-infra/jenkins-job-builder, openstack-infra/nose-html-output, openstack-infra/reviewday, openstack-infra/statusbot, openstack-infra/zuul, openstack/api-site, openstack/ceilometer, openstack/cinder, openstack/compute-api, openstack/glance, openstack/heat, openstack/heat-cfntools, openstack/horizon, openstack/identity-api, openstack/image-api, openstack/keystone, openstack/netconn-api, openstack/nova, openstack/object-api, openstack/openstack-manuals, openstack/oslo-incubator, openstack/oslo.config, openstack/python-ceilometerclient, openstack/python-cinderclient, openstack/python-glanceclient, openstack/python-heatclient, openstack/python-keystoneclient, openstack/python-novaclient, openstack/python-openstackclient, openstack/python-quantumclient, openstack/python-swiftclient, openstack/quantum, openstack/requirements, openstack/swift, openstack/volume-api, stackforge/bufunfa, stackforge/diskimage-builder, stackforge/moniker, stackforge/os-config-applier, stackforge/python-monikerclient, stackforge/python-savannaclient, stackforge/reddwarf, stackforge/savanna, stackforge/tripleo-image-elements"
-                }
-            ],
-            "name": "post"
-        },
-        {
-            "description": "This pipeline runs jobs on projects in response to pre-release tags.",
-            "change_queues": [
-                {
-                    "heads": [],
-                    "name": "openstack-dev/hacking, openstack-dev/pbr, openstack-infra/gerritbot, openstack-infra/gerritlib, openstack-infra/git-review, openstack-infra/jeepyb, openstack-infra/jenkins-job-builder, openstack-infra/nose-html-output, openstack-infra/reviewday, openstack-infra/statusbot, openstack-infra/zuul, openstack/ceilometer, openstack/cinder, openstack/glance, openstack/heat, openstack/heat-cfntools, openstack/horizon, openstack/keystone, openstack/nova, openstack/oslo.config, openstack/python-ceilometerclient, openstack/python-cinderclient, openstack/python-glanceclient, openstack/python-heatclient, openstack/python-keystoneclient, openstack/python-novaclient, openstack/python-openstackclient, openstack/python-quantumclient, openstack/python-swiftclient, openstack/quantum, openstack/swift, stackforge/moniker, stackforge/python-monikerclient, stackforge/python-reddwarfclient, stackforge/python-savannaclient, stackforge/savanna"
-                }
-            ],
-            "name": "pre-release"
-        },
-        {
-            "description": "When a commit is tagged as a release, this pipeline runs jobs that publish archives and documentation.",
-            "change_queues": [
-                {
-                    "heads": [],
-                    "name": "openstack-dev/hacking, openstack-dev/openstack-qa, openstack-dev/pbr, openstack-infra/gerritbot, openstack-infra/gerritlib, openstack-infra/git-review, openstack-infra/jeepyb, openstack-infra/jenkins-job-builder, openstack-infra/nose-html-output, openstack-infra/reviewday, openstack-infra/statusbot, openstack-infra/zuul, openstack/ceilometer, openstack/cinder, openstack/glance, openstack/heat, openstack/heat-cfntools, openstack/horizon, openstack/keystone, openstack/nova, openstack/oslo-incubator, openstack/oslo.config, openstack/python-ceilometerclient, openstack/python-cinderclient, openstack/python-glanceclient, openstack/python-heatclient, openstack/python-keystoneclient, openstack/python-novaclient, openstack/python-openstackclient, openstack/python-quantumclient, openstack/python-swiftclient, openstack/quantum, openstack/swift, stackforge/moniker, stackforge/python-monikerclient, stackforge/python-reddwarfclient, stackforge/python-savannaclient, stackforge/savanna"
-                }
-            ],
-            "name": "release"
-        },
-        {
-            "description": "This pipeline is used for silently testing new jobs.",
-            "change_queues": [
-                {
-                    "heads": [],
-                    "name": ""
-                }
-            ],
-            "name": "silent"
-        }
-    ],
-    "trigger_event_queue": {
-        "length": 0
-    },
-    "result_event_queue": {
-        "length": 0
-    },
-    "zuul_version": "2.0.0.19"
-}
+{
+    "last_reconfigured": 1389381756000,
+    "pipelines": [
+        {
+            "name": "check",
+            "description": "Newly uploaded patchsets enter this pipeline to receive an initial +/-1 Verified vote from Jenkins.",
+            "change_queues": [
+                {
+                    "heads": [],
+                    "name": "stackforge/tripleo-image-elements"
+                }
+            ]
+        },
+        {
+            "description": "Changes that have been approved by core developers are enqueued in order in this pipeline, and .",
+            "change_queues": [
+                {
+                    "heads": [],
+                    "name": "openstack-detackforge/reddwarf-integration"
+                },
+                {
+                    "heads": [],
+                    "name": "stackforge/moniker"
+                },
+                {
+                    "heads": [
+                        [
+                            {
+                                "url": "https://review.openstack.org/26292",
+                                "project": "openstack/quantum",
+                                "jobs": [
+                                    {
+                                        "url": "https://jenkins.openstack.org/job/gate-quantum-docs/5501/consoleFull",
+                                        "voting": true,
+                                        "result": "SUCCESS",
+                                        "name": "gate-quantum-docs"
+                                    },
+                                    {
+                                        "url": "https://jenkins.openstack.org/job/gate-quantum-pep8/6254/consoleFull",
+                                        "voting": true,
+                                        "result": "SUCCESS",
+                                        "name": "gate-quantum-pep8"
+                                    },
+                                    {
+                                        "url": "https://jenkins.openstack.org/job/gate-quantum-python26/5876/",
+                                        "voting": true,
+                                        "result": null,
+                                        "name": "gate-quantum-python26"
+                                    },
+                                    {
+                                        "url": "https://jenkins.openstack.org/job/gate-quantum-python27/5887/",
+                                        "voting": true,
+                                        "result": null,
+                                        "name": "gate-quantum-python27"
+                                    },
+                                    {
+                                        "url": "https://jenkins.openstack.org/job/gate-tempest-devstack-vm-quantum/17548/",
+                                        "voting": true,
+                                        "result": null,
+                                        "name": "gate-tempest-devstack-vm-quantum"
+                                    }
+                                ],
+                                "id": "26292,1"
+                            }
+                        ]
+                    ],
+                    "name": "openstack-dev/devstack, openstack-infra/devstack-gate, openstack/cinder, openstack/glance, openstack/horizon, openstack/keystone, openstack/nova, openstack/python-cinderclient, openstack/python-glanceclient, openstack/python-keystoneclient, openstack/python-novaclient, openstack/python-quantumclient, openstack/quantum, openstack/swift, openstack/tempest, z/tempest"
+                },
+                {
+                    "heads": [],
+                    "name": "openstack/ceilometer"
+                },
+                {
+                    "heads": [],
+                    "name": "stackforge/puppet-openstack"
+                },
+                {
+                    "heads": [],
+                    "name": "stackforge/puppet-cinder"
+                },
+                {
+                    "heads": [],
+                    "name": "stackforge/marconi"
+                },
+                {
+                    "heads": [],
+                    "name": "openstack-infra/config"
+                },
+                {
+                    "heads": [],
+                    "name": "stackforge/tripleo-image-elements"
+                },
+                {
+                    "heads": [],
+                    "name": "stackforge/kwapi"
+                },
+                {
+                    "heads": [],
+                    "name": "stackforge/python-reddwarfclient"
+                },
+                {
+                    "heads": [],
+                    "name": "stackforge/python-savannaclient"
+                },
+                {
+                    "heads": [],
+                    "name": "stackforge/python-monikerclient"
+                },
+                {
+                    "heads": [],
+                    "name": "stackforge/packstack"
+                },
+                {
+                    "heads": [],
+                    "name": "openstack/oslo.config"
+                },
+                {
+                    "heads": [],
+                    "name": "openstack-infra/jenkins-job-builder"
+                },
+                {
+                    "heads": [],
+                    "name": "stackforge/puppet-horizon"
+                },
+                {
+                    "heads": [],
+                    "name": "openstack/heat-cfntools"
+                },
+                {
+                    "heads": [],
+                    "name": "openstack/oslo-incubator"
+                },
+                {
+                    "heads": [],
+                    "name": "stackforge/os-config-applier"
+                },
+                {
+                    "heads": [],
+                    "name": "openstack/requirements"
+                },
+                {
+                    "heads": [],
+                    "name": "stackforge/puppet-glance"
+                },
+                {
+                    "heads": [],
+                    "name": "openstack-infra/gearman-plugin"
+                },
+                {
+                    "heads": [],
+                    "name": "stackforge/puppet-keystone"
+                },
+                {
+                    "heads": [],
+                    "name": "stackforge/puppet-nova"
+                },
+                {
+                    "heads": [],
+                    "name": "stackforge/climate"
+                },
+                {
+                    "heads": [],
+                    "name": "openstack/python-swiftclient"
+                },
+                {
+                    "heads": [],
+                    "name": "openstack/python-ceilometerclient"
+                },
+                {
+                    "heads": [],
+                    "name": "openstack-infra/git-review"
+                },
+                {
+                    "heads": [],
+                    "name": "stackforge/bufunfa"
+                },
+                {
+                    "heads": [],
+                    "name": "stackforge/puppet-swift"
+                },
+                {
+                    "heads": [],
+                    "name": "openstack-infra/statusbot"
+                },
+                {
+                    "heads": [],
+                    "name": "openstack/openstack-planet"
+                },
+                {
+                    "heads": [],
+                    "name": "openstack/python-openstackclient"
+                },
+                {
+                    "heads": [],
+                    "name": "stackforge/diskimage-builder"
+                },
+                {
+                    "heads": [],
+                    "name": "openstack-infra/gerritlib"
+                },
+                {
+                    "heads": [],
+                    "name": "openstack-infra/zuul"
+                },
+                {
+                    "heads": [],
+                    "name": "stackforge/reddwarf"
+                },
+                {
+                    "heads": [],
+                    "name": "openstack-dev/hacking"
+                },
+                {
+                    "heads": [],
+                    "name": "openstack/python-heatclient"
+                },
+                {
+                    "heads": [],
+                    "name": "stackforge/python-libraclient"
+                },
+                {
+                    "heads": [],
+                    "name": "openstack-infra/reviewday"
+                },
+                {
+                    "heads": [],
+                    "name": "openstack-infra/jeepyb"
+                },
+                {
+                    "heads": [],
+                    "name": "openstack/heat"
+                },
+                {
+                    "heads": [],
+                    "name": "stackforge/libra"
+                },
+                {
+                    "heads": [],
+                    "name": "openstack-infra/gerrit"
+                },
+                {
+                    "heads": [],
+                    "name": "stackforge/healthnmon"
+                },
+                {
+                    "heads": [],
+                    "name": "openstack-infra/gerritbot"
+                },
+                {
+                    "heads": [],
+                    "name": "openstack-dev/pbr"
+                },
+                {
+                    "heads": [],
+                    "name": "stackforge/savanna"
+                },
+                {
+                    "heads": [],
+                    "name": "openstack/openstack-manuals"
+                }
+            ],
+            "name": "gate"
+        },
+        {
+            "description": "This pipeline runs jobs that operate after each change is merged.",
+            "change_queues": [
+                {
+                    "heads": [],
+                    "name": "openstack-dev/hacking, openstack-dev/openstack-qa, openstack-dev/pbr, openstack-infra/config, openstack-infra/gearman-plugin, openstack-infra/gerrit, openstack-infra/gerritbot, openstack-infra/git-review, openstack-infra/jenkins-job-builder, openstack-infra/nose-html-output, openstack-infra/reviewday, openstack-infra/statusbot, openstack-infra/zuul, openstack/api-site, openstack/ceilometer, openstack/cinder, openstack/compute-api, openstack/glance, openstack/heat, openstack/heat-cfntools, openstack/horizon, openstack/identity-api, openstack/image-api, openstack/keystone, openstack/netconn-api, openstack/nova, openstack/object-api, openstack/openstack-manuals, openstack/oslo-incubator, openstack/oslo.config, openstack/python-ceilometerclient, openstack/python-cinderclient, openstack/python-glanceclient, openstack/python-heatclient, openstack/python-keystoneclient, openstack/python-novaclient, openstack/python-openstackclient, openstack/python-quantumclient, openstack/python-swiftclient, openstack/quantum, openstack/requirements, openstack/swift, openstack/volume-api, stackforge/bufunfa, stackforge/diskimage-builder, stackforge/moniker, stackforge/os-config-applier, stackforge/python-monikerclient, stackforge/python-savannaclient, stackforge/reddwarf, stackforge/savanna, stackforge/tripleo-image-elements"
+                }
+            ],
+            "name": "post"
+        },
+        {
+            "description": "This pipeline runs jobs on projects in response to pre-release tags.",
+            "change_queues": [
+                {
+                    "heads": [],
+                    "name": "openstack-dev/hacking, openstack-dev/pbr, openstack-infra/gerritbot, openstack-infra/gerritlib, openstack-infra/git-review, openstack-infra/jeepyb, openstack-infra/jenkins-job-builder, openstack-infra/nose-html-output, openstack-infra/reviewday, openstack-infra/statusbot, openstack-infra/zuul, openstack/ceilometer, openstack/cinder, openstack/glance, openstack/heat, openstack/heat-cfntools, openstack/horizon, openstack/keystone, openstack/nova, openstack/oslo.config, openstack/python-ceilometerclient, openstack/python-cinderclient, openstack/python-glanceclient, openstack/python-heatclient, openstack/python-keystoneclient, openstack/python-novaclient, openstack/python-openstackclient, openstack/python-quantumclient, openstack/python-swiftclient, openstack/quantum, openstack/swift, stackforge/moniker, stackforge/python-monikerclient, stackforge/python-reddwarfclient, stackforge/python-savannaclient, stackforge/savanna"
+                }
+            ],
+            "name": "pre-release"
+        },
+        {
+            "description": "When a commit is tagged as a release, this pipeline runs jobs that publish archives and documentation.",
+            "change_queues": [
+                {
+                    "heads": [],
+                    "name": "openstack-dev/hacking, openstack-dev/openstack-qa, openstack-dev/pbr, openstack-infra/gerritbot, openstack-infra/gerritlib, openstack-infra/git-review, openstack-infra/jeepyb, openstack-infra/jenkins-job-builder, openstack-infra/nose-html-output, openstack-infra/reviewday, openstack-infra/statusbot, openstack-infra/zuul, openstack/ceilometer, openstack/cinder, openstack/glance, openstack/heat, openstack/heat-cfntools, openstack/horizon, openstack/keystone, openstack/nova, openstack/oslo-incubator, openstack/oslo.config, openstack/python-ceilometerclient, openstack/python-cinderclient, openstack/python-glanceclient, openstack/python-heatclient, openstack/python-keystoneclient, openstack/python-novaclient, openstack/python-openstackclient, openstack/python-quantumclient, openstack/python-swiftclient, openstack/quantum, openstack/swift, stackforge/moniker, stackforge/python-monikerclient, stackforge/python-reddwarfclient, stackforge/python-savannaclient, stackforge/savanna"
+                }
+            ],
+            "name": "release"
+        },
+        {
+            "description": "This pipeline is used for silently testing new jobs.",
+            "change_queues": [
+                {
+                    "heads": [],
+                    "name": ""
+                }
+            ],
+            "name": "silent"
+        }
+    ],
+    "trigger_event_queue": {
+        "length": 0
+    },
+    "result_event_queue": {
+        "length": 0
+    },
+    "zuul_version": "2.0.0.19"
+}
diff --git a/etc/status/public_html/styles/zuul.css b/etc/status/public_html/styles/zuul.css
index 44fd737..1d51a76 100644
--- a/etc/status/public_html/styles/zuul.css
+++ b/etc/status/public_html/styles/zuul.css
@@ -1,58 +1,58 @@
-.zuul-change {
-    margin-bottom: 10px;
-}
-
-.zuul-change-id {
-    float: right;
-}
-
-.zuul-job-result {
-    float: right;
-    width: 70px;
-    height: 15px;
-    margin: 2px 0 0 0;
-}
-
-.zuul-change-total-result {
-    height: 10px;
-    width: 100px;
-    margin: 0;
-    display: inline-block;
-    vertical-align: middle;
-}
-
-.zuul-spinner,
-.zuul-spinner:hover {
-    opacity: 0;
-    transition: opacity 0.5s ease-out;
-    cursor: default;
-    pointer-events: none;
-}
-
-.zuul-spinner-on,
-.zuul-spinner-on:hover {
-    opacity: 1;
-    transition-duration: 0.2s;
-    cursor: progress;
-}
-
-.zuul-change-cell {
-    padding-left: 5px;
-}
-
-.zuul-change-job {
-    padding: 2px 8px;
-}
-
-.zuul-job-name {
-    font-size: small;
-}
-
-.zuul-non-voting-desc {
-    font-size: smaller;
-}
-
-.zuul-patchset-header {
-    font-size: small;
-    padding: 8px 12px;
+.zuul-change {
+    margin-bottom: 10px;
+}
+
+.zuul-change-id {
+    float: right;
+}
+
+.zuul-job-result {
+    float: right;
+    width: 70px;
+    height: 15px;
+    margin: 2px 0 0 0;
+}
+
+.zuul-change-total-result {
+    height: 10px;
+    width: 100px;
+    margin: 0;
+    display: inline-block;
+    vertical-align: middle;
+}
+
+.zuul-spinner,
+.zuul-spinner:hover {
+    opacity: 0;
+    transition: opacity 0.5s ease-out;
+    cursor: default;
+    pointer-events: none;
+}
+
+.zuul-spinner-on,
+.zuul-spinner-on:hover {
+    opacity: 1;
+    transition-duration: 0.2s;
+    cursor: progress;
+}
+
+.zuul-change-cell {
+    padding-left: 5px;
+}
+
+.zuul-change-job {
+    padding: 2px 8px;
+}
+
+.zuul-job-name {
+    font-size: small;
+}
+
+.zuul-non-voting-desc {
+    font-size: smaller;
+}
+
+.zuul-patchset-header {
+    font-size: small;
+    padding: 8px 12px;
 }
\ No newline at end of file
diff --git a/etc/status/public_html/zuul.app.js b/etc/status/public_html/zuul.app.js
index 6321af8..e9ceb97 100644
--- a/etc/status/public_html/zuul.app.js
+++ b/etc/status/public_html/zuul.app.js
@@ -1,104 +1,104 @@
-// Client script for Zuul status page
-//
-// Copyright 2013 OpenStack Foundation
-// Copyright 2013 Timo Tijhof
-// Copyright 2013 Wikimedia Foundation
-// Copyright 2014 Rackspace Australia
-//
-// Licensed under the Apache License, Version 2.0 (the "License"); you may
-// not use this file except in compliance with the License. You may obtain
-// a copy of the License at
-//
-//      http://www.apache.org/licenses/LICENSE-2.0
-//
-// Unless required by applicable law or agreed to in writing, software
-// distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-// WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-// License for the specific language governing permissions and limitations
-// under the License.
-
-/*exported zuul_build_dom, zuul_start */
-
-function zuul_build_dom($, container) {
-    // Build a default-looking DOM
-    var default_layout = '<div class="container">'
-        + '<h1>Zuul Status</h1>'
-        + '<p>Real-time status monitor of Zuul, the pipeline manager between Gerrit and Workers.</p>'
-        + '<div class="zuul-container" id="zuul-container">'
-        + '<div style="display: none;" class="alert" id="zuul_msg"></div>'
-        + '<button class="btn pull-right zuul-spinner">updating <span class="glyphicon glyphicon-refresh"></span></button>'
-        + '<p>Queue lengths: <span id="zuul_queue_events_num">0</span> events, <span id="zuul_queue_results_num">0</span> results.</p>'
-        + '<div id="zuul_controls"></div>'
-        + '<div id="zuul_pipelines" class="row"></div>'
-        + '<p>Zuul version: <span id="zuul-version-span"></span></p>'
-        + '<p>Last reconfigured: <span id="last-reconfigured-span"></span></p>'
-        + '</div></div>';
-
-    $(function ($) {
-        // DOM ready
-        var $container = $(container);
-        $container.html(default_layout);
-    });
-}
-
-/**
- * @return The $.zuul instance
- */
-function zuul_start($) {
-    // Start the zuul app (expects default dom)
-
-    var $container, $indicator;
-    var demo = location.search.match(/[?&]demo=([^?&]*)/),
-        source_url = location.search.match(/[?&]source_url=([^?&]*)/),
-        source = demo ? './status-' + (demo[1] || 'basic') + '.json-sample' :
-            'status.json';
-    source = source_url ? source_url[1] : source;
-
-    var zuul = $.zuul({
-        source: source,
-        //graphite_url: 'http://graphite.openstack.org/render/'
-    });
-
-    zuul.jq.on('update-start', function () {
-        $container.addClass('zuul-container-loading');
-        $indicator.addClass('zuul-spinner-on');
-    });
-
-    zuul.jq.on('update-end', function () {
-        $container.removeClass('zuul-container-loading');
-        setTimeout(function () {
-            $indicator.removeClass('zuul-spinner-on');
-        }, 500);
-    });
-
-    zuul.jq.one('update-end', function () {
-        // Do this asynchronous so that if the first update adds a
-        // message, it will not animate while we fade in the content.
-        // Instead it simply appears with the rest of the content.
-        setTimeout(function () {
-            // Fade in the content
-            $container.addClass('zuul-container-ready');
-        });
-    });
-
-    $(function ($) {
-        // DOM ready
-        $container = $('#zuul-container');
-        $indicator = $('#zuul-spinner');
-        $('#zuul_controls').append(zuul.app.control_form());
-
-        zuul.app.schedule();
-
-        $(document).on({
-            'show.visibility': function () {
-                zuul.options.enabled = true;
-                zuul.app.update();
-            },
-            'hide.visibility': function () {
-                zuul.options.enabled = false;
-            }
-        });
-    });
-
-    return zuul;
-}
+// Client script for Zuul status page
+//
+// Copyright 2013 OpenStack Foundation
+// Copyright 2013 Timo Tijhof
+// Copyright 2013 Wikimedia Foundation
+// Copyright 2014 Rackspace Australia
+//
+// Licensed under the Apache License, Version 2.0 (the "License"); you may
+// not use this file except in compliance with the License. You may obtain
+// a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+// WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+// License for the specific language governing permissions and limitations
+// under the License.
+
+/*exported zuul_build_dom, zuul_start */
+
+function zuul_build_dom($, container) {
+    // Build a default-looking DOM
+    var default_layout = '<div class="container">'
+        + '<h1>Zuul Status</h1>'
+        + '<p>Real-time status monitor of Zuul, the pipeline manager between Gerrit and Workers.</p>'
+        + '<div class="zuul-container" id="zuul-container">'
+        + '<div style="display: none;" class="alert" id="zuul_msg"></div>'
+        + '<button class="btn pull-right zuul-spinner">updating <span class="glyphicon glyphicon-refresh"></span></button>'
+        + '<p>Queue lengths: <span id="zuul_queue_events_num">0</span> events, <span id="zuul_queue_results_num">0</span> results.</p>'
+        + '<div id="zuul_controls"></div>'
+        + '<div id="zuul_pipelines" class="row"></div>'
+        + '<p>Zuul version: <span id="zuul-version-span"></span></p>'
+        + '<p>Last reconfigured: <span id="last-reconfigured-span"></span></p>'
+        + '</div></div>';
+
+    $(function ($) {
+        // DOM ready
+        var $container = $(container);
+        $container.html(default_layout);
+    });
+}
+
+/**
+ * @return The $.zuul instance
+ */
+function zuul_start($) {
+    // Start the zuul app (expects default dom)
+
+    var $container, $indicator;
+    var demo = location.search.match(/[?&]demo=([^?&]*)/),
+        source_url = location.search.match(/[?&]source_url=([^?&]*)/),
+        source = demo ? './status-' + (demo[1] || 'basic') + '.json-sample' :
+            'status.json';
+    source = source_url ? source_url[1] : source;
+
+    var zuul = $.zuul({
+        source: source,
+        //graphite_url: 'http://graphite.openstack.org/render/'
+    });
+
+    zuul.jq.on('update-start', function () {
+        $container.addClass('zuul-container-loading');
+        $indicator.addClass('zuul-spinner-on');
+    });
+
+    zuul.jq.on('update-end', function () {
+        $container.removeClass('zuul-container-loading');
+        setTimeout(function () {
+            $indicator.removeClass('zuul-spinner-on');
+        }, 500);
+    });
+
+    zuul.jq.one('update-end', function () {
+        // Do this asynchronous so that if the first update adds a
+        // message, it will not animate while we fade in the content.
+        // Instead it simply appears with the rest of the content.
+        setTimeout(function () {
+            // Fade in the content
+            $container.addClass('zuul-container-ready');
+        });
+    });
+
+    $(function ($) {
+        // DOM ready
+        $container = $('#zuul-container');
+        $indicator = $('#zuul-spinner');
+        $('#zuul_controls').append(zuul.app.control_form());
+
+        zuul.app.schedule();
+
+        $(document).on({
+            'show.visibility': function () {
+                zuul.options.enabled = true;
+                zuul.app.update();
+            },
+            'hide.visibility': function () {
+                zuul.options.enabled = false;
+            }
+        });
+    });
+
+    return zuul;
+}
diff --git a/etc/zuul.conf-sample b/etc/zuul.conf-sample
index 77ed59e..37f3d3f 100644
--- a/etc/zuul.conf-sample
+++ b/etc/zuul.conf-sample
@@ -1,42 +1,42 @@
-[gearman]
-server=127.0.0.1
-
-[gearman_server]
-start=true
-
-[zuul]
-layout_config=/etc/zuul/layout.yaml
-log_config=/etc/zuul/logging.conf
-pidfile=/var/run/zuul/zuul.pid
-state_dir=/var/lib/zuul
-status_url=https://jenkins.example.com/zuul/status
-
-[merger]
-git_dir=/var/lib/zuul/git
-;git_user_email=zuul@example.com
-;git_user_name=zuul
-zuul_url=http://zuul.example.com/p
-
-[swift]
-authurl=https://identity.api.example.org/v2.0/
-user=username
-key=password
-
-default_container=logs
-region_name=EXP
-logserver_prefix=http://logs.example.org/server.app/
-
-[connection gerrit]
-driver=gerrit
-server=review.example.com
-;baseurl=https://review.example.com/r
-user=jenkins
-sshkey=/home/jenkins/.ssh/id_rsa
-event_delay=10
-
-[connection smtp]
-driver=smtp
-server=localhost
-port=25
-default_from=zuul@example.com
-default_to=you@example.com
+[gearman]
+server=127.0.0.1
+
+[gearman_server]
+start=true
+
+[zuul]
+layout_config=/etc/zuul/layout.yaml
+log_config=/etc/zuul/logging.conf
+pidfile=/var/run/zuul/zuul.pid
+state_dir=/var/lib/zuul
+status_url=https://jenkins.example.com/zuul/status
+
+[merger]
+git_dir=/var/lib/zuul/git
+;git_user_email=zuul@example.com
+;git_user_name=zuul
+zuul_url=http://zuul.example.com/p
+
+[swift]
+authurl=https://identity.api.example.org/v2.0/
+user=username
+key=password
+
+default_container=logs
+region_name=EXP
+logserver_prefix=http://logs.example.org/server.app/
+
+[connection gerrit]
+driver=gerrit
+server=review.example.com
+;baseurl=https://review.example.com/r
+user=jenkins
+sshkey=/home/jenkins/.ssh/id_rsa
+event_delay=10
+
+[connection smtp]
+driver=smtp
+server=localhost
+port=25
+default_from=zuul@example.com
+default_to=you@example.com
diff --git a/requirements.txt b/requirements.txt
index 0c85d7a..fc63612 100644
--- a/requirements.txt
+++ b/requirements.txt
@@ -1,16 +1,16 @@
-pbr>=0.5.21
-
-PyYAML>=3.1.0
-Paste
-WebOb
-paramiko<2.0.0
-GitPython>=0.3.3
-python-daemon>=2.0.4,<2.1.0
-extras
-statsd>=1.0.0,<3.0
-voluptuous>=0.7
-gear>=0.5.7,<1.0.0
-apscheduler>=3.0,<3.1.0
-PrettyTable>=0.6,<0.8
-babel>=1.0
-six>=1.6.0
+pbr>=0.5.21
+
+PyYAML>=3.1.0
+Paste
+WebOb
+paramiko<2.0.0
+GitPython>=0.3.3
+python-daemon>=2.0.4,<2.1.0
+extras
+statsd>=1.0.0,<3.0
+voluptuous>=0.7
+gear>=0.5.7,<1.0.0
+apscheduler>=3.0,<3.1.0
+PrettyTable>=0.6,<0.8
+babel>=1.0
+six>=1.6.0
diff --git a/setup.cfg b/setup.cfg
index 620e1ac..a5f6751 100644
--- a/setup.cfg
+++ b/setup.cfg
@@ -1,32 +1,32 @@
-[metadata]
-name = zuul
-summary = Trunk Gating System
-description-file =
-    README.rst
-author = OpenStack Infrastructure Team
-author-email = openstack-infra@lists.openstack.org
-home-page = http://docs.openstack.org/infra/system-config/
-classifier =
-    Intended Audience :: Information Technology
-    Intended Audience :: System Administrators
-    License :: OSI Approved :: Apache Software License
-    Operating System :: POSIX :: Linux
-    Programming Language :: Python
-    Programming Language :: Python :: 2
-    Programming Language :: Python :: 2.7
-    Programming Language :: Python :: 2.6
-
-[pbr]
-warnerrors = True
-
-[entry_points]
-console_scripts =
-    zuul-server = zuul.cmd.server:main
-    zuul-merger = zuul.cmd.merger:main
-    zuul = zuul.cmd.client:main
-    zuul-cloner = zuul.cmd.cloner:main
-
-[build_sphinx]
-source-dir = doc/source
-build-dir = doc/build
-all_files = 1
+[metadata]
+name = zuul
+summary = Trunk Gating System
+description-file =
+    README.rst
+author = OpenStack Infrastructure Team
+author-email = openstack-infra@lists.openstack.org
+home-page = http://docs.openstack.org/infra/system-config/
+classifier =
+    Intended Audience :: Information Technology
+    Intended Audience :: System Administrators
+    License :: OSI Approved :: Apache Software License
+    Operating System :: POSIX :: Linux
+    Programming Language :: Python
+    Programming Language :: Python :: 2
+    Programming Language :: Python :: 2.7
+    Programming Language :: Python :: 2.6
+
+[pbr]
+warnerrors = True
+
+[entry_points]
+console_scripts =
+    zuul-server = zuul.cmd.server:main
+    zuul-merger = zuul.cmd.merger:main
+    zuul = zuul.cmd.client:main
+    zuul-cloner = zuul.cmd.cloner:main
+
+[build_sphinx]
+source-dir = doc/source
+build-dir = doc/build
+all_files = 1
diff --git a/setup.py b/setup.py
index 4ec20a6..c304da2 100644
--- a/setup.py
+++ b/setup.py
@@ -1,22 +1,22 @@
-#!/usr/bin/env python2.7
-# Copyright (c) 2013 Hewlett-Packard Development Company, L.P.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#    http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
-# implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-# THIS FILE IS MANAGED BY THE GLOBAL REQUIREMENTS REPO - DO NOT EDIT
-import setuptools
-
-setuptools.setup(
-    setup_requires=['pbr'],
-    pbr=True)
+#!/usr/bin/env python2.7
+# Copyright (c) 2013 Hewlett-Packard Development Company, L.P.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+# implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+# THIS FILE IS MANAGED BY THE GLOBAL REQUIREMENTS REPO - DO NOT EDIT
+import setuptools
+
+setuptools.setup(
+    setup_requires=['pbr'],
+    pbr=True)
diff --git a/test-requirements.txt b/test-requirements.txt
index 88223b0..f04839d 100644
--- a/test-requirements.txt
+++ b/test-requirements.txt
@@ -1,14 +1,14 @@
-hacking>=0.9.2,<0.10
-
-coverage>=3.6
-sphinx>=1.1.2,!=1.2.0,!=1.3b1,<1.3
-sphinxcontrib-blockdiag>=1.1.0
-discover
-fixtures>=0.3.14
-python-keystoneclient>=0.4.2
-python-subunit
-python-swiftclient>=1.6
-testrepository>=0.0.17
-testtools>=0.9.32
-sphinxcontrib-programoutput
-mock
+hacking>=0.9.2,<0.10
+
+coverage>=3.6
+sphinx>=1.1.2,!=1.2.0,!=1.3b1,<1.3
+sphinxcontrib-blockdiag>=1.1.0
+discover
+fixtures>=0.3.14
+python-keystoneclient>=0.4.2
+python-subunit
+python-swiftclient>=1.6
+testrepository>=0.0.17
+testtools>=0.9.32
+sphinxcontrib-programoutput
+mock
diff --git a/tests/base.py b/tests/base.py
index 541a8c1..9860bd2 100755
--- a/tests/base.py
+++ b/tests/base.py
@@ -1,1359 +1,1359 @@
-#!/usr/bin/env python2.7
-
-# Copyright 2012 Hewlett-Packard Development Company, L.P.
-#
-# Licensed under the Apache License, Version 2.0 (the "License"); you may
-# not use this file except in compliance with the License. You may obtain
-# a copy of the License at
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-# License for the specific language governing permissions and limitations
-# under the License.
-
-from six.moves import configparser as ConfigParser
-import gc
-import hashlib
-import json
-import logging
-import os
-import pprint
-from six.moves import queue as Queue
-import random
-import re
-import select
-import shutil
-import socket
-import string
-import subprocess
-import swiftclient
-import threading
-import time
-import urllib2
-
-import git
-import gear
-import fixtures
-import six.moves.urllib.parse as urlparse
-import statsd
-import testtools
-from git import GitCommandError
-
-import zuul.connection.gerrit
-import zuul.connection.smtp
-import zuul.scheduler
-import zuul.webapp
-import zuul.rpclistener
-import zuul.launcher.gearman
-import zuul.lib.swift
-import zuul.merger.client
-import zuul.merger.merger
-import zuul.merger.server
-import zuul.reporter.gerrit
-import zuul.reporter.smtp
-import zuul.source.gerrit
-import zuul.trigger.gerrit
-import zuul.trigger.timer
-import zuul.trigger.zuultrigger
-
-FIXTURE_DIR = os.path.join(os.path.dirname(__file__),
-                           'fixtures')
-USE_TEMPDIR = True
-
-logging.basicConfig(level=logging.DEBUG,
-                    format='%(asctime)s %(name)-32s '
-                    '%(levelname)-8s %(message)s')
-
-
-def repack_repo(path):
-    cmd = ['git', '--git-dir=%s/.git' % path, 'repack', '-afd']
-    output = subprocess.Popen(cmd, close_fds=True,
-                              stdout=subprocess.PIPE,
-                              stderr=subprocess.PIPE)
-    out = output.communicate()
-    if output.returncode:
-        raise Exception("git repack returned %d" % output.returncode)
-    return out
-
-
-def random_sha1():
-    return hashlib.sha1(str(random.random())).hexdigest()
-
-
-def iterate_timeout(max_seconds, purpose):
-    start = time.time()
-    count = 0
-    while (time.time() < start + max_seconds):
-        count += 1
-        yield count
-        time.sleep(0)
-    raise Exception("Timeout waiting for %s" % purpose)
-
-
-class ChangeReference(git.Reference):
-    _common_path_default = "refs/changes"
-    _points_to_commits_only = True
-
-
-class FakeChange(object):
-    categories = {'APRV': ('Approved', -1, 1),
-                  'CRVW': ('Code-Review', -2, 2),
-                  'VRFY': ('Verified', -2, 2)}
-
-    def __init__(self, gerrit, number, project, branch, subject,
-                 status='NEW', upstream_root=None):
-        self.gerrit = gerrit
-        self.reported = 0
-        self.queried = 0
-        self.patchsets = []
-        self.number = number
-        self.project = project
-        self.branch = branch
-        self.subject = subject
-        self.latest_patchset = 0
-        self.depends_on_change = None
-        self.needed_by_changes = []
-        self.fail_merge = False
-        self.messages = []
-        self.data = {
-            'branch': branch,
-            'comments': [],
-            'commitMessage': subject,
-            'createdOn': time.time(),
-            'id': 'I' + random_sha1(),
-            'lastUpdated': time.time(),
-            'number': str(number),
-            'open': status == 'NEW',
-            'owner': {'email': 'user@example.com',
-                      'name': 'User Name',
-                      'username': 'username'},
-            'patchSets': self.patchsets,
-            'project': project,
-            'status': status,
-            'subject': subject,
-            'submitRecords': [],
-            'url': 'https://hostname/%s' % number}
-
-        self.upstream_root = upstream_root
-        self.addPatchset()
-        self.data['submitRecords'] = self.getSubmitRecords()
-        self.open = status == 'NEW'
-
-    def add_fake_change_to_repo(self, msg, fn, large):
-        path = os.path.join(self.upstream_root, self.project)
-        repo = git.Repo(path)
-        ref = ChangeReference.create(repo, '1/%s/%s' % (self.number,
-                                                        self.latest_patchset),
-                                     'refs/tags/init')
-        repo.head.reference = ref
-        zuul.merger.merger.reset_repo_to_head(repo)
-        repo.git.clean('-x', '-f', '-d')
-
-        path = os.path.join(self.upstream_root, self.project)
-        if not large:
-            fn = os.path.join(path, fn)
-            f = open(fn, 'w')
-            f.write("test %s %s %s\n" %
-                    (self.branch, self.number, self.latest_patchset))
-            f.close()
-            repo.index.add([fn])
-        else:
-            for fni in range(100):
-                fn = os.path.join(path, str(fni))
-                f = open(fn, 'w')
-                for ci in range(4096):
-                    f.write(random.choice(string.printable))
-                f.close()
-                repo.index.add([fn])
-
-        r = repo.index.commit(msg)
-        repo.head.reference = 'master'
-        zuul.merger.merger.reset_repo_to_head(repo)
-        repo.git.clean('-x', '-f', '-d')
-        repo.heads['master'].checkout()
-        return r
-
-    def addPatchset(self, files=[], large=False):
-        self.latest_patchset += 1
-        if files:
-            fn = files[0]
-        else:
-            fn = '%s-%s' % (self.branch.replace('/', '_'), self.number)
-        msg = self.subject + '-' + str(self.latest_patchset)
-        c = self.add_fake_change_to_repo(msg, fn, large)
-        ps_files = [{'file': '/COMMIT_MSG',
-                     'type': 'ADDED'},
-                    {'file': 'README',
-                     'type': 'MODIFIED'}]
-        for f in files:
-            ps_files.append({'file': f, 'type': 'ADDED'})
-        d = {'approvals': [],
-             'createdOn': time.time(),
-             'files': ps_files,
-             'number': str(self.latest_patchset),
-             'ref': 'refs/changes/1/%s/%s' % (self.number,
-                                              self.latest_patchset),
-             'revision': c.hexsha,
-             'uploader': {'email': 'user@example.com',
-                          'name': 'User name',
-                          'username': 'user'}}
-        self.data['currentPatchSet'] = d
-        self.patchsets.append(d)
-        self.data['submitRecords'] = self.getSubmitRecords()
-
-    def getPatchsetCreatedEvent(self, patchset):
-        event = {"type": "patchset-created",
-                 "change": {"project": self.project,
-                            "branch": self.branch,
-                            "id": "I5459869c07352a31bfb1e7a8cac379cabfcb25af",
-                            "number": str(self.number),
-                            "subject": self.subject,
-                            "owner": {"name": "User Name"},
-                            "url": "https://hostname/3"},
-                 "patchSet": self.patchsets[patchset - 1],
-                 "uploader": {"name": "User Name"}}
-        return event
-
-    def getChangeRestoredEvent(self):
-        event = {"type": "change-restored",
-                 "change": {"project": self.project,
-                            "branch": self.branch,
-                            "id": "I5459869c07352a31bfb1e7a8cac379cabfcb25af",
-                            "number": str(self.number),
-                            "subject": self.subject,
-                            "owner": {"name": "User Name"},
-                            "url": "https://hostname/3"},
-                 "restorer": {"name": "User Name"},
-                 "patchSet": self.patchsets[-1],
-                 "reason": ""}
-        return event
-
-    def getChangeAbandonedEvent(self):
-        event = {"type": "change-abandoned",
-                 "change": {"project": self.project,
-                            "branch": self.branch,
-                            "id": "I5459869c07352a31bfb1e7a8cac379cabfcb25af",
-                            "number": str(self.number),
-                            "subject": self.subject,
-                            "owner": {"name": "User Name"},
-                            "url": "https://hostname/3"},
-                 "abandoner": {"name": "User Name"},
-                 "patchSet": self.patchsets[-1],
-                 "reason": ""}
-        return event
-
-    def getChangeCommentEvent(self, patchset):
-        event = {"type": "comment-added",
-                 "change": {"project": self.project,
-                            "branch": self.branch,
-                            "id": "I5459869c07352a31bfb1e7a8cac379cabfcb25af",
-                            "number": str(self.number),
-                            "subject": self.subject,
-                            "owner": {"name": "User Name"},
-                            "url": "https://hostname/3"},
-                 "patchSet": self.patchsets[patchset - 1],
-                 "author": {"name": "User Name"},
-                 "approvals": [{"type": "Code-Review",
-                                "description": "Code-Review",
-                                "value": "0"}],
-                 "comment": "This is a comment"}
-        return event
-
-    def addApproval(self, category, value, username='reviewer_john',
-                    granted_on=None, message=''):
-        if not granted_on:
-            granted_on = time.time()
-        approval = {
-            'description': self.categories[category][0],
-            'type': category,
-            'value': str(value),
-            'by': {
-                'username': username,
-                'email': username + '@example.com',
-            },
-            'grantedOn': int(granted_on)
-        }
-        for i, x in enumerate(self.patchsets[-1]['approvals'][:]):
-            if x['by']['username'] == username and x['type'] == category:
-                del self.patchsets[-1]['approvals'][i]
-        self.patchsets[-1]['approvals'].append(approval)
-        event = {'approvals': [approval],
-                 'author': {'email': 'author@example.com',
-                            'name': 'Patchset Author',
-                            'username': 'author_phil'},
-                 'change': {'branch': self.branch,
-                            'id': 'Iaa69c46accf97d0598111724a38250ae76a22c87',
-                            'number': str(self.number),
-                            'owner': {'email': 'owner@example.com',
-                                      'name': 'Change Owner',
-                                      'username': 'owner_jane'},
-                            'project': self.project,
-                            'subject': self.subject,
-                            'topic': 'master',
-                            'url': 'https://hostname/459'},
-                 'comment': message,
-                 'patchSet': self.patchsets[-1],
-                 'type': 'comment-added'}
-        self.data['submitRecords'] = self.getSubmitRecords()
-        return json.loads(json.dumps(event))
-
-    def getSubmitRecords(self):
-        status = {}
-        for cat in self.categories.keys():
-            status[cat] = 0
-
-        for a in self.patchsets[-1]['approvals']:
-            cur = status[a['type']]
-            cat_min, cat_max = self.categories[a['type']][1:]
-            new = int(a['value'])
-            if new == cat_min:
-                cur = new
-            elif abs(new) > abs(cur):
-                cur = new
-            status[a['type']] = cur
-
-        labels = []
-        ok = True
-        for typ, cat in self.categories.items():
-            cur = status[typ]
-            cat_min, cat_max = cat[1:]
-            if cur == cat_min:
-                value = 'REJECT'
-                ok = False
-            elif cur == cat_max:
-                value = 'OK'
-            else:
-                value = 'NEED'
-                ok = False
-            labels.append({'label': cat[0], 'status': value})
-        if ok:
-            return [{'status': 'OK'}]
-        return [{'status': 'NOT_READY',
-                 'labels': labels}]
-
-    def setDependsOn(self, other, patchset):
-        self.depends_on_change = other
-        d = {'id': other.data['id'],
-             'number': other.data['number'],
-             'ref': other.patchsets[patchset - 1]['ref']
-             }
-        self.data['dependsOn'] = [d]
-
-        other.needed_by_changes.append(self)
-        needed = other.data.get('neededBy', [])
-        d = {'id': self.data['id'],
-             'number': self.data['number'],
-             'ref': self.patchsets[patchset - 1]['ref'],
-             'revision': self.patchsets[patchset - 1]['revision']
-             }
-        needed.append(d)
-        other.data['neededBy'] = needed
-
-    def query(self):
-        self.queried += 1
-        d = self.data.get('dependsOn')
-        if d:
-            d = d[0]
-            if (self.depends_on_change.patchsets[-1]['ref'] == d['ref']):
-                d['isCurrentPatchSet'] = True
-            else:
-                d['isCurrentPatchSet'] = False
-        return json.loads(json.dumps(self.data))
-
-    def setMerged(self):
-        if (self.depends_on_change and
-                self.depends_on_change.data['status'] != 'MERGED'):
-            return
-        if self.fail_merge:
-            return
-        self.data['status'] = 'MERGED'
-        self.open = False
-
-        path = os.path.join(self.upstream_root, self.project)
-        repo = git.Repo(path)
-        repo.heads[self.branch].commit = \
-            repo.commit(self.patchsets[-1]['revision'])
-
-    def setReported(self):
-        self.reported += 1
-
-
-class FakeGerritConnection(zuul.connection.gerrit.GerritConnection):
-    log = logging.getLogger("zuul.test.FakeGerritConnection")
-
-    def __init__(self, connection_name, connection_config,
-                 changes_db=None, queues_db=None, upstream_root=None):
-        super(FakeGerritConnection, self).__init__(connection_name,
-                                                   connection_config)
-
-        self.event_queue = queues_db
-        self.fixture_dir = os.path.join(FIXTURE_DIR, 'gerrit')
-        self.change_number = 0
-        self.changes = changes_db
-        self.queries = []
-        self.upstream_root = upstream_root
-
-    def addFakeChange(self, project, branch, subject, status='NEW'):
-        self.change_number += 1
-        c = FakeChange(self, self.change_number, project, branch, subject,
-                       upstream_root=self.upstream_root,
-                       status=status)
-        self.changes[self.change_number] = c
-        return c
-
-    def review(self, project, changeid, message, action):
-        number, ps = changeid.split(',')
-        change = self.changes[int(number)]
-
-        # Add the approval back onto the change (ie simulate what gerrit would
-        # do).
-        # Usually when zuul leaves a review it'll create a feedback loop where
-        # zuul's review enters another gerrit event (which is then picked up by
-        # zuul). However, we can't mimic this behaviour (by adding this
-        # approval event into the queue) as it stops jobs from checking what
-        # happens before this event is triggered. If a job needs to see what
-        # happens they can add their own verified event into the queue.
-        # Nevertheless, we can update change with the new review in gerrit.
-
-        for cat in ['CRVW', 'VRFY', 'APRV']:
-            if cat in action:
-                change.addApproval(cat, action[cat], username=self.user)
-
-        if 'label' in action:
-            parts = action['label'].split('=')
-            change.addApproval(parts[0], parts[2], username=self.user)
-
-        change.messages.append(message)
-
-        if 'submit' in action:
-            change.setMerged()
-        if message:
-            change.setReported()
-
-    def query(self, number):
-        change = self.changes.get(int(number))
-        if change:
-            return change.query()
-        return {}
-
-    def simpleQuery(self, query):
-        self.log.debug("simpleQuery: %s" % query)
-        self.queries.append(query)
-        if query.startswith('change:'):
-            # Query a specific changeid
-            changeid = query[len('change:'):]
-            l = [change.query() for change in self.changes.values()
-                 if change.data['id'] == changeid]
-        elif query.startswith('message:'):
-            # Query the content of a commit message
-            msg = query[len('message:'):].strip()
-            l = [change.query() for change in self.changes.values()
-                 if msg in change.data['commitMessage']]
-        else:
-            # Query all open changes
-            l = [change.query() for change in self.changes.values()]
-        return l
-
-    def _start_watcher_thread(self, *args, **kw):
-        pass
-
-    def getGitUrl(self, project):
-        return os.path.join(self.upstream_root, project.name)
-
-
-class BuildHistory(object):
-    def __init__(self, **kw):
-        self.__dict__.update(kw)
-
-    def __repr__(self):
-        return ("<Completed build, result: %s name: %s #%s changes: %s>" %
-                (self.result, self.name, self.number, self.changes))
-
-
-class FakeURLOpener(object):
-    def __init__(self, upstream_root, url):
-        self.upstream_root = upstream_root
-        self.url = url
-
-    def read(self):
-        res = urlparse.urlparse(self.url)
-        path = res.path
-        project = '/'.join(path.split('/')[2:-2])
-        ret = '001e# service=git-upload-pack\n'
-        ret += ('000000a31270149696713ba7e06f1beb760f20d359c4abed HEAD\x00'
-                'multi_ack thin-pack side-band side-band-64k ofs-delta '
-                'shallow no-progress include-tag multi_ack_detailed no-done\n')
-        path = os.path.join(self.upstream_root, project)
-        repo = git.Repo(path)
-        for ref in repo.refs:
-            r = ref.object.hexsha + ' ' + ref.path + '\n'
-            ret += '%04x%s' % (len(r) + 4, r)
-        ret += '0000'
-        return ret
-
-
-class FakeStatsd(threading.Thread):
-    def __init__(self):
-        threading.Thread.__init__(self)
-        self.daemon = True
-        self.sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
-        self.sock.bind(('', 0))
-        self.port = self.sock.getsockname()[1]
-        self.wake_read, self.wake_write = os.pipe()
-        self.stats = []
-
-    def run(self):
-        while True:
-            poll = select.poll()
-            poll.register(self.sock, select.POLLIN)
-            poll.register(self.wake_read, select.POLLIN)
-            ret = poll.poll()
-            for (fd, event) in ret:
-                if fd == self.sock.fileno():
-                    data = self.sock.recvfrom(1024)
-                    if not data:
-                        return
-                    self.stats.append(data[0])
-                if fd == self.wake_read:
-                    return
-
-    def stop(self):
-        os.write(self.wake_write, '1\n')
-
-
-class FakeBuild(threading.Thread):
-    log = logging.getLogger("zuul.test")
-
-    def __init__(self, worker, job, number, node):
-        threading.Thread.__init__(self)
-        self.daemon = True
-        self.worker = worker
-        self.job = job
-        self.name = job.name.split(':')[1]
-        self.number = number
-        self.node = node
-        self.parameters = json.loads(job.arguments)
-        self.unique = self.parameters['ZUUL_UUID']
-        self.wait_condition = threading.Condition()
-        self.waiting = False
-        self.aborted = False
-        self.created = time.time()
-        self.description = ''
-        self.run_error = False
-
-    def release(self):
-        self.wait_condition.acquire()
-        self.wait_condition.notify()
-        self.waiting = False
-        self.log.debug("Build %s released" % self.unique)
-        self.wait_condition.release()
-
-    def isWaiting(self):
-        self.wait_condition.acquire()
-        if self.waiting:
-            ret = True
-        else:
-            ret = False
-        self.wait_condition.release()
-        return ret
-
-    def _wait(self):
-        self.wait_condition.acquire()
-        self.waiting = True
-        self.log.debug("Build %s waiting" % self.unique)
-        self.wait_condition.wait()
-        self.wait_condition.release()
-
-    def run(self):
-        data = {
-            'url': 'https://server/job/%s/%s/' % (self.name, self.number),
-            'name': self.name,
-            'number': self.number,
-            'manager': self.worker.worker_id,
-            'worker_name': 'My Worker',
-            'worker_hostname': 'localhost',
-            'worker_ips': ['127.0.0.1', '192.168.1.1'],
-            'worker_fqdn': 'zuul.example.org',
-            'worker_program': 'FakeBuilder',
-            'worker_version': 'v1.1',
-            'worker_extra': {'something': 'else'}
-        }
-
-        self.log.debug('Running build %s' % self.unique)
-
-        self.job.sendWorkData(json.dumps(data))
-        self.log.debug('Sent WorkData packet with %s' % json.dumps(data))
-        self.job.sendWorkStatus(0, 100)
-
-        if self.worker.hold_jobs_in_build:
-            self.log.debug('Holding build %s' % self.unique)
-            self._wait()
-        self.log.debug("Build %s continuing" % self.unique)
-
-        self.worker.lock.acquire()
-
-        result = 'SUCCESS'
-        if (('ZUUL_REF' in self.parameters) and
-            self.worker.shouldFailTest(self.name,
-                                       self.parameters['ZUUL_REF'])):
-            result = 'FAILURE'
-        if self.aborted:
-            result = 'ABORTED'
-
-        if self.run_error:
-            work_fail = True
-            result = 'RUN_ERROR'
-        else:
-            data['result'] = result
-            data['node_labels'] = ['bare-necessities']
-            data['node_name'] = 'foo'
-            work_fail = False
-
-        changes = None
-        if 'ZUUL_CHANGE_IDS' in self.parameters:
-            changes = self.parameters['ZUUL_CHANGE_IDS']
-
-        self.worker.build_history.append(
-            BuildHistory(name=self.name, number=self.number,
-                         result=result, changes=changes, node=self.node,
-                         uuid=self.unique, description=self.description,
-                         parameters=self.parameters,
-                         pipeline=self.parameters['ZUUL_PIPELINE'])
-        )
-
-        self.job.sendWorkData(json.dumps(data))
-        if work_fail:
-            self.job.sendWorkFail()
-        else:
-            self.job.sendWorkComplete(json.dumps(data))
-        del self.worker.gearman_jobs[self.job.unique]
-        self.worker.running_builds.remove(self)
-        self.worker.lock.release()
-
-
-class FakeWorker(gear.Worker):
-    def __init__(self, worker_id, test):
-        super(FakeWorker, self).__init__(worker_id)
-        self.gearman_jobs = {}
-        self.build_history = []
-        self.running_builds = []
-        self.build_counter = 0
-        self.fail_tests = {}
-        self.test = test
-
-        self.hold_jobs_in_build = False
-        self.lock = threading.Lock()
-        self.__work_thread = threading.Thread(target=self.work)
-        self.__work_thread.daemon = True
-        self.__work_thread.start()
-
-    def handleJob(self, job):
-        parts = job.name.split(":")
-        cmd = parts[0]
-        name = parts[1]
-        if len(parts) > 2:
-            node = parts[2]
-        else:
-            node = None
-        if cmd == 'build':
-            self.handleBuild(job, name, node)
-        elif cmd == 'stop':
-            self.handleStop(job, name)
-        elif cmd == 'set_description':
-            self.handleSetDescription(job, name)
-
-    def handleBuild(self, job, name, node):
-        build = FakeBuild(self, job, self.build_counter, node)
-        job.build = build
-        self.gearman_jobs[job.unique] = job
-        self.build_counter += 1
-
-        self.running_builds.append(build)
-        build.start()
-
-    def handleStop(self, job, name):
-        self.log.debug("handle stop")
-        parameters = json.loads(job.arguments)
-        name = parameters['name']
-        number = parameters['number']
-        for build in self.running_builds:
-            if build.name == name and build.number == number:
-                build.aborted = True
-                build.release()
-                job.sendWorkComplete()
-                return
-        job.sendWorkFail()
-
-    def handleSetDescription(self, job, name):
-        self.log.debug("handle set description")
-        parameters = json.loads(job.arguments)
-        name = parameters['name']
-        number = parameters['number']
-        descr = parameters['html_description']
-        for build in self.running_builds:
-            if build.name == name and build.number == number:
-                build.description = descr
-                job.sendWorkComplete()
-                return
-        for build in self.build_history:
-            if build.name == name and build.number == number:
-                build.description = descr
-                job.sendWorkComplete()
-                return
-        job.sendWorkFail()
-
-    def work(self):
-        while self.running:
-            try:
-                job = self.getJob()
-            except gear.InterruptedError:
-                continue
-            try:
-                self.handleJob(job)
-            except:
-                self.log.exception("Worker exception:")
-
-    def addFailTest(self, name, change):
-        l = self.fail_tests.get(name, [])
-        l.append(change)
-        self.fail_tests[name] = l
-
-    def shouldFailTest(self, name, ref):
-        l = self.fail_tests.get(name, [])
-        for change in l:
-            if self.test.ref_has_change(ref, change):
-                return True
-        return False
-
-    def release(self, regex=None):
-        builds = self.running_builds[:]
-        self.log.debug("releasing build %s (%s)" % (regex,
-                                                    len(self.running_builds)))
-        for build in builds:
-            if not regex or re.match(regex, build.name):
-                self.log.debug("releasing build %s" %
-                               (build.parameters['ZUUL_UUID']))
-                build.release()
-            else:
-                self.log.debug("not releasing build %s" %
-                               (build.parameters['ZUUL_UUID']))
-        self.log.debug("done releasing builds %s (%s)" %
-                       (regex, len(self.running_builds)))
-
-
-class FakeGearmanServer(gear.Server):
-    def __init__(self):
-        self.hold_jobs_in_queue = False
-        super(FakeGearmanServer, self).__init__(0)
-
-    def getJobForConnection(self, connection, peek=False):
-        for queue in [self.high_queue, self.normal_queue, self.low_queue]:
-            for job in queue:
-                if not hasattr(job, 'waiting'):
-                    if job.name.startswith('build:'):
-                        job.waiting = self.hold_jobs_in_queue
-                    else:
-                        job.waiting = False
-                if job.waiting:
-                    continue
-                if job.name in connection.functions:
-                    if not peek:
-                        queue.remove(job)
-                        connection.related_jobs[job.handle] = job
-                        job.worker_connection = connection
-                    job.running = True
-                    return job
-        return None
-
-    def release(self, regex=None):
-        released = False
-        qlen = (len(self.high_queue) + len(self.normal_queue) +
-                len(self.low_queue))
-        self.log.debug("releasing queued job %s (%s)" % (regex, qlen))
-        for job in self.getQueue():
-            cmd, name = job.name.split(':')
-            if cmd != 'build':
-                continue
-            if not regex or re.match(regex, name):
-                self.log.debug("releasing queued job %s" %
-                               job.unique)
-                job.waiting = False
-                released = True
-            else:
-                self.log.debug("not releasing queued job %s" %
-                               job.unique)
-        if released:
-            self.wakeConnections()
-        qlen = (len(self.high_queue) + len(self.normal_queue) +
-                len(self.low_queue))
-        self.log.debug("done releasing queued jobs %s (%s)" % (regex, qlen))
-
-
-class FakeSMTP(object):
-    log = logging.getLogger('zuul.FakeSMTP')
-
-    def __init__(self, messages, server, port):
-        self.server = server
-        self.port = port
-        self.messages = messages
-
-    def sendmail(self, from_email, to_email, msg):
-        self.log.info("Sending email from %s, to %s, with msg %s" % (
-                      from_email, to_email, msg))
-
-        headers = msg.split('\n\n', 1)[0]
-        body = msg.split('\n\n', 1)[1]
-
-        self.messages.append(dict(
-            from_email=from_email,
-            to_email=to_email,
-            msg=msg,
-            headers=headers,
-            body=body,
-        ))
-
-        return True
-
-    def quit(self):
-        return True
-
-
-class FakeSwiftClientConnection(swiftclient.client.Connection):
-    def post_account(self, headers):
-        # Do nothing
-        pass
-
-    def get_auth(self):
-        # Returns endpoint and (unused) auth token
-        endpoint = os.path.join('https://storage.example.org', 'V1',
-                                'AUTH_account')
-        return endpoint, ''
-
-
-class BaseTestCase(testtools.TestCase):
-    log = logging.getLogger("zuul.test")
-
-    def setUp(self):
-        super(BaseTestCase, self).setUp()
-        test_timeout = os.environ.get('OS_TEST_TIMEOUT', 0)
-        try:
-            test_timeout = int(test_timeout)
-        except ValueError:
-            # If timeout value is invalid do not set a timeout.
-            test_timeout = 0
-        if test_timeout > 0:
-            self.useFixture(fixtures.Timeout(test_timeout, gentle=False))
-
-        if (os.environ.get('OS_STDOUT_CAPTURE') == 'True' or
-            os.environ.get('OS_STDOUT_CAPTURE') == '1'):
-            stdout = self.useFixture(fixtures.StringStream('stdout')).stream
-            self.useFixture(fixtures.MonkeyPatch('sys.stdout', stdout))
-        if (os.environ.get('OS_STDERR_CAPTURE') == 'True' or
-            os.environ.get('OS_STDERR_CAPTURE') == '1'):
-            stderr = self.useFixture(fixtures.StringStream('stderr')).stream
-            self.useFixture(fixtures.MonkeyPatch('sys.stderr', stderr))
-        if (os.environ.get('OS_LOG_CAPTURE') == 'True' or
-            os.environ.get('OS_LOG_CAPTURE') == '1'):
-            self.useFixture(fixtures.FakeLogger(
-                level=logging.DEBUG,
-                format='%(asctime)s %(name)-32s '
-                '%(levelname)-8s %(message)s'))
-
-
-class ZuulTestCase(BaseTestCase):
-
-    def setUp(self):
-        super(ZuulTestCase, self).setUp()
-        if USE_TEMPDIR:
-            tmp_root = self.useFixture(fixtures.TempDir(
-                rootdir=os.environ.get("ZUUL_TEST_ROOT"))
-            ).path
-        else:
-            tmp_root = os.environ.get("ZUUL_TEST_ROOT")
-        self.test_root = os.path.join(tmp_root, "zuul-test")
-        self.upstream_root = os.path.join(self.test_root, "upstream")
-        self.git_root = os.path.join(self.test_root, "git")
-
-        if os.path.exists(self.test_root):
-            shutil.rmtree(self.test_root)
-        os.makedirs(self.test_root)
-        os.makedirs(self.upstream_root)
-
-        # Make per test copy of Configuration.
-        self.setup_config()
-        self.config.set('zuul', 'layout_config',
-                        os.path.join(FIXTURE_DIR,
-                                     self.config.get('zuul', 'layout_config')))
-        self.config.set('merger', 'git_dir', self.git_root)
-
-        # For each project in config:
-        self.init_repo("org/project")
-        self.init_repo("org/project1")
-        self.init_repo("org/project2")
-        self.init_repo("org/project3")
-        self.init_repo("org/project4")
-        self.init_repo("org/project5")
-        self.init_repo("org/project6")
-        self.init_repo("org/one-job-project")
-        self.init_repo("org/nonvoting-project")
-        self.init_repo("org/templated-project")
-        self.init_repo("org/layered-project")
-        self.init_repo("org/node-project")
-        self.init_repo("org/conflict-project")
-        self.init_repo("org/noop-project")
-        self.init_repo("org/experimental-project")
-        self.init_repo("org/no-jobs-project")
-
-        self.statsd = FakeStatsd()
-        # note, use 127.0.0.1 rather than localhost to avoid getting ipv6
-        # see: https://github.com/jsocol/pystatsd/issues/61
-        os.environ['STATSD_HOST'] = '127.0.0.1'
-        os.environ['STATSD_PORT'] = str(self.statsd.port)
-        self.statsd.start()
-        # the statsd client object is configured in the statsd module import
-        reload(statsd)
-        reload(zuul.scheduler)
-
-        self.gearman_server = FakeGearmanServer()
-
-        self.config.set('gearman', 'port', str(self.gearman_server.port))
-
-        self.worker = FakeWorker('fake_worker', self)
-        self.worker.addServer('127.0.0.1', self.gearman_server.port)
-        self.gearman_server.worker = self.worker
-
-        zuul.source.gerrit.GerritSource.replication_timeout = 1.5
-        zuul.source.gerrit.GerritSource.replication_retry_interval = 0.5
-
-        self.sched = zuul.scheduler.Scheduler(self.config)
-
-        self.useFixture(fixtures.MonkeyPatch('swiftclient.client.Connection',
-                                             FakeSwiftClientConnection))
-        self.swift = zuul.lib.swift.Swift(self.config)
-
-        self.event_queues = [
-            self.sched.result_event_queue,
-            self.sched.trigger_event_queue
-        ]
-
-        self.configure_connections()
-        self.sched.registerConnections(self.connections)
-
-        def URLOpenerFactory(*args, **kw):
-            if isinstance(args[0], urllib2.Request):
-                return old_urlopen(*args, **kw)
-            return FakeURLOpener(self.upstream_root, *args, **kw)
-
-        old_urlopen = urllib2.urlopen
-        urllib2.urlopen = URLOpenerFactory
-
-        self.merge_server = zuul.merger.server.MergeServer(self.config,
-                                                           self.connections)
-        self.merge_server.start()
-
-        self.launcher = zuul.launcher.gearman.Gearman(self.config, self.sched,
-                                                      self.swift)
-        self.merge_client = zuul.merger.client.MergeClient(
-            self.config, self.sched)
-
-        self.sched.setLauncher(self.launcher)
-        self.sched.setMerger(self.merge_client)
-
-        self.webapp = zuul.webapp.WebApp(self.sched, port=0)
-        self.rpc = zuul.rpclistener.RPCListener(self.config, self.sched)
-
-        self.sched.start()
-        self.sched.reconfigure(self.config)
-        self.sched.resume()
-        self.webapp.start()
-        self.rpc.start()
-        self.launcher.gearman.waitForServer()
-        self.registerJobs()
-        self.builds = self.worker.running_builds
-        self.history = self.worker.build_history
-
-        self.addCleanup(self.assertFinalState)
-        self.addCleanup(self.shutdown)
-
-    def configure_connections(self):
-        # Register connections from the config
-        self.smtp_messages = []
-
-        def FakeSMTPFactory(*args, **kw):
-            args = [self.smtp_messages] + list(args)
-            return FakeSMTP(*args, **kw)
-
-        self.useFixture(fixtures.MonkeyPatch('smtplib.SMTP', FakeSMTPFactory))
-
-        # Set a changes database so multiple FakeGerrit's can report back to
-        # a virtual canonical database given by the configured hostname
-        self.gerrit_changes_dbs = {}
-        self.gerrit_queues_dbs = {}
-        self.connections = {}
-
-        for section_name in self.config.sections():
-            con_match = re.match(r'^connection ([\'\"]?)(.*)(\1)$',
-                                 section_name, re.I)
-            if not con_match:
-                continue
-            con_name = con_match.group(2)
-            con_config = dict(self.config.items(section_name))
-
-            if 'driver' not in con_config:
-                raise Exception("No driver specified for connection %s."
-                                % con_name)
-
-            con_driver = con_config['driver']
-
-            # TODO(jhesketh): load the required class automatically
-            if con_driver == 'gerrit':
-                if con_config['server'] not in self.gerrit_changes_dbs.keys():
-                    self.gerrit_changes_dbs[con_config['server']] = {}
-                if con_config['server'] not in self.gerrit_queues_dbs.keys():
-                    self.gerrit_queues_dbs[con_config['server']] = \
-                        Queue.Queue()
-                    self.event_queues.append(
-                        self.gerrit_queues_dbs[con_config['server']])
-                con_config['event_delay'] = 0.0
-                self.connections[con_name] = FakeGerritConnection(
-                    con_name, con_config,
-                    changes_db=self.gerrit_changes_dbs[con_config['server']],
-                    queues_db=self.gerrit_queues_dbs[con_config['server']],
-                    upstream_root=self.upstream_root
-                )
-                setattr(self, 'fake_' + con_name, self.connections[con_name])
-            elif con_driver == 'smtp':
-                self.connections[con_name] = \
-                    zuul.connection.smtp.SMTPConnection(con_name, con_config)
-            else:
-                raise Exception("Unknown driver, %s, for connection %s"
-                                % (con_config['driver'], con_name))
-
-        # If the [gerrit] or [smtp] sections still exist, load them in as a
-        # connection named 'gerrit' or 'smtp' respectfully
-
-        if 'gerrit' in self.config.sections():
-            self.gerrit_changes_dbs['gerrit'] = {}
-            self.gerrit_queues_dbs['gerrit'] = Queue.Queue()
-            self.event_queues.append(self.gerrit_queues_dbs['gerrit'])
-            self.connections['gerrit'] = FakeGerritConnection(
-                '_legacy_gerrit', dict(self.config.items('gerrit')),
-                changes_db=self.gerrit_changes_dbs['gerrit'],
-                queues_db=self.gerrit_queues_dbs['gerrit'])
-
-        if 'smtp' in self.config.sections():
-            self.connections['smtp'] = \
-                zuul.connection.smtp.SMTPConnection(
-                    '_legacy_smtp', dict(self.config.items('smtp')))
-
-    def setup_config(self, config_file='zuul.conf'):
-        """Per test config object. Override to set different config."""
-        self.config = ConfigParser.ConfigParser()
-        self.config.read(os.path.join(FIXTURE_DIR, config_file))
-
-    def assertFinalState(self):
-        # Make sure that git.Repo objects have been garbage collected.
-        repos = []
-        gc.collect()
-        for obj in gc.get_objects():
-            if isinstance(obj, git.Repo):
-                repos.append(obj)
-        self.assertEqual(len(repos), 0)
-        self.assertEmptyQueues()
-        for pipeline in self.sched.layout.pipelines.values():
-            if isinstance(pipeline.manager,
-                          zuul.scheduler.IndependentPipelineManager):
-                self.assertEqual(len(pipeline.queues), 0)
-
-    def shutdown(self):
-        self.log.debug("Shutting down after tests")
-        self.launcher.stop()
-        self.merge_server.stop()
-        self.merge_server.join()
-        self.merge_client.stop()
-        self.worker.shutdown()
-        self.sched.stop()
-        self.sched.join()
-        self.statsd.stop()
-        self.statsd.join()
-        self.webapp.stop()
-        self.webapp.join()
-        self.rpc.stop()
-        self.rpc.join()
-        self.gearman_server.shutdown()
-        threads = threading.enumerate()
-        if len(threads) > 1:
-            self.log.error("More than one thread is running: %s" % threads)
-
-    def init_repo(self, project):
-        parts = project.split('/')
-        path = os.path.join(self.upstream_root, *parts[:-1])
-        if not os.path.exists(path):
-            os.makedirs(path)
-        path = os.path.join(self.upstream_root, project)
-        repo = git.Repo.init(path)
-
-        repo.config_writer().set_value('user', 'email', 'user@example.com')
-        repo.config_writer().set_value('user', 'name', 'User Name')
-        repo.config_writer().write()
-
-        fn = os.path.join(path, 'README')
-        f = open(fn, 'w')
-        f.write("test\n")
-        f.close()
-        repo.index.add([fn])
-        repo.index.commit('initial commit')
-        master = repo.create_head('master')
-        repo.create_tag('init')
-
-        repo.head.reference = master
-        zuul.merger.merger.reset_repo_to_head(repo)
-        repo.git.clean('-x', '-f', '-d')
-
-        self.create_branch(project, 'mp')
-
-    def create_branch(self, project, branch):
-        path = os.path.join(self.upstream_root, project)
-        repo = git.Repo.init(path)
-        fn = os.path.join(path, 'README')
-
-        branch_head = repo.create_head(branch)
-        repo.head.reference = branch_head
-        f = open(fn, 'a')
-        f.write("test %s\n" % branch)
-        f.close()
-        repo.index.add([fn])
-        repo.index.commit('%s commit' % branch)
-
-        repo.head.reference = repo.heads['master']
-        zuul.merger.merger.reset_repo_to_head(repo)
-        repo.git.clean('-x', '-f', '-d')
-
-    def ref_has_change(self, ref, change):
-        path = os.path.join(self.git_root, change.project)
-        repo = git.Repo(path)
-        try:
-            for commit in repo.iter_commits(ref):
-                if commit.message.strip() == ('%s-1' % change.subject):
-                    return True
-        except GitCommandError:
-            pass
-        return False
-
-    def job_has_changes(self, *args):
-        job = args[0]
-        commits = args[1:]
-        if isinstance(job, FakeBuild):
-            parameters = job.parameters
-        else:
-            parameters = json.loads(job.arguments)
-        project = parameters['ZUUL_PROJECT']
-        path = os.path.join(self.git_root, project)
-        repo = git.Repo(path)
-        ref = parameters['ZUUL_REF']
-        sha = parameters['ZUUL_COMMIT']
-        repo_messages = [c.message.strip() for c in repo.iter_commits(ref)]
-        repo_shas = [c.hexsha for c in repo.iter_commits(ref)]
-        commit_messages = ['%s-1' % commit.subject for commit in commits]
-        self.log.debug("Checking if job %s has changes; commit_messages %s;"
-                       " repo_messages %s; sha %s" % (job, commit_messages,
-                                                      repo_messages, sha))
-        for msg in commit_messages:
-            if msg not in repo_messages:
-                self.log.debug("  messages do not match")
-                return False
-        if repo_shas[0] != sha:
-            self.log.debug("  sha does not match")
-            return False
-        self.log.debug("  OK")
-        return True
-
-    def registerJobs(self):
-        count = 0
-        for job in self.sched.layout.jobs.keys():
-            self.worker.registerFunction('build:' + job)
-            count += 1
-        self.worker.registerFunction('stop:' + self.worker.worker_id)
-        count += 1
-
-        while len(self.gearman_server.functions) < count:
-            time.sleep(0)
-
-    def orderedRelease(self):
-        # Run one build at a time to ensure non-race order:
-        while len(self.builds):
-            self.release(self.builds[0])
-            self.waitUntilSettled()
-
-    def release(self, job):
-        if isinstance(job, FakeBuild):
-            job.release()
-        else:
-            job.waiting = False
-            self.log.debug("Queued job %s released" % job.unique)
-            self.gearman_server.wakeConnections()
-
-    def getParameter(self, job, name):
-        if isinstance(job, FakeBuild):
-            return job.parameters[name]
-        else:
-            parameters = json.loads(job.arguments)
-            return parameters[name]
-
-    def resetGearmanServer(self):
-        self.worker.setFunctions([])
-        while True:
-            done = True
-            for connection in self.gearman_server.active_connections:
-                if (connection.functions and
-                    connection.client_id not in ['Zuul RPC Listener',
-                                                 'Zuul Merger']):
-                    done = False
-            if done:
-                break
-            time.sleep(0)
-        self.gearman_server.functions = set()
-        self.rpc.register()
-        self.merge_server.register()
-
-    def haveAllBuildsReported(self):
-        # See if Zuul is waiting on a meta job to complete
-        if self.launcher.meta_jobs:
-            return False
-        # Find out if every build that the worker has completed has been
-        # reported back to Zuul.  If it hasn't then that means a Gearman
-        # event is still in transit and the system is not stable.
-        for build in self.worker.build_history:
-            zbuild = self.launcher.builds.get(build.uuid)
-            if not zbuild:
-                # It has already been reported
-                continue
-            # It hasn't been reported yet.
-            return False
-        # Make sure that none of the worker connections are in GRAB_WAIT
-        for connection in self.worker.active_connections:
-            if connection.state == 'GRAB_WAIT':
-                return False
-        return True
-
-    def areAllBuildsWaiting(self):
-        builds = self.launcher.builds.values()
-        for build in builds:
-            client_job = None
-            for conn in self.launcher.gearman.active_connections:
-                for j in conn.related_jobs.values():
-                    if j.unique == build.uuid:
-                        client_job = j
-                        break
-            if not client_job:
-                self.log.debug("%s is not known to the gearman client" %
-                               build)
-                return False
-            if not client_job.handle:
-                self.log.debug("%s has no handle" % client_job)
-                return False
-            server_job = self.gearman_server.jobs.get(client_job.handle)
-            if not server_job:
-                self.log.debug("%s is not known to the gearman server" %
-                               client_job)
-                return False
-            if not hasattr(server_job, 'waiting'):
-                self.log.debug("%s is being enqueued" % server_job)
-                return False
-            if server_job.waiting:
-                continue
-            worker_job = self.worker.gearman_jobs.get(server_job.unique)
-            if worker_job:
-                if build.number is None:
-                    self.log.debug("%s has not reported start" % worker_job)
-                    return False
-                if worker_job.build.isWaiting():
-                    continue
-                else:
-                    self.log.debug("%s is running" % worker_job)
-                    return False
-            else:
-                self.log.debug("%s is unassigned" % server_job)
-                return False
-        return True
-
-    def eventQueuesEmpty(self):
-        for queue in self.event_queues:
-            yield queue.empty()
-
-    def eventQueuesJoin(self):
-        for queue in self.event_queues:
-            queue.join()
-
-    def waitUntilSettled(self):
-        self.log.debug("Waiting until settled...")
-        start = time.time()
-        while True:
-            if time.time() - start > 10:
-                print 'queue status:',
-                print ' '.join(self.eventQueuesEmpty())
-                print self.areAllBuildsWaiting()
-                raise Exception("Timeout waiting for Zuul to settle")
-            # Make sure no new events show up while we're checking
-            self.worker.lock.acquire()
-            # have all build states propogated to zuul?
-            if self.haveAllBuildsReported():
-                # Join ensures that the queue is empty _and_ events have been
-                # processed
-                self.eventQueuesJoin()
-                self.sched.run_handler_lock.acquire()
-                if (not self.merge_client.build_sets and
-                    all(self.eventQueuesEmpty()) and
-                    self.haveAllBuildsReported() and
-                    self.areAllBuildsWaiting()):
-                    self.sched.run_handler_lock.release()
-                    self.worker.lock.release()
-                    self.log.debug("...settled.")
-                    return
-                self.sched.run_handler_lock.release()
-            self.worker.lock.release()
-            self.sched.wake_event.wait(0.1)
-
-    def countJobResults(self, jobs, result):
-        jobs = filter(lambda x: x.result == result, jobs)
-        return len(jobs)
-
-    def getJobFromHistory(self, name):
-        history = self.worker.build_history
-        for job in history:
-            if job.name == name:
-                return job
-        raise Exception("Unable to find job %s in history" % name)
-
-    def assertEmptyQueues(self):
-        # Make sure there are no orphaned jobs
-        for pipeline in self.sched.layout.pipelines.values():
-            for queue in pipeline.queues:
-                if len(queue.queue) != 0:
-                    print 'pipeline %s queue %s contents %s' % (
-                        pipeline.name, queue.name, queue.queue)
-                self.assertEqual(len(queue.queue), 0,
-                                 "Pipelines queues should be empty")
-
-    def assertReportedStat(self, key, value=None, kind=None):
-        start = time.time()
-        while time.time() < (start + 5):
-            for stat in self.statsd.stats:
-                pprint.pprint(self.statsd.stats)
-                k, v = stat.split(':')
-                if key == k:
-                    if value is None and kind is None:
-                        return
-                    elif value:
-                        if value == v:
-                            return
-                    elif kind:
-                        if v.endswith('|' + kind):
-                            return
-            time.sleep(0.1)
-
-        pprint.pprint(self.statsd.stats)
-        raise Exception("Key %s not found in reported stats" % key)
+#!/usr/bin/env python2.7
+
+# Copyright 2012 Hewlett-Packard Development Company, L.P.
+#
+# Licensed under the Apache License, Version 2.0 (the "License"); you may
+# not use this file except in compliance with the License. You may obtain
+# a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+# License for the specific language governing permissions and limitations
+# under the License.
+
+from six.moves import configparser as ConfigParser
+import gc
+import hashlib
+import json
+import logging
+import os
+import pprint
+from six.moves import queue as Queue
+import random
+import re
+import select
+import shutil
+import socket
+import string
+import subprocess
+import swiftclient
+import threading
+import time
+import urllib2
+
+import git
+import gear
+import fixtures
+import six.moves.urllib.parse as urlparse
+import statsd
+import testtools
+from git import GitCommandError
+
+import zuul.connection.gerrit
+import zuul.connection.smtp
+import zuul.scheduler
+import zuul.webapp
+import zuul.rpclistener
+import zuul.launcher.gearman
+import zuul.lib.swift
+import zuul.merger.client
+import zuul.merger.merger
+import zuul.merger.server
+import zuul.reporter.gerrit
+import zuul.reporter.smtp
+import zuul.source.gerrit
+import zuul.trigger.gerrit
+import zuul.trigger.timer
+import zuul.trigger.zuultrigger
+
+FIXTURE_DIR = os.path.join(os.path.dirname(__file__),
+                           'fixtures')
+USE_TEMPDIR = True
+
+logging.basicConfig(level=logging.DEBUG,
+                    format='%(asctime)s %(name)-32s '
+                    '%(levelname)-8s %(message)s')
+
+
+def repack_repo(path):
+    cmd = ['git', '--git-dir=%s/.git' % path, 'repack', '-afd']
+    output = subprocess.Popen(cmd, close_fds=True,
+                              stdout=subprocess.PIPE,
+                              stderr=subprocess.PIPE)
+    out = output.communicate()
+    if output.returncode:
+        raise Exception("git repack returned %d" % output.returncode)
+    return out
+
+
+def random_sha1():
+    return hashlib.sha1(str(random.random())).hexdigest()
+
+
+def iterate_timeout(max_seconds, purpose):
+    start = time.time()
+    count = 0
+    while (time.time() < start + max_seconds):
+        count += 1
+        yield count
+        time.sleep(0)
+    raise Exception("Timeout waiting for %s" % purpose)
+
+
+class ChangeReference(git.Reference):
+    _common_path_default = "refs/changes"
+    _points_to_commits_only = True
+
+
+class FakeChange(object):
+    categories = {'APRV': ('Approved', -1, 1),
+                  'CRVW': ('Code-Review', -2, 2),
+                  'VRFY': ('Verified', -2, 2)}
+
+    def __init__(self, gerrit, number, project, branch, subject,
+                 status='NEW', upstream_root=None):
+        self.gerrit = gerrit
+        self.reported = 0
+        self.queried = 0
+        self.patchsets = []
+        self.number = number
+        self.project = project
+        self.branch = branch
+        self.subject = subject
+        self.latest_patchset = 0
+        self.depends_on_change = None
+        self.needed_by_changes = []
+        self.fail_merge = False
+        self.messages = []
+        self.data = {
+            'branch': branch,
+            'comments': [],
+            'commitMessage': subject,
+            'createdOn': time.time(),
+            'id': 'I' + random_sha1(),
+            'lastUpdated': time.time(),
+            'number': str(number),
+            'open': status == 'NEW',
+            'owner': {'email': 'user@example.com',
+                      'name': 'User Name',
+                      'username': 'username'},
+            'patchSets': self.patchsets,
+            'project': project,
+            'status': status,
+            'subject': subject,
+            'submitRecords': [],
+            'url': 'https://hostname/%s' % number}
+
+        self.upstream_root = upstream_root
+        self.addPatchset()
+        self.data['submitRecords'] = self.getSubmitRecords()
+        self.open = status == 'NEW'
+
+    def add_fake_change_to_repo(self, msg, fn, large):
+        path = os.path.join(self.upstream_root, self.project)
+        repo = git.Repo(path)
+        ref = ChangeReference.create(repo, '1/%s/%s' % (self.number,
+                                                        self.latest_patchset),
+                                     'refs/tags/init')
+        repo.head.reference = ref
+        zuul.merger.merger.reset_repo_to_head(repo)
+        repo.git.clean('-x', '-f', '-d')
+
+        path = os.path.join(self.upstream_root, self.project)
+        if not large:
+            fn = os.path.join(path, fn)
+            f = open(fn, 'w')
+            f.write("test %s %s %s\n" %
+                    (self.branch, self.number, self.latest_patchset))
+            f.close()
+            repo.index.add([fn])
+        else:
+            for fni in range(100):
+                fn = os.path.join(path, str(fni))
+                f = open(fn, 'w')
+                for ci in range(4096):
+                    f.write(random.choice(string.printable))
+                f.close()
+                repo.index.add([fn])
+
+        r = repo.index.commit(msg)
+        repo.head.reference = 'master'
+        zuul.merger.merger.reset_repo_to_head(repo)
+        repo.git.clean('-x', '-f', '-d')
+        repo.heads['master'].checkout()
+        return r
+
+    def addPatchset(self, files=[], large=False):
+        self.latest_patchset += 1
+        if files:
+            fn = files[0]
+        else:
+            fn = '%s-%s' % (self.branch.replace('/', '_'), self.number)
+        msg = self.subject + '-' + str(self.latest_patchset)
+        c = self.add_fake_change_to_repo(msg, fn, large)
+        ps_files = [{'file': '/COMMIT_MSG',
+                     'type': 'ADDED'},
+                    {'file': 'README',
+                     'type': 'MODIFIED'}]
+        for f in files:
+            ps_files.append({'file': f, 'type': 'ADDED'})
+        d = {'approvals': [],
+             'createdOn': time.time(),
+             'files': ps_files,
+             'number': str(self.latest_patchset),
+             'ref': 'refs/changes/1/%s/%s' % (self.number,
+                                              self.latest_patchset),
+             'revision': c.hexsha,
+             'uploader': {'email': 'user@example.com',
+                          'name': 'User name',
+                          'username': 'user'}}
+        self.data['currentPatchSet'] = d
+        self.patchsets.append(d)
+        self.data['submitRecords'] = self.getSubmitRecords()
+
+    def getPatchsetCreatedEvent(self, patchset):
+        event = {"type": "patchset-created",
+                 "change": {"project": self.project,
+                            "branch": self.branch,
+                            "id": "I5459869c07352a31bfb1e7a8cac379cabfcb25af",
+                            "number": str(self.number),
+                            "subject": self.subject,
+                            "owner": {"name": "User Name"},
+                            "url": "https://hostname/3"},
+                 "patchSet": self.patchsets[patchset - 1],
+                 "uploader": {"name": "User Name"}}
+        return event
+
+    def getChangeRestoredEvent(self):
+        event = {"type": "change-restored",
+                 "change": {"project": self.project,
+                            "branch": self.branch,
+                            "id": "I5459869c07352a31bfb1e7a8cac379cabfcb25af",
+                            "number": str(self.number),
+                            "subject": self.subject,
+                            "owner": {"name": "User Name"},
+                            "url": "https://hostname/3"},
+                 "restorer": {"name": "User Name"},
+                 "patchSet": self.patchsets[-1],
+                 "reason": ""}
+        return event
+
+    def getChangeAbandonedEvent(self):
+        event = {"type": "change-abandoned",
+                 "change": {"project": self.project,
+                            "branch": self.branch,
+                            "id": "I5459869c07352a31bfb1e7a8cac379cabfcb25af",
+                            "number": str(self.number),
+                            "subject": self.subject,
+                            "owner": {"name": "User Name"},
+                            "url": "https://hostname/3"},
+                 "abandoner": {"name": "User Name"},
+                 "patchSet": self.patchsets[-1],
+                 "reason": ""}
+        return event
+
+    def getChangeCommentEvent(self, patchset):
+        event = {"type": "comment-added",
+                 "change": {"project": self.project,
+                            "branch": self.branch,
+                            "id": "I5459869c07352a31bfb1e7a8cac379cabfcb25af",
+                            "number": str(self.number),
+                            "subject": self.subject,
+                            "owner": {"name": "User Name"},
+                            "url": "https://hostname/3"},
+                 "patchSet": self.patchsets[patchset - 1],
+                 "author": {"name": "User Name"},
+                 "approvals": [{"type": "Code-Review",
+                                "description": "Code-Review",
+                                "value": "0"}],
+                 "comment": "This is a comment"}
+        return event
+
+    def addApproval(self, category, value, username='reviewer_john',
+                    granted_on=None, message=''):
+        if not granted_on:
+            granted_on = time.time()
+        approval = {
+            'description': self.categories[category][0],
+            'type': category,
+            'value': str(value),
+            'by': {
+                'username': username,
+                'email': username + '@example.com',
+            },
+            'grantedOn': int(granted_on)
+        }
+        for i, x in enumerate(self.patchsets[-1]['approvals'][:]):
+            if x['by']['username'] == username and x['type'] == category:
+                del self.patchsets[-1]['approvals'][i]
+        self.patchsets[-1]['approvals'].append(approval)
+        event = {'approvals': [approval],
+                 'author': {'email': 'author@example.com',
+                            'name': 'Patchset Author',
+                            'username': 'author_phil'},
+                 'change': {'branch': self.branch,
+                            'id': 'Iaa69c46accf97d0598111724a38250ae76a22c87',
+                            'number': str(self.number),
+                            'owner': {'email': 'owner@example.com',
+                                      'name': 'Change Owner',
+                                      'username': 'owner_jane'},
+                            'project': self.project,
+                            'subject': self.subject,
+                            'topic': 'master',
+                            'url': 'https://hostname/459'},
+                 'comment': message,
+                 'patchSet': self.patchsets[-1],
+                 'type': 'comment-added'}
+        self.data['submitRecords'] = self.getSubmitRecords()
+        return json.loads(json.dumps(event))
+
+    def getSubmitRecords(self):
+        status = {}
+        for cat in self.categories.keys():
+            status[cat] = 0
+
+        for a in self.patchsets[-1]['approvals']:
+            cur = status[a['type']]
+            cat_min, cat_max = self.categories[a['type']][1:]
+            new = int(a['value'])
+            if new == cat_min:
+                cur = new
+            elif abs(new) > abs(cur):
+                cur = new
+            status[a['type']] = cur
+
+        labels = []
+        ok = True
+        for typ, cat in self.categories.items():
+            cur = status[typ]
+            cat_min, cat_max = cat[1:]
+            if cur == cat_min:
+                value = 'REJECT'
+                ok = False
+            elif cur == cat_max:
+                value = 'OK'
+            else:
+                value = 'NEED'
+                ok = False
+            labels.append({'label': cat[0], 'status': value})
+        if ok:
+            return [{'status': 'OK'}]
+        return [{'status': 'NOT_READY',
+                 'labels': labels}]
+
+    def setDependsOn(self, other, patchset):
+        self.depends_on_change = other
+        d = {'id': other.data['id'],
+             'number': other.data['number'],
+             'ref': other.patchsets[patchset - 1]['ref']
+             }
+        self.data['dependsOn'] = [d]
+
+        other.needed_by_changes.append(self)
+        needed = other.data.get('neededBy', [])
+        d = {'id': self.data['id'],
+             'number': self.data['number'],
+             'ref': self.patchsets[patchset - 1]['ref'],
+             'revision': self.patchsets[patchset - 1]['revision']
+             }
+        needed.append(d)
+        other.data['neededBy'] = needed
+
+    def query(self):
+        self.queried += 1
+        d = self.data.get('dependsOn')
+        if d:
+            d = d[0]
+            if (self.depends_on_change.patchsets[-1]['ref'] == d['ref']):
+                d['isCurrentPatchSet'] = True
+            else:
+                d['isCurrentPatchSet'] = False
+        return json.loads(json.dumps(self.data))
+
+    def setMerged(self):
+        if (self.depends_on_change and
+                self.depends_on_change.data['status'] != 'MERGED'):
+            return
+        if self.fail_merge:
+            return
+        self.data['status'] = 'MERGED'
+        self.open = False
+
+        path = os.path.join(self.upstream_root, self.project)
+        repo = git.Repo(path)
+        repo.heads[self.branch].commit = \
+            repo.commit(self.patchsets[-1]['revision'])
+
+    def setReported(self):
+        self.reported += 1
+
+
+class FakeGerritConnection(zuul.connection.gerrit.GerritConnection):
+    log = logging.getLogger("zuul.test.FakeGerritConnection")
+
+    def __init__(self, connection_name, connection_config,
+                 changes_db=None, queues_db=None, upstream_root=None):
+        super(FakeGerritConnection, self).__init__(connection_name,
+                                                   connection_config)
+
+        self.event_queue = queues_db
+        self.fixture_dir = os.path.join(FIXTURE_DIR, 'gerrit')
+        self.change_number = 0
+        self.changes = changes_db
+        self.queries = []
+        self.upstream_root = upstream_root
+
+    def addFakeChange(self, project, branch, subject, status='NEW'):
+        self.change_number += 1
+        c = FakeChange(self, self.change_number, project, branch, subject,
+                       upstream_root=self.upstream_root,
+                       status=status)
+        self.changes[self.change_number] = c
+        return c
+
+    def review(self, project, changeid, message, action):
+        number, ps = changeid.split(',')
+        change = self.changes[int(number)]
+
+        # Add the approval back onto the change (ie simulate what gerrit would
+        # do).
+        # Usually when zuul leaves a review it'll create a feedback loop where
+        # zuul's review enters another gerrit event (which is then picked up by
+        # zuul). However, we can't mimic this behaviour (by adding this
+        # approval event into the queue) as it stops jobs from checking what
+        # happens before this event is triggered. If a job needs to see what
+        # happens they can add their own verified event into the queue.
+        # Nevertheless, we can update change with the new review in gerrit.
+
+        for cat in ['CRVW', 'VRFY', 'APRV']:
+            if cat in action:
+                change.addApproval(cat, action[cat], username=self.user)
+
+        if 'label' in action:
+            parts = action['label'].split('=')
+            change.addApproval(parts[0], parts[2], username=self.user)
+
+        change.messages.append(message)
+
+        if 'submit' in action:
+            change.setMerged()
+        if message:
+            change.setReported()
+
+    def query(self, number):
+        change = self.changes.get(int(number))
+        if change:
+            return change.query()
+        return {}
+
+    def simpleQuery(self, query):
+        self.log.debug("simpleQuery: %s" % query)
+        self.queries.append(query)
+        if query.startswith('change:'):
+            # Query a specific changeid
+            changeid = query[len('change:'):]
+            l = [change.query() for change in self.changes.values()
+                 if change.data['id'] == changeid]
+        elif query.startswith('message:'):
+            # Query the content of a commit message
+            msg = query[len('message:'):].strip()
+            l = [change.query() for change in self.changes.values()
+                 if msg in change.data['commitMessage']]
+        else:
+            # Query all open changes
+            l = [change.query() for change in self.changes.values()]
+        return l
+
+    def _start_watcher_thread(self, *args, **kw):
+        pass
+
+    def getGitUrl(self, project):
+        return os.path.join(self.upstream_root, project.name)
+
+
+class BuildHistory(object):
+    def __init__(self, **kw):
+        self.__dict__.update(kw)
+
+    def __repr__(self):
+        return ("<Completed build, result: %s name: %s #%s changes: %s>" %
+                (self.result, self.name, self.number, self.changes))
+
+
+class FakeURLOpener(object):
+    def __init__(self, upstream_root, url):
+        self.upstream_root = upstream_root
+        self.url = url
+
+    def read(self):
+        res = urlparse.urlparse(self.url)
+        path = res.path
+        project = '/'.join(path.split('/')[2:-2])
+        ret = '001e# service=git-upload-pack\n'
+        ret += ('000000a31270149696713ba7e06f1beb760f20d359c4abed HEAD\x00'
+                'multi_ack thin-pack side-band side-band-64k ofs-delta '
+                'shallow no-progress include-tag multi_ack_detailed no-done\n')
+        path = os.path.join(self.upstream_root, project)
+        repo = git.Repo(path)
+        for ref in repo.refs:
+            r = ref.object.hexsha + ' ' + ref.path + '\n'
+            ret += '%04x%s' % (len(r) + 4, r)
+        ret += '0000'
+        return ret
+
+
+class FakeStatsd(threading.Thread):
+    def __init__(self):
+        threading.Thread.__init__(self)
+        self.daemon = True
+        self.sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
+        self.sock.bind(('', 0))
+        self.port = self.sock.getsockname()[1]
+        self.wake_read, self.wake_write = os.pipe()
+        self.stats = []
+
+    def run(self):
+        while True:
+            poll = select.poll()
+            poll.register(self.sock, select.POLLIN)
+            poll.register(self.wake_read, select.POLLIN)
+            ret = poll.poll()
+            for (fd, event) in ret:
+                if fd == self.sock.fileno():
+                    data = self.sock.recvfrom(1024)
+                    if not data:
+                        return
+                    self.stats.append(data[0])
+                if fd == self.wake_read:
+                    return
+
+    def stop(self):
+        os.write(self.wake_write, '1\n')
+
+
+class FakeBuild(threading.Thread):
+    log = logging.getLogger("zuul.test")
+
+    def __init__(self, worker, job, number, node):
+        threading.Thread.__init__(self)
+        self.daemon = True
+        self.worker = worker
+        self.job = job
+        self.name = job.name.split(':')[1]
+        self.number = number
+        self.node = node
+        self.parameters = json.loads(job.arguments)
+        self.unique = self.parameters['ZUUL_UUID']
+        self.wait_condition = threading.Condition()
+        self.waiting = False
+        self.aborted = False
+        self.created = time.time()
+        self.description = ''
+        self.run_error = False
+
+    def release(self):
+        self.wait_condition.acquire()
+        self.wait_condition.notify()
+        self.waiting = False
+        self.log.debug("Build %s released" % self.unique)
+        self.wait_condition.release()
+
+    def isWaiting(self):
+        self.wait_condition.acquire()
+        if self.waiting:
+            ret = True
+        else:
+            ret = False
+        self.wait_condition.release()
+        return ret
+
+    def _wait(self):
+        self.wait_condition.acquire()
+        self.waiting = True
+        self.log.debug("Build %s waiting" % self.unique)
+        self.wait_condition.wait()
+        self.wait_condition.release()
+
+    def run(self):
+        data = {
+            'url': 'https://server/job/%s/%s/' % (self.name, self.number),
+            'name': self.name,
+            'number': self.number,
+            'manager': self.worker.worker_id,
+            'worker_name': 'My Worker',
+            'worker_hostname': 'localhost',
+            'worker_ips': ['127.0.0.1', '192.168.1.1'],
+            'worker_fqdn': 'zuul.example.org',
+            'worker_program': 'FakeBuilder',
+            'worker_version': 'v1.1',
+            'worker_extra': {'something': 'else'}
+        }
+
+        self.log.debug('Running build %s' % self.unique)
+
+        self.job.sendWorkData(json.dumps(data))
+        self.log.debug('Sent WorkData packet with %s' % json.dumps(data))
+        self.job.sendWorkStatus(0, 100)
+
+        if self.worker.hold_jobs_in_build:
+            self.log.debug('Holding build %s' % self.unique)
+            self._wait()
+        self.log.debug("Build %s continuing" % self.unique)
+
+        self.worker.lock.acquire()
+
+        result = 'SUCCESS'
+        if (('ZUUL_REF' in self.parameters) and
+            self.worker.shouldFailTest(self.name,
+                                       self.parameters['ZUUL_REF'])):
+            result = 'FAILURE'
+        if self.aborted:
+            result = 'ABORTED'
+
+        if self.run_error:
+            work_fail = True
+            result = 'RUN_ERROR'
+        else:
+            data['result'] = result
+            data['node_labels'] = ['bare-necessities']
+            data['node_name'] = 'foo'
+            work_fail = False
+
+        changes = None
+        if 'ZUUL_CHANGE_IDS' in self.parameters:
+            changes = self.parameters['ZUUL_CHANGE_IDS']
+
+        self.worker.build_history.append(
+            BuildHistory(name=self.name, number=self.number,
+                         result=result, changes=changes, node=self.node,
+                         uuid=self.unique, description=self.description,
+                         parameters=self.parameters,
+                         pipeline=self.parameters['ZUUL_PIPELINE'])
+        )
+
+        self.job.sendWorkData(json.dumps(data))
+        if work_fail:
+            self.job.sendWorkFail()
+        else:
+            self.job.sendWorkComplete(json.dumps(data))
+        del self.worker.gearman_jobs[self.job.unique]
+        self.worker.running_builds.remove(self)
+        self.worker.lock.release()
+
+
+class FakeWorker(gear.Worker):
+    def __init__(self, worker_id, test):
+        super(FakeWorker, self).__init__(worker_id)
+        self.gearman_jobs = {}
+        self.build_history = []
+        self.running_builds = []
+        self.build_counter = 0
+        self.fail_tests = {}
+        self.test = test
+
+        self.hold_jobs_in_build = False
+        self.lock = threading.Lock()
+        self.__work_thread = threading.Thread(target=self.work)
+        self.__work_thread.daemon = True
+        self.__work_thread.start()
+
+    def handleJob(self, job):
+        parts = job.name.split(":")
+        cmd = parts[0]
+        name = parts[1]
+        if len(parts) > 2:
+            node = parts[2]
+        else:
+            node = None
+        if cmd == 'build':
+            self.handleBuild(job, name, node)
+        elif cmd == 'stop':
+            self.handleStop(job, name)
+        elif cmd == 'set_description':
+            self.handleSetDescription(job, name)
+
+    def handleBuild(self, job, name, node):
+        build = FakeBuild(self, job, self.build_counter, node)
+        job.build = build
+        self.gearman_jobs[job.unique] = job
+        self.build_counter += 1
+
+        self.running_builds.append(build)
+        build.start()
+
+    def handleStop(self, job, name):
+        self.log.debug("handle stop")
+        parameters = json.loads(job.arguments)
+        name = parameters['name']
+        number = parameters['number']
+        for build in self.running_builds:
+            if build.name == name and build.number == number:
+                build.aborted = True
+                build.release()
+                job.sendWorkComplete()
+                return
+        job.sendWorkFail()
+
+    def handleSetDescription(self, job, name):
+        self.log.debug("handle set description")
+        parameters = json.loads(job.arguments)
+        name = parameters['name']
+        number = parameters['number']
+        descr = parameters['html_description']
+        for build in self.running_builds:
+            if build.name == name and build.number == number:
+                build.description = descr
+                job.sendWorkComplete()
+                return
+        for build in self.build_history:
+            if build.name == name and build.number == number:
+                build.description = descr
+                job.sendWorkComplete()
+                return
+        job.sendWorkFail()
+
+    def work(self):
+        while self.running:
+            try:
+                job = self.getJob()
+            except gear.InterruptedError:
+                continue
+            try:
+                self.handleJob(job)
+            except:
+                self.log.exception("Worker exception:")
+
+    def addFailTest(self, name, change):
+        l = self.fail_tests.get(name, [])
+        l.append(change)
+        self.fail_tests[name] = l
+
+    def shouldFailTest(self, name, ref):
+        l = self.fail_tests.get(name, [])
+        for change in l:
+            if self.test.ref_has_change(ref, change):
+                return True
+        return False
+
+    def release(self, regex=None):
+        builds = self.running_builds[:]
+        self.log.debug("releasing build %s (%s)" % (regex,
+                                                    len(self.running_builds)))
+        for build in builds:
+            if not regex or re.match(regex, build.name):
+                self.log.debug("releasing build %s" %
+                               (build.parameters['ZUUL_UUID']))
+                build.release()
+            else:
+                self.log.debug("not releasing build %s" %
+                               (build.parameters['ZUUL_UUID']))
+        self.log.debug("done releasing builds %s (%s)" %
+                       (regex, len(self.running_builds)))
+
+
+class FakeGearmanServer(gear.Server):
+    def __init__(self):
+        self.hold_jobs_in_queue = False
+        super(FakeGearmanServer, self).__init__(0)
+
+    def getJobForConnection(self, connection, peek=False):
+        for queue in [self.high_queue, self.normal_queue, self.low_queue]:
+            for job in queue:
+                if not hasattr(job, 'waiting'):
+                    if job.name.startswith('build:'):
+                        job.waiting = self.hold_jobs_in_queue
+                    else:
+                        job.waiting = False
+                if job.waiting:
+                    continue
+                if job.name in connection.functions:
+                    if not peek:
+                        queue.remove(job)
+                        connection.related_jobs[job.handle] = job
+                        job.worker_connection = connection
+                    job.running = True
+                    return job
+        return None
+
+    def release(self, regex=None):
+        released = False
+        qlen = (len(self.high_queue) + len(self.normal_queue) +
+                len(self.low_queue))
+        self.log.debug("releasing queued job %s (%s)" % (regex, qlen))
+        for job in self.getQueue():
+            cmd, name = job.name.split(':')
+            if cmd != 'build':
+                continue
+            if not regex or re.match(regex, name):
+                self.log.debug("releasing queued job %s" %
+                               job.unique)
+                job.waiting = False
+                released = True
+            else:
+                self.log.debug("not releasing queued job %s" %
+                               job.unique)
+        if released:
+            self.wakeConnections()
+        qlen = (len(self.high_queue) + len(self.normal_queue) +
+                len(self.low_queue))
+        self.log.debug("done releasing queued jobs %s (%s)" % (regex, qlen))
+
+
+class FakeSMTP(object):
+    log = logging.getLogger('zuul.FakeSMTP')
+
+    def __init__(self, messages, server, port):
+        self.server = server
+        self.port = port
+        self.messages = messages
+
+    def sendmail(self, from_email, to_email, msg):
+        self.log.info("Sending email from %s, to %s, with msg %s" % (
+                      from_email, to_email, msg))
+
+        headers = msg.split('\n\n', 1)[0]
+        body = msg.split('\n\n', 1)[1]
+
+        self.messages.append(dict(
+            from_email=from_email,
+            to_email=to_email,
+            msg=msg,
+            headers=headers,
+            body=body,
+        ))
+
+        return True
+
+    def quit(self):
+        return True
+
+
+class FakeSwiftClientConnection(swiftclient.client.Connection):
+    def post_account(self, headers):
+        # Do nothing
+        pass
+
+    def get_auth(self):
+        # Returns endpoint and (unused) auth token
+        endpoint = os.path.join('https://storage.example.org', 'V1',
+                                'AUTH_account')
+        return endpoint, ''
+
+
+class BaseTestCase(testtools.TestCase):
+    log = logging.getLogger("zuul.test")
+
+    def setUp(self):
+        super(BaseTestCase, self).setUp()
+        test_timeout = os.environ.get('OS_TEST_TIMEOUT', 0)
+        try:
+            test_timeout = int(test_timeout)
+        except ValueError:
+            # If timeout value is invalid do not set a timeout.
+            test_timeout = 0
+        if test_timeout > 0:
+            self.useFixture(fixtures.Timeout(test_timeout, gentle=False))
+
+        if (os.environ.get('OS_STDOUT_CAPTURE') == 'True' or
+            os.environ.get('OS_STDOUT_CAPTURE') == '1'):
+            stdout = self.useFixture(fixtures.StringStream('stdout')).stream
+            self.useFixture(fixtures.MonkeyPatch('sys.stdout', stdout))
+        if (os.environ.get('OS_STDERR_CAPTURE') == 'True' or
+            os.environ.get('OS_STDERR_CAPTURE') == '1'):
+            stderr = self.useFixture(fixtures.StringStream('stderr')).stream
+            self.useFixture(fixtures.MonkeyPatch('sys.stderr', stderr))
+        if (os.environ.get('OS_LOG_CAPTURE') == 'True' or
+            os.environ.get('OS_LOG_CAPTURE') == '1'):
+            self.useFixture(fixtures.FakeLogger(
+                level=logging.DEBUG,
+                format='%(asctime)s %(name)-32s '
+                '%(levelname)-8s %(message)s'))
+
+
+class ZuulTestCase(BaseTestCase):
+
+    def setUp(self):
+        super(ZuulTestCase, self).setUp()
+        if USE_TEMPDIR:
+            tmp_root = self.useFixture(fixtures.TempDir(
+                rootdir=os.environ.get("ZUUL_TEST_ROOT"))
+            ).path
+        else:
+            tmp_root = os.environ.get("ZUUL_TEST_ROOT")
+        self.test_root = os.path.join(tmp_root, "zuul-test")
+        self.upstream_root = os.path.join(self.test_root, "upstream")
+        self.git_root = os.path.join(self.test_root, "git")
+
+        if os.path.exists(self.test_root):
+            shutil.rmtree(self.test_root)
+        os.makedirs(self.test_root)
+        os.makedirs(self.upstream_root)
+
+        # Make per test copy of Configuration.
+        self.setup_config()
+        self.config.set('zuul', 'layout_config',
+                        os.path.join(FIXTURE_DIR,
+                                     self.config.get('zuul', 'layout_config')))
+        self.config.set('merger', 'git_dir', self.git_root)
+
+        # For each project in config:
+        self.init_repo("org/project")
+        self.init_repo("org/project1")
+        self.init_repo("org/project2")
+        self.init_repo("org/project3")
+        self.init_repo("org/project4")
+        self.init_repo("org/project5")
+        self.init_repo("org/project6")
+        self.init_repo("org/one-job-project")
+        self.init_repo("org/nonvoting-project")
+        self.init_repo("org/templated-project")
+        self.init_repo("org/layered-project")
+        self.init_repo("org/node-project")
+        self.init_repo("org/conflict-project")
+        self.init_repo("org/noop-project")
+        self.init_repo("org/experimental-project")
+        self.init_repo("org/no-jobs-project")
+
+        self.statsd = FakeStatsd()
+        # note, use 127.0.0.1 rather than localhost to avoid getting ipv6
+        # see: https://github.com/jsocol/pystatsd/issues/61
+        os.environ['STATSD_HOST'] = '127.0.0.1'
+        os.environ['STATSD_PORT'] = str(self.statsd.port)
+        self.statsd.start()
+        # the statsd client object is configured in the statsd module import
+        reload(statsd)
+        reload(zuul.scheduler)
+
+        self.gearman_server = FakeGearmanServer()
+
+        self.config.set('gearman', 'port', str(self.gearman_server.port))
+
+        self.worker = FakeWorker('fake_worker', self)
+        self.worker.addServer('127.0.0.1', self.gearman_server.port)
+        self.gearman_server.worker = self.worker
+
+        zuul.source.gerrit.GerritSource.replication_timeout = 1.5
+        zuul.source.gerrit.GerritSource.replication_retry_interval = 0.5
+
+        self.sched = zuul.scheduler.Scheduler(self.config)
+
+        self.useFixture(fixtures.MonkeyPatch('swiftclient.client.Connection',
+                                             FakeSwiftClientConnection))
+        self.swift = zuul.lib.swift.Swift(self.config)
+
+        self.event_queues = [
+            self.sched.result_event_queue,
+            self.sched.trigger_event_queue
+        ]
+
+        self.configure_connections()
+        self.sched.registerConnections(self.connections)
+
+        def URLOpenerFactory(*args, **kw):
+            if isinstance(args[0], urllib2.Request):
+                return old_urlopen(*args, **kw)
+            return FakeURLOpener(self.upstream_root, *args, **kw)
+
+        old_urlopen = urllib2.urlopen
+        urllib2.urlopen = URLOpenerFactory
+
+        self.merge_server = zuul.merger.server.MergeServer(self.config,
+                                                           self.connections)
+        self.merge_server.start()
+
+        self.launcher = zuul.launcher.gearman.Gearman(self.config, self.sched,
+                                                      self.swift)
+        self.merge_client = zuul.merger.client.MergeClient(
+            self.config, self.sched)
+
+        self.sched.setLauncher(self.launcher)
+        self.sched.setMerger(self.merge_client)
+
+        self.webapp = zuul.webapp.WebApp(self.sched, port=0)
+        self.rpc = zuul.rpclistener.RPCListener(self.config, self.sched)
+
+        self.sched.start()
+        self.sched.reconfigure(self.config)
+        self.sched.resume()
+        self.webapp.start()
+        self.rpc.start()
+        self.launcher.gearman.waitForServer()
+        self.registerJobs()
+        self.builds = self.worker.running_builds
+        self.history = self.worker.build_history
+
+        self.addCleanup(self.assertFinalState)
+        self.addCleanup(self.shutdown)
+
+    def configure_connections(self):
+        # Register connections from the config
+        self.smtp_messages = []
+
+        def FakeSMTPFactory(*args, **kw):
+            args = [self.smtp_messages] + list(args)
+            return FakeSMTP(*args, **kw)
+
+        self.useFixture(fixtures.MonkeyPatch('smtplib.SMTP', FakeSMTPFactory))
+
+        # Set a changes database so multiple FakeGerrit's can report back to
+        # a virtual canonical database given by the configured hostname
+        self.gerrit_changes_dbs = {}
+        self.gerrit_queues_dbs = {}
+        self.connections = {}
+
+        for section_name in self.config.sections():
+            con_match = re.match(r'^connection ([\'\"]?)(.*)(\1)$',
+                                 section_name, re.I)
+            if not con_match:
+                continue
+            con_name = con_match.group(2)
+            con_config = dict(self.config.items(section_name))
+
+            if 'driver' not in con_config:
+                raise Exception("No driver specified for connection %s."
+                                % con_name)
+
+            con_driver = con_config['driver']
+
+            # TODO(jhesketh): load the required class automatically
+            if con_driver == 'gerrit':
+                if con_config['server'] not in self.gerrit_changes_dbs.keys():
+                    self.gerrit_changes_dbs[con_config['server']] = {}
+                if con_config['server'] not in self.gerrit_queues_dbs.keys():
+                    self.gerrit_queues_dbs[con_config['server']] = \
+                        Queue.Queue()
+                    self.event_queues.append(
+                        self.gerrit_queues_dbs[con_config['server']])
+                con_config['event_delay'] = 0.0
+                self.connections[con_name] = FakeGerritConnection(
+                    con_name, con_config,
+                    changes_db=self.gerrit_changes_dbs[con_config['server']],
+                    queues_db=self.gerrit_queues_dbs[con_config['server']],
+                    upstream_root=self.upstream_root
+                )
+                setattr(self, 'fake_' + con_name, self.connections[con_name])
+            elif con_driver == 'smtp':
+                self.connections[con_name] = \
+                    zuul.connection.smtp.SMTPConnection(con_name, con_config)
+            else:
+                raise Exception("Unknown driver, %s, for connection %s"
+                                % (con_config['driver'], con_name))
+
+        # If the [gerrit] or [smtp] sections still exist, load them in as a
+        # connection named 'gerrit' or 'smtp' respectfully
+
+        if 'gerrit' in self.config.sections():
+            self.gerrit_changes_dbs['gerrit'] = {}
+            self.gerrit_queues_dbs['gerrit'] = Queue.Queue()
+            self.event_queues.append(self.gerrit_queues_dbs['gerrit'])
+            self.connections['gerrit'] = FakeGerritConnection(
+                '_legacy_gerrit', dict(self.config.items('gerrit')),
+                changes_db=self.gerrit_changes_dbs['gerrit'],
+                queues_db=self.gerrit_queues_dbs['gerrit'])
+
+        if 'smtp' in self.config.sections():
+            self.connections['smtp'] = \
+                zuul.connection.smtp.SMTPConnection(
+                    '_legacy_smtp', dict(self.config.items('smtp')))
+
+    def setup_config(self, config_file='zuul.conf'):
+        """Per test config object. Override to set different config."""
+        self.config = ConfigParser.ConfigParser()
+        self.config.read(os.path.join(FIXTURE_DIR, config_file))
+
+    def assertFinalState(self):
+        # Make sure that git.Repo objects have been garbage collected.
+        repos = []
+        gc.collect()
+        for obj in gc.get_objects():
+            if isinstance(obj, git.Repo):
+                repos.append(obj)
+        self.assertEqual(len(repos), 0)
+        self.assertEmptyQueues()
+        for pipeline in self.sched.layout.pipelines.values():
+            if isinstance(pipeline.manager,
+                          zuul.scheduler.IndependentPipelineManager):
+                self.assertEqual(len(pipeline.queues), 0)
+
+    def shutdown(self):
+        self.log.debug("Shutting down after tests")
+        self.launcher.stop()
+        self.merge_server.stop()
+        self.merge_server.join()
+        self.merge_client.stop()
+        self.worker.shutdown()
+        self.sched.stop()
+        self.sched.join()
+        self.statsd.stop()
+        self.statsd.join()
+        self.webapp.stop()
+        self.webapp.join()
+        self.rpc.stop()
+        self.rpc.join()
+        self.gearman_server.shutdown()
+        threads = threading.enumerate()
+        if len(threads) > 1:
+            self.log.error("More than one thread is running: %s" % threads)
+
+    def init_repo(self, project):
+        parts = project.split('/')
+        path = os.path.join(self.upstream_root, *parts[:-1])
+        if not os.path.exists(path):
+            os.makedirs(path)
+        path = os.path.join(self.upstream_root, project)
+        repo = git.Repo.init(path)
+
+        repo.config_writer().set_value('user', 'email', 'user@example.com')
+        repo.config_writer().set_value('user', 'name', 'User Name')
+        repo.config_writer().write()
+
+        fn = os.path.join(path, 'README')
+        f = open(fn, 'w')
+        f.write("test\n")
+        f.close()
+        repo.index.add([fn])
+        repo.index.commit('initial commit')
+        master = repo.create_head('master')
+        repo.create_tag('init')
+
+        repo.head.reference = master
+        zuul.merger.merger.reset_repo_to_head(repo)
+        repo.git.clean('-x', '-f', '-d')
+
+        self.create_branch(project, 'mp')
+
+    def create_branch(self, project, branch):
+        path = os.path.join(self.upstream_root, project)
+        repo = git.Repo.init(path)
+        fn = os.path.join(path, 'README')
+
+        branch_head = repo.create_head(branch)
+        repo.head.reference = branch_head
+        f = open(fn, 'a')
+        f.write("test %s\n" % branch)
+        f.close()
+        repo.index.add([fn])
+        repo.index.commit('%s commit' % branch)
+
+        repo.head.reference = repo.heads['master']
+        zuul.merger.merger.reset_repo_to_head(repo)
+        repo.git.clean('-x', '-f', '-d')
+
+    def ref_has_change(self, ref, change):
+        path = os.path.join(self.git_root, change.project)
+        repo = git.Repo(path)
+        try:
+            for commit in repo.iter_commits(ref):
+                if commit.message.strip() == ('%s-1' % change.subject):
+                    return True
+        except GitCommandError:
+            pass
+        return False
+
+    def job_has_changes(self, *args):
+        job = args[0]
+        commits = args[1:]
+        if isinstance(job, FakeBuild):
+            parameters = job.parameters
+        else:
+            parameters = json.loads(job.arguments)
+        project = parameters['ZUUL_PROJECT']
+        path = os.path.join(self.git_root, project)
+        repo = git.Repo(path)
+        ref = parameters['ZUUL_REF']
+        sha = parameters['ZUUL_COMMIT']
+        repo_messages = [c.message.strip() for c in repo.iter_commits(ref)]
+        repo_shas = [c.hexsha for c in repo.iter_commits(ref)]
+        commit_messages = ['%s-1' % commit.subject for commit in commits]
+        self.log.debug("Checking if job %s has changes; commit_messages %s;"
+                       " repo_messages %s; sha %s" % (job, commit_messages,
+                                                      repo_messages, sha))
+        for msg in commit_messages:
+            if msg not in repo_messages:
+                self.log.debug("  messages do not match")
+                return False
+        if repo_shas[0] != sha:
+            self.log.debug("  sha does not match")
+            return False
+        self.log.debug("  OK")
+        return True
+
+    def registerJobs(self):
+        count = 0
+        for job in self.sched.layout.jobs.keys():
+            self.worker.registerFunction('build:' + job)
+            count += 1
+        self.worker.registerFunction('stop:' + self.worker.worker_id)
+        count += 1
+
+        while len(self.gearman_server.functions) < count:
+            time.sleep(0)
+
+    def orderedRelease(self):
+        # Run one build at a time to ensure non-race order:
+        while len(self.builds):
+            self.release(self.builds[0])
+            self.waitUntilSettled()
+
+    def release(self, job):
+        if isinstance(job, FakeBuild):
+            job.release()
+        else:
+            job.waiting = False
+            self.log.debug("Queued job %s released" % job.unique)
+            self.gearman_server.wakeConnections()
+
+    def getParameter(self, job, name):
+        if isinstance(job, FakeBuild):
+            return job.parameters[name]
+        else:
+            parameters = json.loads(job.arguments)
+            return parameters[name]
+
+    def resetGearmanServer(self):
+        self.worker.setFunctions([])
+        while True:
+            done = True
+            for connection in self.gearman_server.active_connections:
+                if (connection.functions and
+                    connection.client_id not in ['Zuul RPC Listener',
+                                                 'Zuul Merger']):
+                    done = False
+            if done:
+                break
+            time.sleep(0)
+        self.gearman_server.functions = set()
+        self.rpc.register()
+        self.merge_server.register()
+
+    def haveAllBuildsReported(self):
+        # See if Zuul is waiting on a meta job to complete
+        if self.launcher.meta_jobs:
+            return False
+        # Find out if every build that the worker has completed has been
+        # reported back to Zuul.  If it hasn't then that means a Gearman
+        # event is still in transit and the system is not stable.
+        for build in self.worker.build_history:
+            zbuild = self.launcher.builds.get(build.uuid)
+            if not zbuild:
+                # It has already been reported
+                continue
+            # It hasn't been reported yet.
+            return False
+        # Make sure that none of the worker connections are in GRAB_WAIT
+        for connection in self.worker.active_connections:
+            if connection.state == 'GRAB_WAIT':
+                return False
+        return True
+
+    def areAllBuildsWaiting(self):
+        builds = self.launcher.builds.values()
+        for build in builds:
+            client_job = None
+            for conn in self.launcher.gearman.active_connections:
+                for j in conn.related_jobs.values():
+                    if j.unique == build.uuid:
+                        client_job = j
+                        break
+            if not client_job:
+                self.log.debug("%s is not known to the gearman client" %
+                               build)
+                return False
+            if not client_job.handle:
+                self.log.debug("%s has no handle" % client_job)
+                return False
+            server_job = self.gearman_server.jobs.get(client_job.handle)
+            if not server_job:
+                self.log.debug("%s is not known to the gearman server" %
+                               client_job)
+                return False
+            if not hasattr(server_job, 'waiting'):
+                self.log.debug("%s is being enqueued" % server_job)
+                return False
+            if server_job.waiting:
+                continue
+            worker_job = self.worker.gearman_jobs.get(server_job.unique)
+            if worker_job:
+                if build.number is None:
+                    self.log.debug("%s has not reported start" % worker_job)
+                    return False
+                if worker_job.build.isWaiting():
+                    continue
+                else:
+                    self.log.debug("%s is running" % worker_job)
+                    return False
+            else:
+                self.log.debug("%s is unassigned" % server_job)
+                return False
+        return True
+
+    def eventQueuesEmpty(self):
+        for queue in self.event_queues:
+            yield queue.empty()
+
+    def eventQueuesJoin(self):
+        for queue in self.event_queues:
+            queue.join()
+
+    def waitUntilSettled(self):
+        self.log.debug("Waiting until settled...")
+        start = time.time()
+        while True:
+            if time.time() - start > 10:
+                print 'queue status:',
+                print ' '.join(self.eventQueuesEmpty())
+                print self.areAllBuildsWaiting()
+                raise Exception("Timeout waiting for Zuul to settle")
+            # Make sure no new events show up while we're checking
+            self.worker.lock.acquire()
+            # have all build states propogated to zuul?
+            if self.haveAllBuildsReported():
+                # Join ensures that the queue is empty _and_ events have been
+                # processed
+                self.eventQueuesJoin()
+                self.sched.run_handler_lock.acquire()
+                if (not self.merge_client.build_sets and
+                    all(self.eventQueuesEmpty()) and
+                    self.haveAllBuildsReported() and
+                    self.areAllBuildsWaiting()):
+                    self.sched.run_handler_lock.release()
+                    self.worker.lock.release()
+                    self.log.debug("...settled.")
+                    return
+                self.sched.run_handler_lock.release()
+            self.worker.lock.release()
+            self.sched.wake_event.wait(0.1)
+
+    def countJobResults(self, jobs, result):
+        jobs = filter(lambda x: x.result == result, jobs)
+        return len(jobs)
+
+    def getJobFromHistory(self, name):
+        history = self.worker.build_history
+        for job in history:
+            if job.name == name:
+                return job
+        raise Exception("Unable to find job %s in history" % name)
+
+    def assertEmptyQueues(self):
+        # Make sure there are no orphaned jobs
+        for pipeline in self.sched.layout.pipelines.values():
+            for queue in pipeline.queues:
+                if len(queue.queue) != 0:
+                    print 'pipeline %s queue %s contents %s' % (
+                        pipeline.name, queue.name, queue.queue)
+                self.assertEqual(len(queue.queue), 0,
+                                 "Pipelines queues should be empty")
+
+    def assertReportedStat(self, key, value=None, kind=None):
+        start = time.time()
+        while time.time() < (start + 5):
+            for stat in self.statsd.stats:
+                pprint.pprint(self.statsd.stats)
+                k, v = stat.split(':')
+                if key == k:
+                    if value is None and kind is None:
+                        return
+                    elif value:
+                        if value == v:
+                            return
+                    elif kind:
+                        if v.endswith('|' + kind):
+                            return
+            time.sleep(0.1)
+
+        pprint.pprint(self.statsd.stats)
+        raise Exception("Key %s not found in reported stats" % key)
diff --git a/tests/cmd/test_cloner.py b/tests/cmd/test_cloner.py
index 9cbb5b8..7559702 100644
--- a/tests/cmd/test_cloner.py
+++ b/tests/cmd/test_cloner.py
@@ -1,56 +1,56 @@
-#!/usr/bin/env python
-
-# Licensed under the Apache License, Version 2.0 (the "License"); you may
-# not use this file except in compliance with the License. You may obtain
-# a copy of the License at
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-# License for the specific language governing permissions and limitations
-# under the License.
-
-import logging
-import os
-
-import testtools
-import zuul.cmd.cloner
-
-logging.basicConfig(level=logging.DEBUG,
-                    format='%(asctime)s %(name)-32s '
-                    '%(levelname)-8s %(message)s')
-
-
-class TestClonerCmdArguments(testtools.TestCase):
-
-    def setUp(self):
-        super(TestClonerCmdArguments, self).setUp()
-        self.app = zuul.cmd.cloner.Cloner()
-
-    def test_default_cache_dir_empty(self):
-        self.app.parse_arguments(['base', 'repo'])
-        self.assertEqual(None, self.app.args.cache_dir)
-
-    def test_default_cache_dir_environ(self):
-        try:
-            os.environ['ZUUL_CACHE_DIR'] = 'fromenviron'
-            self.app.parse_arguments(['base', 'repo'])
-            self.assertEqual('fromenviron', self.app.args.cache_dir)
-        finally:
-            del os.environ['ZUUL_CACHE_DIR']
-
-    def test_default_cache_dir_override_environ(self):
-        try:
-            os.environ['ZUUL_CACHE_DIR'] = 'fromenviron'
-            self.app.parse_arguments(['--cache-dir', 'argument',
-                                      'base', 'repo'])
-            self.assertEqual('argument', self.app.args.cache_dir)
-        finally:
-            del os.environ['ZUUL_CACHE_DIR']
-
-    def test_default_cache_dir_argument(self):
-        self.app.parse_arguments(['--cache-dir', 'argument',
-                                  'base', 'repo'])
-        self.assertEqual('argument', self.app.args.cache_dir)
+#!/usr/bin/env python
+
+# Licensed under the Apache License, Version 2.0 (the "License"); you may
+# not use this file except in compliance with the License. You may obtain
+# a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+# License for the specific language governing permissions and limitations
+# under the License.
+
+import logging
+import os
+
+import testtools
+import zuul.cmd.cloner
+
+logging.basicConfig(level=logging.DEBUG,
+                    format='%(asctime)s %(name)-32s '
+                    '%(levelname)-8s %(message)s')
+
+
+class TestClonerCmdArguments(testtools.TestCase):
+
+    def setUp(self):
+        super(TestClonerCmdArguments, self).setUp()
+        self.app = zuul.cmd.cloner.Cloner()
+
+    def test_default_cache_dir_empty(self):
+        self.app.parse_arguments(['base', 'repo'])
+        self.assertEqual(None, self.app.args.cache_dir)
+
+    def test_default_cache_dir_environ(self):
+        try:
+            os.environ['ZUUL_CACHE_DIR'] = 'fromenviron'
+            self.app.parse_arguments(['base', 'repo'])
+            self.assertEqual('fromenviron', self.app.args.cache_dir)
+        finally:
+            del os.environ['ZUUL_CACHE_DIR']
+
+    def test_default_cache_dir_override_environ(self):
+        try:
+            os.environ['ZUUL_CACHE_DIR'] = 'fromenviron'
+            self.app.parse_arguments(['--cache-dir', 'argument',
+                                      'base', 'repo'])
+            self.assertEqual('argument', self.app.args.cache_dir)
+        finally:
+            del os.environ['ZUUL_CACHE_DIR']
+
+    def test_default_cache_dir_argument(self):
+        self.app.parse_arguments(['--cache-dir', 'argument',
+                                  'base', 'repo'])
+        self.assertEqual('argument', self.app.args.cache_dir)
diff --git a/tests/fixtures/clonemap.yaml b/tests/fixtures/clonemap.yaml
index 0f9e084..e60edbf 100644
--- a/tests/fixtures/clonemap.yaml
+++ b/tests/fixtures/clonemap.yaml
@@ -1,5 +1,5 @@
-clonemap:
- - name: 'mediawiki/core'
-   dest: '.'
- - name: 'mediawiki/extensions/(.*)'
-   dest: '\1'
+clonemap:
+ - name: 'mediawiki/core'
+   dest: '.'
+ - name: 'mediawiki/extensions/(.*)'
+   dest: '\1'
diff --git a/tests/fixtures/custom_functions.py b/tests/fixtures/custom_functions.py
index 4712052..54e7df0 100644
--- a/tests/fixtures/custom_functions.py
+++ b/tests/fixtures/custom_functions.py
@@ -1,2 +1,2 @@
-def select_debian_node(item, params):
-    params['ZUUL_NODE'] = 'debian'
+def select_debian_node(item, params):
+    params['ZUUL_NODE'] = 'debian'
diff --git a/tests/fixtures/custom_functions_live_reconfiguration_functions.py b/tests/fixtures/custom_functions_live_reconfiguration_functions.py
index d8e06f4..fdf6f68 100644
--- a/tests/fixtures/custom_functions_live_reconfiguration_functions.py
+++ b/tests/fixtures/custom_functions_live_reconfiguration_functions.py
@@ -1,2 +1,2 @@
-def select_debian_node(item, params):
-    params['ZUUL_NODE'] = 'wheezy'
+def select_debian_node(item, params):
+    params['ZUUL_NODE'] = 'wheezy'
diff --git a/tests/fixtures/gerrit/simple_query_pagination_new_1 b/tests/fixtures/gerrit/simple_query_pagination_new_1
index b3fdd83..41562f3 100644
--- a/tests/fixtures/gerrit/simple_query_pagination_new_1
+++ b/tests/fixtures/gerrit/simple_query_pagination_new_1
@@ -1,5 +1,5 @@
-gerrit query --format json --commit-message --current-patch-set project:openstack-infra/zuul
-{"project":"openstack-infra/zuul","branch":"master","topic":"(detached","id":"I173251c8b1569755124b7cb1a48b6274bf38c94b","number":"202867","subject":"Report the per-job build wait time to graphite","owner":{"name":"Timothy R. Chavez","email":"timothy.chavez@hp.com","username":"timrchavez"},"url":"https://review.openstack.org/202867","commitMessage":"Report the per-job build wait time to graphite\n\nKnowing how long a job waits to build in aggregate can give useful\ninsights into the performance and capacity of the build system. This\nchange also uses the node labels sent back from the gearman worker to\nsubmit metrics within that context.\n\nChange-Id: I173251c8b1569755124b7cb1a48b6274bf38c94b\nDepends-On: Ibca938fcf8a65facd7e39dab4eb994dfc637722a\n","createdOn":1437104683,"lastUpdated":1440760891,"open":true,"status":"NEW"}
-{"project":"openstack-infra/zuul","branch":"master","topic":"ignore-deletes","id":"Iea75d05ddcb49b0bf748b72b9d2d5472d077f0c6","number":"178833","subject":"Add option to ignore ref-updated events emitted by branch deletions","owner":{"name":"K Jonathan Harker","email":"code@gentlydownthe.net","username":"jesusaurus"},"url":"https://review.openstack.org/178833","commitMessage":"Add option to ignore ref-updated events emitted by branch deletions\n\nWhen a branch is deleted, gerrit emits a ref-updated event with a newrev\nvalue of all zeros. This adds a boolean field to optionally not trigger\non these ref-updated events.\n\nChange-Id: Iea75d05ddcb49b0bf748b72b9d2d5472d077f0c6\n","createdOn":1430339761,"lastUpdated":1440735750,"open":true,"status":"NEW"}
-{"project":"openstack-infra/zuul","branch":"master","topic":"undefined-projects","id":"I7912197fb86c1a7becb7f43ca36078101f632715","number":"207094","subject":"Dependencies from undefined projects","owner":{"name":"Evgeny Antyshev","email":"eantyshev@virtuozzo.com","username":"eantyshev"},"url":"https://review.openstack.org/207094","commitMessage":"Dependencies from undefined projects\n\n3rd party CI layout usually has only a few projects defined,\nso it\u0027s possible that some changes depend on projects\nwhich are unknown to Zuul scheduler.\nThese items had None as a \"item.change.project\", which\nis not handled in many places, for ex. in reconfiguration.\n\nThese cases could be handled by defining these projects in layout\nas \"foreign\" projects: no jobs, no other non-standard attributes.\nChanges to those projects are also dropped, unless\nthey came as dependencies.\n\nChange-Id: I7912197fb86c1a7becb7f43ca36078101f632715\n","createdOn":1438183395,"lastUpdated":1440667433,"open":true,"status":"NEW"}
+gerrit query --format json --commit-message --current-patch-set project:openstack-infra/zuul
+{"project":"openstack-infra/zuul","branch":"master","topic":"(detached","id":"I173251c8b1569755124b7cb1a48b6274bf38c94b","number":"202867","subject":"Report the per-job build wait time to graphite","owner":{"name":"Timothy R. Chavez","email":"timothy.chavez@hp.com","username":"timrchavez"},"url":"https://review.openstack.org/202867","commitMessage":"Report the per-job build wait time to graphite\n\nKnowing how long a job waits to build in aggregate can give useful\ninsights into the performance and capacity of the build system. This\nchange also uses the node labels sent back from the gearman worker to\nsubmit metrics within that context.\n\nChange-Id: I173251c8b1569755124b7cb1a48b6274bf38c94b\nDepends-On: Ibca938fcf8a65facd7e39dab4eb994dfc637722a\n","createdOn":1437104683,"lastUpdated":1440760891,"open":true,"status":"NEW"}
+{"project":"openstack-infra/zuul","branch":"master","topic":"ignore-deletes","id":"Iea75d05ddcb49b0bf748b72b9d2d5472d077f0c6","number":"178833","subject":"Add option to ignore ref-updated events emitted by branch deletions","owner":{"name":"K Jonathan Harker","email":"code@gentlydownthe.net","username":"jesusaurus"},"url":"https://review.openstack.org/178833","commitMessage":"Add option to ignore ref-updated events emitted by branch deletions\n\nWhen a branch is deleted, gerrit emits a ref-updated event with a newrev\nvalue of all zeros. This adds a boolean field to optionally not trigger\non these ref-updated events.\n\nChange-Id: Iea75d05ddcb49b0bf748b72b9d2d5472d077f0c6\n","createdOn":1430339761,"lastUpdated":1440735750,"open":true,"status":"NEW"}
+{"project":"openstack-infra/zuul","branch":"master","topic":"undefined-projects","id":"I7912197fb86c1a7becb7f43ca36078101f632715","number":"207094","subject":"Dependencies from undefined projects","owner":{"name":"Evgeny Antyshev","email":"eantyshev@virtuozzo.com","username":"eantyshev"},"url":"https://review.openstack.org/207094","commitMessage":"Dependencies from undefined projects\n\n3rd party CI layout usually has only a few projects defined,\nso it\u0027s possible that some changes depend on projects\nwhich are unknown to Zuul scheduler.\nThese items had None as a \"item.change.project\", which\nis not handled in many places, for ex. in reconfiguration.\n\nThese cases could be handled by defining these projects in layout\nas \"foreign\" projects: no jobs, no other non-standard attributes.\nChanges to those projects are also dropped, unless\nthey came as dependencies.\n\nChange-Id: I7912197fb86c1a7becb7f43ca36078101f632715\n","createdOn":1438183395,"lastUpdated":1440667433,"open":true,"status":"NEW"}
 {"type":"stats","rowCount":3,"runTimeMilliseconds":12,"moreChanges":true}
\ No newline at end of file
diff --git a/tests/fixtures/gerrit/simple_query_pagination_new_2 b/tests/fixtures/gerrit/simple_query_pagination_new_2
index 9fd8d54..339a393 100644
--- a/tests/fixtures/gerrit/simple_query_pagination_new_2
+++ b/tests/fixtures/gerrit/simple_query_pagination_new_2
@@ -1,4 +1,4 @@
-gerrit query --format json --commit-message --current-patch-set project:openstack-infra/zuul -S 3
-{"project":"openstack-infra/zuul","branch":"master","topic":"github-integration","id":"I95f41088ea160d4e33a507c4a413e3fa7f08906b","number":"192457","subject":"(WIP) Fix job hierarchy bug.","owner":{"name":"Wayne Warren","email":"waynr+launchpad@sdf.org","username":"waynr"},"url":"https://review.openstack.org/192457","commitMessage":"(WIP) Fix job hierarchy bug.\n\nJobTree.addJob may return \u0027None\u0027, this prevents that from happening.\n\nChange-Id: I95f41088ea160d4e33a507c4a413e3fa7f08906b\n","createdOn":1434498278,"lastUpdated":1440608984,"sortKey":"003763050002efc9","open":true,"status":"NEW"}
-{"project":"openstack-infra/zuul","branch":"master","topic":"github-integration","id":"Ic5887d00ff302f67469df5154e9df10b99f1cfcd","number":"215642","subject":"(WIP) Allow using webapp from connections","owner":{"name":"Jan Hruban","email":"jan.hruban@gooddata.com","username":"hrubi"},"url":"https://review.openstack.org/215642","commitMessage":"(WIP) Allow using webapp from connections\n\nAllow connections to register their own handlers for HTTP URIs inside\nthe zuul\u0027s webapp HTTP server. That way, connections can listen for\nevents comming through HTTP.\n\nChange-Id: Ic5887d00ff302f67469df5154e9df10b99f1cfcd\n","createdOn":1440165019,"lastUpdated":1440602591,"sortKey":"0037629b00034a5a","open":true,"status":"NEW"}
+gerrit query --format json --commit-message --current-patch-set project:openstack-infra/zuul -S 3
+{"project":"openstack-infra/zuul","branch":"master","topic":"github-integration","id":"I95f41088ea160d4e33a507c4a413e3fa7f08906b","number":"192457","subject":"(WIP) Fix job hierarchy bug.","owner":{"name":"Wayne Warren","email":"waynr+launchpad@sdf.org","username":"waynr"},"url":"https://review.openstack.org/192457","commitMessage":"(WIP) Fix job hierarchy bug.\n\nJobTree.addJob may return \u0027None\u0027, this prevents that from happening.\n\nChange-Id: I95f41088ea160d4e33a507c4a413e3fa7f08906b\n","createdOn":1434498278,"lastUpdated":1440608984,"sortKey":"003763050002efc9","open":true,"status":"NEW"}
+{"project":"openstack-infra/zuul","branch":"master","topic":"github-integration","id":"Ic5887d00ff302f67469df5154e9df10b99f1cfcd","number":"215642","subject":"(WIP) Allow using webapp from connections","owner":{"name":"Jan Hruban","email":"jan.hruban@gooddata.com","username":"hrubi"},"url":"https://review.openstack.org/215642","commitMessage":"(WIP) Allow using webapp from connections\n\nAllow connections to register their own handlers for HTTP URIs inside\nthe zuul\u0027s webapp HTTP server. That way, connections can listen for\nevents comming through HTTP.\n\nChange-Id: Ic5887d00ff302f67469df5154e9df10b99f1cfcd\n","createdOn":1440165019,"lastUpdated":1440602591,"sortKey":"0037629b00034a5a","open":true,"status":"NEW"}
 {"type":"stats","rowCount":2,"runTimeMilliseconds":12,"moreChanges":false}
\ No newline at end of file
diff --git a/tests/fixtures/gerrit/simple_query_pagination_old_1 b/tests/fixtures/gerrit/simple_query_pagination_old_1
index 8ff1710..5cd30e6 100644
--- a/tests/fixtures/gerrit/simple_query_pagination_old_1
+++ b/tests/fixtures/gerrit/simple_query_pagination_old_1
@@ -1,5 +1,5 @@
-gerrit query --format json --commit-message --current-patch-set project:openstack-infra/zuul
-{"project":"openstack-infra/zuul","branch":"master","topic":"(detached","id":"I173251c8b1569755124b7cb1a48b6274bf38c94b","number":"202867","subject":"Report the per-job build wait time to graphite","owner":{"name":"Timothy R. Chavez","email":"timothy.chavez@hp.com","username":"timrchavez"},"url":"https://review.openstack.org/202867","commitMessage":"Report the per-job build wait time to graphite\n\nKnowing how long a job waits to build in aggregate can give useful\ninsights into the performance and capacity of the build system. This\nchange also uses the node labels sent back from the gearman worker to\nsubmit metrics within that context.\n\nChange-Id: I173251c8b1569755124b7cb1a48b6274bf38c94b\nDepends-On: Ibca938fcf8a65facd7e39dab4eb994dfc637722a\n","createdOn":1437104683,"lastUpdated":1440760891,"sortKey":"00376ce900031873","open":true,"status":"NEW"}
-{"project":"openstack-infra/zuul","branch":"master","topic":"ignore-deletes","id":"Iea75d05ddcb49b0bf748b72b9d2d5472d077f0c6","number":"178833","subject":"Add option to ignore ref-updated events emitted by branch deletions","owner":{"name":"K Jonathan Harker","email":"code@gentlydownthe.net","username":"jesusaurus"},"url":"https://review.openstack.org/178833","commitMessage":"Add option to ignore ref-updated events emitted by branch deletions\n\nWhen a branch is deleted, gerrit emits a ref-updated event with a newrev\nvalue of all zeros. This adds a boolean field to optionally not trigger\non these ref-updated events.\n\nChange-Id: Iea75d05ddcb49b0bf748b72b9d2d5472d077f0c6\n","createdOn":1430339761,"lastUpdated":1440735750,"sortKey":"00376b460002ba91","open":true,"status":"NEW"}
-{"project":"openstack-infra/zuul","branch":"master","topic":"undefined-projects","id":"I7912197fb86c1a7becb7f43ca36078101f632715","number":"207094","subject":"Dependencies from undefined projects","owner":{"name":"Evgeny Antyshev","email":"eantyshev@virtuozzo.com","username":"eantyshev"},"url":"https://review.openstack.org/207094","commitMessage":"Dependencies from undefined projects\n\n3rd party CI layout usually has only a few projects defined,\nso it\u0027s possible that some changes depend on projects\nwhich are unknown to Zuul scheduler.\nThese items had None as a \"item.change.project\", which\nis not handled in many places, for ex. in reconfiguration.\n\nThese cases could be handled by defining these projects in layout\nas \"foreign\" projects: no jobs, no other non-standard attributes.\nChanges to those projects are also dropped, unless\nthey came as dependencies.\n\nChange-Id: I7912197fb86c1a7becb7f43ca36078101f632715\n","createdOn":1438183395,"lastUpdated":1440667433,"sortKey":"003766d3000328f6","open":true,"status":"NEW"}
+gerrit query --format json --commit-message --current-patch-set project:openstack-infra/zuul
+{"project":"openstack-infra/zuul","branch":"master","topic":"(detached","id":"I173251c8b1569755124b7cb1a48b6274bf38c94b","number":"202867","subject":"Report the per-job build wait time to graphite","owner":{"name":"Timothy R. Chavez","email":"timothy.chavez@hp.com","username":"timrchavez"},"url":"https://review.openstack.org/202867","commitMessage":"Report the per-job build wait time to graphite\n\nKnowing how long a job waits to build in aggregate can give useful\ninsights into the performance and capacity of the build system. This\nchange also uses the node labels sent back from the gearman worker to\nsubmit metrics within that context.\n\nChange-Id: I173251c8b1569755124b7cb1a48b6274bf38c94b\nDepends-On: Ibca938fcf8a65facd7e39dab4eb994dfc637722a\n","createdOn":1437104683,"lastUpdated":1440760891,"sortKey":"00376ce900031873","open":true,"status":"NEW"}
+{"project":"openstack-infra/zuul","branch":"master","topic":"ignore-deletes","id":"Iea75d05ddcb49b0bf748b72b9d2d5472d077f0c6","number":"178833","subject":"Add option to ignore ref-updated events emitted by branch deletions","owner":{"name":"K Jonathan Harker","email":"code@gentlydownthe.net","username":"jesusaurus"},"url":"https://review.openstack.org/178833","commitMessage":"Add option to ignore ref-updated events emitted by branch deletions\n\nWhen a branch is deleted, gerrit emits a ref-updated event with a newrev\nvalue of all zeros. This adds a boolean field to optionally not trigger\non these ref-updated events.\n\nChange-Id: Iea75d05ddcb49b0bf748b72b9d2d5472d077f0c6\n","createdOn":1430339761,"lastUpdated":1440735750,"sortKey":"00376b460002ba91","open":true,"status":"NEW"}
+{"project":"openstack-infra/zuul","branch":"master","topic":"undefined-projects","id":"I7912197fb86c1a7becb7f43ca36078101f632715","number":"207094","subject":"Dependencies from undefined projects","owner":{"name":"Evgeny Antyshev","email":"eantyshev@virtuozzo.com","username":"eantyshev"},"url":"https://review.openstack.org/207094","commitMessage":"Dependencies from undefined projects\n\n3rd party CI layout usually has only a few projects defined,\nso it\u0027s possible that some changes depend on projects\nwhich are unknown to Zuul scheduler.\nThese items had None as a \"item.change.project\", which\nis not handled in many places, for ex. in reconfiguration.\n\nThese cases could be handled by defining these projects in layout\nas \"foreign\" projects: no jobs, no other non-standard attributes.\nChanges to those projects are also dropped, unless\nthey came as dependencies.\n\nChange-Id: I7912197fb86c1a7becb7f43ca36078101f632715\n","createdOn":1438183395,"lastUpdated":1440667433,"sortKey":"003766d3000328f6","open":true,"status":"NEW"}
 {"type":"stats","rowCount":3,"runTimeMilliseconds":12}
\ No newline at end of file
diff --git a/tests/fixtures/gerrit/simple_query_pagination_old_2 b/tests/fixtures/gerrit/simple_query_pagination_old_2
index c55cd40..7e3b19b 100644
--- a/tests/fixtures/gerrit/simple_query_pagination_old_2
+++ b/tests/fixtures/gerrit/simple_query_pagination_old_2
@@ -1,4 +1,4 @@
-gerrit query --format json --commit-message --current-patch-set project:openstack-infra/zuul resume_sortkey:'003766d3000328f6'
-{"project":"openstack-infra/zuul","branch":"master","topic":"github-integration","id":"I95f41088ea160d4e33a507c4a413e3fa7f08906b","number":"192457","subject":"(WIP) Fix job hierarchy bug.","owner":{"name":"Wayne Warren","email":"waynr+launchpad@sdf.org","username":"waynr"},"url":"https://review.openstack.org/192457","commitMessage":"(WIP) Fix job hierarchy bug.\n\nJobTree.addJob may return \u0027None\u0027, this prevents that from happening.\n\nChange-Id: I95f41088ea160d4e33a507c4a413e3fa7f08906b\n","createdOn":1434498278,"lastUpdated":1440608984,"sortKey":"003763050002efc9","open":true,"status":"NEW"}
-{"project":"openstack-infra/zuul","branch":"master","topic":"github-integration","id":"Ic5887d00ff302f67469df5154e9df10b99f1cfcd","number":"215642","subject":"(WIP) Allow using webapp from connections","owner":{"name":"Jan Hruban","email":"jan.hruban@gooddata.com","username":"hrubi"},"url":"https://review.openstack.org/215642","commitMessage":"(WIP) Allow using webapp from connections\n\nAllow connections to register their own handlers for HTTP URIs inside\nthe zuul\u0027s webapp HTTP server. That way, connections can listen for\nevents comming through HTTP.\n\nChange-Id: Ic5887d00ff302f67469df5154e9df10b99f1cfcd\n","createdOn":1440165019,"lastUpdated":1440602591,"sortKey":"0037629b00034a5a","open":true,"status":"NEW"}
+gerrit query --format json --commit-message --current-patch-set project:openstack-infra/zuul resume_sortkey:'003766d3000328f6'
+{"project":"openstack-infra/zuul","branch":"master","topic":"github-integration","id":"I95f41088ea160d4e33a507c4a413e3fa7f08906b","number":"192457","subject":"(WIP) Fix job hierarchy bug.","owner":{"name":"Wayne Warren","email":"waynr+launchpad@sdf.org","username":"waynr"},"url":"https://review.openstack.org/192457","commitMessage":"(WIP) Fix job hierarchy bug.\n\nJobTree.addJob may return \u0027None\u0027, this prevents that from happening.\n\nChange-Id: I95f41088ea160d4e33a507c4a413e3fa7f08906b\n","createdOn":1434498278,"lastUpdated":1440608984,"sortKey":"003763050002efc9","open":true,"status":"NEW"}
+{"project":"openstack-infra/zuul","branch":"master","topic":"github-integration","id":"Ic5887d00ff302f67469df5154e9df10b99f1cfcd","number":"215642","subject":"(WIP) Allow using webapp from connections","owner":{"name":"Jan Hruban","email":"jan.hruban@gooddata.com","username":"hrubi"},"url":"https://review.openstack.org/215642","commitMessage":"(WIP) Allow using webapp from connections\n\nAllow connections to register their own handlers for HTTP URIs inside\nthe zuul\u0027s webapp HTTP server. That way, connections can listen for\nevents comming through HTTP.\n\nChange-Id: Ic5887d00ff302f67469df5154e9df10b99f1cfcd\n","createdOn":1440165019,"lastUpdated":1440602591,"sortKey":"0037629b00034a5a","open":true,"status":"NEW"}
 {"type":"stats","rowCount":2,"runTimeMilliseconds":12}
\ No newline at end of file
diff --git a/tests/fixtures/gerrit/simple_query_pagination_old_3 b/tests/fixtures/gerrit/simple_query_pagination_old_3
index b8cdc4a..fe91397 100644
--- a/tests/fixtures/gerrit/simple_query_pagination_old_3
+++ b/tests/fixtures/gerrit/simple_query_pagination_old_3
@@ -1,2 +1,2 @@
-gerrit query --format json --commit-message --current-patch-set project:openstack-infra/zuul resume_sortkey:'0037629b00034a5a'
+gerrit query --format json --commit-message --current-patch-set project:openstack-infra/zuul resume_sortkey:'0037629b00034a5a'
 {"type":"stats","rowCount":0,"runTimeMilliseconds":12}
\ No newline at end of file
diff --git a/tests/fixtures/layout-bad-queue.yaml b/tests/fixtures/layout-bad-queue.yaml
index 3eb2051..58174e1 100644
--- a/tests/fixtures/layout-bad-queue.yaml
+++ b/tests/fixtures/layout-bad-queue.yaml
@@ -1,74 +1,74 @@
-pipelines:
-  - name: check
-    manager: IndependentPipelineManager
-    trigger:
-      gerrit:
-        - event: patchset-created
-    success:
-      gerrit:
-        verified: 1
-    failure:
-      gerrit:
-        verified: -1
-
-  - name: post
-    manager: IndependentPipelineManager
-    trigger:
-      gerrit:
-        - event: ref-updated
-          ref: ^(?!refs/).*$
-
-  - name: gate
-    manager: DependentPipelineManager
-    failure-message: Build failed.  For information on how to proceed, see http://wiki.example.org/Test_Failures
-    trigger:
-      gerrit:
-        - event: comment-added
-          approval:
-            - approved: 1
-    success:
-      gerrit:
-        verified: 2
-        submit: true
-    failure:
-      gerrit:
-        verified: -2
-    start:
-      gerrit:
-        verified: 0
-    precedence: high
-
-jobs:
-  - name: project1-project2-integration
-    queue-name: integration
-  - name: project1-test1
-    queue-name: not_integration
-
-projects:
-  - name: org/project1
-    check:
-      - project1-merge:
-        - project1-test1
-        - project1-test2
-        - project1-project2-integration
-    gate:
-      - project1-merge:
-        - project1-test1
-        - project1-test2
-        - project1-project2-integration
-    post:
-      - project1-post
-
-  - name: org/project2
-    check:
-      - project2-merge:
-        - project2-test1
-        - project2-test2
-        - project1-project2-integration
-    gate:
-      - project2-merge:
-        - project2-test1
-        - project2-test2
-        - project1-project2-integration
-    post:
-      - project2-post
+pipelines:
+  - name: check
+    manager: IndependentPipelineManager
+    trigger:
+      gerrit:
+        - event: patchset-created
+    success:
+      gerrit:
+        verified: 1
+    failure:
+      gerrit:
+        verified: -1
+
+  - name: post
+    manager: IndependentPipelineManager
+    trigger:
+      gerrit:
+        - event: ref-updated
+          ref: ^(?!refs/).*$
+
+  - name: gate
+    manager: DependentPipelineManager
+    failure-message: Build failed.  For information on how to proceed, see http://wiki.example.org/Test_Failures
+    trigger:
+      gerrit:
+        - event: comment-added
+          approval:
+            - approved: 1
+    success:
+      gerrit:
+        verified: 2
+        submit: true
+    failure:
+      gerrit:
+        verified: -2
+    start:
+      gerrit:
+        verified: 0
+    precedence: high
+
+jobs:
+  - name: project1-project2-integration
+    queue-name: integration
+  - name: project1-test1
+    queue-name: not_integration
+
+projects:
+  - name: org/project1
+    check:
+      - project1-merge:
+        - project1-test1
+        - project1-test2
+        - project1-project2-integration
+    gate:
+      - project1-merge:
+        - project1-test1
+        - project1-test2
+        - project1-project2-integration
+    post:
+      - project1-post
+
+  - name: org/project2
+    check:
+      - project2-merge:
+        - project2-test1
+        - project2-test2
+        - project1-project2-integration
+    gate:
+      - project2-merge:
+        - project2-test1
+        - project2-test2
+        - project1-project2-integration
+    post:
+      - project2-post
diff --git a/tests/fixtures/layout-cloner.yaml b/tests/fixtures/layout-cloner.yaml
index e840ed9..b55d7ce 100644
--- a/tests/fixtures/layout-cloner.yaml
+++ b/tests/fixtures/layout-cloner.yaml
@@ -1,45 +1,45 @@
-pipelines:
-  - name: gate
-    manager: DependentPipelineManager
-    failure-message: Build failed.  For information on how to proceed, see http://wiki.example.org/Test_Failures
-    trigger:
-      gerrit:
-        - event: comment-added
-          approval:
-            - approved: 1
-    start:
-      gerrit:
-        verified: 0
-    success:
-      gerrit:
-        verified: 2
-        submit: true
-    failure:
-      gerrit:
-        verified: -2
-
-projects:
-
-  - name: org/project1
-    gate:
-        - integration
-
-  - name: org/project2
-    gate:
-        - integration
-
-  - name: org/project3
-    gate:
-        - integration
-
-  - name: org/project4
-    gate:
-        - integration
-
-  - name: org/project5
-    gate:
-        - integration
-
-  - name: org/project6
-    gate:
-        - integration
+pipelines:
+  - name: gate
+    manager: DependentPipelineManager
+    failure-message: Build failed.  For information on how to proceed, see http://wiki.example.org/Test_Failures
+    trigger:
+      gerrit:
+        - event: comment-added
+          approval:
+            - approved: 1
+    start:
+      gerrit:
+        verified: 0
+    success:
+      gerrit:
+        verified: 2
+        submit: true
+    failure:
+      gerrit:
+        verified: -2
+
+projects:
+
+  - name: org/project1
+    gate:
+        - integration
+
+  - name: org/project2
+    gate:
+        - integration
+
+  - name: org/project3
+    gate:
+        - integration
+
+  - name: org/project4
+    gate:
+        - integration
+
+  - name: org/project5
+    gate:
+        - integration
+
+  - name: org/project6
+    gate:
+        - integration
diff --git a/tests/fixtures/layout-connections-multiple-gerrits.yaml b/tests/fixtures/layout-connections-multiple-gerrits.yaml
index 029f42f..1e5a935 100644
--- a/tests/fixtures/layout-connections-multiple-gerrits.yaml
+++ b/tests/fixtures/layout-connections-multiple-gerrits.yaml
@@ -1,37 +1,37 @@
-pipelines:
-  - name: check
-    manager: IndependentPipelineManager
-    source: review_gerrit
-    trigger:
-      review_gerrit:
-        - event: patchset-created
-    success:
-      review_gerrit:
-        VRFY: 1
-    failure:
-      review_gerrit:
-        VRFY: -1
-
-  - name: another_check
-    manager: IndependentPipelineManager
-    source: another_gerrit
-    trigger:
-      another_gerrit:
-        - event: patchset-created
-    success:
-      another_gerrit:
-        VRFY: 1
-    failure:
-      another_gerrit:
-        VRFY: -1
-
-projects:
-  - name: org/project
-    check:
-      - project-review-gerrit
-    another_check:
-      - project-another-gerrit
-
-  - name: org/project1
-    another_check:
-      - project1-another-gerrit
+pipelines:
+  - name: check
+    manager: IndependentPipelineManager
+    source: review_gerrit
+    trigger:
+      review_gerrit:
+        - event: patchset-created
+    success:
+      review_gerrit:
+        VRFY: 1
+    failure:
+      review_gerrit:
+        VRFY: -1
+
+  - name: another_check
+    manager: IndependentPipelineManager
+    source: another_gerrit
+    trigger:
+      another_gerrit:
+        - event: patchset-created
+    success:
+      another_gerrit:
+        VRFY: 1
+    failure:
+      another_gerrit:
+        VRFY: -1
+
+projects:
+  - name: org/project
+    check:
+      - project-review-gerrit
+    another_check:
+      - project-another-gerrit
+
+  - name: org/project1
+    another_check:
+      - project1-another-gerrit
diff --git a/tests/fixtures/layout-connections-multiple-voters.yaml b/tests/fixtures/layout-connections-multiple-voters.yaml
index 2b8df83..6869394 100644
--- a/tests/fixtures/layout-connections-multiple-voters.yaml
+++ b/tests/fixtures/layout-connections-multiple-voters.yaml
@@ -1,20 +1,20 @@
-pipelines:
-  - name: check
-    manager: IndependentPipelineManager
-    source: review_gerrit
-    trigger:
-      review_gerrit:
-        - event: patchset-created
-    success:
-      review_gerrit:
-        VRFY: 1
-    failure:
-      alt_voting_gerrit:
-        VRFY: -1
-
-projects:
-  - name: org/project
-    check:
-      - project-merge:
-        - project-test1
-        - project-test2
+pipelines:
+  - name: check
+    manager: IndependentPipelineManager
+    source: review_gerrit
+    trigger:
+      review_gerrit:
+        - event: patchset-created
+    success:
+      review_gerrit:
+        VRFY: 1
+    failure:
+      alt_voting_gerrit:
+        VRFY: -1
+
+projects:
+  - name: org/project
+    check:
+      - project-merge:
+        - project-test1
+        - project-test2
diff --git a/tests/fixtures/layout-delayed-repo-init.yaml b/tests/fixtures/layout-delayed-repo-init.yaml
index 6caf622..1b2faec 100644
--- a/tests/fixtures/layout-delayed-repo-init.yaml
+++ b/tests/fixtures/layout-delayed-repo-init.yaml
@@ -1,52 +1,52 @@
-pipelines:
-  - name: check
-    manager: IndependentPipelineManager
-    trigger:
-      gerrit:
-        - event: patchset-created
-    success:
-      gerrit:
-        verified: 1
-    failure:
-      gerrit:
-        verified: -1
-
-  - name: post
-    manager: IndependentPipelineManager
-    trigger:
-      gerrit:
-        - event: ref-updated
-          ref: ^(?!refs/).*$
-
-  - name: gate
-    manager: DependentPipelineManager
-    failure-message: Build failed.  For information on how to proceed, see http://wiki.example.org/Test_Failures
-    trigger:
-      gerrit:
-        - event: comment-added
-          approval:
-            - approved: 1
-    success:
-      gerrit:
-        verified: 2
-        submit: true
-    failure:
-      gerrit:
-        verified: -2
-    start:
-      gerrit:
-        verified: 0
-    precedence: high
-
-projects:
-  - name: org/new-project
-    check:
-      - project-merge:
-        - project-test1
-        - project-test2
-    gate:
-      - project-merge:
-        - project-test1
-        - project-test2
-    post:
-      - project-post
+pipelines:
+  - name: check
+    manager: IndependentPipelineManager
+    trigger:
+      gerrit:
+        - event: patchset-created
+    success:
+      gerrit:
+        verified: 1
+    failure:
+      gerrit:
+        verified: -1
+
+  - name: post
+    manager: IndependentPipelineManager
+    trigger:
+      gerrit:
+        - event: ref-updated
+          ref: ^(?!refs/).*$
+
+  - name: gate
+    manager: DependentPipelineManager
+    failure-message: Build failed.  For information on how to proceed, see http://wiki.example.org/Test_Failures
+    trigger:
+      gerrit:
+        - event: comment-added
+          approval:
+            - approved: 1
+    success:
+      gerrit:
+        verified: 2
+        submit: true
+    failure:
+      gerrit:
+        verified: -2
+    start:
+      gerrit:
+        verified: 0
+    precedence: high
+
+projects:
+  - name: org/new-project
+    check:
+      - project-merge:
+        - project-test1
+        - project-test2
+    gate:
+      - project-merge:
+        - project-test1
+        - project-test2
+    post:
+      - project-post
diff --git a/tests/fixtures/layout-disable-at.yaml b/tests/fixtures/layout-disable-at.yaml
index a2b2526..d894f41 100644
--- a/tests/fixtures/layout-disable-at.yaml
+++ b/tests/fixtures/layout-disable-at.yaml
@@ -1,21 +1,21 @@
-pipelines:
-  - name: check
-    manager: IndependentPipelineManager
-    trigger:
-      gerrit:
-        - event: patchset-created
-    success:
-      gerrit:
-        verified: 1
-    failure:
-      gerrit:
-        verified: -1
-    disabled:
-      smtp:
-        to: you@example.com
-    disable-after-consecutive-failures: 3
-
-projects:
-  - name: org/project
-    check:
-      - project-test1
+pipelines:
+  - name: check
+    manager: IndependentPipelineManager
+    trigger:
+      gerrit:
+        - event: patchset-created
+    success:
+      gerrit:
+        verified: 1
+    failure:
+      gerrit:
+        verified: -1
+    disabled:
+      smtp:
+        to: you@example.com
+    disable-after-consecutive-failures: 3
+
+projects:
+  - name: org/project
+    check:
+      - project-test1
diff --git a/tests/fixtures/layout-dont-ignore-deletes.yaml b/tests/fixtures/layout-dont-ignore-deletes.yaml
index 1cf3c71..f849006 100644
--- a/tests/fixtures/layout-dont-ignore-deletes.yaml
+++ b/tests/fixtures/layout-dont-ignore-deletes.yaml
@@ -1,16 +1,16 @@
-includes:
-  - python-file: custom_functions.py
-
-pipelines:
-  - name: post
-    manager: IndependentPipelineManager
-    trigger:
-      gerrit:
-        - event: ref-updated
-          ref: ^(?!refs/).*$
-          ignore-deletes: False
-
-projects:
-  - name: org/project
-    post:
-      - project-post
+includes:
+  - python-file: custom_functions.py
+
+pipelines:
+  - name: post
+    manager: IndependentPipelineManager
+    trigger:
+      gerrit:
+        - event: ref-updated
+          ref: ^(?!refs/).*$
+          ignore-deletes: False
+
+projects:
+  - name: org/project
+    post:
+      - project-post
diff --git a/tests/fixtures/layout-footer-message.yaml b/tests/fixtures/layout-footer-message.yaml
index 7977c19..aa54e6a 100644
--- a/tests/fixtures/layout-footer-message.yaml
+++ b/tests/fixtures/layout-footer-message.yaml
@@ -1,34 +1,34 @@
-includes:
-  - python-file: custom_functions.py
-
-pipelines:
-  - name: gate
-    manager: DependentPipelineManager
-    failure-message: Build failed.  For information on how to proceed, see http://wiki.example.org/Test_Failures
-    footer-message: For CI problems and help debugging, contact ci@example.org
-    trigger:
-      gerrit:
-        - event: comment-added
-          approval:
-            - approved: 1
-    success:
-      gerrit:
-        verified: 2
-        submit: true
-      smtp:
-        to: success@example.org
-    failure:
-      gerrit:
-        verified: -2
-      smtp:
-        to: failure@example.org
-    start:
-      gerrit:
-        verified: 0
-    precedence: high
-
-projects:
-  - name: org/project
-    gate:
-      - test1
-      - test2
+includes:
+  - python-file: custom_functions.py
+
+pipelines:
+  - name: gate
+    manager: DependentPipelineManager
+    failure-message: Build failed.  For information on how to proceed, see http://wiki.example.org/Test_Failures
+    footer-message: For CI problems and help debugging, contact ci@example.org
+    trigger:
+      gerrit:
+        - event: comment-added
+          approval:
+            - approved: 1
+    success:
+      gerrit:
+        verified: 2
+        submit: true
+      smtp:
+        to: success@example.org
+    failure:
+      gerrit:
+        verified: -2
+      smtp:
+        to: failure@example.org
+    start:
+      gerrit:
+        verified: 0
+    precedence: high
+
+projects:
+  - name: org/project
+    gate:
+      - test1
+      - test2
diff --git a/tests/fixtures/layout-idle.yaml b/tests/fixtures/layout-idle.yaml
index 0870788..b7408cf 100644
--- a/tests/fixtures/layout-idle.yaml
+++ b/tests/fixtures/layout-idle.yaml
@@ -1,12 +1,12 @@
-pipelines:
-  - name: periodic
-    manager: IndependentPipelineManager
-    trigger:
-      timer:
-        - time: '* * * * * */1'
-
-projects:
-  - name: org/project
-    periodic:
-      - project-bitrot-stable-old
-      - project-bitrot-stable-older
+pipelines:
+  - name: periodic
+    manager: IndependentPipelineManager
+    trigger:
+      timer:
+        - time: '* * * * * */1'
+
+projects:
+  - name: org/project
+    periodic:
+      - project-bitrot-stable-old
+      - project-bitrot-stable-older
diff --git a/tests/fixtures/layout-ignore-dependencies.yaml b/tests/fixtures/layout-ignore-dependencies.yaml
index 5c0257c..31c614e 100644
--- a/tests/fixtures/layout-ignore-dependencies.yaml
+++ b/tests/fixtures/layout-ignore-dependencies.yaml
@@ -1,28 +1,28 @@
-pipelines:
-  - name: check
-    manager: IndependentPipelineManager
-    ignore-dependencies: true
-    trigger:
-      gerrit:
-        - event: patchset-created
-    success:
-      gerrit:
-        verified: 1
-    failure:
-      gerrit:
-        verified: -1
-
-projects:
-  - name: org/project1
-    check:
-      - project1-merge:
-        - project1-test1
-        - project1-test2
-        - project1-project2-integration
-
-  - name: org/project2
-    check:
-      - project2-merge:
-        - project2-test1
-        - project2-test2
-        - project1-project2-integration
+pipelines:
+  - name: check
+    manager: IndependentPipelineManager
+    ignore-dependencies: true
+    trigger:
+      gerrit:
+        - event: patchset-created
+    success:
+      gerrit:
+        verified: 1
+    failure:
+      gerrit:
+        verified: -1
+
+projects:
+  - name: org/project1
+    check:
+      - project1-merge:
+        - project1-test1
+        - project1-test2
+        - project1-project2-integration
+
+  - name: org/project2
+    check:
+      - project2-merge:
+        - project2-test1
+        - project2-test2
+        - project1-project2-integration
diff --git a/tests/fixtures/layout-live-reconfiguration-add-job.yaml b/tests/fixtures/layout-live-reconfiguration-add-job.yaml
index e4aea6f..85fc384 100644
--- a/tests/fixtures/layout-live-reconfiguration-add-job.yaml
+++ b/tests/fixtures/layout-live-reconfiguration-add-job.yaml
@@ -1,38 +1,38 @@
-pipelines:
-  - name: gate
-    manager: DependentPipelineManager
-    failure-message: Build failed.  For information on how to proceed, see http://wiki.example.org/Test_Failures
-    trigger:
-      gerrit:
-        - event: comment-added
-          approval:
-            - approved: 1
-    success:
-      gerrit:
-        verified: 2
-        submit: true
-    failure:
-      gerrit:
-        verified: -2
-    start:
-      gerrit:
-        verified: 0
-    precedence: high
-
-jobs:
-  - name: ^.*-merge$
-    failure-message: Unable to merge change
-    hold-following-changes: true
-  - name: project-testfile
-    files:
-      - '.*-requires'
-
-projects:
-  - name: org/project
-    merge-mode: cherry-pick
-    gate:
-      - project-merge:
-        - project-test1
-        - project-test2
-        - project-test3
-        - project-testfile
+pipelines:
+  - name: gate
+    manager: DependentPipelineManager
+    failure-message: Build failed.  For information on how to proceed, see http://wiki.example.org/Test_Failures
+    trigger:
+      gerrit:
+        - event: comment-added
+          approval:
+            - approved: 1
+    success:
+      gerrit:
+        verified: 2
+        submit: true
+    failure:
+      gerrit:
+        verified: -2
+    start:
+      gerrit:
+        verified: 0
+    precedence: high
+
+jobs:
+  - name: ^.*-merge$
+    failure-message: Unable to merge change
+    hold-following-changes: true
+  - name: project-testfile
+    files:
+      - '.*-requires'
+
+projects:
+  - name: org/project
+    merge-mode: cherry-pick
+    gate:
+      - project-merge:
+        - project-test1
+        - project-test2
+        - project-test3
+        - project-testfile
diff --git a/tests/fixtures/layout-live-reconfiguration-del-project.yaml b/tests/fixtures/layout-live-reconfiguration-del-project.yaml
index 07ffb2e..b6d163a 100644
--- a/tests/fixtures/layout-live-reconfiguration-del-project.yaml
+++ b/tests/fixtures/layout-live-reconfiguration-del-project.yaml
@@ -1,21 +1,21 @@
-pipelines:
-  - name: check
-    manager: IndependentPipelineManager
-    trigger:
-      gerrit:
-        - event: patchset-created
-    success:
-      gerrit:
-        verified: 1
-    failure:
-      gerrit:
-        verified: -1
-
-projects:
-  - name: org/project
-    merge-mode: cherry-pick
-    check:
-      - project-merge:
-        - project-test1
-        - project-test2
-        - project-testfile
+pipelines:
+  - name: check
+    manager: IndependentPipelineManager
+    trigger:
+      gerrit:
+        - event: patchset-created
+    success:
+      gerrit:
+        verified: 1
+    failure:
+      gerrit:
+        verified: -1
+
+projects:
+  - name: org/project
+    merge-mode: cherry-pick
+    check:
+      - project-merge:
+        - project-test1
+        - project-test2
+        - project-testfile
diff --git a/tests/fixtures/layout-live-reconfiguration-failed-job.yaml b/tests/fixtures/layout-live-reconfiguration-failed-job.yaml
index e811af1..efe1178 100644
--- a/tests/fixtures/layout-live-reconfiguration-failed-job.yaml
+++ b/tests/fixtures/layout-live-reconfiguration-failed-job.yaml
@@ -1,25 +1,25 @@
-pipelines:
-  - name: check
-    manager: IndependentPipelineManager
-    trigger:
-      gerrit:
-        - event: patchset-created
-    success:
-      gerrit:
-        verified: 1
-    failure:
-      gerrit:
-        verified: -1
-
-jobs:
-  - name: ^.*-merge$
-    failure-message: Unable to merge change
-    hold-following-changes: true
-
-projects:
-  - name: org/project
-    merge-mode: cherry-pick
-    check:
-      - project-merge:
-        - project-test2
-        - project-testfile
+pipelines:
+  - name: check
+    manager: IndependentPipelineManager
+    trigger:
+      gerrit:
+        - event: patchset-created
+    success:
+      gerrit:
+        verified: 1
+    failure:
+      gerrit:
+        verified: -1
+
+jobs:
+  - name: ^.*-merge$
+    failure-message: Unable to merge change
+    hold-following-changes: true
+
+projects:
+  - name: org/project
+    merge-mode: cherry-pick
+    check:
+      - project-merge:
+        - project-test2
+        - project-testfile
diff --git a/tests/fixtures/layout-live-reconfiguration-functions.yaml b/tests/fixtures/layout-live-reconfiguration-functions.yaml
index e261a88..c0b1443 100644
--- a/tests/fixtures/layout-live-reconfiguration-functions.yaml
+++ b/tests/fixtures/layout-live-reconfiguration-functions.yaml
@@ -1,37 +1,37 @@
-includes:
-  - python-file: custom_functions_live_reconfiguration_functions.py
-
-pipelines:
-  - name: gate
-    manager: DependentPipelineManager
-    failure-message: Build failed.  For information on how to proceed, see http://wiki.example.org/Test_Failures
-    trigger:
-      gerrit:
-        - event: comment-added
-          approval:
-            - approved: 1
-    success:
-      gerrit:
-        verified: 2
-        submit: true
-    failure:
-      gerrit:
-        verified: -2
-    start:
-      gerrit:
-        verified: 0
-    precedence: high
-
-jobs:
-  - name: ^.*-merge$
-    failure-message: Unable to merge change
-    hold-following-changes: true
-  - name: node-project-test1
-    parameter-function: select_debian_node
-
-projects:
-  - name: org/node-project
-    gate:
-      - node-project-merge:
-        - node-project-test1
-        - node-project-test2
+includes:
+  - python-file: custom_functions_live_reconfiguration_functions.py
+
+pipelines:
+  - name: gate
+    manager: DependentPipelineManager
+    failure-message: Build failed.  For information on how to proceed, see http://wiki.example.org/Test_Failures
+    trigger:
+      gerrit:
+        - event: comment-added
+          approval:
+            - approved: 1
+    success:
+      gerrit:
+        verified: 2
+        submit: true
+    failure:
+      gerrit:
+        verified: -2
+    start:
+      gerrit:
+        verified: 0
+    precedence: high
+
+jobs:
+  - name: ^.*-merge$
+    failure-message: Unable to merge change
+    hold-following-changes: true
+  - name: node-project-test1
+    parameter-function: select_debian_node
+
+projects:
+  - name: org/node-project
+    gate:
+      - node-project-merge:
+        - node-project-test1
+        - node-project-test2
diff --git a/tests/fixtures/layout-live-reconfiguration-shared-queue.yaml b/tests/fixtures/layout-live-reconfiguration-shared-queue.yaml
index ad3f666..aa35bff 100644
--- a/tests/fixtures/layout-live-reconfiguration-shared-queue.yaml
+++ b/tests/fixtures/layout-live-reconfiguration-shared-queue.yaml
@@ -1,62 +1,62 @@
-pipelines:
-  - name: check
-    manager: IndependentPipelineManager
-    trigger:
-      gerrit:
-        - event: patchset-created
-    success:
-      gerrit:
-        verified: 1
-    failure:
-      gerrit:
-        verified: -1
-
-  - name: gate
-    manager: DependentPipelineManager
-    failure-message: Build failed.  For information on how to proceed, see http://wiki.example.org/Test_Failures
-    trigger:
-      gerrit:
-        - event: comment-added
-          approval:
-            - approved: 1
-    success:
-      gerrit:
-        verified: 2
-        submit: true
-    failure:
-      gerrit:
-        verified: -2
-    start:
-      gerrit:
-        verified: 0
-    precedence: high
-
-jobs:
-  - name: ^.*-merge$
-    failure-message: Unable to merge change
-    hold-following-changes: true
-  - name: project1-project2-integration
-    queue-name: integration
-
-projects:
-  - name: org/project1
-    check:
-      - project1-merge:
-        - project1-test1
-        - project1-test2
-    gate:
-      - project1-merge:
-        - project1-test1
-        - project1-test2
-
-  - name: org/project2
-    check:
-      - project2-merge:
-        - project2-test1
-        - project2-test2
-        - project1-project2-integration
-    gate:
-      - project2-merge:
-        - project2-test1
-        - project2-test2
-        - project1-project2-integration
+pipelines:
+  - name: check
+    manager: IndependentPipelineManager
+    trigger:
+      gerrit:
+        - event: patchset-created
+    success:
+      gerrit:
+        verified: 1
+    failure:
+      gerrit:
+        verified: -1
+
+  - name: gate
+    manager: DependentPipelineManager
+    failure-message: Build failed.  For information on how to proceed, see http://wiki.example.org/Test_Failures
+    trigger:
+      gerrit:
+        - event: comment-added
+          approval:
+            - approved: 1
+    success:
+      gerrit:
+        verified: 2
+        submit: true
+    failure:
+      gerrit:
+        verified: -2
+    start:
+      gerrit:
+        verified: 0
+    precedence: high
+
+jobs:
+  - name: ^.*-merge$
+    failure-message: Unable to merge change
+    hold-following-changes: true
+  - name: project1-project2-integration
+    queue-name: integration
+
+projects:
+  - name: org/project1
+    check:
+      - project1-merge:
+        - project1-test1
+        - project1-test2
+    gate:
+      - project1-merge:
+        - project1-test1
+        - project1-test2
+
+  - name: org/project2
+    check:
+      - project2-merge:
+        - project2-test1
+        - project2-test2
+        - project1-project2-integration
+    gate:
+      - project2-merge:
+        - project2-test1
+        - project2-test2
+        - project1-project2-integration
diff --git a/tests/fixtures/layout-merge-failure.yaml b/tests/fixtures/layout-merge-failure.yaml
index 72bc9c9..92d156d 100644
--- a/tests/fixtures/layout-merge-failure.yaml
+++ b/tests/fixtures/layout-merge-failure.yaml
@@ -1,56 +1,56 @@
-pipelines:
-  - name: check
-    manager: IndependentPipelineManager
-    trigger:
-      gerrit:
-        - event: patchset-created
-    success:
-      gerrit:
-        verified: 1
-    failure:
-      gerrit:
-        verified: -1
-
-  - name: post
-    manager: IndependentPipelineManager
-    trigger:
-      gerrit:
-        - event: ref-updated
-          ref: ^(?!refs/).*$
-
-  - name: gate
-    manager: DependentPipelineManager
-    failure-message: Build failed.  For information on how to proceed, see http://wiki.example.org/Test_Failures
-    merge-failure-message: "The merge failed! For more information..."
-    trigger:
-      gerrit:
-        - event: comment-added
-          approval:
-            - approved: 1
-    success:
-      gerrit:
-        verified: 2
-        submit: true
-    failure:
-      gerrit:
-        verified: -2
-    merge-failure:
-      gerrit:
-        verified: -1
-      smtp:
-        to: you@example.com
-    start:
-      gerrit:
-        verified: 0
-    precedence: high
-
-projects:
-  - name: org/project
-    check:
-      - project-merge:
-        - project-test1
-        - project-test2
-    gate:
-      - project-merge:
-        - project-test1
-        - project-test2
+pipelines:
+  - name: check
+    manager: IndependentPipelineManager
+    trigger:
+      gerrit:
+        - event: patchset-created
+    success:
+      gerrit:
+        verified: 1
+    failure:
+      gerrit:
+        verified: -1
+
+  - name: post
+    manager: IndependentPipelineManager
+    trigger:
+      gerrit:
+        - event: ref-updated
+          ref: ^(?!refs/).*$
+
+  - name: gate
+    manager: DependentPipelineManager
+    failure-message: Build failed.  For information on how to proceed, see http://wiki.example.org/Test_Failures
+    merge-failure-message: "The merge failed! For more information..."
+    trigger:
+      gerrit:
+        - event: comment-added
+          approval:
+            - approved: 1
+    success:
+      gerrit:
+        verified: 2
+        submit: true
+    failure:
+      gerrit:
+        verified: -2
+    merge-failure:
+      gerrit:
+        verified: -1
+      smtp:
+        to: you@example.com
+    start:
+      gerrit:
+        verified: 0
+    precedence: high
+
+projects:
+  - name: org/project
+    check:
+      - project-merge:
+        - project-test1
+        - project-test2
+    gate:
+      - project-merge:
+        - project-test1
+        - project-test2
diff --git a/tests/fixtures/layout-merge-queues.yaml b/tests/fixtures/layout-merge-queues.yaml
index be39a1c..327d40f 100644
--- a/tests/fixtures/layout-merge-queues.yaml
+++ b/tests/fixtures/layout-merge-queues.yaml
@@ -1,25 +1,25 @@
-pipelines:
-  - name: gate
-    manager: DependentPipelineManager
-    precedence: low
-    trigger:
-      gerrit:
-        - event: comment-added
-          approval:
-            - approved: 1
-
-projects:
-  - name: projectA
-    gate:
-      - test-only-a
-      - common-test1
-
-  - name: projectB
-    gate:
-      - test-only-b
-      - common-test2
-
-  - name: projectC
-    gate:
-      - common-test1
-      - common-test2
+pipelines:
+  - name: gate
+    manager: DependentPipelineManager
+    precedence: low
+    trigger:
+      gerrit:
+        - event: comment-added
+          approval:
+            - approved: 1
+
+projects:
+  - name: projectA
+    gate:
+      - test-only-a
+      - common-test1
+
+  - name: projectB
+    gate:
+      - test-only-b
+      - common-test2
+
+  - name: projectC
+    gate:
+      - common-test1
+      - common-test2
diff --git a/tests/fixtures/layout-mutex.yaml b/tests/fixtures/layout-mutex.yaml
index fcd0529..1bd5b66 100644
--- a/tests/fixtures/layout-mutex.yaml
+++ b/tests/fixtures/layout-mutex.yaml
@@ -1,25 +1,25 @@
-pipelines:
-  - name: check
-    manager: IndependentPipelineManager
-    trigger:
-      gerrit:
-        - event: patchset-created
-    success:
-      gerrit:
-        verified: 1
-    failure:
-      gerrit:
-        verified: -1
-
-jobs:
-  - name: mutex-one
-    mutex: test-mutex
-  - name: mutex-two
-    mutex: test-mutex
-
-projects:
-  - name: org/project
-    check:
-      - project-test1
-      - mutex-one
-      - mutex-two
+pipelines:
+  - name: check
+    manager: IndependentPipelineManager
+    trigger:
+      gerrit:
+        - event: patchset-created
+    success:
+      gerrit:
+        verified: 1
+    failure:
+      gerrit:
+        verified: -1
+
+jobs:
+  - name: mutex-one
+    mutex: test-mutex
+  - name: mutex-two
+    mutex: test-mutex
+
+projects:
+  - name: org/project
+    check:
+      - project-test1
+      - mutex-one
+      - mutex-two
diff --git a/tests/fixtures/layout-no-jobs.yaml b/tests/fixtures/layout-no-jobs.yaml
index e860ad5..d2334be 100644
--- a/tests/fixtures/layout-no-jobs.yaml
+++ b/tests/fixtures/layout-no-jobs.yaml
@@ -1,43 +1,43 @@
-includes:
-  - python-file: custom_functions.py
-
-pipelines:
-  - name: check
-    manager: IndependentPipelineManager
-    trigger:
-      gerrit:
-        - event: patchset-created
-    success:
-      gerrit:
-        verified: 1
-    failure:
-      gerrit:
-        verified: -1
-
-  - name: gate
-    manager: DependentPipelineManager
-    failure-message: Build failed.  For information on how to proceed, see http://wiki.example.org/Test_Failures
-    trigger:
-      gerrit:
-        - event: comment-added
-          approval:
-            - approved: 1
-    success:
-      gerrit:
-        verified: 2
-        submit: true
-    failure:
-      gerrit:
-        verified: -2
-    start:
-      gerrit:
-        verified: 0
-    precedence: high
-
-projects:
-  - name: org/project
-    merge-mode: cherry-pick
-    check:
-      - gate-noop
-    gate:
-      - gate-noop
+includes:
+  - python-file: custom_functions.py
+
+pipelines:
+  - name: check
+    manager: IndependentPipelineManager
+    trigger:
+      gerrit:
+        - event: patchset-created
+    success:
+      gerrit:
+        verified: 1
+    failure:
+      gerrit:
+        verified: -1
+
+  - name: gate
+    manager: DependentPipelineManager
+    failure-message: Build failed.  For information on how to proceed, see http://wiki.example.org/Test_Failures
+    trigger:
+      gerrit:
+        - event: comment-added
+          approval:
+            - approved: 1
+    success:
+      gerrit:
+        verified: 2
+        submit: true
+    failure:
+      gerrit:
+        verified: -2
+    start:
+      gerrit:
+        verified: 0
+    precedence: high
+
+projects:
+  - name: org/project
+    merge-mode: cherry-pick
+    check:
+      - gate-noop
+    gate:
+      - gate-noop
diff --git a/tests/fixtures/layout-no-timer.yaml b/tests/fixtures/layout-no-timer.yaml
index ca40d13..44e1d75 100644
--- a/tests/fixtures/layout-no-timer.yaml
+++ b/tests/fixtures/layout-no-timer.yaml
@@ -1,28 +1,28 @@
-pipelines:
-  - name: check
-    manager: IndependentPipelineManager
-    trigger:
-      gerrit:
-        - event: patchset-created
-    success:
-      gerrit:
-        verified: 1
-    failure:
-      gerrit:
-        verified: -1
-
-  - name: periodic
-    manager: IndependentPipelineManager
-    # Trigger is required, set it to one that is a noop
-    # during tests that check the timer trigger.
-    trigger:
-      gerrit:
-        - event: ref-updated
-
-projects:
-  - name: org/project
-    check:
-      - project-test1
-    periodic:
-      - project-bitrot-stable-old
-      - project-bitrot-stable-older
+pipelines:
+  - name: check
+    manager: IndependentPipelineManager
+    trigger:
+      gerrit:
+        - event: patchset-created
+    success:
+      gerrit:
+        verified: 1
+    failure:
+      gerrit:
+        verified: -1
+
+  - name: periodic
+    manager: IndependentPipelineManager
+    # Trigger is required, set it to one that is a noop
+    # during tests that check the timer trigger.
+    trigger:
+      gerrit:
+        - event: ref-updated
+
+projects:
+  - name: org/project
+    check:
+      - project-test1
+    periodic:
+      - project-bitrot-stable-old
+      - project-bitrot-stable-older
diff --git a/tests/fixtures/layout-rate-limit.yaml b/tests/fixtures/layout-rate-limit.yaml
index 9f6748c..65f12ca 100644
--- a/tests/fixtures/layout-rate-limit.yaml
+++ b/tests/fixtures/layout-rate-limit.yaml
@@ -1,32 +1,32 @@
-pipelines:
-  - name: gate
-    manager: DependentPipelineManager
-    failure-message: Build failed.  For information on how to proceed, see http://wiki.example.org/Test_Failures
-    trigger:
-      gerrit:
-        - event: comment-added
-          approval:
-            - approved: 1
-    start:
-      gerrit:
-        verified: 0
-    success:
-      gerrit:
-        verified: 2
-        submit: true
-    failure:
-      gerrit:
-        verified: -2
-    window: 2
-    window-floor: 1
-    window-increase-type: linear
-    window-increase-factor: 1
-    window-decrease-type: exponential
-    window-decrease-factor: 2
-
-projects:
-  - name: org/project
-    gate:
-      - project-merge:
-        - project-test1
-        - project-test2
+pipelines:
+  - name: gate
+    manager: DependentPipelineManager
+    failure-message: Build failed.  For information on how to proceed, see http://wiki.example.org/Test_Failures
+    trigger:
+      gerrit:
+        - event: comment-added
+          approval:
+            - approved: 1
+    start:
+      gerrit:
+        verified: 0
+    success:
+      gerrit:
+        verified: 2
+        submit: true
+    failure:
+      gerrit:
+        verified: -2
+    window: 2
+    window-floor: 1
+    window-increase-type: linear
+    window-increase-factor: 1
+    window-decrease-type: exponential
+    window-decrease-factor: 2
+
+projects:
+  - name: org/project
+    gate:
+      - project-merge:
+        - project-test1
+        - project-test2
diff --git a/tests/fixtures/layout-repo-deleted.yaml b/tests/fixtures/layout-repo-deleted.yaml
index 967009a..7d22288 100644
--- a/tests/fixtures/layout-repo-deleted.yaml
+++ b/tests/fixtures/layout-repo-deleted.yaml
@@ -1,52 +1,52 @@
-pipelines:
-  - name: check
-    manager: IndependentPipelineManager
-    trigger:
-      gerrit:
-        - event: patchset-created
-    success:
-      gerrit:
-        verified: 1
-    failure:
-      gerrit:
-        verified: -1
-
-  - name: post
-    manager: IndependentPipelineManager
-    trigger:
-      gerrit:
-        - event: ref-updated
-          ref: ^(?!refs/).*$
-
-  - name: gate
-    manager: DependentPipelineManager
-    failure-message: Build failed.  For information on how to proceed, see http://wiki.example.org/Test_Failures
-    trigger:
-      gerrit:
-        - event: comment-added
-          approval:
-            - approved: 1
-    success:
-      gerrit:
-        verified: 2
-        submit: true
-    failure:
-      gerrit:
-        verified: -2
-    start:
-      gerrit:
-        verified: 0
-    precedence: high
-
-projects:
-  - name: org/delete-project
-    check:
-      - project-merge:
-        - project-test1
-        - project-test2
-    gate:
-      - project-merge:
-        - project-test1
-        - project-test2
-    post:
-      - project-post
+pipelines:
+  - name: check
+    manager: IndependentPipelineManager
+    trigger:
+      gerrit:
+        - event: patchset-created
+    success:
+      gerrit:
+        verified: 1
+    failure:
+      gerrit:
+        verified: -1
+
+  - name: post
+    manager: IndependentPipelineManager
+    trigger:
+      gerrit:
+        - event: ref-updated
+          ref: ^(?!refs/).*$
+
+  - name: gate
+    manager: DependentPipelineManager
+    failure-message: Build failed.  For information on how to proceed, see http://wiki.example.org/Test_Failures
+    trigger:
+      gerrit:
+        - event: comment-added
+          approval:
+            - approved: 1
+    success:
+      gerrit:
+        verified: 2
+        submit: true
+    failure:
+      gerrit:
+        verified: -2
+    start:
+      gerrit:
+        verified: 0
+    precedence: high
+
+projects:
+  - name: org/delete-project
+    check:
+      - project-merge:
+        - project-test1
+        - project-test2
+    gate:
+      - project-merge:
+        - project-test1
+        - project-test2
+    post:
+      - project-post
diff --git a/tests/fixtures/layout-requirement-current-patchset.yaml b/tests/fixtures/layout-requirement-current-patchset.yaml
index 405077e..d4add47 100644
--- a/tests/fixtures/layout-requirement-current-patchset.yaml
+++ b/tests/fixtures/layout-requirement-current-patchset.yaml
@@ -1,20 +1,20 @@
-pipelines:
-  - name: check
-    manager: IndependentPipelineManager
-    require:
-      current-patchset: True
-    trigger:
-      gerrit:
-        - event: patchset-created
-        - event: comment-added
-    success:
-      gerrit:
-        verified: 1
-    failure:
-      gerrit:
-        verified: -1
-
-projects:
-  - name: org/project
-    check:
-      - project-check
+pipelines:
+  - name: check
+    manager: IndependentPipelineManager
+    require:
+      current-patchset: True
+    trigger:
+      gerrit:
+        - event: patchset-created
+        - event: comment-added
+    success:
+      gerrit:
+        verified: 1
+    failure:
+      gerrit:
+        verified: -1
+
+projects:
+  - name: org/project
+    check:
+      - project-check
diff --git a/tests/fixtures/layout-requirement-email.yaml b/tests/fixtures/layout-requirement-email.yaml
index 4bfb733..23901b3 100644
--- a/tests/fixtures/layout-requirement-email.yaml
+++ b/tests/fixtures/layout-requirement-email.yaml
@@ -1,37 +1,37 @@
-pipelines:
-  - name: pipeline
-    manager: IndependentPipelineManager
-    require:
-      approval:
-        - email: jenkins@example.com
-    trigger:
-      gerrit:
-        - event: comment-added
-    success:
-      gerrit:
-        verified: 1
-    failure:
-      gerrit:
-        verified: -1
-
-  - name: trigger
-    manager: IndependentPipelineManager
-    trigger:
-      gerrit:
-        - event: comment-added
-          require-approval:
-            - email: jenkins@example.com
-    success:
-      gerrit:
-        verified: 1
-    failure:
-      gerrit:
-        verified: -1
-
-projects:
-  - name: org/project1
-    pipeline:
-      - project1-pipeline
-  - name: org/project2
-    trigger:
-      - project2-trigger
+pipelines:
+  - name: pipeline
+    manager: IndependentPipelineManager
+    require:
+      approval:
+        - email: jenkins@example.com
+    trigger:
+      gerrit:
+        - event: comment-added
+    success:
+      gerrit:
+        verified: 1
+    failure:
+      gerrit:
+        verified: -1
+
+  - name: trigger
+    manager: IndependentPipelineManager
+    trigger:
+      gerrit:
+        - event: comment-added
+          require-approval:
+            - email: jenkins@example.com
+    success:
+      gerrit:
+        verified: 1
+    failure:
+      gerrit:
+        verified: -1
+
+projects:
+  - name: org/project1
+    pipeline:
+      - project1-pipeline
+  - name: org/project2
+    trigger:
+      - project2-trigger
diff --git a/tests/fixtures/layout-requirement-newer-than.yaml b/tests/fixtures/layout-requirement-newer-than.yaml
index b6beb35..406e4fb 100644
--- a/tests/fixtures/layout-requirement-newer-than.yaml
+++ b/tests/fixtures/layout-requirement-newer-than.yaml
@@ -1,39 +1,39 @@
-pipelines:
-  - name: pipeline
-    manager: IndependentPipelineManager
-    require:
-      approval:
-        - username: jenkins
-          newer-than: 48h
-    trigger:
-      gerrit:
-        - event: comment-added
-    success:
-      gerrit:
-        verified: 1
-    failure:
-      gerrit:
-        verified: -1
-
-  - name: trigger
-    manager: IndependentPipelineManager
-    trigger:
-      gerrit:
-        - event: comment-added
-          require-approval:
-            - username: jenkins
-              newer-than: 48h
-    success:
-      gerrit:
-        verified: 1
-    failure:
-      gerrit:
-        verified: -1
-
-projects:
-  - name: org/project1
-    pipeline:
-      - project1-pipeline
-  - name: org/project2
-    trigger:
-      - project2-trigger
+pipelines:
+  - name: pipeline
+    manager: IndependentPipelineManager
+    require:
+      approval:
+        - username: jenkins
+          newer-than: 48h
+    trigger:
+      gerrit:
+        - event: comment-added
+    success:
+      gerrit:
+        verified: 1
+    failure:
+      gerrit:
+        verified: -1
+
+  - name: trigger
+    manager: IndependentPipelineManager
+    trigger:
+      gerrit:
+        - event: comment-added
+          require-approval:
+            - username: jenkins
+              newer-than: 48h
+    success:
+      gerrit:
+        verified: 1
+    failure:
+      gerrit:
+        verified: -1
+
+projects:
+  - name: org/project1
+    pipeline:
+      - project1-pipeline
+  - name: org/project2
+    trigger:
+      - project2-trigger
diff --git a/tests/fixtures/layout-requirement-older-than.yaml b/tests/fixtures/layout-requirement-older-than.yaml
index 2edf9df..385e528 100644
--- a/tests/fixtures/layout-requirement-older-than.yaml
+++ b/tests/fixtures/layout-requirement-older-than.yaml
@@ -1,39 +1,39 @@
-pipelines:
-  - name: pipeline
-    manager: IndependentPipelineManager
-    require:
-      approval:
-        - username: jenkins
-          older-than: 48h
-    trigger:
-      gerrit:
-        - event: comment-added
-    success:
-      gerrit:
-        verified: 1
-    failure:
-      gerrit:
-        verified: -1
-
-  - name: trigger
-    manager: IndependentPipelineManager
-    trigger:
-      gerrit:
-        - event: comment-added
-          require-approval:
-            - username: jenkins
-              older-than: 48h
-    success:
-      gerrit:
-        verified: 1
-    failure:
-      gerrit:
-        verified: -1
-
-projects:
-  - name: org/project1
-    pipeline:
-      - project1-pipeline
-  - name: org/project2
-    trigger:
-      - project2-trigger
+pipelines:
+  - name: pipeline
+    manager: IndependentPipelineManager
+    require:
+      approval:
+        - username: jenkins
+          older-than: 48h
+    trigger:
+      gerrit:
+        - event: comment-added
+    success:
+      gerrit:
+        verified: 1
+    failure:
+      gerrit:
+        verified: -1
+
+  - name: trigger
+    manager: IndependentPipelineManager
+    trigger:
+      gerrit:
+        - event: comment-added
+          require-approval:
+            - username: jenkins
+              older-than: 48h
+    success:
+      gerrit:
+        verified: 1
+    failure:
+      gerrit:
+        verified: -1
+
+projects:
+  - name: org/project1
+    pipeline:
+      - project1-pipeline
+  - name: org/project2
+    trigger:
+      - project2-trigger
diff --git a/tests/fixtures/layout-requirement-open.yaml b/tests/fixtures/layout-requirement-open.yaml
index e62719d..17ed37d 100644
--- a/tests/fixtures/layout-requirement-open.yaml
+++ b/tests/fixtures/layout-requirement-open.yaml
@@ -1,20 +1,20 @@
-pipelines:
-  - name: check
-    manager: IndependentPipelineManager
-    require:
-      open: True
-    trigger:
-      gerrit:
-        - event: patchset-created
-        - event: comment-added
-    success:
-      gerrit:
-        verified: 1
-    failure:
-      gerrit:
-        verified: -1
-
-projects:
-  - name: org/project
-    check:
-      - project-check
+pipelines:
+  - name: check
+    manager: IndependentPipelineManager
+    require:
+      open: True
+    trigger:
+      gerrit:
+        - event: patchset-created
+        - event: comment-added
+    success:
+      gerrit:
+        verified: 1
+    failure:
+      gerrit:
+        verified: -1
+
+projects:
+  - name: org/project
+    check:
+      - project-check
diff --git a/tests/fixtures/layout-requirement-reject-username.yaml b/tests/fixtures/layout-requirement-reject-username.yaml
index 9c71045..54ee4b7 100644
--- a/tests/fixtures/layout-requirement-reject-username.yaml
+++ b/tests/fixtures/layout-requirement-reject-username.yaml
@@ -1,37 +1,37 @@
-pipelines:
-  - name: pipeline
-    manager: IndependentPipelineManager
-    reject:
-      approval:
-        - username: 'jenkins'
-    trigger:
-      gerrit:
-        - event: comment-added
-    success:
-      gerrit:
-        verified: 1
-    failure:
-      gerrit:
-        verified: -1
-
-  - name: trigger
-    manager: IndependentPipelineManager
-    trigger:
-      gerrit:
-        - event: comment-added
-          reject-approval:
-            - username: 'jenkins'
-    success:
-      gerrit:
-        verified: 1
-    failure:
-      gerrit:
-        verified: -1
-
-projects:
-  - name: org/project1
-    pipeline:
-      - project1-pipeline
-  - name: org/project2
-    trigger:
+pipelines:
+  - name: pipeline
+    manager: IndependentPipelineManager
+    reject:
+      approval:
+        - username: 'jenkins'
+    trigger:
+      gerrit:
+        - event: comment-added
+    success:
+      gerrit:
+        verified: 1
+    failure:
+      gerrit:
+        verified: -1
+
+  - name: trigger
+    manager: IndependentPipelineManager
+    trigger:
+      gerrit:
+        - event: comment-added
+          reject-approval:
+            - username: 'jenkins'
+    success:
+      gerrit:
+        verified: 1
+    failure:
+      gerrit:
+        verified: -1
+
+projects:
+  - name: org/project1
+    pipeline:
+      - project1-pipeline
+  - name: org/project2
+    trigger:
       - project2-trigger
\ No newline at end of file
diff --git a/tests/fixtures/layout-requirement-reject.yaml b/tests/fixtures/layout-requirement-reject.yaml
index 1f5d714..15611f9 100644
--- a/tests/fixtures/layout-requirement-reject.yaml
+++ b/tests/fixtures/layout-requirement-reject.yaml
@@ -1,44 +1,44 @@
-pipelines:
-  - name: pipeline
-    manager: IndependentPipelineManager
-    require:
-      approval:
-        - username: jenkins
-          verified: [1, 2]
-    reject:
-      approval:
-        - verified: [-1, -2]
-    trigger:
-      gerrit:
-        - event: comment-added
-    success:
-      gerrit:
-        verified: 1
-    failure:
-      gerrit:
-        verified: -1
-
-  - name: trigger
-    manager: IndependentPipelineManager
-    trigger:
-      gerrit:
-        - event: comment-added
-          require-approval:
-            - username: jenkins
-              verified: [1, 2]
-          reject-approval:
-            - verified: [-1, -2]
-    success:
-      gerrit:
-        verified: 1
-    failure:
-      gerrit:
-        verified: -1
-
-projects:
-  - name: org/project1
-    pipeline:
-      - project1-pipeline
-  - name: org/project2
-    trigger:
-      - project2-trigger
+pipelines:
+  - name: pipeline
+    manager: IndependentPipelineManager
+    require:
+      approval:
+        - username: jenkins
+          verified: [1, 2]
+    reject:
+      approval:
+        - verified: [-1, -2]
+    trigger:
+      gerrit:
+        - event: comment-added
+    success:
+      gerrit:
+        verified: 1
+    failure:
+      gerrit:
+        verified: -1
+
+  - name: trigger
+    manager: IndependentPipelineManager
+    trigger:
+      gerrit:
+        - event: comment-added
+          require-approval:
+            - username: jenkins
+              verified: [1, 2]
+          reject-approval:
+            - verified: [-1, -2]
+    success:
+      gerrit:
+        verified: 1
+    failure:
+      gerrit:
+        verified: -1
+
+projects:
+  - name: org/project1
+    pipeline:
+      - project1-pipeline
+  - name: org/project2
+    trigger:
+      - project2-trigger
diff --git a/tests/fixtures/layout-requirement-status.yaml b/tests/fixtures/layout-requirement-status.yaml
index af33468..3a34891 100644
--- a/tests/fixtures/layout-requirement-status.yaml
+++ b/tests/fixtures/layout-requirement-status.yaml
@@ -1,20 +1,20 @@
-pipelines:
-  - name: check
-    manager: IndependentPipelineManager
-    require:
-      status: NEW
-    trigger:
-      gerrit:
-        - event: patchset-created
-        - event: comment-added
-    success:
-      gerrit:
-        verified: 1
-    failure:
-      gerrit:
-        verified: -1
-
-projects:
-  - name: org/project
-    check:
-      - project-check
+pipelines:
+  - name: check
+    manager: IndependentPipelineManager
+    require:
+      status: NEW
+    trigger:
+      gerrit:
+        - event: patchset-created
+        - event: comment-added
+    success:
+      gerrit:
+        verified: 1
+    failure:
+      gerrit:
+        verified: -1
+
+projects:
+  - name: org/project
+    check:
+      - project-check
diff --git a/tests/fixtures/layout-requirement-username.yaml b/tests/fixtures/layout-requirement-username.yaml
index 7a549f0..6d10a1e 100644
--- a/tests/fixtures/layout-requirement-username.yaml
+++ b/tests/fixtures/layout-requirement-username.yaml
@@ -1,37 +1,37 @@
-pipelines:
-  - name: pipeline
-    manager: IndependentPipelineManager
-    require:
-      approval:
-        - username: jenkins
-    trigger:
-      gerrit:
-        - event: comment-added
-    success:
-      gerrit:
-        verified: 1
-    failure:
-      gerrit:
-        verified: -1
-
-  - name: trigger
-    manager: IndependentPipelineManager
-    trigger:
-      gerrit:
-        - event: comment-added
-          require-approval:
-            - username: jenkins
-    success:
-      gerrit:
-        verified: 1
-    failure:
-      gerrit:
-        verified: -1
-
-projects:
-  - name: org/project1
-    pipeline:
-      - project1-pipeline
-  - name: org/project2
-    trigger:
-      - project2-trigger
+pipelines:
+  - name: pipeline
+    manager: IndependentPipelineManager
+    require:
+      approval:
+        - username: jenkins
+    trigger:
+      gerrit:
+        - event: comment-added
+    success:
+      gerrit:
+        verified: 1
+    failure:
+      gerrit:
+        verified: -1
+
+  - name: trigger
+    manager: IndependentPipelineManager
+    trigger:
+      gerrit:
+        - event: comment-added
+          require-approval:
+            - username: jenkins
+    success:
+      gerrit:
+        verified: 1
+    failure:
+      gerrit:
+        verified: -1
+
+projects:
+  - name: org/project1
+    pipeline:
+      - project1-pipeline
+  - name: org/project2
+    trigger:
+      - project2-trigger
diff --git a/tests/fixtures/layout-requirement-vote.yaml b/tests/fixtures/layout-requirement-vote.yaml
index 7ccadff..2cde037 100644
--- a/tests/fixtures/layout-requirement-vote.yaml
+++ b/tests/fixtures/layout-requirement-vote.yaml
@@ -1,39 +1,39 @@
-pipelines:
-  - name: pipeline
-    manager: IndependentPipelineManager
-    require:
-      approval:
-        - username: jenkins
-          verified: 1
-    trigger:
-      gerrit:
-        - event: comment-added
-    success:
-      gerrit:
-        verified: 1
-    failure:
-      gerrit:
-        verified: -1
-
-  - name: trigger
-    manager: IndependentPipelineManager
-    trigger:
-      gerrit:
-        - event: comment-added
-          require-approval:
-            - username: jenkins
-              verified: 1
-    success:
-      gerrit:
-        verified: 1
-    failure:
-      gerrit:
-        verified: -1
-
-projects:
-  - name: org/project1
-    pipeline:
-      - project1-pipeline
-  - name: org/project2
-    trigger:
-      - project2-trigger
+pipelines:
+  - name: pipeline
+    manager: IndependentPipelineManager
+    require:
+      approval:
+        - username: jenkins
+          verified: 1
+    trigger:
+      gerrit:
+        - event: comment-added
+    success:
+      gerrit:
+        verified: 1
+    failure:
+      gerrit:
+        verified: -1
+
+  - name: trigger
+    manager: IndependentPipelineManager
+    trigger:
+      gerrit:
+        - event: comment-added
+          require-approval:
+            - username: jenkins
+              verified: 1
+    success:
+      gerrit:
+        verified: 1
+    failure:
+      gerrit:
+        verified: -1
+
+projects:
+  - name: org/project1
+    pipeline:
+      - project1-pipeline
+  - name: org/project2
+    trigger:
+      - project2-trigger
diff --git a/tests/fixtures/layout-requirement-vote1.yaml b/tests/fixtures/layout-requirement-vote1.yaml
index 7ccadff..2cde037 100644
--- a/tests/fixtures/layout-requirement-vote1.yaml
+++ b/tests/fixtures/layout-requirement-vote1.yaml
@@ -1,39 +1,39 @@
-pipelines:
-  - name: pipeline
-    manager: IndependentPipelineManager
-    require:
-      approval:
-        - username: jenkins
-          verified: 1
-    trigger:
-      gerrit:
-        - event: comment-added
-    success:
-      gerrit:
-        verified: 1
-    failure:
-      gerrit:
-        verified: -1
-
-  - name: trigger
-    manager: IndependentPipelineManager
-    trigger:
-      gerrit:
-        - event: comment-added
-          require-approval:
-            - username: jenkins
-              verified: 1
-    success:
-      gerrit:
-        verified: 1
-    failure:
-      gerrit:
-        verified: -1
-
-projects:
-  - name: org/project1
-    pipeline:
-      - project1-pipeline
-  - name: org/project2
-    trigger:
-      - project2-trigger
+pipelines:
+  - name: pipeline
+    manager: IndependentPipelineManager
+    require:
+      approval:
+        - username: jenkins
+          verified: 1
+    trigger:
+      gerrit:
+        - event: comment-added
+    success:
+      gerrit:
+        verified: 1
+    failure:
+      gerrit:
+        verified: -1
+
+  - name: trigger
+    manager: IndependentPipelineManager
+    trigger:
+      gerrit:
+        - event: comment-added
+          require-approval:
+            - username: jenkins
+              verified: 1
+    success:
+      gerrit:
+        verified: 1
+    failure:
+      gerrit:
+        verified: -1
+
+projects:
+  - name: org/project1
+    pipeline:
+      - project1-pipeline
+  - name: org/project2
+    trigger:
+      - project2-trigger
diff --git a/tests/fixtures/layout-requirement-vote2.yaml b/tests/fixtures/layout-requirement-vote2.yaml
index 33d84d1..1b15893 100644
--- a/tests/fixtures/layout-requirement-vote2.yaml
+++ b/tests/fixtures/layout-requirement-vote2.yaml
@@ -1,39 +1,39 @@
-pipelines:
-  - name: pipeline
-    manager: IndependentPipelineManager
-    require:
-      approval:
-        - username: jenkins
-          verified: [1, 2]
-    trigger:
-      gerrit:
-        - event: comment-added
-    success:
-      gerrit:
-        verified: 1
-    failure:
-      gerrit:
-        verified: -1
-
-  - name: trigger
-    manager: IndependentPipelineManager
-    trigger:
-      gerrit:
-        - event: comment-added
-          require-approval:
-            - username: jenkins
-              verified: [1, 2]
-    success:
-      gerrit:
-        verified: 1
-    failure:
-      gerrit:
-        verified: -1
-
-projects:
-  - name: org/project1
-    pipeline:
-      - project1-pipeline
-  - name: org/project2
-    trigger:
-      - project2-trigger
+pipelines:
+  - name: pipeline
+    manager: IndependentPipelineManager
+    require:
+      approval:
+        - username: jenkins
+          verified: [1, 2]
+    trigger:
+      gerrit:
+        - event: comment-added
+    success:
+      gerrit:
+        verified: 1
+    failure:
+      gerrit:
+        verified: -1
+
+  - name: trigger
+    manager: IndependentPipelineManager
+    trigger:
+      gerrit:
+        - event: comment-added
+          require-approval:
+            - username: jenkins
+              verified: [1, 2]
+    success:
+      gerrit:
+        verified: 1
+    failure:
+      gerrit:
+        verified: -1
+
+projects:
+  - name: org/project1
+    pipeline:
+      - project1-pipeline
+  - name: org/project2
+    trigger:
+      - project2-trigger
diff --git a/tests/fixtures/layout-skip-if.yaml b/tests/fixtures/layout-skip-if.yaml
index 0cfb445..2b62a71 100644
--- a/tests/fixtures/layout-skip-if.yaml
+++ b/tests/fixtures/layout-skip-if.yaml
@@ -1,29 +1,29 @@
-pipelines:
-  - name: check
-    manager: IndependentPipelineManager
-    trigger:
-      gerrit:
-        - event: patchset-created
-    success:
-      gerrit:
-        verified: 1
-    failure:
-      gerrit:
-        verified: -1
-
-
-jobs:
-  # Defining a metajob will validate that the skip-if attribute of the
-  # metajob is correctly copied to the job.
-  - name: ^.*skip-if$
-    skip-if:
-      - project: ^org/project$
-        branch: ^master$
-        all-files-match-any:
-          - ^README$
-  - name: project-test-skip-if
-
-projects:
-  - name: org/project
-    check:
-      - project-test-skip-if
+pipelines:
+  - name: check
+    manager: IndependentPipelineManager
+    trigger:
+      gerrit:
+        - event: patchset-created
+    success:
+      gerrit:
+        verified: 1
+    failure:
+      gerrit:
+        verified: -1
+
+
+jobs:
+  # Defining a metajob will validate that the skip-if attribute of the
+  # metajob is correctly copied to the job.
+  - name: ^.*skip-if$
+    skip-if:
+      - project: ^org/project$
+        branch: ^master$
+        all-files-match-any:
+          - ^README$
+  - name: project-test-skip-if
+
+projects:
+  - name: org/project
+    check:
+      - project-test-skip-if
diff --git a/tests/fixtures/layout-smtp.yaml b/tests/fixtures/layout-smtp.yaml
index 813857b..6026485 100644
--- a/tests/fixtures/layout-smtp.yaml
+++ b/tests/fixtures/layout-smtp.yaml
@@ -1,25 +1,25 @@
-pipelines:
-  - name: check
-    manager: IndependentPipelineManager
-    trigger:
-      gerrit:
-        - event: patchset-created
-    start:
-      smtp:
-        to: you@example.com
-    success:
-      gerrit:
-        verified: 1
-      smtp:
-        to: alternative_me@example.com
-        from: zuul_from@example.com
-    failure:
-      gerrit:
-        verified: -1
-
-projects:
-  - name: org/project
-    check:
-      - project-merge:
-        - project-test1
-        - project-test2
+pipelines:
+  - name: check
+    manager: IndependentPipelineManager
+    trigger:
+      gerrit:
+        - event: patchset-created
+    start:
+      smtp:
+        to: you@example.com
+    success:
+      gerrit:
+        verified: 1
+      smtp:
+        to: alternative_me@example.com
+        from: zuul_from@example.com
+    failure:
+      gerrit:
+        verified: -1
+
+projects:
+  - name: org/project
+    check:
+      - project-merge:
+        - project-test1
+        - project-test2
diff --git a/tests/fixtures/layout-swift.yaml b/tests/fixtures/layout-swift.yaml
index acaaad8..89d6fb7 100644
--- a/tests/fixtures/layout-swift.yaml
+++ b/tests/fixtures/layout-swift.yaml
@@ -1,59 +1,59 @@
-pipelines:
-  - name: check
-    manager: IndependentPipelineManager
-    trigger:
-      gerrit:
-        - event: patchset-created
-    success:
-      gerrit:
-        verified: 1
-    failure:
-      gerrit:
-        verified: -1
-
-  - name: post
-    manager: IndependentPipelineManager
-    trigger:
-      gerrit:
-        - event: ref-updated
-          ref: ^(?!refs/).*$
-
-  - name: gate
-    manager: DependentPipelineManager
-    failure-message: Build failed.  For information on how to proceed, see http://wiki.example.org/Test_Failures
-    trigger:
-      gerrit:
-        - event: comment-added
-          approval:
-            - approved: 1
-    success:
-      gerrit:
-        verified: 2
-        submit: true
-    failure:
-      gerrit:
-        verified: -2
-    start:
-      gerrit:
-        verified: 0
-    precedence: high
-
-jobs:
-  - name: ^.*$
-    swift:
-      - name: logs
-  - name: ^.*-merge$
-    swift:
-      - name: logs
-        container: merge_logs
-    failure-message: Unable to merge change
-  - name: test-test
-    swift:
-      - name: MOSTLY
-        container: stash
-
-projects:
-  - name: org/project
-    gate:
-      - test-merge
-      - test-test
+pipelines:
+  - name: check
+    manager: IndependentPipelineManager
+    trigger:
+      gerrit:
+        - event: patchset-created
+    success:
+      gerrit:
+        verified: 1
+    failure:
+      gerrit:
+        verified: -1
+
+  - name: post
+    manager: IndependentPipelineManager
+    trigger:
+      gerrit:
+        - event: ref-updated
+          ref: ^(?!refs/).*$
+
+  - name: gate
+    manager: DependentPipelineManager
+    failure-message: Build failed.  For information on how to proceed, see http://wiki.example.org/Test_Failures
+    trigger:
+      gerrit:
+        - event: comment-added
+          approval:
+            - approved: 1
+    success:
+      gerrit:
+        verified: 2
+        submit: true
+    failure:
+      gerrit:
+        verified: -2
+    start:
+      gerrit:
+        verified: 0
+    precedence: high
+
+jobs:
+  - name: ^.*$
+    swift:
+      - name: logs
+  - name: ^.*-merge$
+    swift:
+      - name: logs
+        container: merge_logs
+    failure-message: Unable to merge change
+  - name: test-test
+    swift:
+      - name: MOSTLY
+        container: stash
+
+projects:
+  - name: org/project
+    gate:
+      - test-merge
+      - test-test
diff --git a/tests/fixtures/layout-tags.yaml b/tests/fixtures/layout-tags.yaml
index d5b8bf9..bd1daef 100644
--- a/tests/fixtures/layout-tags.yaml
+++ b/tests/fixtures/layout-tags.yaml
@@ -1,42 +1,42 @@
-includes:
-  - python-file: tags_custom_functions.py
-
-pipelines:
-  - name: check
-    manager: IndependentPipelineManager
-    trigger:
-      gerrit:
-        - event: patchset-created
-    success:
-      gerrit:
-        verified: 1
-    failure:
-      gerrit:
-        verified: -1
-
-jobs:
-  - name: ^.*$
-    parameter-function: apply_tags
-  - name: ^.*-merge$
-    failure-message: Unable to merge change
-    hold-following-changes: true
-    tags: merge
-  - name: project1-merge
-    tags:
-      - project1
-      - extratag
-
-projects:
-  - name: org/project1
-    check:
-      - project1-merge:
-        - project1-test1
-        - project1-test2
-        - project1-project2-integration
-
-  - name: org/project2
-    check:
-      - project2-merge:
-        - project2-test1
-        - project2-test2
-        - project1-project2-integration
+includes:
+  - python-file: tags_custom_functions.py
+
+pipelines:
+  - name: check
+    manager: IndependentPipelineManager
+    trigger:
+      gerrit:
+        - event: patchset-created
+    success:
+      gerrit:
+        verified: 1
+    failure:
+      gerrit:
+        verified: -1
+
+jobs:
+  - name: ^.*$
+    parameter-function: apply_tags
+  - name: ^.*-merge$
+    failure-message: Unable to merge change
+    hold-following-changes: true
+    tags: merge
+  - name: project1-merge
+    tags:
+      - project1
+      - extratag
+
+projects:
+  - name: org/project1
+    check:
+      - project1-merge:
+        - project1-test1
+        - project1-test2
+        - project1-project2-integration
+
+  - name: org/project2
+    check:
+      - project2-merge:
+        - project2-test1
+        - project2-test2
+        - project1-project2-integration
diff --git a/tests/fixtures/layout-timer-smtp.yaml b/tests/fixtures/layout-timer-smtp.yaml
index b5a6ce0..7b8f7be 100644
--- a/tests/fixtures/layout-timer-smtp.yaml
+++ b/tests/fixtures/layout-timer-smtp.yaml
@@ -1,23 +1,23 @@
-pipelines:
-  - name: periodic
-    manager: IndependentPipelineManager
-    trigger:
-      timer:
-        - time: '* * * * * */1'
-    success:
-      smtp:
-        to: alternative_me@example.com
-        from: zuul_from@example.com
-        subject: 'Periodic check for {change.project} succeeded'
-
-jobs:
-  - name: project-bitrot-stable-old
-    success-pattern: http://logs.example.com/{job.name}/{build.number}
-  - name: project-bitrot-stable-older
-    success-pattern: http://logs.example.com/{job.name}/{build.number}
-
-projects:
-  - name: org/project
-    periodic:
-      - project-bitrot-stable-old
-      - project-bitrot-stable-older
+pipelines:
+  - name: periodic
+    manager: IndependentPipelineManager
+    trigger:
+      timer:
+        - time: '* * * * * */1'
+    success:
+      smtp:
+        to: alternative_me@example.com
+        from: zuul_from@example.com
+        subject: 'Periodic check for {change.project} succeeded'
+
+jobs:
+  - name: project-bitrot-stable-old
+    success-pattern: http://logs.example.com/{job.name}/{build.number}
+  - name: project-bitrot-stable-older
+    success-pattern: http://logs.example.com/{job.name}/{build.number}
+
+projects:
+  - name: org/project
+    periodic:
+      - project-bitrot-stable-old
+      - project-bitrot-stable-older
diff --git a/tests/fixtures/layout-timer.yaml b/tests/fixtures/layout-timer.yaml
index 4904f87..2570b5e 100644
--- a/tests/fixtures/layout-timer.yaml
+++ b/tests/fixtures/layout-timer.yaml
@@ -1,28 +1,28 @@
-pipelines:
-  - name: check
-    manager: IndependentPipelineManager
-    trigger:
-      gerrit:
-        - event: patchset-created
-    success:
-      gerrit:
-        verified: 1
-    failure:
-      gerrit:
-        verified: -1
-
-  - name: periodic
-    manager: IndependentPipelineManager
-    trigger:
-      timer:
-        - time: '* * * * * */1'
-
-projects:
-  - name: org/project
-    check:
-      - project-merge:
-        - project-test1
-        - project-test2
-    periodic:
-      - project-bitrot-stable-old
-      - project-bitrot-stable-older
+pipelines:
+  - name: check
+    manager: IndependentPipelineManager
+    trigger:
+      gerrit:
+        - event: patchset-created
+    success:
+      gerrit:
+        verified: 1
+    failure:
+      gerrit:
+        verified: -1
+
+  - name: periodic
+    manager: IndependentPipelineManager
+    trigger:
+      timer:
+        - time: '* * * * * */1'
+
+projects:
+  - name: org/project
+    check:
+      - project-merge:
+        - project-test1
+        - project-test2
+    periodic:
+      - project-bitrot-stable-old
+      - project-bitrot-stable-older
diff --git a/tests/fixtures/layout-zuultrigger-enqueued.yaml b/tests/fixtures/layout-zuultrigger-enqueued.yaml
index 8babd9e..cd63700 100644
--- a/tests/fixtures/layout-zuultrigger-enqueued.yaml
+++ b/tests/fixtures/layout-zuultrigger-enqueued.yaml
@@ -1,53 +1,53 @@
-pipelines:
-  - name: check
-    manager: IndependentPipelineManager
-    source: gerrit
-    require:
-      approval:
-        - verified: -1
-    trigger:
-      gerrit:
-        - event: patchset-created
-      zuul:
-        - event: parent-change-enqueued
-          pipeline: gate
-    success:
-      gerrit:
-        verified: 1
-    failure:
-      gerrit:
-        verified: -1
-
-  - name: gate
-    manager: DependentPipelineManager
-    failure-message: Build failed.  For information on how to proceed, see http://wiki.example.org/Test_Failures
-    source: gerrit
-    require:
-      approval:
-        - verified: 1
-    trigger:
-      gerrit:
-        - event: comment-added
-          approval:
-            - approved: 1
-      zuul:
-        - event: parent-change-enqueued
-          pipeline: gate
-    success:
-      gerrit:
-        verified: 2
-        submit: true
-    failure:
-      gerrit:
-        verified: -2
-    start:
-      gerrit:
-        verified: 0
-    precedence: high
-
-projects:
-  - name: org/project
-    check:
-      - project-check
-    gate:
-      - project-gate
+pipelines:
+  - name: check
+    manager: IndependentPipelineManager
+    source: gerrit
+    require:
+      approval:
+        - verified: -1
+    trigger:
+      gerrit:
+        - event: patchset-created
+      zuul:
+        - event: parent-change-enqueued
+          pipeline: gate
+    success:
+      gerrit:
+        verified: 1
+    failure:
+      gerrit:
+        verified: -1
+
+  - name: gate
+    manager: DependentPipelineManager
+    failure-message: Build failed.  For information on how to proceed, see http://wiki.example.org/Test_Failures
+    source: gerrit
+    require:
+      approval:
+        - verified: 1
+    trigger:
+      gerrit:
+        - event: comment-added
+          approval:
+            - approved: 1
+      zuul:
+        - event: parent-change-enqueued
+          pipeline: gate
+    success:
+      gerrit:
+        verified: 2
+        submit: true
+    failure:
+      gerrit:
+        verified: -2
+    start:
+      gerrit:
+        verified: 0
+    precedence: high
+
+projects:
+  - name: org/project
+    check:
+      - project-check
+    gate:
+      - project-gate
diff --git a/tests/fixtures/layout-zuultrigger-merged.yaml b/tests/fixtures/layout-zuultrigger-merged.yaml
index bb06dde..ed7113f 100644
--- a/tests/fixtures/layout-zuultrigger-merged.yaml
+++ b/tests/fixtures/layout-zuultrigger-merged.yaml
@@ -1,54 +1,54 @@
-pipelines:
-  - name: check
-    manager: IndependentPipelineManager
-    source: gerrit
-    trigger:
-      gerrit:
-        - event: patchset-created
-    success:
-      gerrit:
-        verified: 1
-    failure:
-      gerrit:
-        verified: -1
-
-  - name: gate
-    manager: DependentPipelineManager
-    failure-message: Build failed.  For information on how to proceed, see http://wiki.example.org/Test_Failures
-    source: gerrit
-    trigger:
-      gerrit:
-        - event: comment-added
-          approval:
-            - approved: 1
-    success:
-      gerrit:
-        verified: 2
-        submit: true
-    failure:
-      gerrit:
-        verified: -2
-    start:
-      gerrit:
-        verified: 0
-    precedence: high
-
-  - name: merge-check
-    manager: IndependentPipelineManager
-    source: gerrit
-    ignore-dependencies: true
-    trigger:
-      zuul:
-        - event: project-change-merged
-    merge-failure:
-      gerrit:
-        verified: -1
-
-projects:
-  - name: org/project
-    check:
-      - project-check
-    gate:
-      - project-gate
-    merge-check:
-      - noop
+pipelines:
+  - name: check
+    manager: IndependentPipelineManager
+    source: gerrit
+    trigger:
+      gerrit:
+        - event: patchset-created
+    success:
+      gerrit:
+        verified: 1
+    failure:
+      gerrit:
+        verified: -1
+
+  - name: gate
+    manager: DependentPipelineManager
+    failure-message: Build failed.  For information on how to proceed, see http://wiki.example.org/Test_Failures
+    source: gerrit
+    trigger:
+      gerrit:
+        - event: comment-added
+          approval:
+            - approved: 1
+    success:
+      gerrit:
+        verified: 2
+        submit: true
+    failure:
+      gerrit:
+        verified: -2
+    start:
+      gerrit:
+        verified: 0
+    precedence: high
+
+  - name: merge-check
+    manager: IndependentPipelineManager
+    source: gerrit
+    ignore-dependencies: true
+    trigger:
+      zuul:
+        - event: project-change-merged
+    merge-failure:
+      gerrit:
+        verified: -1
+
+projects:
+  - name: org/project
+    check:
+      - project-check
+    gate:
+      - project-gate
+    merge-check:
+      - noop
diff --git a/tests/fixtures/layout.yaml b/tests/fixtures/layout.yaml
index 2e48ff1..07b0147 100644
--- a/tests/fixtures/layout.yaml
+++ b/tests/fixtures/layout.yaml
@@ -1,261 +1,261 @@
-includes:
-  - python-file: custom_functions.py
-
-pipelines:
-  - name: check
-    manager: IndependentPipelineManager
-    trigger:
-      gerrit:
-        - event: patchset-created
-    success:
-      gerrit:
-        verified: 1
-    failure:
-      gerrit:
-        verified: -1
-
-  - name: post
-    manager: IndependentPipelineManager
-    trigger:
-      gerrit:
-        - event: ref-updated
-          ref: ^(?!refs/).*$
-
-  - name: gate
-    manager: DependentPipelineManager
-    failure-message: Build failed.  For information on how to proceed, see http://wiki.example.org/Test_Failures
-    trigger:
-      gerrit:
-        - event: comment-added
-          approval:
-            - approved: 1
-    success:
-      gerrit:
-        verified: 2
-        submit: true
-    failure:
-      gerrit:
-        verified: -2
-    start:
-      gerrit:
-        verified: 0
-    precedence: high
-
-  - name: unused
-    manager: IndependentPipelineManager
-    dequeue-on-new-patchset: false
-    trigger:
-      gerrit:
-        - event: comment-added
-          approval:
-            - approved: 1
-
-  - name: dup1
-    manager: IndependentPipelineManager
-    trigger:
-      gerrit:
-        - event: change-restored
-    success:
-      gerrit:
-        verified: 1
-    failure:
-      gerrit:
-        verified: -1
-
-  - name: dup2
-    manager: IndependentPipelineManager
-    trigger:
-      gerrit:
-        - event: change-restored
-    success:
-      gerrit:
-        verified: 1
-    failure:
-      gerrit:
-        verified: -1
-
-  - name: conflict
-    manager: DependentPipelineManager
-    failure-message: Build failed.  For information on how to proceed, see http://wiki.example.org/Test_Failures
-    trigger:
-      gerrit:
-        - event: comment-added
-          approval:
-            - approved: 1
-    success:
-      gerrit:
-        verified: 2
-        submit: true
-    failure:
-      gerrit:
-        verified: -2
-    start:
-      gerrit:
-        verified: 0
-
-  - name: experimental
-    manager: IndependentPipelineManager
-    trigger:
-      gerrit:
-        - event: patchset-created
-    success:
-      gerrit: {}
-    failure:
-      gerrit: {}
-
-jobs:
-  - name: ^.*-merge$
-    failure-message: Unable to merge change
-    hold-following-changes: true
-    tags: merge
-  - name: nonvoting-project-test2
-    voting: false
-  - name: project-testfile
-    files:
-      - '.*-requires'
-  - name: node-project-test1
-    parameter-function: select_debian_node
-  - name: project1-project2-integration
-    queue-name: integration
-  - name: mutex-one
-    mutex: test-mutex
-  - name: mutex-two
-    mutex: test-mutex
-  - name: project1-merge
-    tags:
-      - project1
-      - extratag
-
-project-templates:
-  - name: test-one-and-two
-    check:
-     - '{projectname}-test1'
-     - '{projectname}-test2'
-  - name: test-three-and-four
-    check:
-     - '{name}-test3'
-     - '{name}-test4'
-  - name: test-five
-    check:
-     - '{name}-{something}-test5'
-
-projects:
-  - name: org/project
-    merge-mode: cherry-pick
-    check:
-      - project-merge:
-        - project-test1
-        - project-test2
-        - project-testfile
-    gate:
-      - project-merge:
-        - project-test1
-        - project-test2
-        - project-testfile
-    post:
-      - project-post
-    dup1:
-      - project-test1
-    dup2:
-      - project-test1
-
-  - name: org/project1
-    check:
-      - project1-merge:
-        - project1-test1
-        - project1-test2
-        - project1-project2-integration
-    gate:
-      - project1-merge:
-        - project1-test1
-        - project1-test2
-        - project1-project2-integration
-    post:
-      - project1-post
-
-  - name: org/project2
-    check:
-      - project2-merge:
-        - project2-test1
-        - project2-test2
-        - project1-project2-integration
-    gate:
-      - project2-merge:
-        - project2-test1
-        - project2-test2
-        - project1-project2-integration
-    post:
-      - project2-post
-
-  - name: org/project3
-    check:
-      - project3-merge:
-        - project3-test1
-        - project3-test2
-        - project1-project2-integration
-    gate:
-      - project3-merge:
-        - project3-test1
-        - project3-test2
-        - project1-project2-integration
-    post:
-      - project3-post
-
-  - name: org/one-job-project
-    check:
-      - one-job-project-merge
-    gate:
-      - one-job-project-merge
-    post:
-      - one-job-project-post
-
-  - name: org/nonvoting-project
-    check:
-      - nonvoting-project-merge:
-        - nonvoting-project-test1
-        - nonvoting-project-test2
-    gate:
-      - nonvoting-project-merge:
-        - nonvoting-project-test1
-        - nonvoting-project-test2
-    post:
-      - nonvoting-project-post
-
-  - name: org/templated-project
-    template:
-      - name: test-one-and-two
-        projectname: project
-
-  - name: org/layered-project
-    template:
-      - name: test-one-and-two
-        projectname: project
-      - name: test-three-and-four
-      - name: test-five
-        something: foo
-    check:
-      - project-test6
-
-  - name: org/node-project
-    gate:
-      - node-project-merge:
-        - node-project-test1
-        - node-project-test2
-
-  - name: org/conflict-project
-    conflict:
-      - conflict-project-merge:
-        - conflict-project-test1
-        - conflict-project-test2
-
-  - name: org/noop-project
-    gate:
-      - noop
-
-  - name: org/experimental-project
-    experimental:
-      - experimental-project-test
-
-  - name: org/no-jobs-project
-    check:
-      - project-testfile
+includes:
+  - python-file: custom_functions.py
+
+pipelines:
+  - name: check
+    manager: IndependentPipelineManager
+    trigger:
+      gerrit:
+        - event: patchset-created
+    success:
+      gerrit:
+        verified: 1
+    failure:
+      gerrit:
+        verified: -1
+
+  - name: post
+    manager: IndependentPipelineManager
+    trigger:
+      gerrit:
+        - event: ref-updated
+          ref: ^(?!refs/).*$
+
+  - name: gate
+    manager: DependentPipelineManager
+    failure-message: Build failed.  For information on how to proceed, see http://wiki.example.org/Test_Failures
+    trigger:
+      gerrit:
+        - event: comment-added
+          approval:
+            - approved: 1
+    success:
+      gerrit:
+        verified: 2
+        submit: true
+    failure:
+      gerrit:
+        verified: -2
+    start:
+      gerrit:
+        verified: 0
+    precedence: high
+
+  - name: unused
+    manager: IndependentPipelineManager
+    dequeue-on-new-patchset: false
+    trigger:
+      gerrit:
+        - event: comment-added
+          approval:
+            - approved: 1
+
+  - name: dup1
+    manager: IndependentPipelineManager
+    trigger:
+      gerrit:
+        - event: change-restored
+    success:
+      gerrit:
+        verified: 1
+    failure:
+      gerrit:
+        verified: -1
+
+  - name: dup2
+    manager: IndependentPipelineManager
+    trigger:
+      gerrit:
+        - event: change-restored
+    success:
+      gerrit:
+        verified: 1
+    failure:
+      gerrit:
+        verified: -1
+
+  - name: conflict
+    manager: DependentPipelineManager
+    failure-message: Build failed.  For information on how to proceed, see http://wiki.example.org/Test_Failures
+    trigger:
+      gerrit:
+        - event: comment-added
+          approval:
+            - approved: 1
+    success:
+      gerrit:
+        verified: 2
+        submit: true
+    failure:
+      gerrit:
+        verified: -2
+    start:
+      gerrit:
+        verified: 0
+
+  - name: experimental
+    manager: IndependentPipelineManager
+    trigger:
+      gerrit:
+        - event: patchset-created
+    success:
+      gerrit: {}
+    failure:
+      gerrit: {}
+
+jobs:
+  - name: ^.*-merge$
+    failure-message: Unable to merge change
+    hold-following-changes: true
+    tags: merge
+  - name: nonvoting-project-test2
+    voting: false
+  - name: project-testfile
+    files:
+      - '.*-requires'
+  - name: node-project-test1
+    parameter-function: select_debian_node
+  - name: project1-project2-integration
+    queue-name: integration
+  - name: mutex-one
+    mutex: test-mutex
+  - name: mutex-two
+    mutex: test-mutex
+  - name: project1-merge
+    tags:
+      - project1
+      - extratag
+
+project-templates:
+  - name: test-one-and-two
+    check:
+     - '{projectname}-test1'
+     - '{projectname}-test2'
+  - name: test-three-and-four
+    check:
+     - '{name}-test3'
+     - '{name}-test4'
+  - name: test-five
+    check:
+     - '{name}-{something}-test5'
+
+projects:
+  - name: org/project
+    merge-mode: cherry-pick
+    check:
+      - project-merge:
+        - project-test1
+        - project-test2
+        - project-testfile
+    gate:
+      - project-merge:
+        - project-test1
+        - project-test2
+        - project-testfile
+    post:
+      - project-post
+    dup1:
+      - project-test1
+    dup2:
+      - project-test1
+
+  - name: org/project1
+    check:
+      - project1-merge:
+        - project1-test1
+        - project1-test2
+        - project1-project2-integration
+    gate:
+      - project1-merge:
+        - project1-test1
+        - project1-test2
+        - project1-project2-integration
+    post:
+      - project1-post
+
+  - name: org/project2
+    check:
+      - project2-merge:
+        - project2-test1
+        - project2-test2
+        - project1-project2-integration
+    gate:
+      - project2-merge:
+        - project2-test1
+        - project2-test2
+        - project1-project2-integration
+    post:
+      - project2-post
+
+  - name: org/project3
+    check:
+      - project3-merge:
+        - project3-test1
+        - project3-test2
+        - project1-project2-integration
+    gate:
+      - project3-merge:
+        - project3-test1
+        - project3-test2
+        - project1-project2-integration
+    post:
+      - project3-post
+
+  - name: org/one-job-project
+    check:
+      - one-job-project-merge
+    gate:
+      - one-job-project-merge
+    post:
+      - one-job-project-post
+
+  - name: org/nonvoting-project
+    check:
+      - nonvoting-project-merge:
+        - nonvoting-project-test1
+        - nonvoting-project-test2
+    gate:
+      - nonvoting-project-merge:
+        - nonvoting-project-test1
+        - nonvoting-project-test2
+    post:
+      - nonvoting-project-post
+
+  - name: org/templated-project
+    template:
+      - name: test-one-and-two
+        projectname: project
+
+  - name: org/layered-project
+    template:
+      - name: test-one-and-two
+        projectname: project
+      - name: test-three-and-four
+      - name: test-five
+        something: foo
+    check:
+      - project-test6
+
+  - name: org/node-project
+    gate:
+      - node-project-merge:
+        - node-project-test1
+        - node-project-test2
+
+  - name: org/conflict-project
+    conflict:
+      - conflict-project-merge:
+        - conflict-project-test1
+        - conflict-project-test2
+
+  - name: org/noop-project
+    gate:
+      - noop
+
+  - name: org/experimental-project
+    experimental:
+      - experimental-project-test
+
+  - name: org/no-jobs-project
+    check:
+      - project-testfile
diff --git a/tests/fixtures/layouts/bad_gerrit_missing.yaml b/tests/fixtures/layouts/bad_gerrit_missing.yaml
index 8db7248..9306d26 100644
--- a/tests/fixtures/layouts/bad_gerrit_missing.yaml
+++ b/tests/fixtures/layouts/bad_gerrit_missing.yaml
@@ -1,18 +1,18 @@
-pipelines:
-  - name: check
-    manager: IndependentPipelineManager
-    trigger:
-      not_gerrit:
-        - event: patchset-created
-    success:
-      review_gerrit:
-        verified: 1
-    failure:
-      review_gerrit:
-        verified: -1
-
-projects:
-  - name: test-org/test
-    check:
-      - test-merge
-      - test-test
+pipelines:
+  - name: check
+    manager: IndependentPipelineManager
+    trigger:
+      not_gerrit:
+        - event: patchset-created
+    success:
+      review_gerrit:
+        verified: 1
+    failure:
+      review_gerrit:
+        verified: -1
+
+projects:
+  - name: test-org/test
+    check:
+      - test-merge
+      - test-test
diff --git a/tests/fixtures/layouts/bad_merge_failure.yaml b/tests/fixtures/layouts/bad_merge_failure.yaml
index d9b973c..7718fc0 100644
--- a/tests/fixtures/layouts/bad_merge_failure.yaml
+++ b/tests/fixtures/layouts/bad_merge_failure.yaml
@@ -1,40 +1,40 @@
-pipelines:
-  - name: check
-    manager: IndependentPipelineManager
-    trigger:
-      review_gerrit:
-        - event: patchset-created
-    success:
-      review_gerrit:
-        verified: 1
-    failure:
-      review_gerrit:
-        verified: -1
-    # merge-failure-message needs a string.
-    merge-failure-message:
-
-  - name: gate
-    manager: DependentPipelineManager
-    failure-message: Build failed.  For information on how to proceed, see http://wiki.example.org/Test_Failures
-    trigger:
-      review_gerrit:
-        - event: comment-added
-          approval:
-            - approved: 1
-    success:
-      review_gerrit:
-        verified: 2
-        submit: true
-    failure:
-      review_gerrit:
-        verified: -2
-    merge-failure:
-    start:
-      review_gerrit:
-        verified: 0
-    precedence: high
-
-projects:
-  - name: org/project
-    check:
-      - project-check
+pipelines:
+  - name: check
+    manager: IndependentPipelineManager
+    trigger:
+      review_gerrit:
+        - event: patchset-created
+    success:
+      review_gerrit:
+        verified: 1
+    failure:
+      review_gerrit:
+        verified: -1
+    # merge-failure-message needs a string.
+    merge-failure-message:
+
+  - name: gate
+    manager: DependentPipelineManager
+    failure-message: Build failed.  For information on how to proceed, see http://wiki.example.org/Test_Failures
+    trigger:
+      review_gerrit:
+        - event: comment-added
+          approval:
+            - approved: 1
+    success:
+      review_gerrit:
+        verified: 2
+        submit: true
+    failure:
+      review_gerrit:
+        verified: -2
+    merge-failure:
+    start:
+      review_gerrit:
+        verified: 0
+    precedence: high
+
+projects:
+  - name: org/project
+    check:
+      - project-check
diff --git a/tests/fixtures/layouts/bad_misplaced_ref.yaml b/tests/fixtures/layouts/bad_misplaced_ref.yaml
index d8bb6bc..6ed8016 100644
--- a/tests/fixtures/layouts/bad_misplaced_ref.yaml
+++ b/tests/fixtures/layouts/bad_misplaced_ref.yaml
@@ -1,13 +1,13 @@
-pipelines:
-  - name: 'check'
-    manager: IndependentPipelineManager
-    trigger:
-      review_gerrit:
-        - event: patchset-created
-          ref: /some/ref/path
-
-projects:
-  - name: org/project
-    merge-mode: cherry-pick
-    check:
-      - project-check
+pipelines:
+  - name: 'check'
+    manager: IndependentPipelineManager
+    trigger:
+      review_gerrit:
+        - event: patchset-created
+          ref: /some/ref/path
+
+projects:
+  - name: org/project
+    merge-mode: cherry-pick
+    check:
+      - project-check
diff --git a/tests/fixtures/layouts/bad_pipelines1.yaml b/tests/fixtures/layouts/bad_pipelines1.yaml
index 09638bc..41c36f9 100644
--- a/tests/fixtures/layouts/bad_pipelines1.yaml
+++ b/tests/fixtures/layouts/bad_pipelines1.yaml
@@ -1,2 +1,2 @@
-# Pipelines completely missing. At least one is required.
-pipelines:
+# Pipelines completely missing. At least one is required.
+pipelines:
diff --git a/tests/fixtures/layouts/bad_pipelines10.yaml b/tests/fixtures/layouts/bad_pipelines10.yaml
index ddde946..6e954f3 100644
--- a/tests/fixtures/layouts/bad_pipelines10.yaml
+++ b/tests/fixtures/layouts/bad_pipelines10.yaml
@@ -1,8 +1,8 @@
-pipelines:
-  - name: check
-    manager: IndependentPipelineManager
-
-projects:
-  - name: foo
-    # merge-mode must be one of merge, merge-resolve, cherry-pick.
-    merge-mode: foo
+pipelines:
+  - name: check
+    manager: IndependentPipelineManager
+
+projects:
+  - name: foo
+    # merge-mode must be one of merge, merge-resolve, cherry-pick.
+    merge-mode: foo
diff --git a/tests/fixtures/layouts/bad_pipelines2.yaml b/tests/fixtures/layouts/bad_pipelines2.yaml
index fc1e154..0172aa1 100644
--- a/tests/fixtures/layouts/bad_pipelines2.yaml
+++ b/tests/fixtures/layouts/bad_pipelines2.yaml
@@ -1,7 +1,7 @@
-pipelines:
-  # name is required for pipelines
-  - noname: check
-    manager: IndependentPipelineManager
-
-projects:
-  - name: foo
+pipelines:
+  # name is required for pipelines
+  - noname: check
+    manager: IndependentPipelineManager
+
+projects:
+  - name: foo
diff --git a/tests/fixtures/layouts/bad_pipelines3.yaml b/tests/fixtures/layouts/bad_pipelines3.yaml
index 93ac266..6658cd4 100644
--- a/tests/fixtures/layouts/bad_pipelines3.yaml
+++ b/tests/fixtures/layouts/bad_pipelines3.yaml
@@ -1,8 +1,8 @@
-pipelines:
-  - name: check
-    # The manager must be one of IndependentPipelineManager
-    # or DependentPipelineManager
-    manager: NonexistentPipelineManager
-
-projects:
-  - name: foo
+pipelines:
+  - name: check
+    # The manager must be one of IndependentPipelineManager
+    # or DependentPipelineManager
+    manager: NonexistentPipelineManager
+
+projects:
+  - name: foo
diff --git a/tests/fixtures/layouts/bad_pipelines4.yaml b/tests/fixtures/layouts/bad_pipelines4.yaml
index 3a91604..3ba7f62 100644
--- a/tests/fixtures/layouts/bad_pipelines4.yaml
+++ b/tests/fixtures/layouts/bad_pipelines4.yaml
@@ -1,10 +1,10 @@
-pipelines:
-  - name: check
-    manager: IndependentPipelineManager
-    trigger:
-      gerrit:
-        # non-event is not a valid gerrit event
-        - event: non-event
-
-projects:
-  - name: foo
+pipelines:
+  - name: check
+    manager: IndependentPipelineManager
+    trigger:
+      gerrit:
+        # non-event is not a valid gerrit event
+        - event: non-event
+
+projects:
+  - name: foo
diff --git a/tests/fixtures/layouts/bad_pipelines5.yaml b/tests/fixtures/layouts/bad_pipelines5.yaml
index a91ac7a..bdc22cf 100644
--- a/tests/fixtures/layouts/bad_pipelines5.yaml
+++ b/tests/fixtures/layouts/bad_pipelines5.yaml
@@ -1,11 +1,11 @@
-pipelines:
-  - name: check
-    manager: IndependentPipelineManager
-    trigger:
-      review_gerrit:
-        # event is a required item but it is missing.
-        - approval:
-            - approved: 1
-
-projects:
-  - name: foo
+pipelines:
+  - name: check
+    manager: IndependentPipelineManager
+    trigger:
+      review_gerrit:
+        # event is a required item but it is missing.
+        - approval:
+            - approved: 1
+
+projects:
+  - name: foo
diff --git a/tests/fixtures/layouts/bad_pipelines6.yaml b/tests/fixtures/layouts/bad_pipelines6.yaml
index bf2d538..a19243d 100644
--- a/tests/fixtures/layouts/bad_pipelines6.yaml
+++ b/tests/fixtures/layouts/bad_pipelines6.yaml
@@ -1,11 +1,11 @@
-pipelines:
-  - name: check
-    manager: IndependentPipelineManager
-    trigger:
-      review_gerrit:
-        - event: comment-added
-          # approved is not a valid entry. Should be approval.
-          approved: 1
-
-projects:
-  - name: foo
+pipelines:
+  - name: check
+    manager: IndependentPipelineManager
+    trigger:
+      review_gerrit:
+        - event: comment-added
+          # approved is not a valid entry. Should be approval.
+          approved: 1
+
+projects:
+  - name: foo
diff --git a/tests/fixtures/layouts/bad_pipelines7.yaml b/tests/fixtures/layouts/bad_pipelines7.yaml
index e2db495..fc54556 100644
--- a/tests/fixtures/layouts/bad_pipelines7.yaml
+++ b/tests/fixtures/layouts/bad_pipelines7.yaml
@@ -1,6 +1,6 @@
-pipelines:
-  # The pipeline must have a name.
-  - manager: IndependentPipelineManager
-
-projects:
-  - name: foo
+pipelines:
+  # The pipeline must have a name.
+  - manager: IndependentPipelineManager
+
+projects:
+  - name: foo
diff --git a/tests/fixtures/layouts/bad_pipelines8.yaml b/tests/fixtures/layouts/bad_pipelines8.yaml
index 9c5918e..35fdd85 100644
--- a/tests/fixtures/layouts/bad_pipelines8.yaml
+++ b/tests/fixtures/layouts/bad_pipelines8.yaml
@@ -1,6 +1,6 @@
-pipelines:
-  # The pipeline must have a manager
-  - name: check
-
-projects:
-  - name: foo
+pipelines:
+  # The pipeline must have a manager
+  - name: check
+
+projects:
+  - name: foo
diff --git a/tests/fixtures/layouts/bad_pipelines9.yaml b/tests/fixtures/layouts/bad_pipelines9.yaml
index 89307d5..7c2738e 100644
--- a/tests/fixtures/layouts/bad_pipelines9.yaml
+++ b/tests/fixtures/layouts/bad_pipelines9.yaml
@@ -1,9 +1,9 @@
-pipelines:
-  # Names must be unique.
-  - name: check
-    manager: IndependentPipelineManager
-  - name: check
-    manager: IndependentPipelineManager
-
-projects:
-  - name: foo
+pipelines:
+  # Names must be unique.
+  - name: check
+    manager: IndependentPipelineManager
+  - name: check
+    manager: IndependentPipelineManager
+
+projects:
+  - name: foo
diff --git a/tests/fixtures/layouts/bad_projects1.yaml b/tests/fixtures/layouts/bad_projects1.yaml
index e3d381f..b10e434 100644
--- a/tests/fixtures/layouts/bad_projects1.yaml
+++ b/tests/fixtures/layouts/bad_projects1.yaml
@@ -1,10 +1,10 @@
-pipelines:
-  - name: check
-    manager: IndependentPipelineManager
-
-projects:
-  - name: foo
-  # gate pipeline is not defined.
-    gate:
-      - test
-
+pipelines:
+  - name: check
+    manager: IndependentPipelineManager
+
+projects:
+  - name: foo
+  # gate pipeline is not defined.
+    gate:
+      - test
+
diff --git a/tests/fixtures/layouts/bad_projects2.yaml b/tests/fixtures/layouts/bad_projects2.yaml
index 9291cc9..1b73af1 100644
--- a/tests/fixtures/layouts/bad_projects2.yaml
+++ b/tests/fixtures/layouts/bad_projects2.yaml
@@ -1,10 +1,10 @@
-pipelines:
-  - name: check
-    manager: IndependentPipelineManager
-
-projects:
-  - name: foo
-    check:
-      # Indentation is one level too deep on the last line.
-      - test
-        - foo
+pipelines:
+  - name: check
+    manager: IndependentPipelineManager
+
+projects:
+  - name: foo
+    check:
+      # Indentation is one level too deep on the last line.
+      - test
+        - foo
diff --git a/tests/fixtures/layouts/bad_reject.yaml b/tests/fixtures/layouts/bad_reject.yaml
index 0549875..c1b2f0b 100644
--- a/tests/fixtures/layouts/bad_reject.yaml
+++ b/tests/fixtures/layouts/bad_reject.yaml
@@ -1,21 +1,21 @@
-# Template is going to be called but missing a parameter
-
-pipelines:
-  - name: 'check'
-    manager: IndependentPipelineManager
-    require:
-      open: True
-      current-patchset: True
-      approval:
-        - verified: [1, 2]
-          username: jenkins
-        - workflow: 1
-    reject:
-      # Reject only takes 'approval', has no need for open etc..
-      open: True
-      approval:
-        - code-review: [-1, -2]
-          username: core-person
-    trigger:
-      review_gerrit:
-        - event: patchset-created
+# Template is going to be called but missing a parameter
+
+pipelines:
+  - name: 'check'
+    manager: IndependentPipelineManager
+    require:
+      open: True
+      current-patchset: True
+      approval:
+        - verified: [1, 2]
+          username: jenkins
+        - workflow: 1
+    reject:
+      # Reject only takes 'approval', has no need for open etc..
+      open: True
+      approval:
+        - code-review: [-1, -2]
+          username: core-person
+    trigger:
+      review_gerrit:
+        - event: patchset-created
diff --git a/tests/fixtures/layouts/bad_swift.yaml b/tests/fixtures/layouts/bad_swift.yaml
index f217821..3c2402d 100644
--- a/tests/fixtures/layouts/bad_swift.yaml
+++ b/tests/fixtures/layouts/bad_swift.yaml
@@ -1,28 +1,28 @@
-pipelines:
-  - name: check
-    manager: IndependentPipelineManager
-    trigger:
-      review_gerrit:
-        - event: patchset-created
-    success:
-      review_gerrit:
-        verified: 1
-    failure:
-      review_gerrit:
-        verified: -1
-
-jobs:
-  - name: ^.*$
-    swift:
-      - name: logs
-  - name: ^.*-merge$
-    # swift requires a name
-    swift:
-        container: merge_assets
-    failure-message: Unable to merge change
-
-projects:
-  - name: test-org/test
-    check:
-      - test-merge
-      - test-test
+pipelines:
+  - name: check
+    manager: IndependentPipelineManager
+    trigger:
+      review_gerrit:
+        - event: patchset-created
+    success:
+      review_gerrit:
+        verified: 1
+    failure:
+      review_gerrit:
+        verified: -1
+
+jobs:
+  - name: ^.*$
+    swift:
+      - name: logs
+  - name: ^.*-merge$
+    # swift requires a name
+    swift:
+        container: merge_assets
+    failure-message: Unable to merge change
+
+projects:
+  - name: test-org/test
+    check:
+      - test-merge
+      - test-test
diff --git a/tests/fixtures/layouts/bad_template1.yaml b/tests/fixtures/layouts/bad_template1.yaml
index 8868edf..d613c3c 100644
--- a/tests/fixtures/layouts/bad_template1.yaml
+++ b/tests/fixtures/layouts/bad_template1.yaml
@@ -1,20 +1,20 @@
-# Template is going to be called but missing a parameter
-
-pipelines:
-  - name: 'check'
-    manager: IndependentPipelineManager
-    trigger:
-      review_gerrit:
-        - event: patchset-created
-
-project-templates:
-  - name: template-generic
-    check:
-     # Template uses the 'project' parameter' which must be provided
-     - '{project}-merge'
-
-projects:
-  - name: organization/project
-    template:
-      - name: template-generic
-      # Here we 'forgot' to pass 'project'
+# Template is going to be called but missing a parameter
+
+pipelines:
+  - name: 'check'
+    manager: IndependentPipelineManager
+    trigger:
+      review_gerrit:
+        - event: patchset-created
+
+project-templates:
+  - name: template-generic
+    check:
+     # Template uses the 'project' parameter' which must be provided
+     - '{project}-merge'
+
+projects:
+  - name: organization/project
+    template:
+      - name: template-generic
+      # Here we 'forgot' to pass 'project'
diff --git a/tests/fixtures/layouts/bad_template2.yaml b/tests/fixtures/layouts/bad_template2.yaml
index 09a5f91..885e3f3 100644
--- a/tests/fixtures/layouts/bad_template2.yaml
+++ b/tests/fixtures/layouts/bad_template2.yaml
@@ -1,23 +1,23 @@
-# Template is going to be called with an extra parameter
-
-pipelines:
-  - name: 'check'
-    manager: IndependentPipelineManager
-    trigger:
-      review_gerrit:
-        - event: patchset-created
-
-project-templates:
-  - name: template-generic
-    check:
-     # Template only uses the 'project' parameter'
-     - '{project}-merge'
-
-projects:
-  - name: organization/project
-    template:
-      - name: template-generic
-        project: 'MyProjectName'
-        # Feed an extra parameters which is not going to be used
-        # by the template.  That is an error.
-        extraparam: 'IShouldNotBeSet'
+# Template is going to be called with an extra parameter
+
+pipelines:
+  - name: 'check'
+    manager: IndependentPipelineManager
+    trigger:
+      review_gerrit:
+        - event: patchset-created
+
+project-templates:
+  - name: template-generic
+    check:
+     # Template only uses the 'project' parameter'
+     - '{project}-merge'
+
+projects:
+  - name: organization/project
+    template:
+      - name: template-generic
+        project: 'MyProjectName'
+        # Feed an extra parameters which is not going to be used
+        # by the template.  That is an error.
+        extraparam: 'IShouldNotBeSet'
diff --git a/tests/fixtures/layouts/bad_template3.yaml b/tests/fixtures/layouts/bad_template3.yaml
index 54697c4..d357e97 100644
--- a/tests/fixtures/layouts/bad_template3.yaml
+++ b/tests/fixtures/layouts/bad_template3.yaml
@@ -1,10 +1,10 @@
-# Template refers to an unexisting pipeline
-
-project-templates:
-  - name: template-generic
-    unexisting-pipeline:  # pipeline does not exist
-
-projects:
-  - name: organization/project
-    template:
-      - name: template-generic
+# Template refers to an unexisting pipeline
+
+project-templates:
+  - name: template-generic
+    unexisting-pipeline:  # pipeline does not exist
+
+projects:
+  - name: organization/project
+    template:
+      - name: template-generic
diff --git a/tests/fixtures/layouts/good_connections1.conf b/tests/fixtures/layouts/good_connections1.conf
index 768cbb0..6b8efca 100644
--- a/tests/fixtures/layouts/good_connections1.conf
+++ b/tests/fixtures/layouts/good_connections1.conf
@@ -1,42 +1,42 @@
-[gearman]
-server=127.0.0.1
-
-[zuul]
-layout_config=layout.yaml
-url_pattern=http://logs.example.com/{change.number}/{change.patchset}/{pipeline.name}/{job.name}/{build.number}
-job_name_in_report=true
-
-[merger]
-git_dir=/tmp/zuul-test/git
-git_user_email=zuul@example.com
-git_user_name=zuul
-zuul_url=http://zuul.example.com/p
-
-[swift]
-authurl=https://identity.api.example.org/v2.0/
-user=username
-key=password
-tenant_name=" "
-
-default_container=logs
-region_name=EXP
-logserver_prefix=http://logs.example.org/server.app/
-
-[connection review_gerrit]
-driver=gerrit
-server=review.example.com
-user=jenkins
-sshkey=none
-
-[connection other_gerrit]
-driver=gerrit
-server=review2.example.com
-user=jenkins2
-sshkey=none
-
-[connection my_smtp]
-driver=smtp
-server=localhost
-port=25
-default_from=zuul@example.com
-default_to=you@example.com
+[gearman]
+server=127.0.0.1
+
+[zuul]
+layout_config=layout.yaml
+url_pattern=http://logs.example.com/{change.number}/{change.patchset}/{pipeline.name}/{job.name}/{build.number}
+job_name_in_report=true
+
+[merger]
+git_dir=/tmp/zuul-test/git
+git_user_email=zuul@example.com
+git_user_name=zuul
+zuul_url=http://zuul.example.com/p
+
+[swift]
+authurl=https://identity.api.example.org/v2.0/
+user=username
+key=password
+tenant_name=" "
+
+default_container=logs
+region_name=EXP
+logserver_prefix=http://logs.example.org/server.app/
+
+[connection review_gerrit]
+driver=gerrit
+server=review.example.com
+user=jenkins
+sshkey=none
+
+[connection other_gerrit]
+driver=gerrit
+server=review2.example.com
+user=jenkins2
+sshkey=none
+
+[connection my_smtp]
+driver=smtp
+server=localhost
+port=25
+default_from=zuul@example.com
+default_to=you@example.com
diff --git a/tests/fixtures/layouts/good_connections1.yaml b/tests/fixtures/layouts/good_connections1.yaml
index f5f55b1..a424b5d 100644
--- a/tests/fixtures/layouts/good_connections1.yaml
+++ b/tests/fixtures/layouts/good_connections1.yaml
@@ -1,18 +1,18 @@
-pipelines:
-  - name: check
-    manager: IndependentPipelineManager
-    source: review_gerrit
-    trigger:
-      review_gerrit:
-        - event: patchset-created
-    success:
-      review_gerrit:
-        verified: 1
-    failure:
-      other_gerrit:
-        verified: -1
-
-projects:
-  - name: org/project
-    check:
-      - project-check
+pipelines:
+  - name: check
+    manager: IndependentPipelineManager
+    source: review_gerrit
+    trigger:
+      review_gerrit:
+        - event: patchset-created
+    success:
+      review_gerrit:
+        verified: 1
+    failure:
+      other_gerrit:
+        verified: -1
+
+projects:
+  - name: org/project
+    check:
+      - project-check
diff --git a/tests/fixtures/layouts/good_layout.yaml b/tests/fixtures/layouts/good_layout.yaml
index 0e21d57..6c0d3ec 100644
--- a/tests/fixtures/layouts/good_layout.yaml
+++ b/tests/fixtures/layouts/good_layout.yaml
@@ -1,102 +1,102 @@
-includes:
-  - python-file: openstack_functions.py
-
-pipelines:
-  - name: check
-    manager: IndependentPipelineManager
-    require:
-      open: True
-      current-patchset: True
-    trigger:
-      review_gerrit:
-        - event: patchset-created
-        - event: comment-added
-          require-approval:
-            - verified: [-1, -2]
-              username: jenkins
-          approval:
-            - workflow: 1
-    success:
-      review_gerrit:
-        verified: 1
-    failure:
-      review_gerrit:
-        verified: -1
-
-  - name: post
-    manager: IndependentPipelineManager
-    trigger:
-      review_gerrit:
-        - event: ref-updated
-          ref: ^(?!refs/).*$
-          ignore-deletes: True
-
-  - name: gate
-    manager: DependentPipelineManager
-    success-message: Your change is awesome.
-    failure-message: Build failed.  For information on how to proceed, see http://wiki.example.org/Test_Failures
-    require:
-      open: True
-      current-patchset: True
-      approval:
-        - verified: [1, 2]
-          username: jenkins
-        - workflow: 1
-    reject:
-      approval:
-        - code-review: [-1, -2]
-    trigger:
-      review_gerrit:
-        - event: comment-added
-          approval:
-            - approved: 1
-    start:
-      review_gerrit:
-        verified: 0
-    success:
-      review_gerrit:
-        verified: 2
-        code-review: 1
-        submit: true
-    failure:
-      review_gerrit:
-        verified: -2
-        workinprogress: true
-
-  - name: merge-check
-    manager: IndependentPipelineManager
-    source: review_gerrit
-    ignore-dependencies: true
-    trigger:
-      zuul:
-        - event: project-change-merged
-    merge-failure:
-      review_gerrit:
-        verified: -1
-
-jobs:
-  - name: ^.*-merge$
-    failure-message: Unable to merge change
-    hold-following-changes: true
-  - name: test-merge
-    parameter-function: devstack_params
-  - name: test-test
-  - name: test-merge2
-    success-pattern: http://logs.example.com/{change.number}/{change.patchset}/{pipeline.name}/{job.name}/{build.number}/success
-    failure-pattern: http://logs.example.com/{change.number}/{change.patchset}/{pipeline.name}/{job.name}/{build.number}/fail
-  - name: project-testfile
-    files:
-      - 'tools/.*-requires'
-
-projects:
-  - name: test-org/test
-    merge-mode: cherry-pick
-    check:
-      - test-merge2:
-          - test-thing1:
-              - test-thing2
-              - test-thing3
-    gate:
-      - test-thing
-    post:
-      - test-post
+includes:
+  - python-file: openstack_functions.py
+
+pipelines:
+  - name: check
+    manager: IndependentPipelineManager
+    require:
+      open: True
+      current-patchset: True
+    trigger:
+      review_gerrit:
+        - event: patchset-created
+        - event: comment-added
+          require-approval:
+            - verified: [-1, -2]
+              username: jenkins
+          approval:
+            - workflow: 1
+    success:
+      review_gerrit:
+        verified: 1
+    failure:
+      review_gerrit:
+        verified: -1
+
+  - name: post
+    manager: IndependentPipelineManager
+    trigger:
+      review_gerrit:
+        - event: ref-updated
+          ref: ^(?!refs/).*$
+          ignore-deletes: True
+
+  - name: gate
+    manager: DependentPipelineManager
+    success-message: Your change is awesome.
+    failure-message: Build failed.  For information on how to proceed, see http://wiki.example.org/Test_Failures
+    require:
+      open: True
+      current-patchset: True
+      approval:
+        - verified: [1, 2]
+          username: jenkins
+        - workflow: 1
+    reject:
+      approval:
+        - code-review: [-1, -2]
+    trigger:
+      review_gerrit:
+        - event: comment-added
+          approval:
+            - approved: 1
+    start:
+      review_gerrit:
+        verified: 0
+    success:
+      review_gerrit:
+        verified: 2
+        code-review: 1
+        submit: true
+    failure:
+      review_gerrit:
+        verified: -2
+        workinprogress: true
+
+  - name: merge-check
+    manager: IndependentPipelineManager
+    source: review_gerrit
+    ignore-dependencies: true
+    trigger:
+      zuul:
+        - event: project-change-merged
+    merge-failure:
+      review_gerrit:
+        verified: -1
+
+jobs:
+  - name: ^.*-merge$
+    failure-message: Unable to merge change
+    hold-following-changes: true
+  - name: test-merge
+    parameter-function: devstack_params
+  - name: test-test
+  - name: test-merge2
+    success-pattern: http://logs.example.com/{change.number}/{change.patchset}/{pipeline.name}/{job.name}/{build.number}/success
+    failure-pattern: http://logs.example.com/{change.number}/{change.patchset}/{pipeline.name}/{job.name}/{build.number}/fail
+  - name: project-testfile
+    files:
+      - 'tools/.*-requires'
+
+projects:
+  - name: test-org/test
+    merge-mode: cherry-pick
+    check:
+      - test-merge2:
+          - test-thing1:
+              - test-thing2
+              - test-thing3
+    gate:
+      - test-thing
+    post:
+      - test-post
diff --git a/tests/fixtures/layouts/good_merge_failure.yaml b/tests/fixtures/layouts/good_merge_failure.yaml
index afede3c..9c317ae 100644
--- a/tests/fixtures/layouts/good_merge_failure.yaml
+++ b/tests/fixtures/layouts/good_merge_failure.yaml
@@ -1,53 +1,53 @@
-pipelines:
-  - name: check
-    manager: IndependentPipelineManager
-    merge-failure-message: "Could not merge the change. Please rebase..."
-    trigger:
-      review_gerrit:
-        - event: patchset-created
-    success:
-      review_gerrit:
-        verified: 1
-    failure:
-      review_gerrit:
-        verified: -1
-
-  - name: post
-    manager: IndependentPipelineManager
-    trigger:
-      review_gerrit:
-        - event: ref-updated
-          ref: ^(?!refs/).*$
-    merge-failure:
-      review_gerrit:
-        verified: -1
-
-  - name: gate
-    manager: DependentPipelineManager
-    failure-message: Build failed.  For information on how to proceed, see http://wiki.example.org/Test_Failures
-    trigger:
-      review_gerrit:
-        - event: comment-added
-          approval:
-            - approved: 1
-    success:
-      review_gerrit:
-        verified: 2
-        submit: true
-    failure:
-      review_gerrit:
-        verified: -2
-    merge-failure:
-      review_gerrit:
-        verified: -1
-      my_smtp:
-        to: you@example.com
-    start:
-      review_gerrit:
-        verified: 0
-    precedence: high
-
-projects:
-  - name: org/project
-    check:
-      - project-check
+pipelines:
+  - name: check
+    manager: IndependentPipelineManager
+    merge-failure-message: "Could not merge the change. Please rebase..."
+    trigger:
+      review_gerrit:
+        - event: patchset-created
+    success:
+      review_gerrit:
+        verified: 1
+    failure:
+      review_gerrit:
+        verified: -1
+
+  - name: post
+    manager: IndependentPipelineManager
+    trigger:
+      review_gerrit:
+        - event: ref-updated
+          ref: ^(?!refs/).*$
+    merge-failure:
+      review_gerrit:
+        verified: -1
+
+  - name: gate
+    manager: DependentPipelineManager
+    failure-message: Build failed.  For information on how to proceed, see http://wiki.example.org/Test_Failures
+    trigger:
+      review_gerrit:
+        - event: comment-added
+          approval:
+            - approved: 1
+    success:
+      review_gerrit:
+        verified: 2
+        submit: true
+    failure:
+      review_gerrit:
+        verified: -2
+    merge-failure:
+      review_gerrit:
+        verified: -1
+      my_smtp:
+        to: you@example.com
+    start:
+      review_gerrit:
+        verified: 0
+    precedence: high
+
+projects:
+  - name: org/project
+    check:
+      - project-check
diff --git a/tests/fixtures/layouts/good_require_approvals.yaml b/tests/fixtures/layouts/good_require_approvals.yaml
index d899765..11e1f3f 100644
--- a/tests/fixtures/layouts/good_require_approvals.yaml
+++ b/tests/fixtures/layouts/good_require_approvals.yaml
@@ -1,36 +1,36 @@
-includes:
-  - python-file: custom_functions.py
-
-pipelines:
-  - name: check
-    manager: IndependentPipelineManager
-    trigger:
-      review_gerrit:
-        - event: comment-added
-          require-approval:
-            - username: jenkins
-              older-than: 48h
-        - event: comment-added
-          require-approval:
-            - email: jenkins@example.com
-              newer-than: 48h
-        - event: comment-added
-          require-approval:
-            - approved: 1
-        - event: comment-added
-          require-approval:
-            - approved: 1
-              username: jenkins
-              email: jenkins@example.com
-    success:
-      review_gerrit:
-        verified: 1
-    failure:
-      review_gerrit:
-        verified: -1
-
-projects:
-  - name: org/project
-    merge-mode: cherry-pick
-    check:
-      - project-check
+includes:
+  - python-file: custom_functions.py
+
+pipelines:
+  - name: check
+    manager: IndependentPipelineManager
+    trigger:
+      review_gerrit:
+        - event: comment-added
+          require-approval:
+            - username: jenkins
+              older-than: 48h
+        - event: comment-added
+          require-approval:
+            - email: jenkins@example.com
+              newer-than: 48h
+        - event: comment-added
+          require-approval:
+            - approved: 1
+        - event: comment-added
+          require-approval:
+            - approved: 1
+              username: jenkins
+              email: jenkins@example.com
+    success:
+      review_gerrit:
+        verified: 1
+    failure:
+      review_gerrit:
+        verified: -1
+
+projects:
+  - name: org/project
+    merge-mode: cherry-pick
+    check:
+      - project-check
diff --git a/tests/fixtures/layouts/good_swift.yaml b/tests/fixtures/layouts/good_swift.yaml
index 48ca7f0..80935a3 100644
--- a/tests/fixtures/layouts/good_swift.yaml
+++ b/tests/fixtures/layouts/good_swift.yaml
@@ -1,32 +1,32 @@
-pipelines:
-  - name: check
-    manager: IndependentPipelineManager
-    trigger:
-      review_gerrit:
-        - event: patchset-created
-    success:
-      review_gerrit:
-        verified: 1
-    failure:
-      review_gerrit:
-        verified: -1
-
-jobs:
-  - name: ^.*$
-    swift:
-      - name: logs
-  - name: ^.*-merge$
-    swift:
-      - name: assets
-        container: merge_assets
-    failure-message: Unable to merge change
-  - name: test-test
-    swift:
-      - name: mostly
-        container: stash
-
-projects:
-  - name: test-org/test
-    check:
-      - test-merge
-      - test-test
+pipelines:
+  - name: check
+    manager: IndependentPipelineManager
+    trigger:
+      review_gerrit:
+        - event: patchset-created
+    success:
+      review_gerrit:
+        verified: 1
+    failure:
+      review_gerrit:
+        verified: -1
+
+jobs:
+  - name: ^.*$
+    swift:
+      - name: logs
+  - name: ^.*-merge$
+    swift:
+      - name: assets
+        container: merge_assets
+    failure-message: Unable to merge change
+  - name: test-test
+    swift:
+      - name: mostly
+        container: stash
+
+projects:
+  - name: test-org/test
+    check:
+      - test-merge
+      - test-test
diff --git a/tests/fixtures/layouts/good_template1.yaml b/tests/fixtures/layouts/good_template1.yaml
index 1680c7b..fbad06a 100644
--- a/tests/fixtures/layouts/good_template1.yaml
+++ b/tests/fixtures/layouts/good_template1.yaml
@@ -1,17 +1,17 @@
-pipelines:
-  - name: 'check'
-    manager: IndependentPipelineManager
-    trigger:
-      review_gerrit:
-        - event: patchset-created
-
-project-templates:
-  - name: template-generic
-    check:
-     - '{project}-merge'
-
-projects:
-  - name: organization/project
-    template:
-      - name: template-generic
-        project: 'myproject'
+pipelines:
+  - name: 'check'
+    manager: IndependentPipelineManager
+    trigger:
+      review_gerrit:
+        - event: patchset-created
+
+project-templates:
+  - name: template-generic
+    check:
+     - '{project}-merge'
+
+projects:
+  - name: organization/project
+    template:
+      - name: template-generic
+        project: 'myproject'
diff --git a/tests/fixtures/layouts/zuul_default.conf b/tests/fixtures/layouts/zuul_default.conf
index 6440027..ebe96cf 100644
--- a/tests/fixtures/layouts/zuul_default.conf
+++ b/tests/fixtures/layouts/zuul_default.conf
@@ -1,36 +1,36 @@
-[gearman]
-server=127.0.0.1
-
-[zuul]
-layout_config=layout.yaml
-url_pattern=http://logs.example.com/{change.number}/{change.patchset}/{pipeline.name}/{job.name}/{build.number}
-job_name_in_report=true
-
-[merger]
-git_dir=/tmp/zuul-test/git
-git_user_email=zuul@example.com
-git_user_name=zuul
-zuul_url=http://zuul.example.com/p
-
-[swift]
-authurl=https://identity.api.example.org/v2.0/
-user=username
-key=password
-tenant_name=" "
-
-default_container=logs
-region_name=EXP
-logserver_prefix=http://logs.example.org/server.app/
-
-[connection review_gerrit]
-driver=gerrit
-server=review.example.com
-user=jenkins
-sshkey=none
-
-[connection my_smtp]
-driver=smtp
-server=localhost
-port=25
-default_from=zuul@example.com
-default_to=you@example.com
+[gearman]
+server=127.0.0.1
+
+[zuul]
+layout_config=layout.yaml
+url_pattern=http://logs.example.com/{change.number}/{change.patchset}/{pipeline.name}/{job.name}/{build.number}
+job_name_in_report=true
+
+[merger]
+git_dir=/tmp/zuul-test/git
+git_user_email=zuul@example.com
+git_user_name=zuul
+zuul_url=http://zuul.example.com/p
+
+[swift]
+authurl=https://identity.api.example.org/v2.0/
+user=username
+key=password
+tenant_name=" "
+
+default_container=logs
+region_name=EXP
+logserver_prefix=http://logs.example.org/server.app/
+
+[connection review_gerrit]
+driver=gerrit
+server=review.example.com
+user=jenkins
+sshkey=none
+
+[connection my_smtp]
+driver=smtp
+server=localhost
+port=25
+default_from=zuul@example.com
+default_to=you@example.com
diff --git a/tests/fixtures/tags_custom_functions.py b/tests/fixtures/tags_custom_functions.py
index 67e7ef1..6621cfd 100644
--- a/tests/fixtures/tags_custom_functions.py
+++ b/tests/fixtures/tags_custom_functions.py
@@ -1,2 +1,2 @@
-def apply_tags(item, job, params):
-    params['BUILD_TAGS'] = ' '.join(sorted(job.tags))
+def apply_tags(item, job, params):
+    params['BUILD_TAGS'] = ' '.join(sorted(job.tags))
diff --git a/tests/fixtures/zuul-connections-multiple-gerrits.conf b/tests/fixtures/zuul-connections-multiple-gerrits.conf
index f067e6e..94c9f54 100644
--- a/tests/fixtures/zuul-connections-multiple-gerrits.conf
+++ b/tests/fixtures/zuul-connections-multiple-gerrits.conf
@@ -1,42 +1,42 @@
-[gearman]
-server=127.0.0.1
-
-[zuul]
-layout_config=layout-connections-multiple-voters.yaml
-url_pattern=http://logs.example.com/{change.number}/{change.patchset}/{pipeline.name}/{job.name}/{build.number}
-job_name_in_report=true
-
-[merger]
-git_dir=/tmp/zuul-test/git
-git_user_email=zuul@example.com
-git_user_name=zuul
-zuul_url=http://zuul.example.com/p
-
-[swift]
-authurl=https://identity.api.example.org/v2.0/
-user=username
-key=password
-tenant_name=" "
-
-default_container=logs
-region_name=EXP
-logserver_prefix=http://logs.example.org/server.app/
-
-[connection review_gerrit]
-driver=gerrit
-server=review.example.com
-user=jenkins
-sshkey=none
-
-[connection another_gerrit]
-driver=gerrit
-server=another.example.com
-user=jenkins
-sshkey=none
-
-[connection outgoing_smtp]
-driver=smtp
-server=localhost
-port=25
-default_from=zuul@example.com
-default_to=you@example.com
+[gearman]
+server=127.0.0.1
+
+[zuul]
+layout_config=layout-connections-multiple-voters.yaml
+url_pattern=http://logs.example.com/{change.number}/{change.patchset}/{pipeline.name}/{job.name}/{build.number}
+job_name_in_report=true
+
+[merger]
+git_dir=/tmp/zuul-test/git
+git_user_email=zuul@example.com
+git_user_name=zuul
+zuul_url=http://zuul.example.com/p
+
+[swift]
+authurl=https://identity.api.example.org/v2.0/
+user=username
+key=password
+tenant_name=" "
+
+default_container=logs
+region_name=EXP
+logserver_prefix=http://logs.example.org/server.app/
+
+[connection review_gerrit]
+driver=gerrit
+server=review.example.com
+user=jenkins
+sshkey=none
+
+[connection another_gerrit]
+driver=gerrit
+server=another.example.com
+user=jenkins
+sshkey=none
+
+[connection outgoing_smtp]
+driver=smtp
+server=localhost
+port=25
+default_from=zuul@example.com
+default_to=you@example.com
diff --git a/tests/fixtures/zuul-connections-same-gerrit.conf b/tests/fixtures/zuul-connections-same-gerrit.conf
index af31c8a..5609267 100644
--- a/tests/fixtures/zuul-connections-same-gerrit.conf
+++ b/tests/fixtures/zuul-connections-same-gerrit.conf
@@ -1,42 +1,42 @@
-[gearman]
-server=127.0.0.1
-
-[zuul]
-layout_config=layout-connections-multiple-voters.yaml
-url_pattern=http://logs.example.com/{change.number}/{change.patchset}/{pipeline.name}/{job.name}/{build.number}
-job_name_in_report=true
-
-[merger]
-git_dir=/tmp/zuul-test/git
-git_user_email=zuul@example.com
-git_user_name=zuul
-zuul_url=http://zuul.example.com/p
-
-[swift]
-authurl=https://identity.api.example.org/v2.0/
-user=username
-key=password
-tenant_name=" "
-
-default_container=logs
-region_name=EXP
-logserver_prefix=http://logs.example.org/server.app/
-
-[connection review_gerrit]
-driver=gerrit
-server=review.example.com
-user=jenkins
-sshkey=none
-
-[connection alt_voting_gerrit]
-driver=gerrit
-server=review.example.com
-user=civoter
-sshkey=none
-
-[connection outgoing_smtp]
-driver=smtp
-server=localhost
-port=25
-default_from=zuul@example.com
-default_to=you@example.com
+[gearman]
+server=127.0.0.1
+
+[zuul]
+layout_config=layout-connections-multiple-voters.yaml
+url_pattern=http://logs.example.com/{change.number}/{change.patchset}/{pipeline.name}/{job.name}/{build.number}
+job_name_in_report=true
+
+[merger]
+git_dir=/tmp/zuul-test/git
+git_user_email=zuul@example.com
+git_user_name=zuul
+zuul_url=http://zuul.example.com/p
+
+[swift]
+authurl=https://identity.api.example.org/v2.0/
+user=username
+key=password
+tenant_name=" "
+
+default_container=logs
+region_name=EXP
+logserver_prefix=http://logs.example.org/server.app/
+
+[connection review_gerrit]
+driver=gerrit
+server=review.example.com
+user=jenkins
+sshkey=none
+
+[connection alt_voting_gerrit]
+driver=gerrit
+server=review.example.com
+user=civoter
+sshkey=none
+
+[connection outgoing_smtp]
+driver=smtp
+server=localhost
+port=25
+default_from=zuul@example.com
+default_to=you@example.com
diff --git a/tests/fixtures/zuul.conf b/tests/fixtures/zuul.conf
index b250c6d..5b3a67d 100644
--- a/tests/fixtures/zuul.conf
+++ b/tests/fixtures/zuul.conf
@@ -1,36 +1,36 @@
-[gearman]
-server=127.0.0.1
-
-[zuul]
-layout_config=layout.yaml
-url_pattern=http://logs.example.com/{change.number}/{change.patchset}/{pipeline.name}/{job.name}/{build.number}
-job_name_in_report=true
-
-[merger]
-git_dir=/tmp/zuul-test/git
-git_user_email=zuul@example.com
-git_user_name=zuul
-zuul_url=http://zuul.example.com/p
-
-[swift]
-authurl=https://identity.api.example.org/v2.0/
-user=username
-key=password
-tenant_name=" "
-
-default_container=logs
-region_name=EXP
-logserver_prefix=http://logs.example.org/server.app/
-
-[connection gerrit]
-driver=gerrit
-server=review.example.com
-user=jenkins
-sshkey=none
-
-[connection smtp]
-driver=smtp
-server=localhost
-port=25
-default_from=zuul@example.com
-default_to=you@example.com
+[gearman]
+server=127.0.0.1
+
+[zuul]
+layout_config=layout.yaml
+url_pattern=http://logs.example.com/{change.number}/{change.patchset}/{pipeline.name}/{job.name}/{build.number}
+job_name_in_report=true
+
+[merger]
+git_dir=/tmp/zuul-test/git
+git_user_email=zuul@example.com
+git_user_name=zuul
+zuul_url=http://zuul.example.com/p
+
+[swift]
+authurl=https://identity.api.example.org/v2.0/
+user=username
+key=password
+tenant_name=" "
+
+default_container=logs
+region_name=EXP
+logserver_prefix=http://logs.example.org/server.app/
+
+[connection gerrit]
+driver=gerrit
+server=review.example.com
+user=jenkins
+sshkey=none
+
+[connection smtp]
+driver=smtp
+server=localhost
+port=25
+default_from=zuul@example.com
+default_to=you@example.com
diff --git a/tests/test_change_matcher.py b/tests/test_change_matcher.py
index 1f4ab93..29b0613 100644
--- a/tests/test_change_matcher.py
+++ b/tests/test_change_matcher.py
@@ -1,154 +1,154 @@
-# Copyright 2015 Red Hat, Inc.
-#
-# Licensed under the Apache License, Version 2.0 (the "License"); you may
-# not use this file except in compliance with the License. You may obtain
-# a copy of the License at
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-# License for the specific language governing permissions and limitations
-# under the License.
-
-from zuul import change_matcher as cm
-from zuul import model
-
-from tests.base import BaseTestCase
-
-
-class BaseTestMatcher(BaseTestCase):
-
-    project = 'project'
-
-    def setUp(self):
-        super(BaseTestMatcher, self).setUp()
-        self.change = model.Change(self.project)
-
-
-class TestAbstractChangeMatcher(BaseTestMatcher):
-
-    def test_str(self):
-        matcher = cm.ProjectMatcher(self.project)
-        self.assertEqual(str(matcher), '{ProjectMatcher:project}')
-
-    def test_repr(self):
-        matcher = cm.ProjectMatcher(self.project)
-        self.assertEqual(repr(matcher), '<ProjectMatcher project>')
-
-
-class TestProjectMatcher(BaseTestMatcher):
-
-    def test_matches_returns_true(self):
-        matcher = cm.ProjectMatcher(self.project)
-        self.assertTrue(matcher.matches(self.change))
-
-    def test_matches_returns_false(self):
-        matcher = cm.ProjectMatcher('not_project')
-        self.assertFalse(matcher.matches(self.change))
-
-
-class TestBranchMatcher(BaseTestMatcher):
-
-    def setUp(self):
-        super(TestBranchMatcher, self).setUp()
-        self.matcher = cm.BranchMatcher('foo')
-
-    def test_matches_returns_true_on_matching_branch(self):
-        self.change.branch = 'foo'
-        self.assertTrue(self.matcher.matches(self.change))
-
-    def test_matches_returns_true_on_matching_ref(self):
-        self.change.branch = 'bar'
-        self.change.ref = 'foo'
-        self.assertTrue(self.matcher.matches(self.change))
-
-    def test_matches_returns_false_for_no_match(self):
-        self.change.branch = 'bar'
-        self.change.ref = 'baz'
-        self.assertFalse(self.matcher.matches(self.change))
-
-    def test_matches_returns_false_for_missing_attrs(self):
-        delattr(self.change, 'branch')
-        # ref is by default not an attribute
-        self.assertFalse(self.matcher.matches(self.change))
-
-
-class TestFileMatcher(BaseTestMatcher):
-
-    def setUp(self):
-        super(TestFileMatcher, self).setUp()
-        self.matcher = cm.FileMatcher('filename')
-
-    def test_matches_returns_true(self):
-        self.change.files = ['filename']
-        self.assertTrue(self.matcher.matches(self.change))
-
-    def test_matches_returns_false_when_no_files(self):
-        self.assertFalse(self.matcher.matches(self.change))
-
-    def test_matches_returns_false_when_files_attr_missing(self):
-        delattr(self.change, 'files')
-        self.assertFalse(self.matcher.matches(self.change))
-
-
-class TestAbstractMatcherCollection(BaseTestMatcher):
-
-    def test_str(self):
-        matcher = cm.MatchAll([cm.FileMatcher('foo')])
-        self.assertEqual(str(matcher), '{MatchAll:{FileMatcher:foo}}')
-
-    def test_repr(self):
-        matcher = cm.MatchAll([])
-        self.assertEqual(repr(matcher), '<MatchAll>')
-
-
-class TestMatchAllFiles(BaseTestMatcher):
-
-    def setUp(self):
-        super(TestMatchAllFiles, self).setUp()
-        self.matcher = cm.MatchAllFiles([cm.FileMatcher('^docs/.*$')])
-
-    def _test_matches(self, expected, files=None):
-        if files is not None:
-            self.change.files = files
-        self.assertEqual(expected, self.matcher.matches(self.change))
-
-    def test_matches_returns_false_when_files_attr_missing(self):
-        delattr(self.change, 'files')
-        self._test_matches(False)
-
-    def test_matches_returns_false_when_no_files(self):
-        self._test_matches(False)
-
-    def test_matches_returns_false_when_not_all_files_match(self):
-        self._test_matches(False, files=['docs/foo', 'foo/bar'])
-
-    def test_matches_returns_true_when_commit_message_matches(self):
-        self._test_matches(True, files=['/COMMIT_MSG'])
-
-    def test_matches_returns_true_when_all_files_match(self):
-        self._test_matches(True, files=['docs/foo'])
-
-
-class TestMatchAll(BaseTestMatcher):
-
-    def test_matches_returns_true(self):
-        matcher = cm.MatchAll([cm.ProjectMatcher(self.project)])
-        self.assertTrue(matcher.matches(self.change))
-
-    def test_matches_returns_false_for_missing_matcher(self):
-        matcher = cm.MatchAll([cm.ProjectMatcher('not_project')])
-        self.assertFalse(matcher.matches(self.change))
-
-
-class TestMatchAny(BaseTestMatcher):
-
-    def test_matches_returns_true(self):
-        matcher = cm.MatchAny([cm.ProjectMatcher(self.project)])
-        self.assertTrue(matcher.matches(self.change))
-
-    def test_matches_returns_false(self):
-        matcher = cm.MatchAny([cm.ProjectMatcher('not_project')])
-        self.assertFalse(matcher.matches(self.change))
+# Copyright 2015 Red Hat, Inc.
+#
+# Licensed under the Apache License, Version 2.0 (the "License"); you may
+# not use this file except in compliance with the License. You may obtain
+# a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+# License for the specific language governing permissions and limitations
+# under the License.
+
+from zuul import change_matcher as cm
+from zuul import model
+
+from tests.base import BaseTestCase
+
+
+class BaseTestMatcher(BaseTestCase):
+
+    project = 'project'
+
+    def setUp(self):
+        super(BaseTestMatcher, self).setUp()
+        self.change = model.Change(self.project)
+
+
+class TestAbstractChangeMatcher(BaseTestMatcher):
+
+    def test_str(self):
+        matcher = cm.ProjectMatcher(self.project)
+        self.assertEqual(str(matcher), '{ProjectMatcher:project}')
+
+    def test_repr(self):
+        matcher = cm.ProjectMatcher(self.project)
+        self.assertEqual(repr(matcher), '<ProjectMatcher project>')
+
+
+class TestProjectMatcher(BaseTestMatcher):
+
+    def test_matches_returns_true(self):
+        matcher = cm.ProjectMatcher(self.project)
+        self.assertTrue(matcher.matches(self.change))
+
+    def test_matches_returns_false(self):
+        matcher = cm.ProjectMatcher('not_project')
+        self.assertFalse(matcher.matches(self.change))
+
+
+class TestBranchMatcher(BaseTestMatcher):
+
+    def setUp(self):
+        super(TestBranchMatcher, self).setUp()
+        self.matcher = cm.BranchMatcher('foo')
+
+    def test_matches_returns_true_on_matching_branch(self):
+        self.change.branch = 'foo'
+        self.assertTrue(self.matcher.matches(self.change))
+
+    def test_matches_returns_true_on_matching_ref(self):
+        self.change.branch = 'bar'
+        self.change.ref = 'foo'
+        self.assertTrue(self.matcher.matches(self.change))
+
+    def test_matches_returns_false_for_no_match(self):
+        self.change.branch = 'bar'
+        self.change.ref = 'baz'
+        self.assertFalse(self.matcher.matches(self.change))
+
+    def test_matches_returns_false_for_missing_attrs(self):
+        delattr(self.change, 'branch')
+        # ref is by default not an attribute
+        self.assertFalse(self.matcher.matches(self.change))
+
+
+class TestFileMatcher(BaseTestMatcher):
+
+    def setUp(self):
+        super(TestFileMatcher, self).setUp()
+        self.matcher = cm.FileMatcher('filename')
+
+    def test_matches_returns_true(self):
+        self.change.files = ['filename']
+        self.assertTrue(self.matcher.matches(self.change))
+
+    def test_matches_returns_false_when_no_files(self):
+        self.assertFalse(self.matcher.matches(self.change))
+
+    def test_matches_returns_false_when_files_attr_missing(self):
+        delattr(self.change, 'files')
+        self.assertFalse(self.matcher.matches(self.change))
+
+
+class TestAbstractMatcherCollection(BaseTestMatcher):
+
+    def test_str(self):
+        matcher = cm.MatchAll([cm.FileMatcher('foo')])
+        self.assertEqual(str(matcher), '{MatchAll:{FileMatcher:foo}}')
+
+    def test_repr(self):
+        matcher = cm.MatchAll([])
+        self.assertEqual(repr(matcher), '<MatchAll>')
+
+
+class TestMatchAllFiles(BaseTestMatcher):
+
+    def setUp(self):
+        super(TestMatchAllFiles, self).setUp()
+        self.matcher = cm.MatchAllFiles([cm.FileMatcher('^docs/.*$')])
+
+    def _test_matches(self, expected, files=None):
+        if files is not None:
+            self.change.files = files
+        self.assertEqual(expected, self.matcher.matches(self.change))
+
+    def test_matches_returns_false_when_files_attr_missing(self):
+        delattr(self.change, 'files')
+        self._test_matches(False)
+
+    def test_matches_returns_false_when_no_files(self):
+        self._test_matches(False)
+
+    def test_matches_returns_false_when_not_all_files_match(self):
+        self._test_matches(False, files=['docs/foo', 'foo/bar'])
+
+    def test_matches_returns_true_when_commit_message_matches(self):
+        self._test_matches(True, files=['/COMMIT_MSG'])
+
+    def test_matches_returns_true_when_all_files_match(self):
+        self._test_matches(True, files=['docs/foo'])
+
+
+class TestMatchAll(BaseTestMatcher):
+
+    def test_matches_returns_true(self):
+        matcher = cm.MatchAll([cm.ProjectMatcher(self.project)])
+        self.assertTrue(matcher.matches(self.change))
+
+    def test_matches_returns_false_for_missing_matcher(self):
+        matcher = cm.MatchAll([cm.ProjectMatcher('not_project')])
+        self.assertFalse(matcher.matches(self.change))
+
+
+class TestMatchAny(BaseTestMatcher):
+
+    def test_matches_returns_true(self):
+        matcher = cm.MatchAny([cm.ProjectMatcher(self.project)])
+        self.assertTrue(matcher.matches(self.change))
+
+    def test_matches_returns_false(self):
+        matcher = cm.MatchAny([cm.ProjectMatcher('not_project')])
+        self.assertFalse(matcher.matches(self.change))
diff --git a/tests/test_clonemapper.py b/tests/test_clonemapper.py
index b7814f8..7901220 100644
--- a/tests/test_clonemapper.py
+++ b/tests/test_clonemapper.py
@@ -1,84 +1,84 @@
-# Copyright 2014 Antoine "hashar" Musso
-# Copyright 2014 Wikimedia Foundation Inc.
-#
-# Licensed under the Apache License, Version 2.0 (the "License"); you may
-# not use this file except in compliance with the License. You may obtain
-# a copy of the License at
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-# License for the specific language governing permissions and limitations
-# under the License.
-
-import logging
-import testtools
-from zuul.lib.clonemapper import CloneMapper
-
-logging.basicConfig(level=logging.DEBUG,
-                    format='%(asctime)s %(name)-17s '
-                    '%(levelname)-8s %(message)s')
-
-
-class TestCloneMapper(testtools.TestCase):
-
-    def test_empty_mapper(self):
-        """Given an empty map, the slashes in project names are directory
-           separators"""
-        cmap = CloneMapper(
-            {},
-            [
-                'project1',
-                'plugins/plugin1'
-            ])
-
-        self.assertEqual(
-            {'project1': '/basepath/project1',
-             'plugins/plugin1': '/basepath/plugins/plugin1'},
-            cmap.expand('/basepath')
-        )
-
-    def test_map_to_a_dot_dir(self):
-        """Verify we normalize path, hence '.' refers to the basepath"""
-        cmap = CloneMapper(
-            [{'name': 'mediawiki/core', 'dest': '.'}],
-            ['mediawiki/core'])
-        self.assertEqual(
-            {'mediawiki/core': '/basepath'},
-            cmap.expand('/basepath'))
-
-    def test_map_using_regex(self):
-        """One can use regex in maps and use \\1 to forge the directory"""
-        cmap = CloneMapper(
-            [{'name': 'plugins/(.*)', 'dest': 'project/plugins/\\1'}],
-            ['plugins/PluginFirst'])
-        self.assertEqual(
-            {'plugins/PluginFirst': '/basepath/project/plugins/PluginFirst'},
-            cmap.expand('/basepath'))
-
-    def test_map_discarding_regex_group(self):
-        cmap = CloneMapper(
-            [{'name': 'plugins/(.*)', 'dest': 'project/'}],
-            ['plugins/Plugin_1'])
-        self.assertEqual(
-            {'plugins/Plugin_1': '/basepath/project'},
-            cmap.expand('/basepath'))
-
-    def test_cant_dupe_destinations(self):
-        """We cant clone multiple projects in the same directory"""
-        cmap = CloneMapper(
-            [{'name': 'plugins/(.*)', 'dest': 'catchall/'}],
-            ['plugins/plugin1', 'plugins/plugin2']
-        )
-        self.assertRaises(Exception, cmap.expand, '/basepath')
-
-    def test_map_with_dot_and_regex(self):
-        """Combining relative path and regex"""
-        cmap = CloneMapper(
-            [{'name': 'plugins/(.*)', 'dest': './\\1'}],
-            ['plugins/PluginInBasePath'])
-        self.assertEqual(
-            {'plugins/PluginInBasePath': '/basepath/PluginInBasePath'},
-            cmap.expand('/basepath'))
+# Copyright 2014 Antoine "hashar" Musso
+# Copyright 2014 Wikimedia Foundation Inc.
+#
+# Licensed under the Apache License, Version 2.0 (the "License"); you may
+# not use this file except in compliance with the License. You may obtain
+# a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+# License for the specific language governing permissions and limitations
+# under the License.
+
+import logging
+import testtools
+from zuul.lib.clonemapper import CloneMapper
+
+logging.basicConfig(level=logging.DEBUG,
+                    format='%(asctime)s %(name)-17s '
+                    '%(levelname)-8s %(message)s')
+
+
+class TestCloneMapper(testtools.TestCase):
+
+    def test_empty_mapper(self):
+        """Given an empty map, the slashes in project names are directory
+           separators"""
+        cmap = CloneMapper(
+            {},
+            [
+                'project1',
+                'plugins/plugin1'
+            ])
+
+        self.assertEqual(
+            {'project1': '/basepath/project1',
+             'plugins/plugin1': '/basepath/plugins/plugin1'},
+            cmap.expand('/basepath')
+        )
+
+    def test_map_to_a_dot_dir(self):
+        """Verify we normalize path, hence '.' refers to the basepath"""
+        cmap = CloneMapper(
+            [{'name': 'mediawiki/core', 'dest': '.'}],
+            ['mediawiki/core'])
+        self.assertEqual(
+            {'mediawiki/core': '/basepath'},
+            cmap.expand('/basepath'))
+
+    def test_map_using_regex(self):
+        """One can use regex in maps and use \\1 to forge the directory"""
+        cmap = CloneMapper(
+            [{'name': 'plugins/(.*)', 'dest': 'project/plugins/\\1'}],
+            ['plugins/PluginFirst'])
+        self.assertEqual(
+            {'plugins/PluginFirst': '/basepath/project/plugins/PluginFirst'},
+            cmap.expand('/basepath'))
+
+    def test_map_discarding_regex_group(self):
+        cmap = CloneMapper(
+            [{'name': 'plugins/(.*)', 'dest': 'project/'}],
+            ['plugins/Plugin_1'])
+        self.assertEqual(
+            {'plugins/Plugin_1': '/basepath/project'},
+            cmap.expand('/basepath'))
+
+    def test_cant_dupe_destinations(self):
+        """We cant clone multiple projects in the same directory"""
+        cmap = CloneMapper(
+            [{'name': 'plugins/(.*)', 'dest': 'catchall/'}],
+            ['plugins/plugin1', 'plugins/plugin2']
+        )
+        self.assertRaises(Exception, cmap.expand, '/basepath')
+
+    def test_map_with_dot_and_regex(self):
+        """Combining relative path and regex"""
+        cmap = CloneMapper(
+            [{'name': 'plugins/(.*)', 'dest': './\\1'}],
+            ['plugins/PluginInBasePath'])
+        self.assertEqual(
+            {'plugins/PluginInBasePath': '/basepath/PluginInBasePath'},
+            cmap.expand('/basepath'))
diff --git a/tests/test_cloner.py b/tests/test_cloner.py
index 2f5f8a7..366f1c4 100644
--- a/tests/test_cloner.py
+++ b/tests/test_cloner.py
@@ -1,591 +1,591 @@
-#!/usr/bin/env python2.7
-
-# Copyright 2012 Hewlett-Packard Development Company, L.P.
-# Copyright 2014 Wikimedia Foundation Inc.
-#
-# Licensed under the Apache License, Version 2.0 (the "License"); you may
-# not use this file except in compliance with the License. You may obtain
-# a copy of the License at
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-# License for the specific language governing permissions and limitations
-# under the License.
-
-import fixtures
-import logging
-import os
-import shutil
-import time
-
-import git
-
-import zuul.lib.cloner
-
-from tests.base import ZuulTestCase
-
-logging.basicConfig(level=logging.DEBUG,
-                    format='%(asctime)s %(name)-32s '
-                    '%(levelname)-8s %(message)s')
-
-
-class TestCloner(ZuulTestCase):
-
-    log = logging.getLogger("zuul.test.cloner")
-    workspace_root = None
-
-    def setUp(self):
-        super(TestCloner, self).setUp()
-        self.workspace_root = os.path.join(self.test_root, 'workspace')
-
-        self.config.set('zuul', 'layout_config',
-                        'tests/fixtures/layout-cloner.yaml')
-        self.sched.reconfigure(self.config)
-        self.registerJobs()
-
-    def getWorkspaceRepos(self, projects):
-        repos = {}
-        for project in projects:
-            repos[project] = git.Repo(
-                os.path.join(self.workspace_root, project))
-        return repos
-
-    def getUpstreamRepos(self, projects):
-        repos = {}
-        for project in projects:
-            repos[project] = git.Repo(
-                os.path.join(self.upstream_root, project))
-        return repos
-
-    def test_cache_dir(self):
-        projects = ['org/project1', 'org/project2']
-        cache_root = os.path.join(self.test_root, "cache")
-        for project in projects:
-            upstream_repo_path = os.path.join(self.upstream_root, project)
-            cache_repo_path = os.path.join(cache_root, project)
-            git.Repo.clone_from(upstream_repo_path, cache_repo_path)
-
-        self.worker.hold_jobs_in_build = True
-        A = self.fake_gerrit.addFakeChange('org/project1', 'master', 'A')
-        A.addApproval('CRVW', 2)
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-
-        self.waitUntilSettled()
-
-        self.assertEquals(1, len(self.builds), "One build is running")
-
-        B = self.fake_gerrit.addFakeChange('org/project2', 'master', 'B')
-        B.setMerged()
-
-        upstream = self.getUpstreamRepos(projects)
-        states = [{
-            'org/project1': self.builds[0].parameters['ZUUL_COMMIT'],
-            'org/project2': str(upstream['org/project2'].commit('master')),
-        }]
-
-        for number, build in enumerate(self.builds):
-            self.log.debug("Build parameters: %s", build.parameters)
-            cloner = zuul.lib.cloner.Cloner(
-                git_base_url=self.upstream_root,
-                projects=projects,
-                workspace=self.workspace_root,
-                zuul_branch=build.parameters['ZUUL_BRANCH'],
-                zuul_ref=build.parameters['ZUUL_REF'],
-                zuul_url=self.git_root,
-                cache_dir=cache_root,
-            )
-            cloner.execute()
-            work = self.getWorkspaceRepos(projects)
-            state = states[number]
-
-            for project in projects:
-                self.assertEquals(state[project],
-                                  str(work[project].commit('HEAD')),
-                                  'Project %s commit for build %s should '
-                                  'be correct' % (project, number))
-
-        work = self.getWorkspaceRepos(projects)
-        upstream_repo_path = os.path.join(self.upstream_root, 'org/project1')
-        self.assertEquals(
-            work['org/project1'].remotes.origin.url,
-            upstream_repo_path,
-            'workspace repo origin should be upstream, not cache'
-        )
-
-        self.worker.hold_jobs_in_build = False
-        self.worker.release()
-        self.waitUntilSettled()
-
-    def test_recognize_bare_cache(self):
-        cache_root = os.path.join(self.test_root, "cache")
-        upstream_repo_path = os.path.join(self.upstream_root, 'org/project1')
-        cache_bare_path = os.path.join(cache_root, 'org/project1.git')
-        cache_repo = git.Repo.clone_from(upstream_repo_path, cache_bare_path,
-                                         bare=True)
-        self.assertTrue(type(cache_repo.bare), msg='Cache repo is bare')
-
-        log_fixture = self.useFixture(fixtures.FakeLogger(level=logging.INFO))
-        cloner = zuul.lib.cloner.Cloner(
-            git_base_url=self.upstream_root,
-            projects=['org/project1'],
-            workspace=self.workspace_root,
-            zuul_branch='HEAD',
-            zuul_ref='HEAD',
-            zuul_url=self.git_root,
-            cache_dir=cache_root
-        )
-        cloner.execute()
-        self.assertIn('Creating repo org/project1 from cache file://%s' % (
-                      cache_bare_path), log_fixture.output)
-
-    def test_one_branch(self):
-        self.worker.hold_jobs_in_build = True
-
-        projects = ['org/project1', 'org/project2']
-        A = self.fake_gerrit.addFakeChange('org/project1', 'master', 'A')
-        B = self.fake_gerrit.addFakeChange('org/project2', 'master', 'B')
-        A.addApproval('CRVW', 2)
-        B.addApproval('CRVW', 2)
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
-
-        self.waitUntilSettled()
-
-        self.assertEquals(2, len(self.builds), "Two builds are running")
-
-        upstream = self.getUpstreamRepos(projects)
-        states = [
-            {'org/project1': self.builds[0].parameters['ZUUL_COMMIT'],
-             'org/project2': str(upstream['org/project2'].commit('master')),
-             },
-            {'org/project1': self.builds[0].parameters['ZUUL_COMMIT'],
-             'org/project2': self.builds[1].parameters['ZUUL_COMMIT'],
-             },
-        ]
-
-        for number, build in enumerate(self.builds):
-            self.log.debug("Build parameters: %s", build.parameters)
-            cloner = zuul.lib.cloner.Cloner(
-                git_base_url=self.upstream_root,
-                projects=projects,
-                workspace=self.workspace_root,
-                zuul_branch=build.parameters['ZUUL_BRANCH'],
-                zuul_ref=build.parameters['ZUUL_REF'],
-                zuul_url=self.git_root,
-            )
-            cloner.execute()
-            work = self.getWorkspaceRepos(projects)
-            state = states[number]
-
-            for project in projects:
-                self.assertEquals(state[project],
-                                  str(work[project].commit('HEAD')),
-                                  'Project %s commit for build %s should '
-                                  'be correct' % (project, number))
-
-            shutil.rmtree(self.workspace_root)
-
-        self.worker.hold_jobs_in_build = False
-        self.worker.release()
-        self.waitUntilSettled()
-
-    def test_multi_branch(self):
-        self.worker.hold_jobs_in_build = True
-        projects = ['org/project1', 'org/project2',
-                    'org/project3', 'org/project4']
-
-        self.create_branch('org/project2', 'stable/havana')
-        self.create_branch('org/project4', 'stable/havana')
-        A = self.fake_gerrit.addFakeChange('org/project1', 'master', 'A')
-        B = self.fake_gerrit.addFakeChange('org/project2', 'stable/havana',
-                                           'B')
-        C = self.fake_gerrit.addFakeChange('org/project3', 'master', 'C')
-        A.addApproval('CRVW', 2)
-        B.addApproval('CRVW', 2)
-        C.addApproval('CRVW', 2)
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
-        self.fake_gerrit.addEvent(C.addApproval('APRV', 1))
-
-        self.waitUntilSettled()
-
-        self.assertEquals(3, len(self.builds), "Three builds are running")
-
-        upstream = self.getUpstreamRepos(projects)
-        states = [
-            {'org/project1': self.builds[0].parameters['ZUUL_COMMIT'],
-             'org/project2': str(upstream['org/project2'].commit('master')),
-             'org/project3': str(upstream['org/project3'].commit('master')),
-             'org/project4': str(upstream['org/project4'].
-                                 commit('master')),
-             },
-            {'org/project1': self.builds[0].parameters['ZUUL_COMMIT'],
-             'org/project2': self.builds[1].parameters['ZUUL_COMMIT'],
-             'org/project3': str(upstream['org/project3'].commit('master')),
-             'org/project4': str(upstream['org/project4'].
-                                 commit('stable/havana')),
-             },
-            {'org/project1': self.builds[0].parameters['ZUUL_COMMIT'],
-             'org/project2': str(upstream['org/project2'].commit('master')),
-             'org/project3': self.builds[2].parameters['ZUUL_COMMIT'],
-             'org/project4': str(upstream['org/project4'].
-                                 commit('master')),
-             },
-        ]
-
-        for number, build in enumerate(self.builds):
-            self.log.debug("Build parameters: %s", build.parameters)
-            cloner = zuul.lib.cloner.Cloner(
-                git_base_url=self.upstream_root,
-                projects=projects,
-                workspace=self.workspace_root,
-                zuul_branch=build.parameters['ZUUL_BRANCH'],
-                zuul_ref=build.parameters['ZUUL_REF'],
-                zuul_url=self.git_root,
-            )
-            cloner.execute()
-            work = self.getWorkspaceRepos(projects)
-            state = states[number]
-
-            for project in projects:
-                self.assertEquals(state[project],
-                                  str(work[project].commit('HEAD')),
-                                  'Project %s commit for build %s should '
-                                  'be correct' % (project, number))
-            shutil.rmtree(self.workspace_root)
-
-        self.worker.hold_jobs_in_build = False
-        self.worker.release()
-        self.waitUntilSettled()
-
-    def test_upgrade(self):
-        # Simulates an upgrade test
-        self.worker.hold_jobs_in_build = True
-        projects = ['org/project1', 'org/project2', 'org/project3',
-                    'org/project4', 'org/project5', 'org/project6']
-
-        self.create_branch('org/project2', 'stable/havana')
-        self.create_branch('org/project3', 'stable/havana')
-        self.create_branch('org/project4', 'stable/havana')
-        self.create_branch('org/project5', 'stable/havana')
-        A = self.fake_gerrit.addFakeChange('org/project1', 'master', 'A')
-        B = self.fake_gerrit.addFakeChange('org/project2', 'master', 'B')
-        C = self.fake_gerrit.addFakeChange('org/project3', 'stable/havana',
-                                           'C')
-        D = self.fake_gerrit.addFakeChange('org/project3', 'master', 'D')
-        E = self.fake_gerrit.addFakeChange('org/project4', 'stable/havana',
-                                           'E')
-        A.addApproval('CRVW', 2)
-        B.addApproval('CRVW', 2)
-        C.addApproval('CRVW', 2)
-        D.addApproval('CRVW', 2)
-        E.addApproval('CRVW', 2)
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
-        self.fake_gerrit.addEvent(C.addApproval('APRV', 1))
-        self.fake_gerrit.addEvent(D.addApproval('APRV', 1))
-        self.fake_gerrit.addEvent(E.addApproval('APRV', 1))
-
-        self.waitUntilSettled()
-
-        self.assertEquals(5, len(self.builds), "Five builds are running")
-
-        # Check the old side of the upgrade first
-        upstream = self.getUpstreamRepos(projects)
-        states = [
-            {'org/project1': self.builds[0].parameters['ZUUL_COMMIT'],
-             'org/project2': str(upstream['org/project2'].commit(
-                                 'stable/havana')),
-             'org/project3': str(upstream['org/project3'].commit(
-                                 'stable/havana')),
-             'org/project4': str(upstream['org/project4'].commit(
-                                 'stable/havana')),
-             'org/project5': str(upstream['org/project5'].commit(
-                                 'stable/havana')),
-             'org/project6': str(upstream['org/project6'].commit('master')),
-             },
-            {'org/project1': self.builds[0].parameters['ZUUL_COMMIT'],
-             'org/project2': str(upstream['org/project2'].commit(
-                                 'stable/havana')),
-             'org/project3': str(upstream['org/project3'].commit(
-                                 'stable/havana')),
-             'org/project4': str(upstream['org/project4'].commit(
-                                 'stable/havana')),
-             'org/project5': str(upstream['org/project5'].commit(
-                                 'stable/havana')),
-             'org/project6': str(upstream['org/project6'].commit('master')),
-             },
-            {'org/project1': self.builds[0].parameters['ZUUL_COMMIT'],
-             'org/project2': str(upstream['org/project2'].commit(
-                                 'stable/havana')),
-             'org/project3': self.builds[2].parameters['ZUUL_COMMIT'],
-             'org/project4': str(upstream['org/project4'].commit(
-                                 'stable/havana')),
-
-             'org/project5': str(upstream['org/project5'].commit(
-                                 'stable/havana')),
-             'org/project6': str(upstream['org/project6'].commit('master')),
-             },
-            {'org/project1': self.builds[0].parameters['ZUUL_COMMIT'],
-             'org/project2': str(upstream['org/project2'].commit(
-                                 'stable/havana')),
-             'org/project3': self.builds[2].parameters['ZUUL_COMMIT'],
-             'org/project4': str(upstream['org/project4'].commit(
-                                 'stable/havana')),
-             'org/project5': str(upstream['org/project5'].commit(
-                                 'stable/havana')),
-             'org/project6': str(upstream['org/project6'].commit('master')),
-             },
-            {'org/project1': self.builds[0].parameters['ZUUL_COMMIT'],
-             'org/project2': str(upstream['org/project2'].commit(
-                                 'stable/havana')),
-             'org/project3': self.builds[2].parameters['ZUUL_COMMIT'],
-             'org/project4': self.builds[4].parameters['ZUUL_COMMIT'],
-             'org/project5': str(upstream['org/project5'].commit(
-                                 'stable/havana')),
-             'org/project6': str(upstream['org/project6'].commit('master')),
-             },
-        ]
-
-        for number, build in enumerate(self.builds):
-            self.log.debug("Build parameters: %s", build.parameters)
-            cloner = zuul.lib.cloner.Cloner(
-                git_base_url=self.upstream_root,
-                projects=projects,
-                workspace=self.workspace_root,
-                zuul_branch=build.parameters['ZUUL_BRANCH'],
-                zuul_ref=build.parameters['ZUUL_REF'],
-                zuul_url=self.git_root,
-                branch='stable/havana',  # Old branch for upgrade
-            )
-            cloner.execute()
-            work = self.getWorkspaceRepos(projects)
-            state = states[number]
-
-            for project in projects:
-                self.assertEquals(state[project],
-                                  str(work[project].commit('HEAD')),
-                                  'Project %s commit for build %s should '
-                                  'be correct on old side of upgrade' %
-                                  (project, number))
-            shutil.rmtree(self.workspace_root)
-
-        # Check the new side of the upgrade
-        states = [
-            {'org/project1': self.builds[0].parameters['ZUUL_COMMIT'],
-             'org/project2': str(upstream['org/project2'].commit('master')),
-             'org/project3': str(upstream['org/project3'].commit('master')),
-             'org/project4': str(upstream['org/project4'].commit('master')),
-             'org/project5': str(upstream['org/project5'].commit('master')),
-             'org/project6': str(upstream['org/project6'].commit('master')),
-             },
-            {'org/project1': self.builds[0].parameters['ZUUL_COMMIT'],
-             'org/project2': self.builds[1].parameters['ZUUL_COMMIT'],
-             'org/project3': str(upstream['org/project3'].commit('master')),
-             'org/project4': str(upstream['org/project4'].commit('master')),
-             'org/project5': str(upstream['org/project5'].commit('master')),
-             'org/project6': str(upstream['org/project6'].commit('master')),
-             },
-            {'org/project1': self.builds[0].parameters['ZUUL_COMMIT'],
-             'org/project2': self.builds[1].parameters['ZUUL_COMMIT'],
-             'org/project3': str(upstream['org/project3'].commit('master')),
-             'org/project4': str(upstream['org/project4'].commit('master')),
-             'org/project5': str(upstream['org/project5'].commit('master')),
-             'org/project6': str(upstream['org/project6'].commit('master')),
-             },
-            {'org/project1': self.builds[0].parameters['ZUUL_COMMIT'],
-             'org/project2': self.builds[1].parameters['ZUUL_COMMIT'],
-             'org/project3': self.builds[3].parameters['ZUUL_COMMIT'],
-             'org/project4': str(upstream['org/project4'].commit('master')),
-             'org/project5': str(upstream['org/project5'].commit('master')),
-             'org/project6': str(upstream['org/project6'].commit('master')),
-             },
-            {'org/project1': self.builds[0].parameters['ZUUL_COMMIT'],
-             'org/project2': self.builds[1].parameters['ZUUL_COMMIT'],
-             'org/project3': self.builds[3].parameters['ZUUL_COMMIT'],
-             'org/project4': str(upstream['org/project4'].commit('master')),
-             'org/project5': str(upstream['org/project5'].commit('master')),
-             'org/project6': str(upstream['org/project6'].commit('master')),
-             },
-        ]
-
-        for number, build in enumerate(self.builds):
-            self.log.debug("Build parameters: %s", build.parameters)
-            cloner = zuul.lib.cloner.Cloner(
-                git_base_url=self.upstream_root,
-                projects=projects,
-                workspace=self.workspace_root,
-                zuul_branch=build.parameters['ZUUL_BRANCH'],
-                zuul_ref=build.parameters['ZUUL_REF'],
-                zuul_url=self.git_root,
-                branch='master',  # New branch for upgrade
-            )
-            cloner.execute()
-            work = self.getWorkspaceRepos(projects)
-            state = states[number]
-
-            for project in projects:
-                self.assertEquals(state[project],
-                                  str(work[project].commit('HEAD')),
-                                  'Project %s commit for build %s should '
-                                  'be correct on old side of upgrade' %
-                                  (project, number))
-            shutil.rmtree(self.workspace_root)
-
-        self.worker.hold_jobs_in_build = False
-        self.worker.release()
-        self.waitUntilSettled()
-
-    def test_project_override(self):
-        self.worker.hold_jobs_in_build = True
-        projects = ['org/project1', 'org/project2', 'org/project3',
-                    'org/project4', 'org/project5', 'org/project6']
-
-        self.create_branch('org/project3', 'stable/havana')
-        self.create_branch('org/project4', 'stable/havana')
-        self.create_branch('org/project6', 'stable/havana')
-        A = self.fake_gerrit.addFakeChange('org/project1', 'master', 'A')
-        B = self.fake_gerrit.addFakeChange('org/project1', 'master', 'B')
-        C = self.fake_gerrit.addFakeChange('org/project2', 'master', 'C')
-        D = self.fake_gerrit.addFakeChange('org/project3', 'stable/havana',
-                                           'D')
-        A.addApproval('CRVW', 2)
-        B.addApproval('CRVW', 2)
-        C.addApproval('CRVW', 2)
-        D.addApproval('CRVW', 2)
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
-        self.fake_gerrit.addEvent(C.addApproval('APRV', 1))
-        self.fake_gerrit.addEvent(D.addApproval('APRV', 1))
-
-        self.waitUntilSettled()
-
-        self.assertEquals(4, len(self.builds), "Four builds are running")
-
-        upstream = self.getUpstreamRepos(projects)
-        states = [
-            {'org/project1': self.builds[0].parameters['ZUUL_COMMIT'],
-             'org/project2': str(upstream['org/project2'].commit('master')),
-             'org/project3': str(upstream['org/project3'].commit('master')),
-             'org/project4': str(upstream['org/project4'].commit('master')),
-             'org/project5': str(upstream['org/project5'].commit('master')),
-             'org/project6': str(upstream['org/project6'].commit('master')),
-             },
-            {'org/project1': self.builds[1].parameters['ZUUL_COMMIT'],
-             'org/project2': str(upstream['org/project2'].commit('master')),
-             'org/project3': str(upstream['org/project3'].commit('master')),
-             'org/project4': str(upstream['org/project4'].commit('master')),
-             'org/project5': str(upstream['org/project5'].commit('master')),
-             'org/project6': str(upstream['org/project6'].commit('master')),
-             },
-            {'org/project1': self.builds[1].parameters['ZUUL_COMMIT'],
-             'org/project2': self.builds[2].parameters['ZUUL_COMMIT'],
-             'org/project3': str(upstream['org/project3'].commit('master')),
-             'org/project4': str(upstream['org/project4'].commit('master')),
-             'org/project5': str(upstream['org/project5'].commit('master')),
-             'org/project6': str(upstream['org/project6'].commit('master')),
-             },
-            {'org/project1': self.builds[1].parameters['ZUUL_COMMIT'],
-             'org/project2': self.builds[2].parameters['ZUUL_COMMIT'],
-             'org/project3': self.builds[3].parameters['ZUUL_COMMIT'],
-             'org/project4': str(upstream['org/project4'].commit('master')),
-             'org/project5': str(upstream['org/project5'].commit('master')),
-             'org/project6': str(upstream['org/project6'].commit(
-                                 'stable/havana')),
-             },
-        ]
-
-        for number, build in enumerate(self.builds):
-            self.log.debug("Build parameters: %s", build.parameters)
-            cloner = zuul.lib.cloner.Cloner(
-                git_base_url=self.upstream_root,
-                projects=projects,
-                workspace=self.workspace_root,
-                zuul_branch=build.parameters['ZUUL_BRANCH'],
-                zuul_ref=build.parameters['ZUUL_REF'],
-                zuul_url=self.git_root,
-                project_branches={'org/project4': 'master'},
-            )
-            cloner.execute()
-            work = self.getWorkspaceRepos(projects)
-            state = states[number]
-
-            for project in projects:
-                self.assertEquals(state[project],
-                                  str(work[project].commit('HEAD')),
-                                  'Project %s commit for build %s should '
-                                  'be correct' % (project, number))
-            shutil.rmtree(self.workspace_root)
-
-        self.worker.hold_jobs_in_build = False
-        self.worker.release()
-        self.waitUntilSettled()
-
-    def test_periodic(self):
-        self.worker.hold_jobs_in_build = True
-        self.create_branch('org/project', 'stable/havana')
-        self.config.set('zuul', 'layout_config',
-                        'tests/fixtures/layout-timer.yaml')
-        self.sched.reconfigure(self.config)
-        self.registerJobs()
-
-        # The pipeline triggers every second, so we should have seen
-        # several by now.
-        time.sleep(5)
-        self.waitUntilSettled()
-
-        builds = self.builds[:]
-
-        self.worker.hold_jobs_in_build = False
-        # Stop queuing timer triggered jobs so that the assertions
-        # below don't race against more jobs being queued.
-        self.config.set('zuul', 'layout_config',
-                        'tests/fixtures/layout-no-timer.yaml')
-        self.sched.reconfigure(self.config)
-        self.registerJobs()
-        self.worker.release()
-        self.waitUntilSettled()
-
-        projects = ['org/project']
-
-        self.assertEquals(2, len(builds), "Two builds are running")
-
-        upstream = self.getUpstreamRepos(projects)
-        states = [
-            {'org/project':
-                str(upstream['org/project'].commit('stable/havana')),
-             },
-            {'org/project':
-                str(upstream['org/project'].commit('stable/havana')),
-             },
-        ]
-
-        for number, build in enumerate(builds):
-            self.log.debug("Build parameters: %s", build.parameters)
-            cloner = zuul.lib.cloner.Cloner(
-                git_base_url=self.upstream_root,
-                projects=projects,
-                workspace=self.workspace_root,
-                zuul_branch=build.parameters.get('ZUUL_BRANCH', None),
-                zuul_ref=build.parameters.get('ZUUL_REF', None),
-                zuul_url=self.git_root,
-                branch='stable/havana',
-            )
-            cloner.execute()
-            work = self.getWorkspaceRepos(projects)
-            state = states[number]
-
-            for project in projects:
-                self.assertEquals(state[project],
-                                  str(work[project].commit('HEAD')),
-                                  'Project %s commit for build %s should '
-                                  'be correct' % (project, number))
-
-            shutil.rmtree(self.workspace_root)
-
-        self.worker.hold_jobs_in_build = False
-        self.worker.release()
-        self.waitUntilSettled()
+#!/usr/bin/env python2.7
+
+# Copyright 2012 Hewlett-Packard Development Company, L.P.
+# Copyright 2014 Wikimedia Foundation Inc.
+#
+# Licensed under the Apache License, Version 2.0 (the "License"); you may
+# not use this file except in compliance with the License. You may obtain
+# a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+# License for the specific language governing permissions and limitations
+# under the License.
+
+import fixtures
+import logging
+import os
+import shutil
+import time
+
+import git
+
+import zuul.lib.cloner
+
+from tests.base import ZuulTestCase
+
+logging.basicConfig(level=logging.DEBUG,
+                    format='%(asctime)s %(name)-32s '
+                    '%(levelname)-8s %(message)s')
+
+
+class TestCloner(ZuulTestCase):
+
+    log = logging.getLogger("zuul.test.cloner")
+    workspace_root = None
+
+    def setUp(self):
+        super(TestCloner, self).setUp()
+        self.workspace_root = os.path.join(self.test_root, 'workspace')
+
+        self.config.set('zuul', 'layout_config',
+                        'tests/fixtures/layout-cloner.yaml')
+        self.sched.reconfigure(self.config)
+        self.registerJobs()
+
+    def getWorkspaceRepos(self, projects):
+        repos = {}
+        for project in projects:
+            repos[project] = git.Repo(
+                os.path.join(self.workspace_root, project))
+        return repos
+
+    def getUpstreamRepos(self, projects):
+        repos = {}
+        for project in projects:
+            repos[project] = git.Repo(
+                os.path.join(self.upstream_root, project))
+        return repos
+
+    def test_cache_dir(self):
+        projects = ['org/project1', 'org/project2']
+        cache_root = os.path.join(self.test_root, "cache")
+        for project in projects:
+            upstream_repo_path = os.path.join(self.upstream_root, project)
+            cache_repo_path = os.path.join(cache_root, project)
+            git.Repo.clone_from(upstream_repo_path, cache_repo_path)
+
+        self.worker.hold_jobs_in_build = True
+        A = self.fake_gerrit.addFakeChange('org/project1', 'master', 'A')
+        A.addApproval('CRVW', 2)
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+
+        self.waitUntilSettled()
+
+        self.assertEquals(1, len(self.builds), "One build is running")
+
+        B = self.fake_gerrit.addFakeChange('org/project2', 'master', 'B')
+        B.setMerged()
+
+        upstream = self.getUpstreamRepos(projects)
+        states = [{
+            'org/project1': self.builds[0].parameters['ZUUL_COMMIT'],
+            'org/project2': str(upstream['org/project2'].commit('master')),
+        }]
+
+        for number, build in enumerate(self.builds):
+            self.log.debug("Build parameters: %s", build.parameters)
+            cloner = zuul.lib.cloner.Cloner(
+                git_base_url=self.upstream_root,
+                projects=projects,
+                workspace=self.workspace_root,
+                zuul_branch=build.parameters['ZUUL_BRANCH'],
+                zuul_ref=build.parameters['ZUUL_REF'],
+                zuul_url=self.git_root,
+                cache_dir=cache_root,
+            )
+            cloner.execute()
+            work = self.getWorkspaceRepos(projects)
+            state = states[number]
+
+            for project in projects:
+                self.assertEquals(state[project],
+                                  str(work[project].commit('HEAD')),
+                                  'Project %s commit for build %s should '
+                                  'be correct' % (project, number))
+
+        work = self.getWorkspaceRepos(projects)
+        upstream_repo_path = os.path.join(self.upstream_root, 'org/project1')
+        self.assertEquals(
+            work['org/project1'].remotes.origin.url,
+            upstream_repo_path,
+            'workspace repo origin should be upstream, not cache'
+        )
+
+        self.worker.hold_jobs_in_build = False
+        self.worker.release()
+        self.waitUntilSettled()
+
+    def test_recognize_bare_cache(self):
+        cache_root = os.path.join(self.test_root, "cache")
+        upstream_repo_path = os.path.join(self.upstream_root, 'org/project1')
+        cache_bare_path = os.path.join(cache_root, 'org/project1.git')
+        cache_repo = git.Repo.clone_from(upstream_repo_path, cache_bare_path,
+                                         bare=True)
+        self.assertTrue(type(cache_repo.bare), msg='Cache repo is bare')
+
+        log_fixture = self.useFixture(fixtures.FakeLogger(level=logging.INFO))
+        cloner = zuul.lib.cloner.Cloner(
+            git_base_url=self.upstream_root,
+            projects=['org/project1'],
+            workspace=self.workspace_root,
+            zuul_branch='HEAD',
+            zuul_ref='HEAD',
+            zuul_url=self.git_root,
+            cache_dir=cache_root
+        )
+        cloner.execute()
+        self.assertIn('Creating repo org/project1 from cache file://%s' % (
+                      cache_bare_path), log_fixture.output)
+
+    def test_one_branch(self):
+        self.worker.hold_jobs_in_build = True
+
+        projects = ['org/project1', 'org/project2']
+        A = self.fake_gerrit.addFakeChange('org/project1', 'master', 'A')
+        B = self.fake_gerrit.addFakeChange('org/project2', 'master', 'B')
+        A.addApproval('CRVW', 2)
+        B.addApproval('CRVW', 2)
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
+
+        self.waitUntilSettled()
+
+        self.assertEquals(2, len(self.builds), "Two builds are running")
+
+        upstream = self.getUpstreamRepos(projects)
+        states = [
+            {'org/project1': self.builds[0].parameters['ZUUL_COMMIT'],
+             'org/project2': str(upstream['org/project2'].commit('master')),
+             },
+            {'org/project1': self.builds[0].parameters['ZUUL_COMMIT'],
+             'org/project2': self.builds[1].parameters['ZUUL_COMMIT'],
+             },
+        ]
+
+        for number, build in enumerate(self.builds):
+            self.log.debug("Build parameters: %s", build.parameters)
+            cloner = zuul.lib.cloner.Cloner(
+                git_base_url=self.upstream_root,
+                projects=projects,
+                workspace=self.workspace_root,
+                zuul_branch=build.parameters['ZUUL_BRANCH'],
+                zuul_ref=build.parameters['ZUUL_REF'],
+                zuul_url=self.git_root,
+            )
+            cloner.execute()
+            work = self.getWorkspaceRepos(projects)
+            state = states[number]
+
+            for project in projects:
+                self.assertEquals(state[project],
+                                  str(work[project].commit('HEAD')),
+                                  'Project %s commit for build %s should '
+                                  'be correct' % (project, number))
+
+            shutil.rmtree(self.workspace_root)
+
+        self.worker.hold_jobs_in_build = False
+        self.worker.release()
+        self.waitUntilSettled()
+
+    def test_multi_branch(self):
+        self.worker.hold_jobs_in_build = True
+        projects = ['org/project1', 'org/project2',
+                    'org/project3', 'org/project4']
+
+        self.create_branch('org/project2', 'stable/havana')
+        self.create_branch('org/project4', 'stable/havana')
+        A = self.fake_gerrit.addFakeChange('org/project1', 'master', 'A')
+        B = self.fake_gerrit.addFakeChange('org/project2', 'stable/havana',
+                                           'B')
+        C = self.fake_gerrit.addFakeChange('org/project3', 'master', 'C')
+        A.addApproval('CRVW', 2)
+        B.addApproval('CRVW', 2)
+        C.addApproval('CRVW', 2)
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
+        self.fake_gerrit.addEvent(C.addApproval('APRV', 1))
+
+        self.waitUntilSettled()
+
+        self.assertEquals(3, len(self.builds), "Three builds are running")
+
+        upstream = self.getUpstreamRepos(projects)
+        states = [
+            {'org/project1': self.builds[0].parameters['ZUUL_COMMIT'],
+             'org/project2': str(upstream['org/project2'].commit('master')),
+             'org/project3': str(upstream['org/project3'].commit('master')),
+             'org/project4': str(upstream['org/project4'].
+                                 commit('master')),
+             },
+            {'org/project1': self.builds[0].parameters['ZUUL_COMMIT'],
+             'org/project2': self.builds[1].parameters['ZUUL_COMMIT'],
+             'org/project3': str(upstream['org/project3'].commit('master')),
+             'org/project4': str(upstream['org/project4'].
+                                 commit('stable/havana')),
+             },
+            {'org/project1': self.builds[0].parameters['ZUUL_COMMIT'],
+             'org/project2': str(upstream['org/project2'].commit('master')),
+             'org/project3': self.builds[2].parameters['ZUUL_COMMIT'],
+             'org/project4': str(upstream['org/project4'].
+                                 commit('master')),
+             },
+        ]
+
+        for number, build in enumerate(self.builds):
+            self.log.debug("Build parameters: %s", build.parameters)
+            cloner = zuul.lib.cloner.Cloner(
+                git_base_url=self.upstream_root,
+                projects=projects,
+                workspace=self.workspace_root,
+                zuul_branch=build.parameters['ZUUL_BRANCH'],
+                zuul_ref=build.parameters['ZUUL_REF'],
+                zuul_url=self.git_root,
+            )
+            cloner.execute()
+            work = self.getWorkspaceRepos(projects)
+            state = states[number]
+
+            for project in projects:
+                self.assertEquals(state[project],
+                                  str(work[project].commit('HEAD')),
+                                  'Project %s commit for build %s should '
+                                  'be correct' % (project, number))
+            shutil.rmtree(self.workspace_root)
+
+        self.worker.hold_jobs_in_build = False
+        self.worker.release()
+        self.waitUntilSettled()
+
+    def test_upgrade(self):
+        # Simulates an upgrade test
+        self.worker.hold_jobs_in_build = True
+        projects = ['org/project1', 'org/project2', 'org/project3',
+                    'org/project4', 'org/project5', 'org/project6']
+
+        self.create_branch('org/project2', 'stable/havana')
+        self.create_branch('org/project3', 'stable/havana')
+        self.create_branch('org/project4', 'stable/havana')
+        self.create_branch('org/project5', 'stable/havana')
+        A = self.fake_gerrit.addFakeChange('org/project1', 'master', 'A')
+        B = self.fake_gerrit.addFakeChange('org/project2', 'master', 'B')
+        C = self.fake_gerrit.addFakeChange('org/project3', 'stable/havana',
+                                           'C')
+        D = self.fake_gerrit.addFakeChange('org/project3', 'master', 'D')
+        E = self.fake_gerrit.addFakeChange('org/project4', 'stable/havana',
+                                           'E')
+        A.addApproval('CRVW', 2)
+        B.addApproval('CRVW', 2)
+        C.addApproval('CRVW', 2)
+        D.addApproval('CRVW', 2)
+        E.addApproval('CRVW', 2)
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
+        self.fake_gerrit.addEvent(C.addApproval('APRV', 1))
+        self.fake_gerrit.addEvent(D.addApproval('APRV', 1))
+        self.fake_gerrit.addEvent(E.addApproval('APRV', 1))
+
+        self.waitUntilSettled()
+
+        self.assertEquals(5, len(self.builds), "Five builds are running")
+
+        # Check the old side of the upgrade first
+        upstream = self.getUpstreamRepos(projects)
+        states = [
+            {'org/project1': self.builds[0].parameters['ZUUL_COMMIT'],
+             'org/project2': str(upstream['org/project2'].commit(
+                                 'stable/havana')),
+             'org/project3': str(upstream['org/project3'].commit(
+                                 'stable/havana')),
+             'org/project4': str(upstream['org/project4'].commit(
+                                 'stable/havana')),
+             'org/project5': str(upstream['org/project5'].commit(
+                                 'stable/havana')),
+             'org/project6': str(upstream['org/project6'].commit('master')),
+             },
+            {'org/project1': self.builds[0].parameters['ZUUL_COMMIT'],
+             'org/project2': str(upstream['org/project2'].commit(
+                                 'stable/havana')),
+             'org/project3': str(upstream['org/project3'].commit(
+                                 'stable/havana')),
+             'org/project4': str(upstream['org/project4'].commit(
+                                 'stable/havana')),
+             'org/project5': str(upstream['org/project5'].commit(
+                                 'stable/havana')),
+             'org/project6': str(upstream['org/project6'].commit('master')),
+             },
+            {'org/project1': self.builds[0].parameters['ZUUL_COMMIT'],
+             'org/project2': str(upstream['org/project2'].commit(
+                                 'stable/havana')),
+             'org/project3': self.builds[2].parameters['ZUUL_COMMIT'],
+             'org/project4': str(upstream['org/project4'].commit(
+                                 'stable/havana')),
+
+             'org/project5': str(upstream['org/project5'].commit(
+                                 'stable/havana')),
+             'org/project6': str(upstream['org/project6'].commit('master')),
+             },
+            {'org/project1': self.builds[0].parameters['ZUUL_COMMIT'],
+             'org/project2': str(upstream['org/project2'].commit(
+                                 'stable/havana')),
+             'org/project3': self.builds[2].parameters['ZUUL_COMMIT'],
+             'org/project4': str(upstream['org/project4'].commit(
+                                 'stable/havana')),
+             'org/project5': str(upstream['org/project5'].commit(
+                                 'stable/havana')),
+             'org/project6': str(upstream['org/project6'].commit('master')),
+             },
+            {'org/project1': self.builds[0].parameters['ZUUL_COMMIT'],
+             'org/project2': str(upstream['org/project2'].commit(
+                                 'stable/havana')),
+             'org/project3': self.builds[2].parameters['ZUUL_COMMIT'],
+             'org/project4': self.builds[4].parameters['ZUUL_COMMIT'],
+             'org/project5': str(upstream['org/project5'].commit(
+                                 'stable/havana')),
+             'org/project6': str(upstream['org/project6'].commit('master')),
+             },
+        ]
+
+        for number, build in enumerate(self.builds):
+            self.log.debug("Build parameters: %s", build.parameters)
+            cloner = zuul.lib.cloner.Cloner(
+                git_base_url=self.upstream_root,
+                projects=projects,
+                workspace=self.workspace_root,
+                zuul_branch=build.parameters['ZUUL_BRANCH'],
+                zuul_ref=build.parameters['ZUUL_REF'],
+                zuul_url=self.git_root,
+                branch='stable/havana',  # Old branch for upgrade
+            )
+            cloner.execute()
+            work = self.getWorkspaceRepos(projects)
+            state = states[number]
+
+            for project in projects:
+                self.assertEquals(state[project],
+                                  str(work[project].commit('HEAD')),
+                                  'Project %s commit for build %s should '
+                                  'be correct on old side of upgrade' %
+                                  (project, number))
+            shutil.rmtree(self.workspace_root)
+
+        # Check the new side of the upgrade
+        states = [
+            {'org/project1': self.builds[0].parameters['ZUUL_COMMIT'],
+             'org/project2': str(upstream['org/project2'].commit('master')),
+             'org/project3': str(upstream['org/project3'].commit('master')),
+             'org/project4': str(upstream['org/project4'].commit('master')),
+             'org/project5': str(upstream['org/project5'].commit('master')),
+             'org/project6': str(upstream['org/project6'].commit('master')),
+             },
+            {'org/project1': self.builds[0].parameters['ZUUL_COMMIT'],
+             'org/project2': self.builds[1].parameters['ZUUL_COMMIT'],
+             'org/project3': str(upstream['org/project3'].commit('master')),
+             'org/project4': str(upstream['org/project4'].commit('master')),
+             'org/project5': str(upstream['org/project5'].commit('master')),
+             'org/project6': str(upstream['org/project6'].commit('master')),
+             },
+            {'org/project1': self.builds[0].parameters['ZUUL_COMMIT'],
+             'org/project2': self.builds[1].parameters['ZUUL_COMMIT'],
+             'org/project3': str(upstream['org/project3'].commit('master')),
+             'org/project4': str(upstream['org/project4'].commit('master')),
+             'org/project5': str(upstream['org/project5'].commit('master')),
+             'org/project6': str(upstream['org/project6'].commit('master')),
+             },
+            {'org/project1': self.builds[0].parameters['ZUUL_COMMIT'],
+             'org/project2': self.builds[1].parameters['ZUUL_COMMIT'],
+             'org/project3': self.builds[3].parameters['ZUUL_COMMIT'],
+             'org/project4': str(upstream['org/project4'].commit('master')),
+             'org/project5': str(upstream['org/project5'].commit('master')),
+             'org/project6': str(upstream['org/project6'].commit('master')),
+             },
+            {'org/project1': self.builds[0].parameters['ZUUL_COMMIT'],
+             'org/project2': self.builds[1].parameters['ZUUL_COMMIT'],
+             'org/project3': self.builds[3].parameters['ZUUL_COMMIT'],
+             'org/project4': str(upstream['org/project4'].commit('master')),
+             'org/project5': str(upstream['org/project5'].commit('master')),
+             'org/project6': str(upstream['org/project6'].commit('master')),
+             },
+        ]
+
+        for number, build in enumerate(self.builds):
+            self.log.debug("Build parameters: %s", build.parameters)
+            cloner = zuul.lib.cloner.Cloner(
+                git_base_url=self.upstream_root,
+                projects=projects,
+                workspace=self.workspace_root,
+                zuul_branch=build.parameters['ZUUL_BRANCH'],
+                zuul_ref=build.parameters['ZUUL_REF'],
+                zuul_url=self.git_root,
+                branch='master',  # New branch for upgrade
+            )
+            cloner.execute()
+            work = self.getWorkspaceRepos(projects)
+            state = states[number]
+
+            for project in projects:
+                self.assertEquals(state[project],
+                                  str(work[project].commit('HEAD')),
+                                  'Project %s commit for build %s should '
+                                  'be correct on old side of upgrade' %
+                                  (project, number))
+            shutil.rmtree(self.workspace_root)
+
+        self.worker.hold_jobs_in_build = False
+        self.worker.release()
+        self.waitUntilSettled()
+
+    def test_project_override(self):
+        self.worker.hold_jobs_in_build = True
+        projects = ['org/project1', 'org/project2', 'org/project3',
+                    'org/project4', 'org/project5', 'org/project6']
+
+        self.create_branch('org/project3', 'stable/havana')
+        self.create_branch('org/project4', 'stable/havana')
+        self.create_branch('org/project6', 'stable/havana')
+        A = self.fake_gerrit.addFakeChange('org/project1', 'master', 'A')
+        B = self.fake_gerrit.addFakeChange('org/project1', 'master', 'B')
+        C = self.fake_gerrit.addFakeChange('org/project2', 'master', 'C')
+        D = self.fake_gerrit.addFakeChange('org/project3', 'stable/havana',
+                                           'D')
+        A.addApproval('CRVW', 2)
+        B.addApproval('CRVW', 2)
+        C.addApproval('CRVW', 2)
+        D.addApproval('CRVW', 2)
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
+        self.fake_gerrit.addEvent(C.addApproval('APRV', 1))
+        self.fake_gerrit.addEvent(D.addApproval('APRV', 1))
+
+        self.waitUntilSettled()
+
+        self.assertEquals(4, len(self.builds), "Four builds are running")
+
+        upstream = self.getUpstreamRepos(projects)
+        states = [
+            {'org/project1': self.builds[0].parameters['ZUUL_COMMIT'],
+             'org/project2': str(upstream['org/project2'].commit('master')),
+             'org/project3': str(upstream['org/project3'].commit('master')),
+             'org/project4': str(upstream['org/project4'].commit('master')),
+             'org/project5': str(upstream['org/project5'].commit('master')),
+             'org/project6': str(upstream['org/project6'].commit('master')),
+             },
+            {'org/project1': self.builds[1].parameters['ZUUL_COMMIT'],
+             'org/project2': str(upstream['org/project2'].commit('master')),
+             'org/project3': str(upstream['org/project3'].commit('master')),
+             'org/project4': str(upstream['org/project4'].commit('master')),
+             'org/project5': str(upstream['org/project5'].commit('master')),
+             'org/project6': str(upstream['org/project6'].commit('master')),
+             },
+            {'org/project1': self.builds[1].parameters['ZUUL_COMMIT'],
+             'org/project2': self.builds[2].parameters['ZUUL_COMMIT'],
+             'org/project3': str(upstream['org/project3'].commit('master')),
+             'org/project4': str(upstream['org/project4'].commit('master')),
+             'org/project5': str(upstream['org/project5'].commit('master')),
+             'org/project6': str(upstream['org/project6'].commit('master')),
+             },
+            {'org/project1': self.builds[1].parameters['ZUUL_COMMIT'],
+             'org/project2': self.builds[2].parameters['ZUUL_COMMIT'],
+             'org/project3': self.builds[3].parameters['ZUUL_COMMIT'],
+             'org/project4': str(upstream['org/project4'].commit('master')),
+             'org/project5': str(upstream['org/project5'].commit('master')),
+             'org/project6': str(upstream['org/project6'].commit(
+                                 'stable/havana')),
+             },
+        ]
+
+        for number, build in enumerate(self.builds):
+            self.log.debug("Build parameters: %s", build.parameters)
+            cloner = zuul.lib.cloner.Cloner(
+                git_base_url=self.upstream_root,
+                projects=projects,
+                workspace=self.workspace_root,
+                zuul_branch=build.parameters['ZUUL_BRANCH'],
+                zuul_ref=build.parameters['ZUUL_REF'],
+                zuul_url=self.git_root,
+                project_branches={'org/project4': 'master'},
+            )
+            cloner.execute()
+            work = self.getWorkspaceRepos(projects)
+            state = states[number]
+
+            for project in projects:
+                self.assertEquals(state[project],
+                                  str(work[project].commit('HEAD')),
+                                  'Project %s commit for build %s should '
+                                  'be correct' % (project, number))
+            shutil.rmtree(self.workspace_root)
+
+        self.worker.hold_jobs_in_build = False
+        self.worker.release()
+        self.waitUntilSettled()
+
+    def test_periodic(self):
+        self.worker.hold_jobs_in_build = True
+        self.create_branch('org/project', 'stable/havana')
+        self.config.set('zuul', 'layout_config',
+                        'tests/fixtures/layout-timer.yaml')
+        self.sched.reconfigure(self.config)
+        self.registerJobs()
+
+        # The pipeline triggers every second, so we should have seen
+        # several by now.
+        time.sleep(5)
+        self.waitUntilSettled()
+
+        builds = self.builds[:]
+
+        self.worker.hold_jobs_in_build = False
+        # Stop queuing timer triggered jobs so that the assertions
+        # below don't race against more jobs being queued.
+        self.config.set('zuul', 'layout_config',
+                        'tests/fixtures/layout-no-timer.yaml')
+        self.sched.reconfigure(self.config)
+        self.registerJobs()
+        self.worker.release()
+        self.waitUntilSettled()
+
+        projects = ['org/project']
+
+        self.assertEquals(2, len(builds), "Two builds are running")
+
+        upstream = self.getUpstreamRepos(projects)
+        states = [
+            {'org/project':
+                str(upstream['org/project'].commit('stable/havana')),
+             },
+            {'org/project':
+                str(upstream['org/project'].commit('stable/havana')),
+             },
+        ]
+
+        for number, build in enumerate(builds):
+            self.log.debug("Build parameters: %s", build.parameters)
+            cloner = zuul.lib.cloner.Cloner(
+                git_base_url=self.upstream_root,
+                projects=projects,
+                workspace=self.workspace_root,
+                zuul_branch=build.parameters.get('ZUUL_BRANCH', None),
+                zuul_ref=build.parameters.get('ZUUL_REF', None),
+                zuul_url=self.git_root,
+                branch='stable/havana',
+            )
+            cloner.execute()
+            work = self.getWorkspaceRepos(projects)
+            state = states[number]
+
+            for project in projects:
+                self.assertEquals(state[project],
+                                  str(work[project].commit('HEAD')),
+                                  'Project %s commit for build %s should '
+                                  'be correct' % (project, number))
+
+            shutil.rmtree(self.workspace_root)
+
+        self.worker.hold_jobs_in_build = False
+        self.worker.release()
+        self.waitUntilSettled()
diff --git a/tests/test_connection.py b/tests/test_connection.py
index c3458ac..7eb08e8 100644
--- a/tests/test_connection.py
+++ b/tests/test_connection.py
@@ -1,85 +1,85 @@
-# Copyright 2014 Rackspace Australia
-#
-# Licensed under the Apache License, Version 2.0 (the "License"); you may
-# not use this file except in compliance with the License. You may obtain
-# a copy of the License at
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-# License for the specific language governing permissions and limitations
-# under the License.
-
-import logging
-import testtools
-
-import zuul.connection.gerrit
-
-from tests.base import ZuulTestCase
-
-
-class TestGerritConnection(testtools.TestCase):
-    log = logging.getLogger("zuul.test_connection")
-
-    def test_driver_name(self):
-        self.assertEqual('gerrit',
-                         zuul.connection.gerrit.GerritConnection.driver_name)
-
-
-class TestConnections(ZuulTestCase):
-    def setup_config(self, config_file='zuul-connections-same-gerrit.conf'):
-        super(TestConnections, self).setup_config(config_file)
-
-    def test_multiple_connections(self):
-        "Test multiple connections to the one gerrit"
-
-        A = self.fake_review_gerrit.addFakeChange('org/project', 'master', 'A')
-        self.fake_review_gerrit.addEvent(A.getPatchsetCreatedEvent(1))
-
-        self.waitUntilSettled()
-
-        self.assertEqual(len(A.patchsets[-1]['approvals']), 1)
-        self.assertEqual(A.patchsets[-1]['approvals'][0]['type'], 'VRFY')
-        self.assertEqual(A.patchsets[-1]['approvals'][0]['value'], '1')
-        self.assertEqual(A.patchsets[-1]['approvals'][0]['by']['username'],
-                         'jenkins')
-
-        B = self.fake_review_gerrit.addFakeChange('org/project', 'master', 'B')
-        self.worker.addFailTest('project-test2', B)
-        self.fake_review_gerrit.addEvent(B.getPatchsetCreatedEvent(1))
-
-        self.waitUntilSettled()
-
-        self.assertEqual(len(B.patchsets[-1]['approvals']), 1)
-        self.assertEqual(B.patchsets[-1]['approvals'][0]['type'], 'VRFY')
-        self.assertEqual(B.patchsets[-1]['approvals'][0]['value'], '-1')
-        self.assertEqual(B.patchsets[-1]['approvals'][0]['by']['username'],
-                         'civoter')
-
-
-class TestMultipleGerrits(ZuulTestCase):
-    def setup_config(self,
-                     config_file='zuul-connections-multiple-gerrits.conf'):
-        super(TestMultipleGerrits, self).setup_config(config_file)
-        self.config.set(
-            'zuul', 'layout_config',
-            'layout-connections-multiple-gerrits.yaml')
-
-    def test_multiple_project_separate_gerrits(self):
-        self.worker.hold_jobs_in_build = True
-
-        A = self.fake_another_gerrit.addFakeChange(
-            'org/project', 'master', 'A')
-        self.fake_another_gerrit.addEvent(A.getPatchsetCreatedEvent(1))
-
-        self.waitUntilSettled()
-
-        self.assertEqual(1, len(self.builds))
-        self.assertEqual('project-another-gerrit', self.builds[0].name)
-        self.assertTrue(self.job_has_changes(self.builds[0], A))
-
-        self.worker.hold_jobs_in_build = False
-        self.worker.release()
-        self.waitUntilSettled()
+# Copyright 2014 Rackspace Australia
+#
+# Licensed under the Apache License, Version 2.0 (the "License"); you may
+# not use this file except in compliance with the License. You may obtain
+# a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+# License for the specific language governing permissions and limitations
+# under the License.
+
+import logging
+import testtools
+
+import zuul.connection.gerrit
+
+from tests.base import ZuulTestCase
+
+
+class TestGerritConnection(testtools.TestCase):
+    log = logging.getLogger("zuul.test_connection")
+
+    def test_driver_name(self):
+        self.assertEqual('gerrit',
+                         zuul.connection.gerrit.GerritConnection.driver_name)
+
+
+class TestConnections(ZuulTestCase):
+    def setup_config(self, config_file='zuul-connections-same-gerrit.conf'):
+        super(TestConnections, self).setup_config(config_file)
+
+    def test_multiple_connections(self):
+        "Test multiple connections to the one gerrit"
+
+        A = self.fake_review_gerrit.addFakeChange('org/project', 'master', 'A')
+        self.fake_review_gerrit.addEvent(A.getPatchsetCreatedEvent(1))
+
+        self.waitUntilSettled()
+
+        self.assertEqual(len(A.patchsets[-1]['approvals']), 1)
+        self.assertEqual(A.patchsets[-1]['approvals'][0]['type'], 'VRFY')
+        self.assertEqual(A.patchsets[-1]['approvals'][0]['value'], '1')
+        self.assertEqual(A.patchsets[-1]['approvals'][0]['by']['username'],
+                         'jenkins')
+
+        B = self.fake_review_gerrit.addFakeChange('org/project', 'master', 'B')
+        self.worker.addFailTest('project-test2', B)
+        self.fake_review_gerrit.addEvent(B.getPatchsetCreatedEvent(1))
+
+        self.waitUntilSettled()
+
+        self.assertEqual(len(B.patchsets[-1]['approvals']), 1)
+        self.assertEqual(B.patchsets[-1]['approvals'][0]['type'], 'VRFY')
+        self.assertEqual(B.patchsets[-1]['approvals'][0]['value'], '-1')
+        self.assertEqual(B.patchsets[-1]['approvals'][0]['by']['username'],
+                         'civoter')
+
+
+class TestMultipleGerrits(ZuulTestCase):
+    def setup_config(self,
+                     config_file='zuul-connections-multiple-gerrits.conf'):
+        super(TestMultipleGerrits, self).setup_config(config_file)
+        self.config.set(
+            'zuul', 'layout_config',
+            'layout-connections-multiple-gerrits.yaml')
+
+    def test_multiple_project_separate_gerrits(self):
+        self.worker.hold_jobs_in_build = True
+
+        A = self.fake_another_gerrit.addFakeChange(
+            'org/project', 'master', 'A')
+        self.fake_another_gerrit.addEvent(A.getPatchsetCreatedEvent(1))
+
+        self.waitUntilSettled()
+
+        self.assertEqual(1, len(self.builds))
+        self.assertEqual('project-another-gerrit', self.builds[0].name)
+        self.assertTrue(self.job_has_changes(self.builds[0], A))
+
+        self.worker.hold_jobs_in_build = False
+        self.worker.release()
+        self.waitUntilSettled()
diff --git a/tests/test_daemon.py b/tests/test_daemon.py
index 689d4f7..7150bed 100644
--- a/tests/test_daemon.py
+++ b/tests/test_daemon.py
@@ -1,63 +1,63 @@
-#!/usr/bin/env python
-
-# Copyright 2014 Hewlett-Packard Development Company, L.P.
-#
-# Licensed under the Apache License, Version 2.0 (the "License"); you may
-# not use this file except in compliance with the License. You may obtain
-# a copy of the License at
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-# License for the specific language governing permissions and limitations
-# under the License.
-
-import daemon
-import logging
-import os
-import sys
-
-import extras
-import fixtures
-import testtools
-
-from tests.base import iterate_timeout
-
-# as of python-daemon 1.6 it doesn't bundle pidlockfile anymore
-# instead it depends on lockfile-0.9.1 which uses pidfile.
-pid_file_module = extras.try_imports(['daemon.pidlockfile', 'daemon.pidfile'])
-
-
-def daemon_test(pidfile, flagfile):
-    pid = pid_file_module.TimeoutPIDLockFile(pidfile, 10)
-    with daemon.DaemonContext(pidfile=pid):
-        for x in iterate_timeout(30, "flagfile to be removed"):
-            if not os.path.exists(flagfile):
-                break
-    sys.exit(0)
-
-
-class TestDaemon(testtools.TestCase):
-    log = logging.getLogger("zuul.test.daemon")
-
-    def setUp(self):
-        super(TestDaemon, self).setUp()
-        self.test_root = self.useFixture(fixtures.TempDir(
-            rootdir=os.environ.get("ZUUL_TEST_ROOT"))).path
-
-    def test_daemon(self):
-        pidfile = os.path.join(self.test_root, "daemon.pid")
-        flagfile = os.path.join(self.test_root, "daemon.flag")
-        open(flagfile, 'w').close()
-        if not os.fork():
-            self._cleanups = []
-            daemon_test(pidfile, flagfile)
-        for x in iterate_timeout(30, "daemon to start"):
-            if os.path.exists(pidfile):
-                break
-        os.unlink(flagfile)
-        for x in iterate_timeout(30, "daemon to stop"):
-            if not os.path.exists(pidfile):
-                break
+#!/usr/bin/env python
+
+# Copyright 2014 Hewlett-Packard Development Company, L.P.
+#
+# Licensed under the Apache License, Version 2.0 (the "License"); you may
+# not use this file except in compliance with the License. You may obtain
+# a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+# License for the specific language governing permissions and limitations
+# under the License.
+
+import daemon
+import logging
+import os
+import sys
+
+import extras
+import fixtures
+import testtools
+
+from tests.base import iterate_timeout
+
+# as of python-daemon 1.6 it doesn't bundle pidlockfile anymore
+# instead it depends on lockfile-0.9.1 which uses pidfile.
+pid_file_module = extras.try_imports(['daemon.pidlockfile', 'daemon.pidfile'])
+
+
+def daemon_test(pidfile, flagfile):
+    pid = pid_file_module.TimeoutPIDLockFile(pidfile, 10)
+    with daemon.DaemonContext(pidfile=pid):
+        for x in iterate_timeout(30, "flagfile to be removed"):
+            if not os.path.exists(flagfile):
+                break
+    sys.exit(0)
+
+
+class TestDaemon(testtools.TestCase):
+    log = logging.getLogger("zuul.test.daemon")
+
+    def setUp(self):
+        super(TestDaemon, self).setUp()
+        self.test_root = self.useFixture(fixtures.TempDir(
+            rootdir=os.environ.get("ZUUL_TEST_ROOT"))).path
+
+    def test_daemon(self):
+        pidfile = os.path.join(self.test_root, "daemon.pid")
+        flagfile = os.path.join(self.test_root, "daemon.flag")
+        open(flagfile, 'w').close()
+        if not os.fork():
+            self._cleanups = []
+            daemon_test(pidfile, flagfile)
+        for x in iterate_timeout(30, "daemon to start"):
+            if os.path.exists(pidfile):
+                break
+        os.unlink(flagfile)
+        for x in iterate_timeout(30, "daemon to stop"):
+            if not os.path.exists(pidfile):
+                break
diff --git a/tests/test_gerrit.py b/tests/test_gerrit.py
index 93ce122..c0c4db1 100644
--- a/tests/test_gerrit.py
+++ b/tests/test_gerrit.py
@@ -1,80 +1,80 @@
-#!/usr/bin/env python
-
-# Copyright 2015 BMW Car IT GmbH
-#
-# Licensed under the Apache License, Version 2.0 (the "License"); you may
-# not use this file except in compliance with the License. You may obtain
-# a copy of the License at
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-# License for the specific language governing permissions and limitations
-# under the License.
-import os
-
-try:
-    from unittest import mock
-except ImportError:
-    import mock
-
-from tests.base import BaseTestCase
-from zuul.connection.gerrit import GerritConnection
-
-FIXTURE_DIR = os.path.join(os.path.dirname(__file__), 'fixtures/gerrit')
-
-
-def read_fixture(file):
-    with open('%s/%s' % (FIXTURE_DIR, file), 'r') as fixturefile:
-        lines = fixturefile.readlines()
-        command = lines[0].replace('\n', '')
-        value = ''.join(lines[1:])
-        return command, value
-
-
-def read_fixtures(files):
-    calls = []
-    values = []
-    for fixture_file in files:
-        command, value = read_fixture(fixture_file)
-        calls.append(mock.call(command))
-        values.append([value, ''])
-    return calls, values
-
-
-class TestGerrit(BaseTestCase):
-
-    @mock.patch('zuul.connection.gerrit.GerritConnection._ssh')
-    def run_query(self, files, expected_patches, _ssh_mock):
-        gerrit_config = {
-            'user': 'gerrit',
-            'server': 'localhost',
-        }
-        gerrit = GerritConnection('review_gerrit', gerrit_config)
-
-        calls, values = read_fixtures(files)
-        _ssh_mock.side_effect = values
-
-        result = gerrit.simpleQuery('project:openstack-infra/zuul')
-
-        _ssh_mock.assert_has_calls(calls)
-        self.assertEquals(len(calls), _ssh_mock.call_count,
-                          '_ssh should be called %d times' % len(calls))
-        self.assertIsNotNone(result, 'Result is not none')
-        self.assertEquals(len(result), expected_patches,
-                          'There must be %d patches.' % expected_patches)
-
-    def test_simple_query_pagination_new(self):
-        files = ['simple_query_pagination_new_1',
-                 'simple_query_pagination_new_2']
-        expected_patches = 5
-        self.run_query(files, expected_patches)
-
-    def test_simple_query_pagination_old(self):
-        files = ['simple_query_pagination_old_1',
-                 'simple_query_pagination_old_2',
-                 'simple_query_pagination_old_3']
-        expected_patches = 5
-        self.run_query(files, expected_patches)
+#!/usr/bin/env python
+
+# Copyright 2015 BMW Car IT GmbH
+#
+# Licensed under the Apache License, Version 2.0 (the "License"); you may
+# not use this file except in compliance with the License. You may obtain
+# a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+# License for the specific language governing permissions and limitations
+# under the License.
+import os
+
+try:
+    from unittest import mock
+except ImportError:
+    import mock
+
+from tests.base import BaseTestCase
+from zuul.connection.gerrit import GerritConnection
+
+FIXTURE_DIR = os.path.join(os.path.dirname(__file__), 'fixtures/gerrit')
+
+
+def read_fixture(file):
+    with open('%s/%s' % (FIXTURE_DIR, file), 'r') as fixturefile:
+        lines = fixturefile.readlines()
+        command = lines[0].replace('\n', '')
+        value = ''.join(lines[1:])
+        return command, value
+
+
+def read_fixtures(files):
+    calls = []
+    values = []
+    for fixture_file in files:
+        command, value = read_fixture(fixture_file)
+        calls.append(mock.call(command))
+        values.append([value, ''])
+    return calls, values
+
+
+class TestGerrit(BaseTestCase):
+
+    @mock.patch('zuul.connection.gerrit.GerritConnection._ssh')
+    def run_query(self, files, expected_patches, _ssh_mock):
+        gerrit_config = {
+            'user': 'gerrit',
+            'server': 'localhost',
+        }
+        gerrit = GerritConnection('review_gerrit', gerrit_config)
+
+        calls, values = read_fixtures(files)
+        _ssh_mock.side_effect = values
+
+        result = gerrit.simpleQuery('project:openstack-infra/zuul')
+
+        _ssh_mock.assert_has_calls(calls)
+        self.assertEquals(len(calls), _ssh_mock.call_count,
+                          '_ssh should be called %d times' % len(calls))
+        self.assertIsNotNone(result, 'Result is not none')
+        self.assertEquals(len(result), expected_patches,
+                          'There must be %d patches.' % expected_patches)
+
+    def test_simple_query_pagination_new(self):
+        files = ['simple_query_pagination_new_1',
+                 'simple_query_pagination_new_2']
+        expected_patches = 5
+        self.run_query(files, expected_patches)
+
+    def test_simple_query_pagination_old(self):
+        files = ['simple_query_pagination_old_1',
+                 'simple_query_pagination_old_2',
+                 'simple_query_pagination_old_3']
+        expected_patches = 5
+        self.run_query(files, expected_patches)
diff --git a/tests/test_layoutvalidator.py b/tests/test_layoutvalidator.py
index 99732a5..1a12a99 100644
--- a/tests/test_layoutvalidator.py
+++ b/tests/test_layoutvalidator.py
@@ -1,78 +1,78 @@
-#!/usr/bin/env python2.7
-
-# Copyright 2013 OpenStack Foundation
-#
-# Licensed under the Apache License, Version 2.0 (the "License"); you may
-# not use this file except in compliance with the License. You may obtain
-# a copy of the License at
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-# License for the specific language governing permissions and limitations
-# under the License.
-
-import ConfigParser
-import os
-import re
-
-import testtools
-import voluptuous
-import yaml
-
-import zuul.layoutvalidator
-import zuul.lib.connections
-
-FIXTURE_DIR = os.path.join(os.path.dirname(__file__),
-                           'fixtures')
-LAYOUT_RE = re.compile(r'^(good|bad)_.*\.yaml$')
-
-
-class TestLayoutValidator(testtools.TestCase):
-    def test_layouts(self):
-        """Test layout file validation"""
-        print
-        errors = []
-        for fn in os.listdir(os.path.join(FIXTURE_DIR, 'layouts')):
-            m = LAYOUT_RE.match(fn)
-            if not m:
-                continue
-            print fn
-
-            # Load any .conf file by the same name but .conf extension.
-            config_file = ("%s.conf" %
-                           os.path.join(FIXTURE_DIR, 'layouts',
-                                        fn.split('.yaml')[0]))
-            if not os.path.isfile(config_file):
-                config_file = os.path.join(FIXTURE_DIR, 'layouts',
-                                           'zuul_default.conf')
-            config = ConfigParser.ConfigParser()
-            config.read(config_file)
-            connections = zuul.lib.connections.configure_connections(config)
-
-            layout = os.path.join(FIXTURE_DIR, 'layouts', fn)
-            data = yaml.load(open(layout))
-            validator = zuul.layoutvalidator.LayoutValidator()
-            if m.group(1) == 'good':
-                try:
-                    validator.validate(data, connections)
-                except voluptuous.Invalid as e:
-                    raise Exception(
-                        'Unexpected YAML syntax error in %s:\n  %s' %
-                        (fn, str(e)))
-            else:
-                try:
-                    validator.validate(data, connections)
-                    raise Exception("Expected a YAML syntax error in %s." %
-                                    fn)
-                except voluptuous.Invalid as e:
-                    error = str(e)
-                    print '  ', error
-                    if error in errors:
-                        raise Exception("Error has already been tested: %s" %
-                                        error)
-                    else:
-                        errors.append(error)
-                    pass
+#!/usr/bin/env python2.7
+
+# Copyright 2013 OpenStack Foundation
+#
+# Licensed under the Apache License, Version 2.0 (the "License"); you may
+# not use this file except in compliance with the License. You may obtain
+# a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+# License for the specific language governing permissions and limitations
+# under the License.
+
+import ConfigParser
+import os
+import re
+
+import testtools
+import voluptuous
+import yaml
+
+import zuul.layoutvalidator
+import zuul.lib.connections
+
+FIXTURE_DIR = os.path.join(os.path.dirname(__file__),
+                           'fixtures')
+LAYOUT_RE = re.compile(r'^(good|bad)_.*\.yaml$')
+
+
+class TestLayoutValidator(testtools.TestCase):
+    def test_layouts(self):
+        """Test layout file validation"""
+        print
+        errors = []
+        for fn in os.listdir(os.path.join(FIXTURE_DIR, 'layouts')):
+            m = LAYOUT_RE.match(fn)
+            if not m:
+                continue
+            print fn
+
+            # Load any .conf file by the same name but .conf extension.
+            config_file = ("%s.conf" %
+                           os.path.join(FIXTURE_DIR, 'layouts',
+                                        fn.split('.yaml')[0]))
+            if not os.path.isfile(config_file):
+                config_file = os.path.join(FIXTURE_DIR, 'layouts',
+                                           'zuul_default.conf')
+            config = ConfigParser.ConfigParser()
+            config.read(config_file)
+            connections = zuul.lib.connections.configure_connections(config)
+
+            layout = os.path.join(FIXTURE_DIR, 'layouts', fn)
+            data = yaml.load(open(layout))
+            validator = zuul.layoutvalidator.LayoutValidator()
+            if m.group(1) == 'good':
+                try:
+                    validator.validate(data, connections)
+                except voluptuous.Invalid as e:
+                    raise Exception(
+                        'Unexpected YAML syntax error in %s:\n  %s' %
+                        (fn, str(e)))
+            else:
+                try:
+                    validator.validate(data, connections)
+                    raise Exception("Expected a YAML syntax error in %s." %
+                                    fn)
+                except voluptuous.Invalid as e:
+                    error = str(e)
+                    print '  ', error
+                    if error in errors:
+                        raise Exception("Error has already been tested: %s" %
+                                        error)
+                    else:
+                        errors.append(error)
+                    pass
diff --git a/tests/test_merger_repo.py b/tests/test_merger_repo.py
index 55d16a0..8b57101 100644
--- a/tests/test_merger_repo.py
+++ b/tests/test_merger_repo.py
@@ -1,79 +1,79 @@
-#!/usr/bin/env python2.7
-
-# Copyright 2012 Hewlett-Packard Development Company, L.P.
-# Copyright 2014 Wikimedia Foundation Inc.
-#
-# Licensed under the Apache License, Version 2.0 (the "License"); you may
-# not use this file except in compliance with the License. You may obtain
-# a copy of the License at
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-# License for the specific language governing permissions and limitations
-# under the License.
-
-import logging
-import os
-
-import git
-
-from zuul.merger.merger import Repo
-from tests.base import ZuulTestCase
-
-logging.basicConfig(level=logging.DEBUG,
-                    format='%(asctime)s %(name)-32s '
-                    '%(levelname)-8s %(message)s')
-
-
-class TestMergerRepo(ZuulTestCase):
-
-    log = logging.getLogger("zuul.test.merger.repo")
-    workspace_root = None
-
-    def setUp(self):
-        super(TestMergerRepo, self).setUp()
-        self.workspace_root = os.path.join(self.test_root, 'workspace')
-
-    def test_ensure_cloned(self):
-        parent_path = os.path.join(self.upstream_root, 'org/project1')
-
-        # Forge a repo having a submodule
-        parent_repo = git.Repo(parent_path)
-        parent_repo.git.submodule('add', os.path.join(
-            self.upstream_root, 'org/project2'), 'subdir')
-        parent_repo.index.commit('Adding project2 as a submodule in subdir')
-        # git 1.7.8 changed .git from being a directory to a file pointing
-        # to the parent repository /.git/modules/*
-        self.assertTrue(os.path.exists(
-            os.path.join(parent_path, 'subdir', '.git')),
-            msg='.git file in submodule should be a file')
-
-        work_repo = Repo(parent_path, self.workspace_root,
-                         'none@example.org', 'User Name')
-        self.assertTrue(
-            os.path.isdir(os.path.join(self.workspace_root, 'subdir')),
-            msg='Cloned repository has a submodule placeholder directory')
-        self.assertFalse(os.path.exists(
-            os.path.join(self.workspace_root, 'subdir', '.git')),
-            msg='Submodule is not initialized')
-
-        sub_repo = Repo(
-            os.path.join(self.upstream_root, 'org/project2'),
-            os.path.join(self.workspace_root, 'subdir'),
-            'none@example.org', 'User Name')
-        self.assertTrue(os.path.exists(
-            os.path.join(self.workspace_root, 'subdir', '.git')),
-            msg='Cloned over the submodule placeholder')
-
-        self.assertEquals(
-            os.path.join(self.upstream_root, 'org/project1'),
-            work_repo.createRepoObject().remotes[0].url,
-            message="Parent clone still point to upstream project1")
-
-        self.assertEquals(
-            os.path.join(self.upstream_root, 'org/project2'),
-            sub_repo.createRepoObject().remotes[0].url,
-            message="Sub repository points to upstream project2")
+#!/usr/bin/env python2.7
+
+# Copyright 2012 Hewlett-Packard Development Company, L.P.
+# Copyright 2014 Wikimedia Foundation Inc.
+#
+# Licensed under the Apache License, Version 2.0 (the "License"); you may
+# not use this file except in compliance with the License. You may obtain
+# a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+# License for the specific language governing permissions and limitations
+# under the License.
+
+import logging
+import os
+
+import git
+
+from zuul.merger.merger import Repo
+from tests.base import ZuulTestCase
+
+logging.basicConfig(level=logging.DEBUG,
+                    format='%(asctime)s %(name)-32s '
+                    '%(levelname)-8s %(message)s')
+
+
+class TestMergerRepo(ZuulTestCase):
+
+    log = logging.getLogger("zuul.test.merger.repo")
+    workspace_root = None
+
+    def setUp(self):
+        super(TestMergerRepo, self).setUp()
+        self.workspace_root = os.path.join(self.test_root, 'workspace')
+
+    def test_ensure_cloned(self):
+        parent_path = os.path.join(self.upstream_root, 'org/project1')
+
+        # Forge a repo having a submodule
+        parent_repo = git.Repo(parent_path)
+        parent_repo.git.submodule('add', os.path.join(
+            self.upstream_root, 'org/project2'), 'subdir')
+        parent_repo.index.commit('Adding project2 as a submodule in subdir')
+        # git 1.7.8 changed .git from being a directory to a file pointing
+        # to the parent repository /.git/modules/*
+        self.assertTrue(os.path.exists(
+            os.path.join(parent_path, 'subdir', '.git')),
+            msg='.git file in submodule should be a file')
+
+        work_repo = Repo(parent_path, self.workspace_root,
+                         'none@example.org', 'User Name')
+        self.assertTrue(
+            os.path.isdir(os.path.join(self.workspace_root, 'subdir')),
+            msg='Cloned repository has a submodule placeholder directory')
+        self.assertFalse(os.path.exists(
+            os.path.join(self.workspace_root, 'subdir', '.git')),
+            msg='Submodule is not initialized')
+
+        sub_repo = Repo(
+            os.path.join(self.upstream_root, 'org/project2'),
+            os.path.join(self.workspace_root, 'subdir'),
+            'none@example.org', 'User Name')
+        self.assertTrue(os.path.exists(
+            os.path.join(self.workspace_root, 'subdir', '.git')),
+            msg='Cloned over the submodule placeholder')
+
+        self.assertEquals(
+            os.path.join(self.upstream_root, 'org/project1'),
+            work_repo.createRepoObject().remotes[0].url,
+            message="Parent clone still point to upstream project1")
+
+        self.assertEquals(
+            os.path.join(self.upstream_root, 'org/project2'),
+            sub_repo.createRepoObject().remotes[0].url,
+            message="Sub repository points to upstream project2")
diff --git a/tests/test_model.py b/tests/test_model.py
index 2711618..5add9f4 100644
--- a/tests/test_model.py
+++ b/tests/test_model.py
@@ -1,64 +1,64 @@
-# Copyright 2015 Red Hat, Inc.
-#
-# Licensed under the Apache License, Version 2.0 (the "License"); you may
-# not use this file except in compliance with the License. You may obtain
-# a copy of the License at
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-# License for the specific language governing permissions and limitations
-# under the License.
-
-from zuul import change_matcher as cm
-from zuul import model
-
-from tests.base import BaseTestCase
-
-
-class TestJob(BaseTestCase):
-
-    @property
-    def job(self):
-        job = model.Job('job')
-        job.skip_if_matcher = cm.MatchAll([
-            cm.ProjectMatcher('^project$'),
-            cm.MatchAllFiles([cm.FileMatcher('^docs/.*$')]),
-        ])
-        return job
-
-    def test_change_matches_returns_false_for_matched_skip_if(self):
-        change = model.Change('project')
-        change.files = ['docs/foo']
-        self.assertFalse(self.job.changeMatches(change))
-
-    def test_change_matches_returns_true_for_unmatched_skip_if(self):
-        change = model.Change('project')
-        change.files = ['foo']
-        self.assertTrue(self.job.changeMatches(change))
-
-    def test_copy_retains_skip_if(self):
-        job = model.Job('job')
-        job.copy(self.job)
-        self.assertTrue(job.skip_if_matcher)
-
-    def _assert_job_booleans_are_not_none(self, job):
-        self.assertIsNotNone(job.voting)
-        self.assertIsNotNone(job.hold_following_changes)
-
-    def test_job_sets_defaults_for_boolean_attributes(self):
-        job = model.Job('job')
-        self._assert_job_booleans_are_not_none(job)
-
-    def test_metajob_does_not_set_defaults_for_boolean_attributes(self):
-        job = model.Job('^job')
-        self.assertIsNone(job.voting)
-        self.assertIsNone(job.hold_following_changes)
-
-    def test_metajob_copy_does_not_set_undefined_boolean_attributes(self):
-        job = model.Job('job')
-        metajob = model.Job('^job')
-        job.copy(metajob)
-        self._assert_job_booleans_are_not_none(job)
+# Copyright 2015 Red Hat, Inc.
+#
+# Licensed under the Apache License, Version 2.0 (the "License"); you may
+# not use this file except in compliance with the License. You may obtain
+# a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+# License for the specific language governing permissions and limitations
+# under the License.
+
+from zuul import change_matcher as cm
+from zuul import model
+
+from tests.base import BaseTestCase
+
+
+class TestJob(BaseTestCase):
+
+    @property
+    def job(self):
+        job = model.Job('job')
+        job.skip_if_matcher = cm.MatchAll([
+            cm.ProjectMatcher('^project$'),
+            cm.MatchAllFiles([cm.FileMatcher('^docs/.*$')]),
+        ])
+        return job
+
+    def test_change_matches_returns_false_for_matched_skip_if(self):
+        change = model.Change('project')
+        change.files = ['docs/foo']
+        self.assertFalse(self.job.changeMatches(change))
+
+    def test_change_matches_returns_true_for_unmatched_skip_if(self):
+        change = model.Change('project')
+        change.files = ['foo']
+        self.assertTrue(self.job.changeMatches(change))
+
+    def test_copy_retains_skip_if(self):
+        job = model.Job('job')
+        job.copy(self.job)
+        self.assertTrue(job.skip_if_matcher)
+
+    def _assert_job_booleans_are_not_none(self, job):
+        self.assertIsNotNone(job.voting)
+        self.assertIsNotNone(job.hold_following_changes)
+
+    def test_job_sets_defaults_for_boolean_attributes(self):
+        job = model.Job('job')
+        self._assert_job_booleans_are_not_none(job)
+
+    def test_metajob_does_not_set_defaults_for_boolean_attributes(self):
+        job = model.Job('^job')
+        self.assertIsNone(job.voting)
+        self.assertIsNone(job.hold_following_changes)
+
+    def test_metajob_copy_does_not_set_undefined_boolean_attributes(self):
+        job = model.Job('job')
+        metajob = model.Job('^job')
+        job.copy(metajob)
+        self._assert_job_booleans_are_not_none(job)
diff --git a/tests/test_reporter.py b/tests/test_reporter.py
index 8d3090a..f00d171 100644
--- a/tests/test_reporter.py
+++ b/tests/test_reporter.py
@@ -1,46 +1,46 @@
-# Copyright 2014 Rackspace Australia
-#
-# Licensed under the Apache License, Version 2.0 (the "License"); you may
-# not use this file except in compliance with the License. You may obtain
-# a copy of the License at
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-# License for the specific language governing permissions and limitations
-# under the License.
-
-import logging
-import testtools
-
-import zuul.reporter
-
-
-class TestSMTPReporter(testtools.TestCase):
-    log = logging.getLogger("zuul.test_reporter")
-
-    def setUp(self):
-        super(TestSMTPReporter, self).setUp()
-
-    def test_reporter_abc(self):
-        # We only need to instantiate a class for this
-        reporter = zuul.reporter.smtp.SMTPReporter({})  # noqa
-
-    def test_reporter_name(self):
-        self.assertEqual('smtp', zuul.reporter.smtp.SMTPReporter.name)
-
-
-class TestGerritReporter(testtools.TestCase):
-    log = logging.getLogger("zuul.test_reporter")
-
-    def setUp(self):
-        super(TestGerritReporter, self).setUp()
-
-    def test_reporter_abc(self):
-        # We only need to instantiate a class for this
-        reporter = zuul.reporter.gerrit.GerritReporter(None)  # noqa
-
-    def test_reporter_name(self):
-        self.assertEqual('gerrit', zuul.reporter.gerrit.GerritReporter.name)
+# Copyright 2014 Rackspace Australia
+#
+# Licensed under the Apache License, Version 2.0 (the "License"); you may
+# not use this file except in compliance with the License. You may obtain
+# a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+# License for the specific language governing permissions and limitations
+# under the License.
+
+import logging
+import testtools
+
+import zuul.reporter
+
+
+class TestSMTPReporter(testtools.TestCase):
+    log = logging.getLogger("zuul.test_reporter")
+
+    def setUp(self):
+        super(TestSMTPReporter, self).setUp()
+
+    def test_reporter_abc(self):
+        # We only need to instantiate a class for this
+        reporter = zuul.reporter.smtp.SMTPReporter({})  # noqa
+
+    def test_reporter_name(self):
+        self.assertEqual('smtp', zuul.reporter.smtp.SMTPReporter.name)
+
+
+class TestGerritReporter(testtools.TestCase):
+    log = logging.getLogger("zuul.test_reporter")
+
+    def setUp(self):
+        super(TestGerritReporter, self).setUp()
+
+    def test_reporter_abc(self):
+        # We only need to instantiate a class for this
+        reporter = zuul.reporter.gerrit.GerritReporter(None)  # noqa
+
+    def test_reporter_name(self):
+        self.assertEqual('gerrit', zuul.reporter.gerrit.GerritReporter.name)
diff --git a/tests/test_requirements.py b/tests/test_requirements.py
index 202010c..f3e8fdb 100644
--- a/tests/test_requirements.py
+++ b/tests/test_requirements.py
@@ -1,427 +1,427 @@
-#!/usr/bin/env python2.7
-
-# Copyright 2012-2014 Hewlett-Packard Development Company, L.P.
-#
-# Licensed under the Apache License, Version 2.0 (the "License"); you may
-# not use this file except in compliance with the License. You may obtain
-# a copy of the License at
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-# License for the specific language governing permissions and limitations
-# under the License.
-
-import logging
-import time
-
-from tests.base import ZuulTestCase
-
-logging.basicConfig(level=logging.DEBUG,
-                    format='%(asctime)s %(name)-32s '
-                    '%(levelname)-8s %(message)s')
-
-
-class TestRequirements(ZuulTestCase):
-    """Test pipeline and trigger requirements"""
-
-    def test_pipeline_require_approval_newer_than(self):
-        "Test pipeline requirement: approval newer than"
-        return self._test_require_approval_newer_than('org/project1',
-                                                      'project1-pipeline')
-
-    def test_trigger_require_approval_newer_than(self):
-        "Test trigger requirement: approval newer than"
-        return self._test_require_approval_newer_than('org/project2',
-                                                      'project2-trigger')
-
-    def _test_require_approval_newer_than(self, project, job):
-        self.config.set('zuul', 'layout_config',
-                        'tests/fixtures/layout-requirement-newer-than.yaml')
-        self.sched.reconfigure(self.config)
-        self.registerJobs()
-
-        A = self.fake_gerrit.addFakeChange(project, 'master', 'A')
-        # A comment event that we will keep submitting to trigger
-        comment = A.addApproval('CRVW', 2, username='nobody')
-        self.fake_gerrit.addEvent(comment)
-        self.waitUntilSettled()
-        # No +1 from Jenkins so should not be enqueued
-        self.assertEqual(len(self.history), 0)
-
-        # Add a too-old +1, should not be enqueued
-        A.addApproval('VRFY', 1, username='jenkins',
-                      granted_on=time.time() - 72 * 60 * 60)
-        self.fake_gerrit.addEvent(comment)
-        self.waitUntilSettled()
-        self.assertEqual(len(self.history), 0)
-
-        # Add a recent +1
-        self.fake_gerrit.addEvent(A.addApproval('VRFY', 1, username='jenkins'))
-        self.fake_gerrit.addEvent(comment)
-        self.waitUntilSettled()
-        self.assertEqual(len(self.history), 1)
-        self.assertEqual(self.history[0].name, job)
-
-    def test_pipeline_require_approval_older_than(self):
-        "Test pipeline requirement: approval older than"
-        return self._test_require_approval_older_than('org/project1',
-                                                      'project1-pipeline')
-
-    def test_trigger_require_approval_older_than(self):
-        "Test trigger requirement: approval older than"
-        return self._test_require_approval_older_than('org/project2',
-                                                      'project2-trigger')
-
-    def _test_require_approval_older_than(self, project, job):
-        self.config.set('zuul', 'layout_config',
-                        'tests/fixtures/layout-requirement-older-than.yaml')
-        self.sched.reconfigure(self.config)
-        self.registerJobs()
-
-        A = self.fake_gerrit.addFakeChange(project, 'master', 'A')
-        # A comment event that we will keep submitting to trigger
-        comment = A.addApproval('CRVW', 2, username='nobody')
-        self.fake_gerrit.addEvent(comment)
-        self.waitUntilSettled()
-        # No +1 from Jenkins so should not be enqueued
-        self.assertEqual(len(self.history), 0)
-
-        # Add a recent +1 which should not be enqueued
-        A.addApproval('VRFY', 1)
-        self.fake_gerrit.addEvent(comment)
-        self.waitUntilSettled()
-        self.assertEqual(len(self.history), 0)
-
-        # Add an old +1 which should be enqueued
-        A.addApproval('VRFY', 1, username='jenkins',
-                      granted_on=time.time() - 72 * 60 * 60)
-        self.fake_gerrit.addEvent(comment)
-        self.waitUntilSettled()
-        self.assertEqual(len(self.history), 1)
-        self.assertEqual(self.history[0].name, job)
-
-    def test_pipeline_require_approval_username(self):
-        "Test pipeline requirement: approval username"
-        return self._test_require_approval_username('org/project1',
-                                                    'project1-pipeline')
-
-    def test_trigger_require_approval_username(self):
-        "Test trigger requirement: approval username"
-        return self._test_require_approval_username('org/project2',
-                                                    'project2-trigger')
-
-    def _test_require_approval_username(self, project, job):
-        self.config.set('zuul', 'layout_config',
-                        'tests/fixtures/layout-requirement-username.yaml')
-        self.sched.reconfigure(self.config)
-        self.registerJobs()
-
-        A = self.fake_gerrit.addFakeChange(project, 'master', 'A')
-        # A comment event that we will keep submitting to trigger
-        comment = A.addApproval('CRVW', 2, username='nobody')
-        self.fake_gerrit.addEvent(comment)
-        self.waitUntilSettled()
-        # No approval from Jenkins so should not be enqueued
-        self.assertEqual(len(self.history), 0)
-
-        # Add an approval from Jenkins
-        A.addApproval('VRFY', 1, username='jenkins')
-        self.fake_gerrit.addEvent(comment)
-        self.waitUntilSettled()
-        self.assertEqual(len(self.history), 1)
-        self.assertEqual(self.history[0].name, job)
-
-    def test_pipeline_require_approval_email(self):
-        "Test pipeline requirement: approval email"
-        return self._test_require_approval_email('org/project1',
-                                                 'project1-pipeline')
-
-    def test_trigger_require_approval_email(self):
-        "Test trigger requirement: approval email"
-        return self._test_require_approval_email('org/project2',
-                                                 'project2-trigger')
-
-    def _test_require_approval_email(self, project, job):
-        self.config.set('zuul', 'layout_config',
-                        'tests/fixtures/layout-requirement-email.yaml')
-        self.sched.reconfigure(self.config)
-        self.registerJobs()
-
-        A = self.fake_gerrit.addFakeChange(project, 'master', 'A')
-        # A comment event that we will keep submitting to trigger
-        comment = A.addApproval('CRVW', 2, username='nobody')
-        self.fake_gerrit.addEvent(comment)
-        self.waitUntilSettled()
-        # No approval from Jenkins so should not be enqueued
-        self.assertEqual(len(self.history), 0)
-
-        # Add an approval from Jenkins
-        A.addApproval('VRFY', 1, username='jenkins')
-        self.fake_gerrit.addEvent(comment)
-        self.waitUntilSettled()
-        self.assertEqual(len(self.history), 1)
-        self.assertEqual(self.history[0].name, job)
-
-    def test_pipeline_require_approval_vote1(self):
-        "Test pipeline requirement: approval vote with one value"
-        return self._test_require_approval_vote1('org/project1',
-                                                 'project1-pipeline')
-
-    def test_trigger_require_approval_vote1(self):
-        "Test trigger requirement: approval vote with one value"
-        return self._test_require_approval_vote1('org/project2',
-                                                 'project2-trigger')
-
-    def _test_require_approval_vote1(self, project, job):
-        self.config.set('zuul', 'layout_config',
-                        'tests/fixtures/layout-requirement-vote1.yaml')
-        self.sched.reconfigure(self.config)
-        self.registerJobs()
-
-        A = self.fake_gerrit.addFakeChange(project, 'master', 'A')
-        # A comment event that we will keep submitting to trigger
-        comment = A.addApproval('CRVW', 2, username='nobody')
-        self.fake_gerrit.addEvent(comment)
-        self.waitUntilSettled()
-        # No approval from Jenkins so should not be enqueued
-        self.assertEqual(len(self.history), 0)
-
-        # A -1 from jenkins should not cause it to be enqueued
-        A.addApproval('VRFY', -1, username='jenkins')
-        self.fake_gerrit.addEvent(comment)
-        self.waitUntilSettled()
-        self.assertEqual(len(self.history), 0)
-
-        # A +1 should allow it to be enqueued
-        A.addApproval('VRFY', 1, username='jenkins')
-        self.fake_gerrit.addEvent(comment)
-        self.waitUntilSettled()
-        self.assertEqual(len(self.history), 1)
-        self.assertEqual(self.history[0].name, job)
-
-    def test_pipeline_require_approval_vote2(self):
-        "Test pipeline requirement: approval vote with two values"
-        return self._test_require_approval_vote2('org/project1',
-                                                 'project1-pipeline')
-
-    def test_trigger_require_approval_vote2(self):
-        "Test trigger requirement: approval vote with two values"
-        return self._test_require_approval_vote2('org/project2',
-                                                 'project2-trigger')
-
-    def _test_require_approval_vote2(self, project, job):
-        self.config.set('zuul', 'layout_config',
-                        'tests/fixtures/layout-requirement-vote2.yaml')
-        self.sched.reconfigure(self.config)
-        self.registerJobs()
-
-        A = self.fake_gerrit.addFakeChange(project, 'master', 'A')
-        # A comment event that we will keep submitting to trigger
-        comment = A.addApproval('CRVW', 2, username='nobody')
-        self.fake_gerrit.addEvent(comment)
-        self.waitUntilSettled()
-        # No approval from Jenkins so should not be enqueued
-        self.assertEqual(len(self.history), 0)
-
-        # A -1 from jenkins should not cause it to be enqueued
-        A.addApproval('VRFY', -1, username='jenkins')
-        self.fake_gerrit.addEvent(comment)
-        self.waitUntilSettled()
-        self.assertEqual(len(self.history), 0)
-
-        # A -2 from jenkins should not cause it to be enqueued
-        A.addApproval('VRFY', -2, username='jenkins')
-        self.fake_gerrit.addEvent(comment)
-        self.waitUntilSettled()
-        self.assertEqual(len(self.history), 0)
-
-        # A +1 from jenkins should allow it to be enqueued
-        A.addApproval('VRFY', 1, username='jenkins')
-        self.fake_gerrit.addEvent(comment)
-        self.waitUntilSettled()
-        self.assertEqual(len(self.history), 1)
-        self.assertEqual(self.history[0].name, job)
-
-        # A +2 should allow it to be enqueued
-        B = self.fake_gerrit.addFakeChange(project, 'master', 'B')
-        # A comment event that we will keep submitting to trigger
-        comment = B.addApproval('CRVW', 2, username='nobody')
-        self.fake_gerrit.addEvent(comment)
-        self.waitUntilSettled()
-        self.assertEqual(len(self.history), 1)
-
-        B.addApproval('VRFY', 2, username='jenkins')
-        self.fake_gerrit.addEvent(comment)
-        self.waitUntilSettled()
-        self.assertEqual(len(self.history), 2)
-        self.assertEqual(self.history[1].name, job)
-
-    def test_pipeline_require_current_patchset(self):
-        "Test pipeline requirement: current-patchset"
-        self.config.set('zuul', 'layout_config',
-                        'tests/fixtures/layout-requirement-'
-                        'current-patchset.yaml')
-        self.sched.reconfigure(self.config)
-        self.registerJobs()
-        # Create two patchsets and let their tests settle out. Then
-        # comment on first patchset and check that no additional
-        # jobs are run.
-        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
-        self.fake_gerrit.addEvent(A.addApproval('CRVW', 1))
-        self.waitUntilSettled()
-        A.addPatchset()
-        self.fake_gerrit.addEvent(A.addApproval('CRVW', 1))
-        self.waitUntilSettled()
-
-        self.assertEqual(len(self.history), 2)  # one job for each ps
-        self.fake_gerrit.addEvent(A.getChangeCommentEvent(1))
-        self.waitUntilSettled()
-
-        # Assert no new jobs ran after event for old patchset.
-        self.assertEqual(len(self.history), 2)
-
-        # Make sure the same event on a new PS will trigger
-        self.fake_gerrit.addEvent(A.getChangeCommentEvent(2))
-        self.waitUntilSettled()
-        self.assertEqual(len(self.history), 3)
-
-    def test_pipeline_require_open(self):
-        "Test pipeline requirement: open"
-        self.config.set('zuul', 'layout_config',
-                        'tests/fixtures/layout-requirement-open.yaml')
-        self.sched.reconfigure(self.config)
-        self.registerJobs()
-
-        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A',
-                                           status='MERGED')
-        self.fake_gerrit.addEvent(A.addApproval('CRVW', 2))
-        self.waitUntilSettled()
-        self.assertEqual(len(self.history), 0)
-
-        B = self.fake_gerrit.addFakeChange('org/project', 'master', 'B')
-        self.fake_gerrit.addEvent(B.addApproval('CRVW', 2))
-        self.waitUntilSettled()
-        self.assertEqual(len(self.history), 1)
-
-    def test_pipeline_require_status(self):
-        "Test pipeline requirement: status"
-        self.config.set('zuul', 'layout_config',
-                        'tests/fixtures/layout-requirement-status.yaml')
-        self.sched.reconfigure(self.config)
-        self.registerJobs()
-
-        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A',
-                                           status='MERGED')
-        self.fake_gerrit.addEvent(A.addApproval('CRVW', 2))
-        self.waitUntilSettled()
-        self.assertEqual(len(self.history), 0)
-
-        B = self.fake_gerrit.addFakeChange('org/project', 'master', 'B')
-        self.fake_gerrit.addEvent(B.addApproval('CRVW', 2))
-        self.waitUntilSettled()
-        self.assertEqual(len(self.history), 1)
-
-    def _test_require_reject_username(self, project, job):
-        "Test negative username's match"
-        # Should only trigger if Jenkins hasn't voted.
-        self.config.set(
-            'zuul', 'layout_config',
-            'tests/fixtures/layout-requirement-reject-username.yaml')
-        self.sched.reconfigure(self.config)
-        self.registerJobs()
-
-        # add in a change with no comments
-        A = self.fake_gerrit.addFakeChange(project, 'master', 'A')
-        self.waitUntilSettled()
-        self.assertEqual(len(self.history), 0)
-
-        # add in a comment that will trigger
-        self.fake_gerrit.addEvent(A.addApproval('CRVW', 1,
-                                                username='reviewer'))
-        self.waitUntilSettled()
-        self.assertEqual(len(self.history), 1)
-        self.assertEqual(self.history[0].name, job)
-
-        # add in a comment from jenkins user which shouldn't trigger
-        self.fake_gerrit.addEvent(A.addApproval('VRFY', 1, username='jenkins'))
-        self.waitUntilSettled()
-        self.assertEqual(len(self.history), 1)
-
-        # Check future reviews also won't trigger as a 'jenkins' user has
-        # commented previously
-        self.fake_gerrit.addEvent(A.addApproval('CRVW', 1,
-                                                username='reviewer'))
-        self.waitUntilSettled()
-        self.assertEqual(len(self.history), 1)
-
-    def test_pipeline_reject_username(self):
-        "Test negative pipeline requirement: no comment from jenkins"
-        return self._test_require_reject_username('org/project1',
-                                                  'project1-pipeline')
-
-    def test_trigger_reject_username(self):
-        "Test negative trigger requirement: no comment from jenkins"
-        return self._test_require_reject_username('org/project2',
-                                                  'project2-trigger')
-
-    def _test_require_reject(self, project, job):
-        "Test no approval matches a reject param"
-        self.config.set(
-            'zuul', 'layout_config',
-            'tests/fixtures/layout-requirement-reject.yaml')
-        self.sched.reconfigure(self.config)
-        self.registerJobs()
-
-        A = self.fake_gerrit.addFakeChange(project, 'master', 'A')
-        self.waitUntilSettled()
-        self.assertEqual(len(self.history), 0)
-
-        # First positive vote should not queue until jenkins has +1'd
-        comment = A.addApproval('VRFY', 1, username='reviewer_a')
-        self.fake_gerrit.addEvent(comment)
-        self.waitUntilSettled()
-        self.assertEqual(len(self.history), 0)
-
-        # Jenkins should put in a +1 which will also queue
-        comment = A.addApproval('VRFY', 1, username='jenkins')
-        self.fake_gerrit.addEvent(comment)
-        self.waitUntilSettled()
-        self.assertEqual(len(self.history), 1)
-        self.assertEqual(self.history[0].name, job)
-
-        # Negative vote should not queue
-        comment = A.addApproval('VRFY', -1, username='reviewer_b')
-        self.fake_gerrit.addEvent(comment)
-        self.waitUntilSettled()
-        self.assertEqual(len(self.history), 1)
-
-        # Future approvals should do nothing
-        comment = A.addApproval('VRFY', 1, username='reviewer_c')
-        self.fake_gerrit.addEvent(comment)
-        self.waitUntilSettled()
-        self.assertEqual(len(self.history), 1)
-
-        # Change/update negative vote should queue
-        comment = A.addApproval('VRFY', 1, username='reviewer_b')
-        self.fake_gerrit.addEvent(comment)
-        self.waitUntilSettled()
-        self.assertEqual(len(self.history), 2)
-        self.assertEqual(self.history[1].name, job)
-
-        # Future approvals should also queue
-        comment = A.addApproval('VRFY', 1, username='reviewer_d')
-        self.fake_gerrit.addEvent(comment)
-        self.waitUntilSettled()
-        self.assertEqual(len(self.history), 3)
-        self.assertEqual(self.history[2].name, job)
-
-    def test_pipeline_require_reject(self):
-        "Test pipeline requirement: rejections absent"
-        return self._test_require_reject('org/project1', 'project1-pipeline')
-
-    def test_trigger_require_reject(self):
-        "Test trigger requirement: rejections absent"
-        return self._test_require_reject('org/project2', 'project2-trigger')
+#!/usr/bin/env python2.7
+
+# Copyright 2012-2014 Hewlett-Packard Development Company, L.P.
+#
+# Licensed under the Apache License, Version 2.0 (the "License"); you may
+# not use this file except in compliance with the License. You may obtain
+# a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+# License for the specific language governing permissions and limitations
+# under the License.
+
+import logging
+import time
+
+from tests.base import ZuulTestCase
+
+logging.basicConfig(level=logging.DEBUG,
+                    format='%(asctime)s %(name)-32s '
+                    '%(levelname)-8s %(message)s')
+
+
+class TestRequirements(ZuulTestCase):
+    """Test pipeline and trigger requirements"""
+
+    def test_pipeline_require_approval_newer_than(self):
+        "Test pipeline requirement: approval newer than"
+        return self._test_require_approval_newer_than('org/project1',
+                                                      'project1-pipeline')
+
+    def test_trigger_require_approval_newer_than(self):
+        "Test trigger requirement: approval newer than"
+        return self._test_require_approval_newer_than('org/project2',
+                                                      'project2-trigger')
+
+    def _test_require_approval_newer_than(self, project, job):
+        self.config.set('zuul', 'layout_config',
+                        'tests/fixtures/layout-requirement-newer-than.yaml')
+        self.sched.reconfigure(self.config)
+        self.registerJobs()
+
+        A = self.fake_gerrit.addFakeChange(project, 'master', 'A')
+        # A comment event that we will keep submitting to trigger
+        comment = A.addApproval('CRVW', 2, username='nobody')
+        self.fake_gerrit.addEvent(comment)
+        self.waitUntilSettled()
+        # No +1 from Jenkins so should not be enqueued
+        self.assertEqual(len(self.history), 0)
+
+        # Add a too-old +1, should not be enqueued
+        A.addApproval('VRFY', 1, username='jenkins',
+                      granted_on=time.time() - 72 * 60 * 60)
+        self.fake_gerrit.addEvent(comment)
+        self.waitUntilSettled()
+        self.assertEqual(len(self.history), 0)
+
+        # Add a recent +1
+        self.fake_gerrit.addEvent(A.addApproval('VRFY', 1, username='jenkins'))
+        self.fake_gerrit.addEvent(comment)
+        self.waitUntilSettled()
+        self.assertEqual(len(self.history), 1)
+        self.assertEqual(self.history[0].name, job)
+
+    def test_pipeline_require_approval_older_than(self):
+        "Test pipeline requirement: approval older than"
+        return self._test_require_approval_older_than('org/project1',
+                                                      'project1-pipeline')
+
+    def test_trigger_require_approval_older_than(self):
+        "Test trigger requirement: approval older than"
+        return self._test_require_approval_older_than('org/project2',
+                                                      'project2-trigger')
+
+    def _test_require_approval_older_than(self, project, job):
+        self.config.set('zuul', 'layout_config',
+                        'tests/fixtures/layout-requirement-older-than.yaml')
+        self.sched.reconfigure(self.config)
+        self.registerJobs()
+
+        A = self.fake_gerrit.addFakeChange(project, 'master', 'A')
+        # A comment event that we will keep submitting to trigger
+        comment = A.addApproval('CRVW', 2, username='nobody')
+        self.fake_gerrit.addEvent(comment)
+        self.waitUntilSettled()
+        # No +1 from Jenkins so should not be enqueued
+        self.assertEqual(len(self.history), 0)
+
+        # Add a recent +1 which should not be enqueued
+        A.addApproval('VRFY', 1)
+        self.fake_gerrit.addEvent(comment)
+        self.waitUntilSettled()
+        self.assertEqual(len(self.history), 0)
+
+        # Add an old +1 which should be enqueued
+        A.addApproval('VRFY', 1, username='jenkins',
+                      granted_on=time.time() - 72 * 60 * 60)
+        self.fake_gerrit.addEvent(comment)
+        self.waitUntilSettled()
+        self.assertEqual(len(self.history), 1)
+        self.assertEqual(self.history[0].name, job)
+
+    def test_pipeline_require_approval_username(self):
+        "Test pipeline requirement: approval username"
+        return self._test_require_approval_username('org/project1',
+                                                    'project1-pipeline')
+
+    def test_trigger_require_approval_username(self):
+        "Test trigger requirement: approval username"
+        return self._test_require_approval_username('org/project2',
+                                                    'project2-trigger')
+
+    def _test_require_approval_username(self, project, job):
+        self.config.set('zuul', 'layout_config',
+                        'tests/fixtures/layout-requirement-username.yaml')
+        self.sched.reconfigure(self.config)
+        self.registerJobs()
+
+        A = self.fake_gerrit.addFakeChange(project, 'master', 'A')
+        # A comment event that we will keep submitting to trigger
+        comment = A.addApproval('CRVW', 2, username='nobody')
+        self.fake_gerrit.addEvent(comment)
+        self.waitUntilSettled()
+        # No approval from Jenkins so should not be enqueued
+        self.assertEqual(len(self.history), 0)
+
+        # Add an approval from Jenkins
+        A.addApproval('VRFY', 1, username='jenkins')
+        self.fake_gerrit.addEvent(comment)
+        self.waitUntilSettled()
+        self.assertEqual(len(self.history), 1)
+        self.assertEqual(self.history[0].name, job)
+
+    def test_pipeline_require_approval_email(self):
+        "Test pipeline requirement: approval email"
+        return self._test_require_approval_email('org/project1',
+                                                 'project1-pipeline')
+
+    def test_trigger_require_approval_email(self):
+        "Test trigger requirement: approval email"
+        return self._test_require_approval_email('org/project2',
+                                                 'project2-trigger')
+
+    def _test_require_approval_email(self, project, job):
+        self.config.set('zuul', 'layout_config',
+                        'tests/fixtures/layout-requirement-email.yaml')
+        self.sched.reconfigure(self.config)
+        self.registerJobs()
+
+        A = self.fake_gerrit.addFakeChange(project, 'master', 'A')
+        # A comment event that we will keep submitting to trigger
+        comment = A.addApproval('CRVW', 2, username='nobody')
+        self.fake_gerrit.addEvent(comment)
+        self.waitUntilSettled()
+        # No approval from Jenkins so should not be enqueued
+        self.assertEqual(len(self.history), 0)
+
+        # Add an approval from Jenkins
+        A.addApproval('VRFY', 1, username='jenkins')
+        self.fake_gerrit.addEvent(comment)
+        self.waitUntilSettled()
+        self.assertEqual(len(self.history), 1)
+        self.assertEqual(self.history[0].name, job)
+
+    def test_pipeline_require_approval_vote1(self):
+        "Test pipeline requirement: approval vote with one value"
+        return self._test_require_approval_vote1('org/project1',
+                                                 'project1-pipeline')
+
+    def test_trigger_require_approval_vote1(self):
+        "Test trigger requirement: approval vote with one value"
+        return self._test_require_approval_vote1('org/project2',
+                                                 'project2-trigger')
+
+    def _test_require_approval_vote1(self, project, job):
+        self.config.set('zuul', 'layout_config',
+                        'tests/fixtures/layout-requirement-vote1.yaml')
+        self.sched.reconfigure(self.config)
+        self.registerJobs()
+
+        A = self.fake_gerrit.addFakeChange(project, 'master', 'A')
+        # A comment event that we will keep submitting to trigger
+        comment = A.addApproval('CRVW', 2, username='nobody')
+        self.fake_gerrit.addEvent(comment)
+        self.waitUntilSettled()
+        # No approval from Jenkins so should not be enqueued
+        self.assertEqual(len(self.history), 0)
+
+        # A -1 from jenkins should not cause it to be enqueued
+        A.addApproval('VRFY', -1, username='jenkins')
+        self.fake_gerrit.addEvent(comment)
+        self.waitUntilSettled()
+        self.assertEqual(len(self.history), 0)
+
+        # A +1 should allow it to be enqueued
+        A.addApproval('VRFY', 1, username='jenkins')
+        self.fake_gerrit.addEvent(comment)
+        self.waitUntilSettled()
+        self.assertEqual(len(self.history), 1)
+        self.assertEqual(self.history[0].name, job)
+
+    def test_pipeline_require_approval_vote2(self):
+        "Test pipeline requirement: approval vote with two values"
+        return self._test_require_approval_vote2('org/project1',
+                                                 'project1-pipeline')
+
+    def test_trigger_require_approval_vote2(self):
+        "Test trigger requirement: approval vote with two values"
+        return self._test_require_approval_vote2('org/project2',
+                                                 'project2-trigger')
+
+    def _test_require_approval_vote2(self, project, job):
+        self.config.set('zuul', 'layout_config',
+                        'tests/fixtures/layout-requirement-vote2.yaml')
+        self.sched.reconfigure(self.config)
+        self.registerJobs()
+
+        A = self.fake_gerrit.addFakeChange(project, 'master', 'A')
+        # A comment event that we will keep submitting to trigger
+        comment = A.addApproval('CRVW', 2, username='nobody')
+        self.fake_gerrit.addEvent(comment)
+        self.waitUntilSettled()
+        # No approval from Jenkins so should not be enqueued
+        self.assertEqual(len(self.history), 0)
+
+        # A -1 from jenkins should not cause it to be enqueued
+        A.addApproval('VRFY', -1, username='jenkins')
+        self.fake_gerrit.addEvent(comment)
+        self.waitUntilSettled()
+        self.assertEqual(len(self.history), 0)
+
+        # A -2 from jenkins should not cause it to be enqueued
+        A.addApproval('VRFY', -2, username='jenkins')
+        self.fake_gerrit.addEvent(comment)
+        self.waitUntilSettled()
+        self.assertEqual(len(self.history), 0)
+
+        # A +1 from jenkins should allow it to be enqueued
+        A.addApproval('VRFY', 1, username='jenkins')
+        self.fake_gerrit.addEvent(comment)
+        self.waitUntilSettled()
+        self.assertEqual(len(self.history), 1)
+        self.assertEqual(self.history[0].name, job)
+
+        # A +2 should allow it to be enqueued
+        B = self.fake_gerrit.addFakeChange(project, 'master', 'B')
+        # A comment event that we will keep submitting to trigger
+        comment = B.addApproval('CRVW', 2, username='nobody')
+        self.fake_gerrit.addEvent(comment)
+        self.waitUntilSettled()
+        self.assertEqual(len(self.history), 1)
+
+        B.addApproval('VRFY', 2, username='jenkins')
+        self.fake_gerrit.addEvent(comment)
+        self.waitUntilSettled()
+        self.assertEqual(len(self.history), 2)
+        self.assertEqual(self.history[1].name, job)
+
+    def test_pipeline_require_current_patchset(self):
+        "Test pipeline requirement: current-patchset"
+        self.config.set('zuul', 'layout_config',
+                        'tests/fixtures/layout-requirement-'
+                        'current-patchset.yaml')
+        self.sched.reconfigure(self.config)
+        self.registerJobs()
+        # Create two patchsets and let their tests settle out. Then
+        # comment on first patchset and check that no additional
+        # jobs are run.
+        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
+        self.fake_gerrit.addEvent(A.addApproval('CRVW', 1))
+        self.waitUntilSettled()
+        A.addPatchset()
+        self.fake_gerrit.addEvent(A.addApproval('CRVW', 1))
+        self.waitUntilSettled()
+
+        self.assertEqual(len(self.history), 2)  # one job for each ps
+        self.fake_gerrit.addEvent(A.getChangeCommentEvent(1))
+        self.waitUntilSettled()
+
+        # Assert no new jobs ran after event for old patchset.
+        self.assertEqual(len(self.history), 2)
+
+        # Make sure the same event on a new PS will trigger
+        self.fake_gerrit.addEvent(A.getChangeCommentEvent(2))
+        self.waitUntilSettled()
+        self.assertEqual(len(self.history), 3)
+
+    def test_pipeline_require_open(self):
+        "Test pipeline requirement: open"
+        self.config.set('zuul', 'layout_config',
+                        'tests/fixtures/layout-requirement-open.yaml')
+        self.sched.reconfigure(self.config)
+        self.registerJobs()
+
+        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A',
+                                           status='MERGED')
+        self.fake_gerrit.addEvent(A.addApproval('CRVW', 2))
+        self.waitUntilSettled()
+        self.assertEqual(len(self.history), 0)
+
+        B = self.fake_gerrit.addFakeChange('org/project', 'master', 'B')
+        self.fake_gerrit.addEvent(B.addApproval('CRVW', 2))
+        self.waitUntilSettled()
+        self.assertEqual(len(self.history), 1)
+
+    def test_pipeline_require_status(self):
+        "Test pipeline requirement: status"
+        self.config.set('zuul', 'layout_config',
+                        'tests/fixtures/layout-requirement-status.yaml')
+        self.sched.reconfigure(self.config)
+        self.registerJobs()
+
+        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A',
+                                           status='MERGED')
+        self.fake_gerrit.addEvent(A.addApproval('CRVW', 2))
+        self.waitUntilSettled()
+        self.assertEqual(len(self.history), 0)
+
+        B = self.fake_gerrit.addFakeChange('org/project', 'master', 'B')
+        self.fake_gerrit.addEvent(B.addApproval('CRVW', 2))
+        self.waitUntilSettled()
+        self.assertEqual(len(self.history), 1)
+
+    def _test_require_reject_username(self, project, job):
+        "Test negative username's match"
+        # Should only trigger if Jenkins hasn't voted.
+        self.config.set(
+            'zuul', 'layout_config',
+            'tests/fixtures/layout-requirement-reject-username.yaml')
+        self.sched.reconfigure(self.config)
+        self.registerJobs()
+
+        # add in a change with no comments
+        A = self.fake_gerrit.addFakeChange(project, 'master', 'A')
+        self.waitUntilSettled()
+        self.assertEqual(len(self.history), 0)
+
+        # add in a comment that will trigger
+        self.fake_gerrit.addEvent(A.addApproval('CRVW', 1,
+                                                username='reviewer'))
+        self.waitUntilSettled()
+        self.assertEqual(len(self.history), 1)
+        self.assertEqual(self.history[0].name, job)
+
+        # add in a comment from jenkins user which shouldn't trigger
+        self.fake_gerrit.addEvent(A.addApproval('VRFY', 1, username='jenkins'))
+        self.waitUntilSettled()
+        self.assertEqual(len(self.history), 1)
+
+        # Check future reviews also won't trigger as a 'jenkins' user has
+        # commented previously
+        self.fake_gerrit.addEvent(A.addApproval('CRVW', 1,
+                                                username='reviewer'))
+        self.waitUntilSettled()
+        self.assertEqual(len(self.history), 1)
+
+    def test_pipeline_reject_username(self):
+        "Test negative pipeline requirement: no comment from jenkins"
+        return self._test_require_reject_username('org/project1',
+                                                  'project1-pipeline')
+
+    def test_trigger_reject_username(self):
+        "Test negative trigger requirement: no comment from jenkins"
+        return self._test_require_reject_username('org/project2',
+                                                  'project2-trigger')
+
+    def _test_require_reject(self, project, job):
+        "Test no approval matches a reject param"
+        self.config.set(
+            'zuul', 'layout_config',
+            'tests/fixtures/layout-requirement-reject.yaml')
+        self.sched.reconfigure(self.config)
+        self.registerJobs()
+
+        A = self.fake_gerrit.addFakeChange(project, 'master', 'A')
+        self.waitUntilSettled()
+        self.assertEqual(len(self.history), 0)
+
+        # First positive vote should not queue until jenkins has +1'd
+        comment = A.addApproval('VRFY', 1, username='reviewer_a')
+        self.fake_gerrit.addEvent(comment)
+        self.waitUntilSettled()
+        self.assertEqual(len(self.history), 0)
+
+        # Jenkins should put in a +1 which will also queue
+        comment = A.addApproval('VRFY', 1, username='jenkins')
+        self.fake_gerrit.addEvent(comment)
+        self.waitUntilSettled()
+        self.assertEqual(len(self.history), 1)
+        self.assertEqual(self.history[0].name, job)
+
+        # Negative vote should not queue
+        comment = A.addApproval('VRFY', -1, username='reviewer_b')
+        self.fake_gerrit.addEvent(comment)
+        self.waitUntilSettled()
+        self.assertEqual(len(self.history), 1)
+
+        # Future approvals should do nothing
+        comment = A.addApproval('VRFY', 1, username='reviewer_c')
+        self.fake_gerrit.addEvent(comment)
+        self.waitUntilSettled()
+        self.assertEqual(len(self.history), 1)
+
+        # Change/update negative vote should queue
+        comment = A.addApproval('VRFY', 1, username='reviewer_b')
+        self.fake_gerrit.addEvent(comment)
+        self.waitUntilSettled()
+        self.assertEqual(len(self.history), 2)
+        self.assertEqual(self.history[1].name, job)
+
+        # Future approvals should also queue
+        comment = A.addApproval('VRFY', 1, username='reviewer_d')
+        self.fake_gerrit.addEvent(comment)
+        self.waitUntilSettled()
+        self.assertEqual(len(self.history), 3)
+        self.assertEqual(self.history[2].name, job)
+
+    def test_pipeline_require_reject(self):
+        "Test pipeline requirement: rejections absent"
+        return self._test_require_reject('org/project1', 'project1-pipeline')
+
+    def test_trigger_require_reject(self):
+        "Test trigger requirement: rejections absent"
+        return self._test_require_reject('org/project2', 'project2-trigger')
diff --git a/tests/test_scheduler.py b/tests/test_scheduler.py
index cbf1495..30be8d3 100755
--- a/tests/test_scheduler.py
+++ b/tests/test_scheduler.py
@@ -1,4394 +1,4394 @@
-#!/usr/bin/env python2.7
-
-# Copyright 2012 Hewlett-Packard Development Company, L.P.
-#
-# Licensed under the Apache License, Version 2.0 (the "License"); you may
-# not use this file except in compliance with the License. You may obtain
-# a copy of the License at
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-# License for the specific language governing permissions and limitations
-# under the License.
-
-import json
-import logging
-import os
-import re
-import shutil
-import time
-import urllib
-import urllib2
-import yaml
-
-import git
-import testtools
-
-import zuul.change_matcher
-import zuul.scheduler
-import zuul.rpcclient
-import zuul.reporter.gerrit
-import zuul.reporter.smtp
-
-from tests.base import (
-    BaseTestCase,
-    ZuulTestCase,
-    repack_repo,
-)
-
-logging.basicConfig(level=logging.DEBUG,
-                    format='%(asctime)s %(name)-32s '
-                    '%(levelname)-8s %(message)s')
-
-
-class TestSchedulerConfigParsing(BaseTestCase):
-
-    def test_parse_skip_if(self):
-        job_yaml = """
-jobs:
-  - name: job_name
-    skip-if:
-      - project: ^project_name$
-        branch: ^stable/icehouse$
-        all-files-match-any:
-          - ^filename$
-      - project: ^project2_name$
-        all-files-match-any:
-          - ^filename2$
-    """.strip()
-        data = yaml.load(job_yaml)
-        config_job = data.get('jobs')[0]
-        sched = zuul.scheduler.Scheduler({})
-        cm = zuul.change_matcher
-        expected = cm.MatchAny([
-            cm.MatchAll([
-                cm.ProjectMatcher('^project_name$'),
-                cm.BranchMatcher('^stable/icehouse$'),
-                cm.MatchAllFiles([cm.FileMatcher('^filename$')]),
-            ]),
-            cm.MatchAll([
-                cm.ProjectMatcher('^project2_name$'),
-                cm.MatchAllFiles([cm.FileMatcher('^filename2$')]),
-            ]),
-        ])
-        matcher = sched._parseSkipIf(config_job)
-        self.assertEqual(expected, matcher)
-
-
-class TestScheduler(ZuulTestCase):
-
-    def test_jobs_launched(self):
-        "Test that jobs are launched and a change is merged"
-
-        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
-        A.addApproval('CRVW', 2)
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-        self.waitUntilSettled()
-        self.assertEqual(self.getJobFromHistory('project-merge').result,
-                         'SUCCESS')
-        self.assertEqual(self.getJobFromHistory('project-test1').result,
-                         'SUCCESS')
-        self.assertEqual(self.getJobFromHistory('project-test2').result,
-                         'SUCCESS')
-        self.assertEqual(A.data['status'], 'MERGED')
-        self.assertEqual(A.reported, 2)
-
-        self.assertReportedStat('gerrit.event.comment-added', value='1|c')
-        self.assertReportedStat('zuul.pipeline.gate.current_changes',
-                                value='1|g')
-        self.assertReportedStat('zuul.pipeline.gate.job.project-merge.SUCCESS',
-                                kind='ms')
-        self.assertReportedStat('zuul.pipeline.gate.job.project-merge.SUCCESS',
-                                value='1|c')
-        self.assertReportedStat('zuul.pipeline.gate.resident_time', kind='ms')
-        self.assertReportedStat('zuul.pipeline.gate.total_changes',
-                                value='1|c')
-        self.assertReportedStat(
-            'zuul.pipeline.gate.org.project.resident_time', kind='ms')
-        self.assertReportedStat(
-            'zuul.pipeline.gate.org.project.total_changes', value='1|c')
-
-        for build in self.builds:
-            self.assertEqual(build.parameters['ZUUL_VOTING'], '1')
-
-    def test_initial_pipeline_gauges(self):
-        "Test that each pipeline reported its length on start"
-        pipeline_names = self.sched.layout.pipelines.keys()
-        self.assertNotEqual(len(pipeline_names), 0)
-        for name in pipeline_names:
-            self.assertReportedStat('zuul.pipeline.%s.current_changes' % name,
-                                    value='0|g')
-
-    def test_duplicate_pipelines(self):
-        "Test that a change matching multiple pipelines works"
-
-        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
-        self.fake_gerrit.addEvent(A.getChangeRestoredEvent())
-        self.waitUntilSettled()
-
-        self.assertEqual(len(self.history), 2)
-        self.history[0].name == 'project-test1'
-        self.history[1].name == 'project-test1'
-
-        self.assertEqual(len(A.messages), 2)
-        if 'dup1/project-test1' in A.messages[0]:
-            self.assertIn('dup1/project-test1', A.messages[0])
-            self.assertNotIn('dup2/project-test1', A.messages[0])
-            self.assertNotIn('dup1/project-test1', A.messages[1])
-            self.assertIn('dup2/project-test1', A.messages[1])
-        else:
-            self.assertIn('dup1/project-test1', A.messages[1])
-            self.assertNotIn('dup2/project-test1', A.messages[1])
-            self.assertNotIn('dup1/project-test1', A.messages[0])
-            self.assertIn('dup2/project-test1', A.messages[0])
-
-    def test_parallel_changes(self):
-        "Test that changes are tested in parallel and merged in series"
-
-        self.worker.hold_jobs_in_build = True
-        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
-        B = self.fake_gerrit.addFakeChange('org/project', 'master', 'B')
-        C = self.fake_gerrit.addFakeChange('org/project', 'master', 'C')
-        A.addApproval('CRVW', 2)
-        B.addApproval('CRVW', 2)
-        C.addApproval('CRVW', 2)
-
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
-        self.fake_gerrit.addEvent(C.addApproval('APRV', 1))
-
-        self.waitUntilSettled()
-        self.assertEqual(len(self.builds), 1)
-        self.assertEqual(self.builds[0].name, 'project-merge')
-        self.assertTrue(self.job_has_changes(self.builds[0], A))
-
-        self.worker.release('.*-merge')
-        self.waitUntilSettled()
-        self.assertEqual(len(self.builds), 3)
-        self.assertEqual(self.builds[0].name, 'project-test1')
-        self.assertTrue(self.job_has_changes(self.builds[0], A))
-        self.assertEqual(self.builds[1].name, 'project-test2')
-        self.assertTrue(self.job_has_changes(self.builds[1], A))
-        self.assertEqual(self.builds[2].name, 'project-merge')
-        self.assertTrue(self.job_has_changes(self.builds[2], A, B))
-
-        self.worker.release('.*-merge')
-        self.waitUntilSettled()
-        self.assertEqual(len(self.builds), 5)
-        self.assertEqual(self.builds[0].name, 'project-test1')
-        self.assertTrue(self.job_has_changes(self.builds[0], A))
-        self.assertEqual(self.builds[1].name, 'project-test2')
-        self.assertTrue(self.job_has_changes(self.builds[1], A))
-
-        self.assertEqual(self.builds[2].name, 'project-test1')
-        self.assertTrue(self.job_has_changes(self.builds[2], A, B))
-        self.assertEqual(self.builds[3].name, 'project-test2')
-        self.assertTrue(self.job_has_changes(self.builds[3], A, B))
-
-        self.assertEqual(self.builds[4].name, 'project-merge')
-        self.assertTrue(self.job_has_changes(self.builds[4], A, B, C))
-
-        self.worker.release('.*-merge')
-        self.waitUntilSettled()
-        self.assertEqual(len(self.builds), 6)
-        self.assertEqual(self.builds[0].name, 'project-test1')
-        self.assertTrue(self.job_has_changes(self.builds[0], A))
-        self.assertEqual(self.builds[1].name, 'project-test2')
-        self.assertTrue(self.job_has_changes(self.builds[1], A))
-
-        self.assertEqual(self.builds[2].name, 'project-test1')
-        self.assertTrue(self.job_has_changes(self.builds[2], A, B))
-        self.assertEqual(self.builds[3].name, 'project-test2')
-        self.assertTrue(self.job_has_changes(self.builds[3], A, B))
-
-        self.assertEqual(self.builds[4].name, 'project-test1')
-        self.assertTrue(self.job_has_changes(self.builds[4], A, B, C))
-        self.assertEqual(self.builds[5].name, 'project-test2')
-        self.assertTrue(self.job_has_changes(self.builds[5], A, B, C))
-
-        self.worker.hold_jobs_in_build = False
-        self.worker.release()
-        self.waitUntilSettled()
-        self.assertEqual(len(self.builds), 0)
-
-        self.assertEqual(len(self.history), 9)
-        self.assertEqual(A.data['status'], 'MERGED')
-        self.assertEqual(B.data['status'], 'MERGED')
-        self.assertEqual(C.data['status'], 'MERGED')
-        self.assertEqual(A.reported, 2)
-        self.assertEqual(B.reported, 2)
-        self.assertEqual(C.reported, 2)
-
-    def test_failed_changes(self):
-        "Test that a change behind a failed change is retested"
-        self.worker.hold_jobs_in_build = True
-
-        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
-        B = self.fake_gerrit.addFakeChange('org/project', 'master', 'B')
-        A.addApproval('CRVW', 2)
-        B.addApproval('CRVW', 2)
-
-        self.worker.addFailTest('project-test1', A)
-
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
-        self.waitUntilSettled()
-
-        self.worker.release('.*-merge')
-        self.waitUntilSettled()
-
-        self.worker.hold_jobs_in_build = False
-        self.worker.release()
-
-        self.waitUntilSettled()
-        # It's certain that the merge job for change 2 will run, but
-        # the test1 and test2 jobs may or may not run.
-        self.assertTrue(len(self.history) > 6)
-        self.assertEqual(A.data['status'], 'NEW')
-        self.assertEqual(B.data['status'], 'MERGED')
-        self.assertEqual(A.reported, 2)
-        self.assertEqual(B.reported, 2)
-
-    def test_independent_queues(self):
-        "Test that changes end up in the right queues"
-
-        self.worker.hold_jobs_in_build = True
-        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
-        B = self.fake_gerrit.addFakeChange('org/project1', 'master', 'B')
-        C = self.fake_gerrit.addFakeChange('org/project2', 'master', 'C')
-        A.addApproval('CRVW', 2)
-        B.addApproval('CRVW', 2)
-        C.addApproval('CRVW', 2)
-
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
-        self.fake_gerrit.addEvent(C.addApproval('APRV', 1))
-
-        self.waitUntilSettled()
-
-        # There should be one merge job at the head of each queue running
-        self.assertEqual(len(self.builds), 2)
-        self.assertEqual(self.builds[0].name, 'project-merge')
-        self.assertTrue(self.job_has_changes(self.builds[0], A))
-        self.assertEqual(self.builds[1].name, 'project1-merge')
-        self.assertTrue(self.job_has_changes(self.builds[1], B))
-
-        # Release the current merge builds
-        self.worker.release('.*-merge')
-        self.waitUntilSettled()
-        # Release the merge job for project2 which is behind project1
-        self.worker.release('.*-merge')
-        self.waitUntilSettled()
-
-        # All the test builds should be running:
-        # project1 (3) + project2 (3) + project (2) = 8
-        self.assertEqual(len(self.builds), 8)
-
-        self.worker.release()
-        self.waitUntilSettled()
-        self.assertEqual(len(self.builds), 0)
-
-        self.assertEqual(len(self.history), 11)
-        self.assertEqual(A.data['status'], 'MERGED')
-        self.assertEqual(B.data['status'], 'MERGED')
-        self.assertEqual(C.data['status'], 'MERGED')
-        self.assertEqual(A.reported, 2)
-        self.assertEqual(B.reported, 2)
-        self.assertEqual(C.reported, 2)
-
-    def test_failed_change_at_head(self):
-        "Test that if a change at the head fails, jobs behind it are canceled"
-
-        self.worker.hold_jobs_in_build = True
-        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
-        B = self.fake_gerrit.addFakeChange('org/project', 'master', 'B')
-        C = self.fake_gerrit.addFakeChange('org/project', 'master', 'C')
-        A.addApproval('CRVW', 2)
-        B.addApproval('CRVW', 2)
-        C.addApproval('CRVW', 2)
-
-        self.worker.addFailTest('project-test1', A)
-
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
-        self.fake_gerrit.addEvent(C.addApproval('APRV', 1))
-
-        self.waitUntilSettled()
-
-        self.assertEqual(len(self.builds), 1)
-        self.assertEqual(self.builds[0].name, 'project-merge')
-        self.assertTrue(self.job_has_changes(self.builds[0], A))
-
-        self.worker.release('.*-merge')
-        self.waitUntilSettled()
-        self.worker.release('.*-merge')
-        self.waitUntilSettled()
-        self.worker.release('.*-merge')
-        self.waitUntilSettled()
-
-        self.assertEqual(len(self.builds), 6)
-        self.assertEqual(self.builds[0].name, 'project-test1')
-        self.assertEqual(self.builds[1].name, 'project-test2')
-        self.assertEqual(self.builds[2].name, 'project-test1')
-        self.assertEqual(self.builds[3].name, 'project-test2')
-        self.assertEqual(self.builds[4].name, 'project-test1')
-        self.assertEqual(self.builds[5].name, 'project-test2')
-
-        self.release(self.builds[0])
-        self.waitUntilSettled()
-
-        # project-test2, project-merge for B
-        self.assertEqual(len(self.builds), 2)
-        self.assertEqual(self.countJobResults(self.history, 'ABORTED'), 4)
-
-        self.worker.hold_jobs_in_build = False
-        self.worker.release()
-        self.waitUntilSettled()
-
-        self.assertEqual(len(self.builds), 0)
-        self.assertEqual(len(self.history), 15)
-        self.assertEqual(A.data['status'], 'NEW')
-        self.assertEqual(B.data['status'], 'MERGED')
-        self.assertEqual(C.data['status'], 'MERGED')
-        self.assertEqual(A.reported, 2)
-        self.assertEqual(B.reported, 2)
-        self.assertEqual(C.reported, 2)
-
-    def test_failed_change_in_middle(self):
-        "Test a failed change in the middle of the queue"
-
-        self.worker.hold_jobs_in_build = True
-        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
-        B = self.fake_gerrit.addFakeChange('org/project', 'master', 'B')
-        C = self.fake_gerrit.addFakeChange('org/project', 'master', 'C')
-        A.addApproval('CRVW', 2)
-        B.addApproval('CRVW', 2)
-        C.addApproval('CRVW', 2)
-
-        self.worker.addFailTest('project-test1', B)
-
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
-        self.fake_gerrit.addEvent(C.addApproval('APRV', 1))
-
-        self.waitUntilSettled()
-
-        self.worker.release('.*-merge')
-        self.waitUntilSettled()
-        self.worker.release('.*-merge')
-        self.waitUntilSettled()
-        self.worker.release('.*-merge')
-        self.waitUntilSettled()
-
-        self.assertEqual(len(self.builds), 6)
-        self.assertEqual(self.builds[0].name, 'project-test1')
-        self.assertEqual(self.builds[1].name, 'project-test2')
-        self.assertEqual(self.builds[2].name, 'project-test1')
-        self.assertEqual(self.builds[3].name, 'project-test2')
-        self.assertEqual(self.builds[4].name, 'project-test1')
-        self.assertEqual(self.builds[5].name, 'project-test2')
-
-        self.release(self.builds[2])
-        self.waitUntilSettled()
-
-        # project-test1 and project-test2 for A
-        # project-test2 for B
-        # project-merge for C (without B)
-        self.assertEqual(len(self.builds), 4)
-        self.assertEqual(self.countJobResults(self.history, 'ABORTED'), 2)
-
-        self.worker.release('.*-merge')
-        self.waitUntilSettled()
-
-        # project-test1 and project-test2 for A
-        # project-test2 for B
-        # project-test1 and project-test2 for C
-        self.assertEqual(len(self.builds), 5)
-
-        items = self.sched.layout.pipelines['gate'].getAllItems()
-        builds = items[0].current_build_set.getBuilds()
-        self.assertEqual(self.countJobResults(builds, 'SUCCESS'), 1)
-        self.assertEqual(self.countJobResults(builds, None), 2)
-        builds = items[1].current_build_set.getBuilds()
-        self.assertEqual(self.countJobResults(builds, 'SUCCESS'), 1)
-        self.assertEqual(self.countJobResults(builds, 'FAILURE'), 1)
-        self.assertEqual(self.countJobResults(builds, None), 1)
-        builds = items[2].current_build_set.getBuilds()
-        self.assertEqual(self.countJobResults(builds, 'SUCCESS'), 1)
-        self.assertEqual(self.countJobResults(builds, None), 2)
-
-        self.worker.hold_jobs_in_build = False
-        self.worker.release()
-        self.waitUntilSettled()
-
-        self.assertEqual(len(self.builds), 0)
-        self.assertEqual(len(self.history), 12)
-        self.assertEqual(A.data['status'], 'MERGED')
-        self.assertEqual(B.data['status'], 'NEW')
-        self.assertEqual(C.data['status'], 'MERGED')
-        self.assertEqual(A.reported, 2)
-        self.assertEqual(B.reported, 2)
-        self.assertEqual(C.reported, 2)
-
-    def test_failed_change_at_head_with_queue(self):
-        "Test that if a change at the head fails, queued jobs are canceled"
-
-        self.gearman_server.hold_jobs_in_queue = True
-        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
-        B = self.fake_gerrit.addFakeChange('org/project', 'master', 'B')
-        C = self.fake_gerrit.addFakeChange('org/project', 'master', 'C')
-        A.addApproval('CRVW', 2)
-        B.addApproval('CRVW', 2)
-        C.addApproval('CRVW', 2)
-
-        self.worker.addFailTest('project-test1', A)
-
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
-        self.fake_gerrit.addEvent(C.addApproval('APRV', 1))
-
-        self.waitUntilSettled()
-        queue = self.gearman_server.getQueue()
-        self.assertEqual(len(self.builds), 0)
-        self.assertEqual(len(queue), 1)
-        self.assertEqual(queue[0].name, 'build:project-merge')
-        self.assertTrue(self.job_has_changes(queue[0], A))
-
-        self.gearman_server.release('.*-merge')
-        self.waitUntilSettled()
-        self.gearman_server.release('.*-merge')
-        self.waitUntilSettled()
-        self.gearman_server.release('.*-merge')
-        self.waitUntilSettled()
-        queue = self.gearman_server.getQueue()
-
-        self.assertEqual(len(self.builds), 0)
-        self.assertEqual(len(queue), 6)
-        self.assertEqual(queue[0].name, 'build:project-test1')
-        self.assertEqual(queue[1].name, 'build:project-test2')
-        self.assertEqual(queue[2].name, 'build:project-test1')
-        self.assertEqual(queue[3].name, 'build:project-test2')
-        self.assertEqual(queue[4].name, 'build:project-test1')
-        self.assertEqual(queue[5].name, 'build:project-test2')
-
-        self.release(queue[0])
-        self.waitUntilSettled()
-
-        self.assertEqual(len(self.builds), 0)
-        queue = self.gearman_server.getQueue()
-        self.assertEqual(len(queue), 2)  # project-test2, project-merge for B
-        self.assertEqual(self.countJobResults(self.history, 'ABORTED'), 0)
-
-        self.gearman_server.hold_jobs_in_queue = False
-        self.gearman_server.release()
-        self.waitUntilSettled()
-
-        self.assertEqual(len(self.builds), 0)
-        self.assertEqual(len(self.history), 11)
-        self.assertEqual(A.data['status'], 'NEW')
-        self.assertEqual(B.data['status'], 'MERGED')
-        self.assertEqual(C.data['status'], 'MERGED')
-        self.assertEqual(A.reported, 2)
-        self.assertEqual(B.reported, 2)
-        self.assertEqual(C.reported, 2)
-
-    def test_two_failed_changes_at_head(self):
-        "Test that changes are reparented correctly if 2 fail at head"
-
-        self.worker.hold_jobs_in_build = True
-        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
-        B = self.fake_gerrit.addFakeChange('org/project', 'master', 'B')
-        C = self.fake_gerrit.addFakeChange('org/project', 'master', 'C')
-        A.addApproval('CRVW', 2)
-        B.addApproval('CRVW', 2)
-        C.addApproval('CRVW', 2)
-
-        self.worker.addFailTest('project-test1', A)
-        self.worker.addFailTest('project-test1', B)
-
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
-        self.fake_gerrit.addEvent(C.addApproval('APRV', 1))
-        self.waitUntilSettled()
-
-        self.worker.release('.*-merge')
-        self.waitUntilSettled()
-        self.worker.release('.*-merge')
-        self.waitUntilSettled()
-        self.worker.release('.*-merge')
-        self.waitUntilSettled()
-
-        self.assertEqual(len(self.builds), 6)
-        self.assertEqual(self.builds[0].name, 'project-test1')
-        self.assertEqual(self.builds[1].name, 'project-test2')
-        self.assertEqual(self.builds[2].name, 'project-test1')
-        self.assertEqual(self.builds[3].name, 'project-test2')
-        self.assertEqual(self.builds[4].name, 'project-test1')
-        self.assertEqual(self.builds[5].name, 'project-test2')
-
-        self.assertTrue(self.job_has_changes(self.builds[0], A))
-        self.assertTrue(self.job_has_changes(self.builds[2], A))
-        self.assertTrue(self.job_has_changes(self.builds[2], B))
-        self.assertTrue(self.job_has_changes(self.builds[4], A))
-        self.assertTrue(self.job_has_changes(self.builds[4], B))
-        self.assertTrue(self.job_has_changes(self.builds[4], C))
-
-        # Fail change B first
-        self.release(self.builds[2])
-        self.waitUntilSettled()
-
-        # restart of C after B failure
-        self.worker.release('.*-merge')
-        self.waitUntilSettled()
-
-        self.assertEqual(len(self.builds), 5)
-        self.assertEqual(self.builds[0].name, 'project-test1')
-        self.assertEqual(self.builds[1].name, 'project-test2')
-        self.assertEqual(self.builds[2].name, 'project-test2')
-        self.assertEqual(self.builds[3].name, 'project-test1')
-        self.assertEqual(self.builds[4].name, 'project-test2')
-
-        self.assertTrue(self.job_has_changes(self.builds[1], A))
-        self.assertTrue(self.job_has_changes(self.builds[2], A))
-        self.assertTrue(self.job_has_changes(self.builds[2], B))
-        self.assertTrue(self.job_has_changes(self.builds[4], A))
-        self.assertFalse(self.job_has_changes(self.builds[4], B))
-        self.assertTrue(self.job_has_changes(self.builds[4], C))
-
-        # Finish running all passing jobs for change A
-        self.release(self.builds[1])
-        self.waitUntilSettled()
-        # Fail and report change A
-        self.release(self.builds[0])
-        self.waitUntilSettled()
-
-        # restart of B,C after A failure
-        self.worker.release('.*-merge')
-        self.waitUntilSettled()
-        self.worker.release('.*-merge')
-        self.waitUntilSettled()
-
-        self.assertEqual(len(self.builds), 4)
-        self.assertEqual(self.builds[0].name, 'project-test1')  # B
-        self.assertEqual(self.builds[1].name, 'project-test2')  # B
-        self.assertEqual(self.builds[2].name, 'project-test1')  # C
-        self.assertEqual(self.builds[3].name, 'project-test2')  # C
-
-        self.assertFalse(self.job_has_changes(self.builds[1], A))
-        self.assertTrue(self.job_has_changes(self.builds[1], B))
-        self.assertFalse(self.job_has_changes(self.builds[1], C))
-
-        self.assertFalse(self.job_has_changes(self.builds[2], A))
-        # After A failed and B and C restarted, B should be back in
-        # C's tests because it has not failed yet.
-        self.assertTrue(self.job_has_changes(self.builds[2], B))
-        self.assertTrue(self.job_has_changes(self.builds[2], C))
-
-        self.worker.hold_jobs_in_build = False
-        self.worker.release()
-        self.waitUntilSettled()
-
-        self.assertEqual(len(self.builds), 0)
-        self.assertEqual(len(self.history), 21)
-        self.assertEqual(A.data['status'], 'NEW')
-        self.assertEqual(B.data['status'], 'NEW')
-        self.assertEqual(C.data['status'], 'MERGED')
-        self.assertEqual(A.reported, 2)
-        self.assertEqual(B.reported, 2)
-        self.assertEqual(C.reported, 2)
-
-    def test_patch_order(self):
-        "Test that dependent patches are tested in the right order"
-        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
-        B = self.fake_gerrit.addFakeChange('org/project', 'master', 'B')
-        C = self.fake_gerrit.addFakeChange('org/project', 'master', 'C')
-        A.addApproval('CRVW', 2)
-        B.addApproval('CRVW', 2)
-        C.addApproval('CRVW', 2)
-
-        M2 = self.fake_gerrit.addFakeChange('org/project', 'master', 'M2')
-        M1 = self.fake_gerrit.addFakeChange('org/project', 'master', 'M1')
-        M2.setMerged()
-        M1.setMerged()
-
-        # C -> B -> A -> M1 -> M2
-        # M2 is here to make sure it is never queried.  If it is, it
-        # means zuul is walking down the entire history of merged
-        # changes.
-
-        C.setDependsOn(B, 1)
-        B.setDependsOn(A, 1)
-        A.setDependsOn(M1, 1)
-        M1.setDependsOn(M2, 1)
-
-        self.fake_gerrit.addEvent(C.addApproval('APRV', 1))
-
-        self.waitUntilSettled()
-
-        self.assertEqual(A.data['status'], 'NEW')
-        self.assertEqual(B.data['status'], 'NEW')
-        self.assertEqual(C.data['status'], 'NEW')
-
-        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-
-        self.waitUntilSettled()
-        self.assertEqual(M2.queried, 0)
-        self.assertEqual(A.data['status'], 'MERGED')
-        self.assertEqual(B.data['status'], 'MERGED')
-        self.assertEqual(C.data['status'], 'MERGED')
-        self.assertEqual(A.reported, 2)
-        self.assertEqual(B.reported, 2)
-        self.assertEqual(C.reported, 2)
-
-    def test_needed_changes_enqueue(self):
-        "Test that a needed change is enqueued ahead"
-        #          A      Given a git tree like this, if we enqueue
-        #         / \     change C, we should walk up and down the tree
-        #        B   G    and enqueue changes in the order ABCDEFG.
-        #       /|\       This is also the order that you would get if
-        #     *C E F      you enqueued changes in the order ABCDEFG, so
-        #     /           the ordering is stable across re-enqueue events.
-        #    D
-
-        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
-        B = self.fake_gerrit.addFakeChange('org/project', 'master', 'B')
-        C = self.fake_gerrit.addFakeChange('org/project', 'master', 'C')
-        D = self.fake_gerrit.addFakeChange('org/project', 'master', 'D')
-        E = self.fake_gerrit.addFakeChange('org/project', 'master', 'E')
-        F = self.fake_gerrit.addFakeChange('org/project', 'master', 'F')
-        G = self.fake_gerrit.addFakeChange('org/project', 'master', 'G')
-        B.setDependsOn(A, 1)
-        C.setDependsOn(B, 1)
-        D.setDependsOn(C, 1)
-        E.setDependsOn(B, 1)
-        F.setDependsOn(B, 1)
-        G.setDependsOn(A, 1)
-
-        A.addApproval('CRVW', 2)
-        B.addApproval('CRVW', 2)
-        C.addApproval('CRVW', 2)
-        D.addApproval('CRVW', 2)
-        E.addApproval('CRVW', 2)
-        F.addApproval('CRVW', 2)
-        G.addApproval('CRVW', 2)
-        self.fake_gerrit.addEvent(C.addApproval('APRV', 1))
-
-        self.waitUntilSettled()
-
-        self.assertEqual(A.data['status'], 'NEW')
-        self.assertEqual(B.data['status'], 'NEW')
-        self.assertEqual(C.data['status'], 'NEW')
-        self.assertEqual(D.data['status'], 'NEW')
-        self.assertEqual(E.data['status'], 'NEW')
-        self.assertEqual(F.data['status'], 'NEW')
-        self.assertEqual(G.data['status'], 'NEW')
-
-        # We're about to add approvals to changes without adding the
-        # triggering events to Zuul, so that we can be sure that it is
-        # enqueing the changes based on dependencies, not because of
-        # triggering events.  Since it will have the changes cached
-        # already (without approvals), we need to clear the cache
-        # first.
-        for connection in self.connections.values():
-            connection.maintainCache([])
-
-        self.worker.hold_jobs_in_build = True
-        A.addApproval('APRV', 1)
-        B.addApproval('APRV', 1)
-        D.addApproval('APRV', 1)
-        E.addApproval('APRV', 1)
-        F.addApproval('APRV', 1)
-        G.addApproval('APRV', 1)
-        self.fake_gerrit.addEvent(C.addApproval('APRV', 1))
-
-        for x in range(8):
-            self.worker.release('.*-merge')
-            self.waitUntilSettled()
-        self.worker.hold_jobs_in_build = False
-        self.worker.release()
-        self.waitUntilSettled()
-
-        self.assertEqual(A.data['status'], 'MERGED')
-        self.assertEqual(B.data['status'], 'MERGED')
-        self.assertEqual(C.data['status'], 'MERGED')
-        self.assertEqual(D.data['status'], 'MERGED')
-        self.assertEqual(E.data['status'], 'MERGED')
-        self.assertEqual(F.data['status'], 'MERGED')
-        self.assertEqual(G.data['status'], 'MERGED')
-        self.assertEqual(A.reported, 2)
-        self.assertEqual(B.reported, 2)
-        self.assertEqual(C.reported, 2)
-        self.assertEqual(D.reported, 2)
-        self.assertEqual(E.reported, 2)
-        self.assertEqual(F.reported, 2)
-        self.assertEqual(G.reported, 2)
-        self.assertEqual(self.history[6].changes,
-                         '1,1 2,1 3,1 4,1 5,1 6,1 7,1')
-
-    def test_source_cache(self):
-        "Test that the source cache operates correctly"
-        self.worker.hold_jobs_in_build = True
-
-        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
-        B = self.fake_gerrit.addFakeChange('org/project', 'master', 'B')
-        X = self.fake_gerrit.addFakeChange('org/project', 'master', 'X')
-        A.addApproval('CRVW', 2)
-        B.addApproval('CRVW', 2)
-
-        M1 = self.fake_gerrit.addFakeChange('org/project', 'master', 'M1')
-        M1.setMerged()
-
-        B.setDependsOn(A, 1)
-        A.setDependsOn(M1, 1)
-
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-        self.fake_gerrit.addEvent(X.getPatchsetCreatedEvent(1))
-
-        self.waitUntilSettled()
-
-        for build in self.builds:
-            if build.parameters['ZUUL_PIPELINE'] == 'check':
-                build.release()
-        self.waitUntilSettled()
-        for build in self.builds:
-            if build.parameters['ZUUL_PIPELINE'] == 'check':
-                build.release()
-        self.waitUntilSettled()
-
-        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
-        self.waitUntilSettled()
-
-        self.log.debug("len %s" % self.fake_gerrit._change_cache.keys())
-        # there should still be changes in the cache
-        self.assertNotEqual(len(self.fake_gerrit._change_cache.keys()), 0)
-
-        self.worker.hold_jobs_in_build = False
-        self.worker.release()
-        self.waitUntilSettled()
-
-        self.assertEqual(A.data['status'], 'MERGED')
-        self.assertEqual(B.data['status'], 'MERGED')
-        self.assertEqual(A.queried, 2)  # Initial and isMerged
-        self.assertEqual(B.queried, 3)  # Initial A, refresh from B, isMerged
-
-    def test_can_merge(self):
-        "Test whether a change is ready to merge"
-        # TODO: move to test_gerrit (this is a unit test!)
-        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
-        source = self.sched.layout.pipelines['gate'].source
-        a = source._getChange(1, 2)
-        mgr = self.sched.layout.pipelines['gate'].manager
-        self.assertFalse(source.canMerge(a, mgr.getSubmitAllowNeeds()))
-
-        A.addApproval('CRVW', 2)
-        a = source._getChange(1, 2, refresh=True)
-        self.assertFalse(source.canMerge(a, mgr.getSubmitAllowNeeds()))
-
-        A.addApproval('APRV', 1)
-        a = source._getChange(1, 2, refresh=True)
-        self.assertTrue(source.canMerge(a, mgr.getSubmitAllowNeeds()))
-
-    def test_build_configuration(self):
-        "Test that zuul merges the right commits for testing"
-
-        self.gearman_server.hold_jobs_in_queue = True
-        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
-        B = self.fake_gerrit.addFakeChange('org/project', 'master', 'B')
-        C = self.fake_gerrit.addFakeChange('org/project', 'master', 'C')
-        A.addApproval('CRVW', 2)
-        B.addApproval('CRVW', 2)
-        C.addApproval('CRVW', 2)
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
-        self.fake_gerrit.addEvent(C.addApproval('APRV', 1))
-        self.waitUntilSettled()
-
-        self.gearman_server.release('.*-merge')
-        self.waitUntilSettled()
-        self.gearman_server.release('.*-merge')
-        self.waitUntilSettled()
-        self.gearman_server.release('.*-merge')
-        self.waitUntilSettled()
-        queue = self.gearman_server.getQueue()
-        ref = self.getParameter(queue[-1], 'ZUUL_REF')
-        self.gearman_server.hold_jobs_in_queue = False
-        self.gearman_server.release()
-        self.waitUntilSettled()
-
-        path = os.path.join(self.git_root, "org/project")
-        repo = git.Repo(path)
-        repo_messages = [c.message.strip() for c in repo.iter_commits(ref)]
-        repo_messages.reverse()
-        correct_messages = ['initial commit', 'A-1', 'B-1', 'C-1']
-        self.assertEqual(repo_messages, correct_messages)
-
-    def test_build_configuration_conflict(self):
-        "Test that merge conflicts are handled"
-
-        self.gearman_server.hold_jobs_in_queue = True
-        A = self.fake_gerrit.addFakeChange('org/conflict-project',
-                                           'master', 'A')
-        A.addPatchset(['conflict'])
-        B = self.fake_gerrit.addFakeChange('org/conflict-project',
-                                           'master', 'B')
-        B.addPatchset(['conflict'])
-        C = self.fake_gerrit.addFakeChange('org/conflict-project',
-                                           'master', 'C')
-        A.addApproval('CRVW', 2)
-        B.addApproval('CRVW', 2)
-        C.addApproval('CRVW', 2)
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
-        self.fake_gerrit.addEvent(C.addApproval('APRV', 1))
-        self.waitUntilSettled()
-
-        self.assertEqual(A.reported, 1)
-        self.assertEqual(B.reported, 1)
-        self.assertEqual(C.reported, 1)
-
-        self.gearman_server.release('.*-merge')
-        self.waitUntilSettled()
-        self.gearman_server.release('.*-merge')
-        self.waitUntilSettled()
-        self.gearman_server.release('.*-merge')
-        self.waitUntilSettled()
-
-        self.assertEqual(len(self.history), 2)  # A and C merge jobs
-
-        self.gearman_server.hold_jobs_in_queue = False
-        self.gearman_server.release()
-        self.waitUntilSettled()
-
-        self.assertEqual(A.data['status'], 'MERGED')
-        self.assertEqual(B.data['status'], 'NEW')
-        self.assertEqual(C.data['status'], 'MERGED')
-        self.assertEqual(A.reported, 2)
-        self.assertEqual(B.reported, 2)
-        self.assertEqual(C.reported, 2)
-        self.assertEqual(len(self.history), 6)
-
-    def test_post(self):
-        "Test that post jobs run"
-
-        e = {
-            "type": "ref-updated",
-            "submitter": {
-                "name": "User Name",
-            },
-            "refUpdate": {
-                "oldRev": "90f173846e3af9154517b88543ffbd1691f31366",
-                "newRev": "d479a0bfcb34da57a31adb2a595c0cf687812543",
-                "refName": "master",
-                "project": "org/project",
-            }
-        }
-        self.fake_gerrit.addEvent(e)
-        self.waitUntilSettled()
-
-        job_names = [x.name for x in self.history]
-        self.assertEqual(len(self.history), 1)
-        self.assertIn('project-post', job_names)
-
-    def test_post_ignore_deletes(self):
-        "Test that deleting refs does not trigger post jobs"
-
-        e = {
-            "type": "ref-updated",
-            "submitter": {
-                "name": "User Name",
-            },
-            "refUpdate": {
-                "oldRev": "90f173846e3af9154517b88543ffbd1691f31366",
-                "newRev": "0000000000000000000000000000000000000000",
-                "refName": "master",
-                "project": "org/project",
-            }
-        }
-        self.fake_gerrit.addEvent(e)
-        self.waitUntilSettled()
-
-        job_names = [x.name for x in self.history]
-        self.assertEqual(len(self.history), 0)
-        self.assertNotIn('project-post', job_names)
-
-    def test_post_ignore_deletes_negative(self):
-        "Test that deleting refs does trigger post jobs"
-
-        self.config.set('zuul', 'layout_config',
-                        'tests/fixtures/layout-dont-ignore-deletes.yaml')
-        self.sched.reconfigure(self.config)
-
-        e = {
-            "type": "ref-updated",
-            "submitter": {
-                "name": "User Name",
-            },
-            "refUpdate": {
-                "oldRev": "90f173846e3af9154517b88543ffbd1691f31366",
-                "newRev": "0000000000000000000000000000000000000000",
-                "refName": "master",
-                "project": "org/project",
-            }
-        }
-        self.fake_gerrit.addEvent(e)
-        self.waitUntilSettled()
-
-        job_names = [x.name for x in self.history]
-        self.assertEqual(len(self.history), 1)
-        self.assertIn('project-post', job_names)
-
-    def test_build_configuration_branch(self):
-        "Test that the right commits are on alternate branches"
-
-        self.gearman_server.hold_jobs_in_queue = True
-        A = self.fake_gerrit.addFakeChange('org/project', 'mp', 'A')
-        B = self.fake_gerrit.addFakeChange('org/project', 'mp', 'B')
-        C = self.fake_gerrit.addFakeChange('org/project', 'mp', 'C')
-        A.addApproval('CRVW', 2)
-        B.addApproval('CRVW', 2)
-        C.addApproval('CRVW', 2)
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
-        self.fake_gerrit.addEvent(C.addApproval('APRV', 1))
-        self.waitUntilSettled()
-
-        self.gearman_server.release('.*-merge')
-        self.waitUntilSettled()
-        self.gearman_server.release('.*-merge')
-        self.waitUntilSettled()
-        self.gearman_server.release('.*-merge')
-        self.waitUntilSettled()
-        queue = self.gearman_server.getQueue()
-        ref = self.getParameter(queue[-1], 'ZUUL_REF')
-        self.gearman_server.hold_jobs_in_queue = False
-        self.gearman_server.release()
-        self.waitUntilSettled()
-
-        path = os.path.join(self.git_root, "org/project")
-        repo = git.Repo(path)
-        repo_messages = [c.message.strip() for c in repo.iter_commits(ref)]
-        repo_messages.reverse()
-        correct_messages = ['initial commit', 'mp commit', 'A-1', 'B-1', 'C-1']
-        self.assertEqual(repo_messages, correct_messages)
-
-    def test_build_configuration_branch_interaction(self):
-        "Test that switching between branches works"
-        self.test_build_configuration()
-        self.test_build_configuration_branch()
-        # C has been merged, undo that
-        path = os.path.join(self.upstream_root, "org/project")
-        repo = git.Repo(path)
-        repo.heads.master.commit = repo.commit('init')
-        self.test_build_configuration()
-
-    def test_build_configuration_multi_branch(self):
-        "Test that dependent changes on multiple branches are merged"
-
-        self.gearman_server.hold_jobs_in_queue = True
-        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
-        B = self.fake_gerrit.addFakeChange('org/project', 'mp', 'B')
-        C = self.fake_gerrit.addFakeChange('org/project', 'master', 'C')
-        A.addApproval('CRVW', 2)
-        B.addApproval('CRVW', 2)
-        C.addApproval('CRVW', 2)
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
-        self.fake_gerrit.addEvent(C.addApproval('APRV', 1))
-        self.waitUntilSettled()
-        queue = self.gearman_server.getQueue()
-        job_A = None
-        for job in queue:
-            if 'project-merge' in job.name:
-                job_A = job
-        ref_A = self.getParameter(job_A, 'ZUUL_REF')
-        commit_A = self.getParameter(job_A, 'ZUUL_COMMIT')
-        self.log.debug("Got Zuul ref for change A: %s" % ref_A)
-        self.log.debug("Got Zuul commit for change A: %s" % commit_A)
-
-        self.gearman_server.release('.*-merge')
-        self.waitUntilSettled()
-        queue = self.gearman_server.getQueue()
-        job_B = None
-        for job in queue:
-            if 'project-merge' in job.name:
-                job_B = job
-        ref_B = self.getParameter(job_B, 'ZUUL_REF')
-        commit_B = self.getParameter(job_B, 'ZUUL_COMMIT')
-        self.log.debug("Got Zuul ref for change B: %s" % ref_B)
-        self.log.debug("Got Zuul commit for change B: %s" % commit_B)
-
-        self.gearman_server.release('.*-merge')
-        self.waitUntilSettled()
-        queue = self.gearman_server.getQueue()
-        for job in queue:
-            if 'project-merge' in job.name:
-                job_C = job
-        ref_C = self.getParameter(job_C, 'ZUUL_REF')
-        commit_C = self.getParameter(job_C, 'ZUUL_COMMIT')
-        self.log.debug("Got Zuul ref for change C: %s" % ref_C)
-        self.log.debug("Got Zuul commit for change C: %s" % commit_C)
-        self.gearman_server.hold_jobs_in_queue = False
-        self.gearman_server.release()
-        self.waitUntilSettled()
-
-        path = os.path.join(self.git_root, "org/project")
-        repo = git.Repo(path)
-
-        repo_messages = [c.message.strip()
-                         for c in repo.iter_commits(ref_C)]
-        repo_shas = [c.hexsha for c in repo.iter_commits(ref_C)]
-        repo_messages.reverse()
-        correct_messages = ['initial commit', 'A-1', 'C-1']
-        # Ensure the right commits are in the history for this ref
-        self.assertEqual(repo_messages, correct_messages)
-        # Ensure ZUUL_REF -> ZUUL_COMMIT
-        self.assertEqual(repo_shas[0], commit_C)
-
-        repo_messages = [c.message.strip()
-                         for c in repo.iter_commits(ref_B)]
-        repo_shas = [c.hexsha for c in repo.iter_commits(ref_B)]
-        repo_messages.reverse()
-        correct_messages = ['initial commit', 'mp commit', 'B-1']
-        self.assertEqual(repo_messages, correct_messages)
-        self.assertEqual(repo_shas[0], commit_B)
-
-        repo_messages = [c.message.strip()
-                         for c in repo.iter_commits(ref_A)]
-        repo_shas = [c.hexsha for c in repo.iter_commits(ref_A)]
-        repo_messages.reverse()
-        correct_messages = ['initial commit', 'A-1']
-        self.assertEqual(repo_messages, correct_messages)
-        self.assertEqual(repo_shas[0], commit_A)
-
-        self.assertNotEqual(ref_A, ref_B, ref_C)
-        self.assertNotEqual(commit_A, commit_B, commit_C)
-
-    def test_one_job_project(self):
-        "Test that queueing works with one job"
-        A = self.fake_gerrit.addFakeChange('org/one-job-project',
-                                           'master', 'A')
-        B = self.fake_gerrit.addFakeChange('org/one-job-project',
-                                           'master', 'B')
-        A.addApproval('CRVW', 2)
-        B.addApproval('CRVW', 2)
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
-        self.waitUntilSettled()
-
-        self.assertEqual(A.data['status'], 'MERGED')
-        self.assertEqual(A.reported, 2)
-        self.assertEqual(B.data['status'], 'MERGED')
-        self.assertEqual(B.reported, 2)
-
-    def test_job_from_templates_launched(self):
-        "Test whether a job generated via a template can be launched"
-
-        A = self.fake_gerrit.addFakeChange(
-            'org/templated-project', 'master', 'A')
-        self.fake_gerrit.addEvent(A.getPatchsetCreatedEvent(1))
-        self.waitUntilSettled()
-
-        self.assertEqual(self.getJobFromHistory('project-test1').result,
-                         'SUCCESS')
-        self.assertEqual(self.getJobFromHistory('project-test2').result,
-                         'SUCCESS')
-
-    def test_layered_templates(self):
-        "Test whether a job generated via a template can be launched"
-
-        A = self.fake_gerrit.addFakeChange(
-            'org/layered-project', 'master', 'A')
-        self.fake_gerrit.addEvent(A.getPatchsetCreatedEvent(1))
-        self.waitUntilSettled()
-
-        self.assertEqual(self.getJobFromHistory('project-test1').result,
-                         'SUCCESS')
-        self.assertEqual(self.getJobFromHistory('project-test2').result,
-                         'SUCCESS')
-        self.assertEqual(self.getJobFromHistory('layered-project-test3'
-                                                ).result, 'SUCCESS')
-        self.assertEqual(self.getJobFromHistory('layered-project-test4'
-                                                ).result, 'SUCCESS')
-        self.assertEqual(self.getJobFromHistory('layered-project-foo-test5'
-                                                ).result, 'SUCCESS')
-        self.assertEqual(self.getJobFromHistory('project-test6').result,
-                         'SUCCESS')
-
-    def test_dependent_changes_dequeue(self):
-        "Test that dependent patches are not needlessly tested"
-
-        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
-        B = self.fake_gerrit.addFakeChange('org/project', 'master', 'B')
-        C = self.fake_gerrit.addFakeChange('org/project', 'master', 'C')
-        A.addApproval('CRVW', 2)
-        B.addApproval('CRVW', 2)
-        C.addApproval('CRVW', 2)
-
-        M1 = self.fake_gerrit.addFakeChange('org/project', 'master', 'M1')
-        M1.setMerged()
-
-        # C -> B -> A -> M1
-
-        C.setDependsOn(B, 1)
-        B.setDependsOn(A, 1)
-        A.setDependsOn(M1, 1)
-
-        self.worker.addFailTest('project-merge', A)
-
-        self.fake_gerrit.addEvent(C.addApproval('APRV', 1))
-        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-
-        self.waitUntilSettled()
-
-        self.assertEqual(A.data['status'], 'NEW')
-        self.assertEqual(A.reported, 2)
-        self.assertEqual(B.data['status'], 'NEW')
-        self.assertEqual(B.reported, 2)
-        self.assertEqual(C.data['status'], 'NEW')
-        self.assertEqual(C.reported, 2)
-        self.assertEqual(len(self.history), 1)
-
-    def test_failing_dependent_changes(self):
-        "Test that failing dependent patches are taken out of stream"
-        self.worker.hold_jobs_in_build = True
-        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
-        B = self.fake_gerrit.addFakeChange('org/project', 'master', 'B')
-        C = self.fake_gerrit.addFakeChange('org/project', 'master', 'C')
-        D = self.fake_gerrit.addFakeChange('org/project', 'master', 'D')
-        E = self.fake_gerrit.addFakeChange('org/project', 'master', 'E')
-        A.addApproval('CRVW', 2)
-        B.addApproval('CRVW', 2)
-        C.addApproval('CRVW', 2)
-        D.addApproval('CRVW', 2)
-        E.addApproval('CRVW', 2)
-
-        # E, D -> C -> B, A
-
-        D.setDependsOn(C, 1)
-        C.setDependsOn(B, 1)
-
-        self.worker.addFailTest('project-test1', B)
-
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-        self.fake_gerrit.addEvent(D.addApproval('APRV', 1))
-        self.fake_gerrit.addEvent(C.addApproval('APRV', 1))
-        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
-        self.fake_gerrit.addEvent(E.addApproval('APRV', 1))
-
-        self.waitUntilSettled()
-        self.worker.release('.*-merge')
-        self.waitUntilSettled()
-        self.worker.release('.*-merge')
-        self.waitUntilSettled()
-        self.worker.release('.*-merge')
-        self.waitUntilSettled()
-        self.worker.release('.*-merge')
-        self.waitUntilSettled()
-        self.worker.release('.*-merge')
-        self.waitUntilSettled()
-
-        self.worker.hold_jobs_in_build = False
-        for build in self.builds:
-            if build.parameters['ZUUL_CHANGE'] != '1':
-                build.release()
-                self.waitUntilSettled()
-
-        self.worker.release()
-        self.waitUntilSettled()
-
-        self.assertEqual(A.data['status'], 'MERGED')
-        self.assertEqual(A.reported, 2)
-        self.assertIn('Build succeeded', A.messages[1])
-        self.assertEqual(B.data['status'], 'NEW')
-        self.assertEqual(B.reported, 2)
-        self.assertIn('Build failed', B.messages[1])
-        self.assertEqual(C.data['status'], 'NEW')
-        self.assertEqual(C.reported, 2)
-        self.assertIn('depends on a change', C.messages[1])
-        self.assertEqual(D.data['status'], 'NEW')
-        self.assertEqual(D.reported, 2)
-        self.assertIn('depends on a change', D.messages[1])
-        self.assertEqual(E.data['status'], 'MERGED')
-        self.assertEqual(E.reported, 2)
-        self.assertIn('Build succeeded', E.messages[1])
-        self.assertEqual(len(self.history), 18)
-
-    def test_head_is_dequeued_once(self):
-        "Test that if a change at the head fails it is dequeued only once"
-        # If it's dequeued more than once, we should see extra
-        # aborted jobs.
-
-        self.worker.hold_jobs_in_build = True
-        A = self.fake_gerrit.addFakeChange('org/project1', 'master', 'A')
-        B = self.fake_gerrit.addFakeChange('org/project1', 'master', 'B')
-        C = self.fake_gerrit.addFakeChange('org/project1', 'master', 'C')
-        A.addApproval('CRVW', 2)
-        B.addApproval('CRVW', 2)
-        C.addApproval('CRVW', 2)
-
-        self.worker.addFailTest('project1-test1', A)
-        self.worker.addFailTest('project1-test2', A)
-        self.worker.addFailTest('project1-project2-integration', A)
-
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
-        self.fake_gerrit.addEvent(C.addApproval('APRV', 1))
-
-        self.waitUntilSettled()
-
-        self.assertEqual(len(self.builds), 1)
-        self.assertEqual(self.builds[0].name, 'project1-merge')
-        self.assertTrue(self.job_has_changes(self.builds[0], A))
-
-        self.worker.release('.*-merge')
-        self.waitUntilSettled()
-        self.worker.release('.*-merge')
-        self.waitUntilSettled()
-        self.worker.release('.*-merge')
-        self.waitUntilSettled()
-
-        self.assertEqual(len(self.builds), 9)
-        self.assertEqual(self.builds[0].name, 'project1-test1')
-        self.assertEqual(self.builds[1].name, 'project1-test2')
-        self.assertEqual(self.builds[2].name, 'project1-project2-integration')
-        self.assertEqual(self.builds[3].name, 'project1-test1')
-        self.assertEqual(self.builds[4].name, 'project1-test2')
-        self.assertEqual(self.builds[5].name, 'project1-project2-integration')
-        self.assertEqual(self.builds[6].name, 'project1-test1')
-        self.assertEqual(self.builds[7].name, 'project1-test2')
-        self.assertEqual(self.builds[8].name, 'project1-project2-integration')
-
-        self.release(self.builds[0])
-        self.waitUntilSettled()
-
-        self.assertEqual(len(self.builds), 3)  # test2,integration, merge for B
-        self.assertEqual(self.countJobResults(self.history, 'ABORTED'), 6)
-
-        self.worker.hold_jobs_in_build = False
-        self.worker.release()
-        self.waitUntilSettled()
-
-        self.assertEqual(len(self.builds), 0)
-        self.assertEqual(len(self.history), 20)
-
-        self.assertEqual(A.data['status'], 'NEW')
-        self.assertEqual(B.data['status'], 'MERGED')
-        self.assertEqual(C.data['status'], 'MERGED')
-        self.assertEqual(A.reported, 2)
-        self.assertEqual(B.reported, 2)
-        self.assertEqual(C.reported, 2)
-
-    def test_nonvoting_job(self):
-        "Test that non-voting jobs don't vote."
-
-        A = self.fake_gerrit.addFakeChange('org/nonvoting-project',
-                                           'master', 'A')
-        A.addApproval('CRVW', 2)
-        self.worker.addFailTest('nonvoting-project-test2', A)
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-
-        self.waitUntilSettled()
-
-        self.assertEqual(A.data['status'], 'MERGED')
-        self.assertEqual(A.reported, 2)
-        self.assertEqual(
-            self.getJobFromHistory('nonvoting-project-merge').result,
-            'SUCCESS')
-        self.assertEqual(
-            self.getJobFromHistory('nonvoting-project-test1').result,
-            'SUCCESS')
-        self.assertEqual(
-            self.getJobFromHistory('nonvoting-project-test2').result,
-            'FAILURE')
-
-        for build in self.builds:
-            self.assertEqual(build.parameters['ZUUL_VOTING'], '0')
-
-    def test_check_queue_success(self):
-        "Test successful check queue jobs."
-
-        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
-        self.fake_gerrit.addEvent(A.getPatchsetCreatedEvent(1))
-
-        self.waitUntilSettled()
-
-        self.assertEqual(A.data['status'], 'NEW')
-        self.assertEqual(A.reported, 1)
-        self.assertEqual(self.getJobFromHistory('project-merge').result,
-                         'SUCCESS')
-        self.assertEqual(self.getJobFromHistory('project-test1').result,
-                         'SUCCESS')
-        self.assertEqual(self.getJobFromHistory('project-test2').result,
-                         'SUCCESS')
-
-    def test_check_queue_failure(self):
-        "Test failed check queue jobs."
-
-        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
-        self.worker.addFailTest('project-test2', A)
-        self.fake_gerrit.addEvent(A.getPatchsetCreatedEvent(1))
-
-        self.waitUntilSettled()
-
-        self.assertEqual(A.data['status'], 'NEW')
-        self.assertEqual(A.reported, 1)
-        self.assertEqual(self.getJobFromHistory('project-merge').result,
-                         'SUCCESS')
-        self.assertEqual(self.getJobFromHistory('project-test1').result,
-                         'SUCCESS')
-        self.assertEqual(self.getJobFromHistory('project-test2').result,
-                         'FAILURE')
-
-    def test_dependent_behind_dequeue(self):
-        "test that dependent changes behind dequeued changes work"
-        # This complicated test is a reproduction of a real life bug
-        self.sched.reconfigure(self.config)
-
-        self.worker.hold_jobs_in_build = True
-        A = self.fake_gerrit.addFakeChange('org/project1', 'master', 'A')
-        B = self.fake_gerrit.addFakeChange('org/project1', 'master', 'B')
-        C = self.fake_gerrit.addFakeChange('org/project2', 'master', 'C')
-        D = self.fake_gerrit.addFakeChange('org/project2', 'master', 'D')
-        E = self.fake_gerrit.addFakeChange('org/project2', 'master', 'E')
-        F = self.fake_gerrit.addFakeChange('org/project3', 'master', 'F')
-        D.setDependsOn(C, 1)
-        E.setDependsOn(D, 1)
-        A.addApproval('CRVW', 2)
-        B.addApproval('CRVW', 2)
-        C.addApproval('CRVW', 2)
-        D.addApproval('CRVW', 2)
-        E.addApproval('CRVW', 2)
-        F.addApproval('CRVW', 2)
-
-        A.fail_merge = True
-
-        # Change object re-use in the gerrit trigger is hidden if
-        # changes are added in quick succession; waiting makes it more
-        # like real life.
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-        self.waitUntilSettled()
-        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
-        self.waitUntilSettled()
-
-        self.worker.release('.*-merge')
-        self.waitUntilSettled()
-        self.worker.release('.*-merge')
-        self.waitUntilSettled()
-
-        self.fake_gerrit.addEvent(C.addApproval('APRV', 1))
-        self.waitUntilSettled()
-        self.fake_gerrit.addEvent(D.addApproval('APRV', 1))
-        self.waitUntilSettled()
-        self.fake_gerrit.addEvent(E.addApproval('APRV', 1))
-        self.waitUntilSettled()
-        self.fake_gerrit.addEvent(F.addApproval('APRV', 1))
-        self.waitUntilSettled()
-
-        self.worker.release('.*-merge')
-        self.waitUntilSettled()
-        self.worker.release('.*-merge')
-        self.waitUntilSettled()
-        self.worker.release('.*-merge')
-        self.waitUntilSettled()
-        self.worker.release('.*-merge')
-        self.waitUntilSettled()
-
-        # all jobs running
-
-        # Grab pointers to the jobs we want to release before
-        # releasing any, because list indexes may change as
-        # the jobs complete.
-        a, b, c = self.builds[:3]
-        a.release()
-        b.release()
-        c.release()
-        self.waitUntilSettled()
-
-        self.worker.hold_jobs_in_build = False
-        self.worker.release()
-        self.waitUntilSettled()
-
-        self.assertEqual(A.data['status'], 'NEW')
-        self.assertEqual(B.data['status'], 'MERGED')
-        self.assertEqual(C.data['status'], 'MERGED')
-        self.assertEqual(D.data['status'], 'MERGED')
-        self.assertEqual(E.data['status'], 'MERGED')
-        self.assertEqual(F.data['status'], 'MERGED')
-
-        self.assertEqual(A.reported, 2)
-        self.assertEqual(B.reported, 2)
-        self.assertEqual(C.reported, 2)
-        self.assertEqual(D.reported, 2)
-        self.assertEqual(E.reported, 2)
-        self.assertEqual(F.reported, 2)
-
-        self.assertEqual(self.countJobResults(self.history, 'ABORTED'), 15)
-        self.assertEqual(len(self.history), 44)
-
-    def test_merger_repack(self):
-        "Test that the merger works after a repack"
-
-        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
-        A.addApproval('CRVW', 2)
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-        self.waitUntilSettled()
-        self.assertEqual(self.getJobFromHistory('project-merge').result,
-                         'SUCCESS')
-        self.assertEqual(self.getJobFromHistory('project-test1').result,
-                         'SUCCESS')
-        self.assertEqual(self.getJobFromHistory('project-test2').result,
-                         'SUCCESS')
-        self.assertEqual(A.data['status'], 'MERGED')
-        self.assertEqual(A.reported, 2)
-        self.assertEmptyQueues()
-        self.worker.build_history = []
-
-        path = os.path.join(self.git_root, "org/project")
-        print repack_repo(path)
-
-        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
-        A.addApproval('CRVW', 2)
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-        self.waitUntilSettled()
-        self.assertEqual(self.getJobFromHistory('project-merge').result,
-                         'SUCCESS')
-        self.assertEqual(self.getJobFromHistory('project-test1').result,
-                         'SUCCESS')
-        self.assertEqual(self.getJobFromHistory('project-test2').result,
-                         'SUCCESS')
-        self.assertEqual(A.data['status'], 'MERGED')
-        self.assertEqual(A.reported, 2)
-
-    def test_merger_repack_large_change(self):
-        "Test that the merger works with large changes after a repack"
-        # https://bugs.launchpad.net/zuul/+bug/1078946
-        # This test assumes the repo is already cloned; make sure it is
-        url = self.fake_gerrit.getGitUrl(
-            self.sched.layout.projects['org/project1'])
-        self.merge_server.merger.addProject('org/project1', url)
-        A = self.fake_gerrit.addFakeChange('org/project1', 'master', 'A')
-        A.addPatchset(large=True)
-        path = os.path.join(self.upstream_root, "org/project1")
-        print repack_repo(path)
-        path = os.path.join(self.git_root, "org/project1")
-        print repack_repo(path)
-
-        A.addApproval('CRVW', 2)
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-        self.waitUntilSettled()
-        self.assertEqual(self.getJobFromHistory('project1-merge').result,
-                         'SUCCESS')
-        self.assertEqual(self.getJobFromHistory('project1-test1').result,
-                         'SUCCESS')
-        self.assertEqual(self.getJobFromHistory('project1-test2').result,
-                         'SUCCESS')
-        self.assertEqual(A.data['status'], 'MERGED')
-        self.assertEqual(A.reported, 2)
-
-    def test_nonexistent_job(self):
-        "Test launching a job that doesn't exist"
-        # Set to the state immediately after a restart
-        self.resetGearmanServer()
-        self.launcher.negative_function_cache_ttl = 0
-
-        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
-        A.addApproval('CRVW', 2)
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-        # There may be a thread about to report a lost change
-        while A.reported < 2:
-            self.waitUntilSettled()
-        job_names = [x.name for x in self.history]
-        self.assertFalse(job_names)
-        self.assertEqual(A.data['status'], 'NEW')
-        self.assertEqual(A.reported, 2)
-        self.assertEmptyQueues()
-
-        # Make sure things still work:
-        self.registerJobs()
-        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
-        A.addApproval('CRVW', 2)
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-        self.waitUntilSettled()
-        self.assertEqual(self.getJobFromHistory('project-merge').result,
-                         'SUCCESS')
-        self.assertEqual(self.getJobFromHistory('project-test1').result,
-                         'SUCCESS')
-        self.assertEqual(self.getJobFromHistory('project-test2').result,
-                         'SUCCESS')
-        self.assertEqual(A.data['status'], 'MERGED')
-        self.assertEqual(A.reported, 2)
-
-    def test_single_nonexistent_post_job(self):
-        "Test launching a single post job that doesn't exist"
-        e = {
-            "type": "ref-updated",
-            "submitter": {
-                "name": "User Name",
-            },
-            "refUpdate": {
-                "oldRev": "90f173846e3af9154517b88543ffbd1691f31366",
-                "newRev": "d479a0bfcb34da57a31adb2a595c0cf687812543",
-                "refName": "master",
-                "project": "org/project",
-            }
-        }
-        # Set to the state immediately after a restart
-        self.resetGearmanServer()
-        self.launcher.negative_function_cache_ttl = 0
-
-        self.fake_gerrit.addEvent(e)
-        self.waitUntilSettled()
-
-        self.assertEqual(len(self.history), 0)
-
-    def test_new_patchset_dequeues_old(self):
-        "Test that a new patchset causes the old to be dequeued"
-        # D -> C (depends on B) -> B (depends on A) -> A -> M
-        self.worker.hold_jobs_in_build = True
-        M = self.fake_gerrit.addFakeChange('org/project', 'master', 'M')
-        M.setMerged()
-
-        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
-        B = self.fake_gerrit.addFakeChange('org/project', 'master', 'B')
-        C = self.fake_gerrit.addFakeChange('org/project', 'master', 'C')
-        D = self.fake_gerrit.addFakeChange('org/project', 'master', 'D')
-        A.addApproval('CRVW', 2)
-        B.addApproval('CRVW', 2)
-        C.addApproval('CRVW', 2)
-        D.addApproval('CRVW', 2)
-
-        C.setDependsOn(B, 1)
-        B.setDependsOn(A, 1)
-        A.setDependsOn(M, 1)
-
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
-        self.fake_gerrit.addEvent(C.addApproval('APRV', 1))
-        self.fake_gerrit.addEvent(D.addApproval('APRV', 1))
-        self.waitUntilSettled()
-
-        B.addPatchset()
-        self.fake_gerrit.addEvent(B.getPatchsetCreatedEvent(2))
-        self.waitUntilSettled()
-
-        self.worker.hold_jobs_in_build = False
-        self.worker.release()
-        self.waitUntilSettled()
-
-        self.assertEqual(A.data['status'], 'MERGED')
-        self.assertEqual(A.reported, 2)
-        self.assertEqual(B.data['status'], 'NEW')
-        self.assertEqual(B.reported, 2)
-        self.assertEqual(C.data['status'], 'NEW')
-        self.assertEqual(C.reported, 2)
-        self.assertEqual(D.data['status'], 'MERGED')
-        self.assertEqual(D.reported, 2)
-        self.assertEqual(len(self.history), 9)  # 3 each for A, B, D.
-
-    def test_new_patchset_check(self):
-        "Test a new patchset in check"
-
-        self.worker.hold_jobs_in_build = True
-
-        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
-        B = self.fake_gerrit.addFakeChange('org/project', 'master', 'B')
-        check_pipeline = self.sched.layout.pipelines['check']
-
-        # Add two git-dependent changes
-        B.setDependsOn(A, 1)
-        self.fake_gerrit.addEvent(B.getPatchsetCreatedEvent(1))
-        self.waitUntilSettled()
-        self.fake_gerrit.addEvent(A.getPatchsetCreatedEvent(1))
-        self.waitUntilSettled()
-
-        # A live item, and a non-live/live pair
-        items = check_pipeline.getAllItems()
-        self.assertEqual(len(items), 3)
-
-        self.assertEqual(items[0].change.number, '1')
-        self.assertEqual(items[0].change.patchset, '1')
-        self.assertFalse(items[0].live)
-
-        self.assertEqual(items[1].change.number, '2')
-        self.assertEqual(items[1].change.patchset, '1')
-        self.assertTrue(items[1].live)
-
-        self.assertEqual(items[2].change.number, '1')
-        self.assertEqual(items[2].change.patchset, '1')
-        self.assertTrue(items[2].live)
-
-        # Add a new patchset to A
-        A.addPatchset()
-        self.fake_gerrit.addEvent(A.getPatchsetCreatedEvent(2))
-        self.waitUntilSettled()
-
-        # The live copy of A,1 should be gone, but the non-live and B
-        # should continue, and we should have a new A,2
-        items = check_pipeline.getAllItems()
-        self.assertEqual(len(items), 3)
-
-        self.assertEqual(items[0].change.number, '1')
-        self.assertEqual(items[0].change.patchset, '1')
-        self.assertFalse(items[0].live)
-
-        self.assertEqual(items[1].change.number, '2')
-        self.assertEqual(items[1].change.patchset, '1')
-        self.assertTrue(items[1].live)
-
-        self.assertEqual(items[2].change.number, '1')
-        self.assertEqual(items[2].change.patchset, '2')
-        self.assertTrue(items[2].live)
-
-        # Add a new patchset to B
-        B.addPatchset()
-        self.fake_gerrit.addEvent(B.getPatchsetCreatedEvent(2))
-        self.waitUntilSettled()
-
-        # The live copy of B,1 should be gone, and it's non-live copy of A,1
-        # but we should have a new B,2 (still based on A,1)
-        items = check_pipeline.getAllItems()
-        self.assertEqual(len(items), 3)
-
-        self.assertEqual(items[0].change.number, '1')
-        self.assertEqual(items[0].change.patchset, '2')
-        self.assertTrue(items[0].live)
-
-        self.assertEqual(items[1].change.number, '1')
-        self.assertEqual(items[1].change.patchset, '1')
-        self.assertFalse(items[1].live)
-
-        self.assertEqual(items[2].change.number, '2')
-        self.assertEqual(items[2].change.patchset, '2')
-        self.assertTrue(items[2].live)
-
-        self.builds[0].release()
-        self.waitUntilSettled()
-        self.builds[0].release()
-        self.waitUntilSettled()
-        self.worker.hold_jobs_in_build = False
-        self.worker.release()
-        self.waitUntilSettled()
-
-        self.assertEqual(A.reported, 1)
-        self.assertEqual(B.reported, 1)
-        self.assertEqual(self.history[0].result, 'ABORTED')
-        self.assertEqual(self.history[0].changes, '1,1')
-        self.assertEqual(self.history[1].result, 'ABORTED')
-        self.assertEqual(self.history[1].changes, '1,1 2,1')
-        self.assertEqual(self.history[2].result, 'SUCCESS')
-        self.assertEqual(self.history[2].changes, '1,2')
-        self.assertEqual(self.history[3].result, 'SUCCESS')
-        self.assertEqual(self.history[3].changes, '1,1 2,2')
-
-    def test_abandoned_gate(self):
-        "Test that an abandoned change is dequeued from gate"
-
-        self.worker.hold_jobs_in_build = True
-
-        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
-        A.addApproval('CRVW', 2)
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-        self.waitUntilSettled()
-        self.assertEqual(len(self.builds), 1, "One job being built (on hold)")
-        self.assertEqual(self.builds[0].name, 'project-merge')
-
-        self.fake_gerrit.addEvent(A.getChangeAbandonedEvent())
-        self.waitUntilSettled()
-
-        self.worker.release('.*-merge')
-        self.waitUntilSettled()
-
-        self.assertEqual(len(self.builds), 0, "No job running")
-        self.assertEqual(len(self.history), 1, "Only one build in history")
-        self.assertEqual(self.history[0].result, 'ABORTED',
-                         "Build should have been aborted")
-        self.assertEqual(A.reported, 1,
-                         "Abandoned gate change should report only start")
-
-    def test_abandoned_check(self):
-        "Test that an abandoned change is dequeued from check"
-
-        self.worker.hold_jobs_in_build = True
-
-        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
-        B = self.fake_gerrit.addFakeChange('org/project', 'master', 'B')
-        check_pipeline = self.sched.layout.pipelines['check']
-
-        # Add two git-dependent changes
-        B.setDependsOn(A, 1)
-        self.fake_gerrit.addEvent(B.getPatchsetCreatedEvent(1))
-        self.waitUntilSettled()
-        self.fake_gerrit.addEvent(A.getPatchsetCreatedEvent(1))
-        self.waitUntilSettled()
-        # A live item, and a non-live/live pair
-        items = check_pipeline.getAllItems()
-        self.assertEqual(len(items), 3)
-
-        self.assertEqual(items[0].change.number, '1')
-        self.assertFalse(items[0].live)
-
-        self.assertEqual(items[1].change.number, '2')
-        self.assertTrue(items[1].live)
-
-        self.assertEqual(items[2].change.number, '1')
-        self.assertTrue(items[2].live)
-
-        # Abandon A
-        self.fake_gerrit.addEvent(A.getChangeAbandonedEvent())
-        self.waitUntilSettled()
-
-        # The live copy of A should be gone, but the non-live and B
-        # should continue
-        items = check_pipeline.getAllItems()
-        self.assertEqual(len(items), 2)
-
-        self.assertEqual(items[0].change.number, '1')
-        self.assertFalse(items[0].live)
-
-        self.assertEqual(items[1].change.number, '2')
-        self.assertTrue(items[1].live)
-
-        self.worker.hold_jobs_in_build = False
-        self.worker.release()
-        self.waitUntilSettled()
-
-        self.assertEqual(len(self.history), 4)
-        self.assertEqual(self.history[0].result, 'ABORTED',
-                         'Build should have been aborted')
-        self.assertEqual(A.reported, 0, "Abandoned change should not report")
-        self.assertEqual(B.reported, 1, "Change should report")
-
-    def test_abandoned_not_timer(self):
-        "Test that an abandoned change does not cancel timer jobs"
-
-        self.worker.hold_jobs_in_build = True
-
-        # Start timer trigger - also org/project
-        self.config.set('zuul', 'layout_config',
-                        'tests/fixtures/layout-idle.yaml')
-        self.sched.reconfigure(self.config)
-        self.registerJobs()
-        # The pipeline triggers every second, so we should have seen
-        # several by now.
-        time.sleep(5)
-        self.waitUntilSettled()
-        # Stop queuing timer triggered jobs so that the assertions
-        # below don't race against more jobs being queued.
-        self.config.set('zuul', 'layout_config',
-                        'tests/fixtures/layout-no-timer.yaml')
-        self.sched.reconfigure(self.config)
-        self.registerJobs()
-        self.assertEqual(len(self.builds), 2, "Two timer jobs")
-
-        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
-        self.fake_gerrit.addEvent(A.getPatchsetCreatedEvent(1))
-        self.waitUntilSettled()
-        self.assertEqual(len(self.builds), 3, "One change plus two timer jobs")
-
-        self.fake_gerrit.addEvent(A.getChangeAbandonedEvent())
-        self.waitUntilSettled()
-
-        self.assertEqual(len(self.builds), 2, "Two timer jobs remain")
-
-        self.worker.release()
-        self.waitUntilSettled()
-
-    def test_zuul_url_return(self):
-        "Test if ZUUL_URL is returning when zuul_url is set in zuul.conf"
-        self.assertTrue(self.sched.config.has_option('merger', 'zuul_url'))
-        self.worker.hold_jobs_in_build = True
-
-        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
-        A.addApproval('CRVW', 2)
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-        self.waitUntilSettled()
-
-        self.assertEqual(len(self.builds), 1)
-        for build in self.builds:
-            self.assertTrue('ZUUL_URL' in build.parameters)
-
-        self.worker.hold_jobs_in_build = False
-        self.worker.release()
-        self.waitUntilSettled()
-
-    def test_new_patchset_dequeues_old_on_head(self):
-        "Test that a new patchset causes the old to be dequeued (at head)"
-        # D -> C (depends on B) -> B (depends on A) -> A -> M
-        self.worker.hold_jobs_in_build = True
-        M = self.fake_gerrit.addFakeChange('org/project', 'master', 'M')
-        M.setMerged()
-        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
-        B = self.fake_gerrit.addFakeChange('org/project', 'master', 'B')
-        C = self.fake_gerrit.addFakeChange('org/project', 'master', 'C')
-        D = self.fake_gerrit.addFakeChange('org/project', 'master', 'D')
-        A.addApproval('CRVW', 2)
-        B.addApproval('CRVW', 2)
-        C.addApproval('CRVW', 2)
-        D.addApproval('CRVW', 2)
-
-        C.setDependsOn(B, 1)
-        B.setDependsOn(A, 1)
-        A.setDependsOn(M, 1)
-
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
-        self.fake_gerrit.addEvent(C.addApproval('APRV', 1))
-        self.fake_gerrit.addEvent(D.addApproval('APRV', 1))
-        self.waitUntilSettled()
-
-        A.addPatchset()
-        self.fake_gerrit.addEvent(A.getPatchsetCreatedEvent(2))
-        self.waitUntilSettled()
-
-        self.worker.hold_jobs_in_build = False
-        self.worker.release()
-        self.waitUntilSettled()
-
-        self.assertEqual(A.data['status'], 'NEW')
-        self.assertEqual(A.reported, 2)
-        self.assertEqual(B.data['status'], 'NEW')
-        self.assertEqual(B.reported, 2)
-        self.assertEqual(C.data['status'], 'NEW')
-        self.assertEqual(C.reported, 2)
-        self.assertEqual(D.data['status'], 'MERGED')
-        self.assertEqual(D.reported, 2)
-        self.assertEqual(len(self.history), 7)
-
-    def test_new_patchset_dequeues_old_without_dependents(self):
-        "Test that a new patchset causes only the old to be dequeued"
-        self.worker.hold_jobs_in_build = True
-        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
-        B = self.fake_gerrit.addFakeChange('org/project', 'master', 'B')
-        C = self.fake_gerrit.addFakeChange('org/project', 'master', 'C')
-        A.addApproval('CRVW', 2)
-        B.addApproval('CRVW', 2)
-        C.addApproval('CRVW', 2)
-
-        self.fake_gerrit.addEvent(C.addApproval('APRV', 1))
-        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-        self.waitUntilSettled()
-
-        B.addPatchset()
-        self.fake_gerrit.addEvent(B.getPatchsetCreatedEvent(2))
-        self.waitUntilSettled()
-
-        self.worker.hold_jobs_in_build = False
-        self.worker.release()
-        self.waitUntilSettled()
-
-        self.assertEqual(A.data['status'], 'MERGED')
-        self.assertEqual(A.reported, 2)
-        self.assertEqual(B.data['status'], 'NEW')
-        self.assertEqual(B.reported, 2)
-        self.assertEqual(C.data['status'], 'MERGED')
-        self.assertEqual(C.reported, 2)
-        self.assertEqual(len(self.history), 9)
-
-    def test_new_patchset_dequeues_old_independent_queue(self):
-        "Test that a new patchset causes the old to be dequeued (independent)"
-        self.worker.hold_jobs_in_build = True
-        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
-        B = self.fake_gerrit.addFakeChange('org/project', 'master', 'B')
-        C = self.fake_gerrit.addFakeChange('org/project', 'master', 'C')
-        self.fake_gerrit.addEvent(A.getPatchsetCreatedEvent(1))
-        self.fake_gerrit.addEvent(B.getPatchsetCreatedEvent(1))
-        self.fake_gerrit.addEvent(C.getPatchsetCreatedEvent(1))
-        self.waitUntilSettled()
-
-        B.addPatchset()
-        self.fake_gerrit.addEvent(B.getPatchsetCreatedEvent(2))
-        self.waitUntilSettled()
-
-        self.worker.hold_jobs_in_build = False
-        self.worker.release()
-        self.waitUntilSettled()
-
-        self.assertEqual(A.data['status'], 'NEW')
-        self.assertEqual(A.reported, 1)
-        self.assertEqual(B.data['status'], 'NEW')
-        self.assertEqual(B.reported, 1)
-        self.assertEqual(C.data['status'], 'NEW')
-        self.assertEqual(C.reported, 1)
-        self.assertEqual(len(self.history), 10)
-        self.assertEqual(self.countJobResults(self.history, 'ABORTED'), 1)
-
-    def test_noop_job(self):
-        "Test that the internal noop job works"
-        A = self.fake_gerrit.addFakeChange('org/noop-project', 'master', 'A')
-        A.addApproval('CRVW', 2)
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-        self.waitUntilSettled()
-
-        self.assertEqual(len(self.gearman_server.getQueue()), 0)
-        self.assertTrue(self.sched._areAllBuildsComplete())
-        self.assertEqual(len(self.history), 0)
-        self.assertEqual(A.data['status'], 'MERGED')
-        self.assertEqual(A.reported, 2)
-
-    def test_no_job_project(self):
-        "Test that reports with no jobs don't get sent"
-        A = self.fake_gerrit.addFakeChange('org/no-jobs-project',
-                                           'master', 'A')
-        self.fake_gerrit.addEvent(A.getPatchsetCreatedEvent(1))
-        self.waitUntilSettled()
-
-        # Change wasn't reported to
-        self.assertEqual(A.reported, False)
-
-        # Check queue is empty afterwards
-        check_pipeline = self.sched.layout.pipelines['check']
-        items = check_pipeline.getAllItems()
-        self.assertEqual(len(items), 0)
-
-        self.assertEqual(len(self.history), 0)
-
-    def test_zuul_refs(self):
-        "Test that zuul refs exist and have the right changes"
-        self.worker.hold_jobs_in_build = True
-        M1 = self.fake_gerrit.addFakeChange('org/project1', 'master', 'M1')
-        M1.setMerged()
-        M2 = self.fake_gerrit.addFakeChange('org/project2', 'master', 'M2')
-        M2.setMerged()
-
-        A = self.fake_gerrit.addFakeChange('org/project1', 'master', 'A')
-        B = self.fake_gerrit.addFakeChange('org/project1', 'master', 'B')
-        C = self.fake_gerrit.addFakeChange('org/project2', 'master', 'C')
-        D = self.fake_gerrit.addFakeChange('org/project2', 'master', 'D')
-        A.addApproval('CRVW', 2)
-        B.addApproval('CRVW', 2)
-        C.addApproval('CRVW', 2)
-        D.addApproval('CRVW', 2)
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
-        self.fake_gerrit.addEvent(C.addApproval('APRV', 1))
-        self.fake_gerrit.addEvent(D.addApproval('APRV', 1))
-
-        self.waitUntilSettled()
-        self.worker.release('.*-merge')
-        self.waitUntilSettled()
-        self.worker.release('.*-merge')
-        self.waitUntilSettled()
-        self.worker.release('.*-merge')
-        self.waitUntilSettled()
-        self.worker.release('.*-merge')
-        self.waitUntilSettled()
-
-        a_zref = b_zref = c_zref = d_zref = None
-        for x in self.builds:
-            if x.parameters['ZUUL_CHANGE'] == '3':
-                a_zref = x.parameters['ZUUL_REF']
-            if x.parameters['ZUUL_CHANGE'] == '4':
-                b_zref = x.parameters['ZUUL_REF']
-            if x.parameters['ZUUL_CHANGE'] == '5':
-                c_zref = x.parameters['ZUUL_REF']
-            if x.parameters['ZUUL_CHANGE'] == '6':
-                d_zref = x.parameters['ZUUL_REF']
-
-        # There are... four... refs.
-        self.assertIsNotNone(a_zref)
-        self.assertIsNotNone(b_zref)
-        self.assertIsNotNone(c_zref)
-        self.assertIsNotNone(d_zref)
-
-        # And they should all be different
-        refs = set([a_zref, b_zref, c_zref, d_zref])
-        self.assertEqual(len(refs), 4)
-
-        # a ref should have a, not b, and should not be in project2
-        self.assertTrue(self.ref_has_change(a_zref, A))
-        self.assertFalse(self.ref_has_change(a_zref, B))
-        self.assertFalse(self.ref_has_change(a_zref, M2))
-
-        # b ref should have a and b, and should not be in project2
-        self.assertTrue(self.ref_has_change(b_zref, A))
-        self.assertTrue(self.ref_has_change(b_zref, B))
-        self.assertFalse(self.ref_has_change(b_zref, M2))
-
-        # c ref should have a and b in 1, c in 2
-        self.assertTrue(self.ref_has_change(c_zref, A))
-        self.assertTrue(self.ref_has_change(c_zref, B))
-        self.assertTrue(self.ref_has_change(c_zref, C))
-        self.assertFalse(self.ref_has_change(c_zref, D))
-
-        # d ref should have a and b in 1, c and d in 2
-        self.assertTrue(self.ref_has_change(d_zref, A))
-        self.assertTrue(self.ref_has_change(d_zref, B))
-        self.assertTrue(self.ref_has_change(d_zref, C))
-        self.assertTrue(self.ref_has_change(d_zref, D))
-
-        self.worker.hold_jobs_in_build = False
-        self.worker.release()
-        self.waitUntilSettled()
-
-        self.assertEqual(A.data['status'], 'MERGED')
-        self.assertEqual(A.reported, 2)
-        self.assertEqual(B.data['status'], 'MERGED')
-        self.assertEqual(B.reported, 2)
-        self.assertEqual(C.data['status'], 'MERGED')
-        self.assertEqual(C.reported, 2)
-        self.assertEqual(D.data['status'], 'MERGED')
-        self.assertEqual(D.reported, 2)
-
-    def test_rerun_on_error(self):
-        "Test that if a worker fails to run a job, it is run again"
-        self.worker.hold_jobs_in_build = True
-        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
-        A.addApproval('CRVW', 2)
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-        self.waitUntilSettled()
-
-        self.builds[0].run_error = True
-        self.worker.hold_jobs_in_build = False
-        self.worker.release()
-        self.waitUntilSettled()
-        self.assertEqual(self.countJobResults(self.history, 'RUN_ERROR'), 1)
-        self.assertEqual(self.countJobResults(self.history, 'SUCCESS'), 3)
-
-    def test_statsd(self):
-        "Test each of the statsd methods used in the scheduler"
-        import extras
-        statsd = extras.try_import('statsd.statsd')
-        statsd.incr('test-incr')
-        statsd.timing('test-timing', 3)
-        statsd.gauge('test-gauge', 12)
-        self.assertReportedStat('test-incr', '1|c')
-        self.assertReportedStat('test-timing', '3|ms')
-        self.assertReportedStat('test-gauge', '12|g')
-
-    def test_stuck_job_cleanup(self):
-        "Test that pending jobs are cleaned up if removed from layout"
-        # This job won't be registered at startup because it is not in
-        # the standard layout, but we need it to already be registerd
-        # for when we reconfigure, as that is when Zuul will attempt
-        # to run the new job.
-        self.worker.registerFunction('build:gate-noop')
-        self.gearman_server.hold_jobs_in_queue = True
-        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
-        A.addApproval('CRVW', 2)
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-        self.waitUntilSettled()
-        self.assertEqual(len(self.gearman_server.getQueue()), 1)
-
-        self.config.set('zuul', 'layout_config',
-                        'tests/fixtures/layout-no-jobs.yaml')
-        self.sched.reconfigure(self.config)
-        self.waitUntilSettled()
-
-        self.gearman_server.release('gate-noop')
-        self.waitUntilSettled()
-        self.assertEqual(len(self.gearman_server.getQueue()), 0)
-        self.assertTrue(self.sched._areAllBuildsComplete())
-
-        self.assertEqual(len(self.history), 1)
-        self.assertEqual(self.history[0].name, 'gate-noop')
-        self.assertEqual(self.history[0].result, 'SUCCESS')
-
-    def test_file_head(self):
-        # This is a regression test for an observed bug.  A change
-        # with a file named "HEAD" in the root directory of the repo
-        # was processed by a merger.  It then was unable to reset the
-        # repo because of:
-        #   GitCommandError: 'git reset --hard HEAD' returned
-        #       with exit code 128
-        #   stderr: 'fatal: ambiguous argument 'HEAD': both revision
-        #       and filename
-        #   Use '--' to separate filenames from revisions'
-
-        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
-        A.addPatchset(['HEAD'])
-        B = self.fake_gerrit.addFakeChange('org/project', 'master', 'B')
-
-        self.fake_gerrit.addEvent(A.getPatchsetCreatedEvent(2))
-        self.waitUntilSettled()
-
-        self.fake_gerrit.addEvent(B.getPatchsetCreatedEvent(1))
-        self.waitUntilSettled()
-
-        self.assertIn('Build succeeded', A.messages[0])
-        self.assertIn('Build succeeded', B.messages[0])
-
-    def test_file_jobs(self):
-        "Test that file jobs run only when appropriate"
-        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
-        A.addPatchset(['pip-requires'])
-        B = self.fake_gerrit.addFakeChange('org/project', 'master', 'B')
-        A.addApproval('CRVW', 2)
-        B.addApproval('CRVW', 2)
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
-        self.waitUntilSettled()
-
-        testfile_jobs = [x for x in self.history
-                         if x.name == 'project-testfile']
-
-        self.assertEqual(len(testfile_jobs), 1)
-        self.assertEqual(testfile_jobs[0].changes, '1,2')
-        self.assertEqual(A.data['status'], 'MERGED')
-        self.assertEqual(A.reported, 2)
-        self.assertEqual(B.data['status'], 'MERGED')
-        self.assertEqual(B.reported, 2)
-
-    def _test_skip_if_jobs(self, branch, should_skip):
-        "Test that jobs with a skip-if filter run only when appropriate"
-        self.config.set('zuul', 'layout_config',
-                        'tests/fixtures/layout-skip-if.yaml')
-        self.sched.reconfigure(self.config)
-        self.registerJobs()
-
-        change = self.fake_gerrit.addFakeChange('org/project',
-                                                branch,
-                                                'test skip-if')
-        self.fake_gerrit.addEvent(change.getPatchsetCreatedEvent(1))
-        self.waitUntilSettled()
-
-        tested_change_ids = [x.changes[0] for x in self.history
-                             if x.name == 'project-test-skip-if']
-
-        if should_skip:
-            self.assertEqual([], tested_change_ids)
-        else:
-            self.assertIn(change.data['number'], tested_change_ids)
-
-    def test_skip_if_match_skips_job(self):
-        self._test_skip_if_jobs(branch='master', should_skip=True)
-
-    def test_skip_if_no_match_runs_job(self):
-        self._test_skip_if_jobs(branch='mp', should_skip=False)
-
-    def test_test_config(self):
-        "Test that we can test the config"
-        self.sched.testConfig(self.config.get('zuul', 'layout_config'),
-                              self.connections)
-
-    def test_build_description(self):
-        "Test that build descriptions update"
-        self.worker.registerFunction('set_description:' +
-                                     self.worker.worker_id)
-
-        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
-        A.addApproval('CRVW', 2)
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-        self.waitUntilSettled()
-        desc = self.history[0].description
-        self.log.debug("Description: %s" % desc)
-        self.assertTrue(re.search("Branch.*master", desc))
-        self.assertTrue(re.search("Pipeline.*gate", desc))
-        self.assertTrue(re.search("project-merge.*SUCCESS", desc))
-        self.assertTrue(re.search("project-test1.*SUCCESS", desc))
-        self.assertTrue(re.search("project-test2.*SUCCESS", desc))
-        self.assertTrue(re.search("Reported result.*SUCCESS", desc))
-
-    def test_queue_names(self):
-        "Test shared change queue names"
-        project1 = self.sched.layout.projects['org/project1']
-        project2 = self.sched.layout.projects['org/project2']
-        q1 = self.sched.layout.pipelines['gate'].getQueue(project1)
-        q2 = self.sched.layout.pipelines['gate'].getQueue(project2)
-        self.assertEqual(q1.name, 'integration')
-        self.assertEqual(q2.name, 'integration')
-
-        self.config.set('zuul', 'layout_config',
-                        'tests/fixtures/layout-bad-queue.yaml')
-        with testtools.ExpectedException(
-            Exception, "More than one name assigned to change queue"):
-            self.sched.reconfigure(self.config)
-
-    def test_queue_precedence(self):
-        "Test that queue precedence works"
-
-        self.gearman_server.hold_jobs_in_queue = True
-        self.worker.hold_jobs_in_build = True
-        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
-        self.fake_gerrit.addEvent(A.getPatchsetCreatedEvent(1))
-        A.addApproval('CRVW', 2)
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-
-        self.waitUntilSettled()
-        self.gearman_server.hold_jobs_in_queue = False
-        self.gearman_server.release()
-        self.waitUntilSettled()
-
-        # Run one build at a time to ensure non-race order:
-        self.orderedRelease()
-        self.worker.hold_jobs_in_build = False
-        self.waitUntilSettled()
-
-        self.log.debug(self.history)
-        self.assertEqual(self.history[0].pipeline, 'gate')
-        self.assertEqual(self.history[1].pipeline, 'check')
-        self.assertEqual(self.history[2].pipeline, 'gate')
-        self.assertEqual(self.history[3].pipeline, 'gate')
-        self.assertEqual(self.history[4].pipeline, 'check')
-        self.assertEqual(self.history[5].pipeline, 'check')
-
-    def test_json_status(self):
-        "Test that we can retrieve JSON status info"
-        self.worker.hold_jobs_in_build = True
-        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
-        A.addApproval('CRVW', 2)
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-        self.waitUntilSettled()
-
-        self.worker.release('project-merge')
-        self.waitUntilSettled()
-
-        port = self.webapp.server.socket.getsockname()[1]
-
-        req = urllib2.Request("http://localhost:%s/status.json" % port)
-        f = urllib2.urlopen(req)
-        headers = f.info()
-        self.assertIn('Content-Length', headers)
-        self.assertIn('Content-Type', headers)
-        self.assertIsNotNone(re.match('^application/json(; charset=UTF-8)?$',
-                                      headers['Content-Type']))
-        self.assertIn('Access-Control-Allow-Origin', headers)
-        self.assertIn('Cache-Control', headers)
-        self.assertIn('Last-Modified', headers)
-        self.assertIn('Expires', headers)
-        data = f.read()
-
-        self.worker.hold_jobs_in_build = False
-        self.worker.release()
-        self.waitUntilSettled()
-
-        data = json.loads(data)
-        status_jobs = []
-        for p in data['pipelines']:
-            for q in p['change_queues']:
-                if p['name'] in ['gate', 'conflict']:
-                    self.assertEqual(q['window'], 20)
-                else:
-                    self.assertEqual(q['window'], 0)
-                for head in q['heads']:
-                    for change in head:
-                        self.assertTrue(change['active'])
-                        self.assertEqual(change['id'], '1,1')
-                        for job in change['jobs']:
-                            status_jobs.append(job)
-        self.assertEqual('project-merge', status_jobs[0]['name'])
-        self.assertEqual('https://server/job/project-merge/0/',
-                         status_jobs[0]['url'])
-        self.assertEqual('http://logs.example.com/1/1/gate/project-merge/0',
-                         status_jobs[0]['report_url'])
-
-        self.assertEqual('project-test1', status_jobs[1]['name'])
-        self.assertEqual('https://server/job/project-test1/1/',
-                         status_jobs[1]['url'])
-        self.assertEqual('http://logs.example.com/1/1/gate/project-test1/1',
-                         status_jobs[1]['report_url'])
-
-        self.assertEqual('project-test2', status_jobs[2]['name'])
-        self.assertEqual('https://server/job/project-test2/2/',
-                         status_jobs[2]['url'])
-        self.assertEqual('http://logs.example.com/1/1/gate/project-test2/2',
-                         status_jobs[2]['report_url'])
-
-    def test_merging_queues(self):
-        "Test that transitively-connected change queues are merged"
-        self.config.set('zuul', 'layout_config',
-                        'tests/fixtures/layout-merge-queues.yaml')
-        self.sched.reconfigure(self.config)
-        self.assertEqual(len(self.sched.layout.pipelines['gate'].queues), 1)
-
-    def test_mutex(self):
-        "Test job mutexes"
-        self.config.set('zuul', 'layout_config',
-                        'tests/fixtures/layout-mutex.yaml')
-        self.sched.reconfigure(self.config)
-
-        self.worker.hold_jobs_in_build = True
-        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
-        B = self.fake_gerrit.addFakeChange('org/project', 'master', 'B')
-        self.assertFalse('test-mutex' in self.sched.mutex.mutexes)
-
-        self.fake_gerrit.addEvent(A.getPatchsetCreatedEvent(1))
-        self.fake_gerrit.addEvent(B.getPatchsetCreatedEvent(1))
-        self.waitUntilSettled()
-        self.assertEqual(len(self.builds), 3)
-        self.assertEqual(self.builds[0].name, 'project-test1')
-        self.assertEqual(self.builds[1].name, 'mutex-one')
-        self.assertEqual(self.builds[2].name, 'project-test1')
-
-        self.worker.release('mutex-one')
-        self.waitUntilSettled()
-
-        self.assertEqual(len(self.builds), 3)
-        self.assertEqual(self.builds[0].name, 'project-test1')
-        self.assertEqual(self.builds[1].name, 'project-test1')
-        self.assertEqual(self.builds[2].name, 'mutex-two')
-        self.assertTrue('test-mutex' in self.sched.mutex.mutexes)
-
-        self.worker.release('mutex-two')
-        self.waitUntilSettled()
-
-        self.assertEqual(len(self.builds), 3)
-        self.assertEqual(self.builds[0].name, 'project-test1')
-        self.assertEqual(self.builds[1].name, 'project-test1')
-        self.assertEqual(self.builds[2].name, 'mutex-one')
-        self.assertTrue('test-mutex' in self.sched.mutex.mutexes)
-
-        self.worker.release('mutex-one')
-        self.waitUntilSettled()
-
-        self.assertEqual(len(self.builds), 3)
-        self.assertEqual(self.builds[0].name, 'project-test1')
-        self.assertEqual(self.builds[1].name, 'project-test1')
-        self.assertEqual(self.builds[2].name, 'mutex-two')
-        self.assertTrue('test-mutex' in self.sched.mutex.mutexes)
-
-        self.worker.release('mutex-two')
-        self.waitUntilSettled()
-
-        self.assertEqual(len(self.builds), 2)
-        self.assertEqual(self.builds[0].name, 'project-test1')
-        self.assertEqual(self.builds[1].name, 'project-test1')
-        self.assertFalse('test-mutex' in self.sched.mutex.mutexes)
-
-        self.worker.hold_jobs_in_build = False
-        self.worker.release()
-
-        self.waitUntilSettled()
-        self.assertEqual(len(self.builds), 0)
-
-        self.assertEqual(A.reported, 1)
-        self.assertEqual(B.reported, 1)
-        self.assertFalse('test-mutex' in self.sched.mutex.mutexes)
-
-    def test_node_label(self):
-        "Test that a job runs on a specific node label"
-        self.worker.registerFunction('build:node-project-test1:debian')
-
-        A = self.fake_gerrit.addFakeChange('org/node-project', 'master', 'A')
-        A.addApproval('CRVW', 2)
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-        self.waitUntilSettled()
-
-        self.assertIsNone(self.getJobFromHistory('node-project-merge').node)
-        self.assertEqual(self.getJobFromHistory('node-project-test1').node,
-                         'debian')
-        self.assertIsNone(self.getJobFromHistory('node-project-test2').node)
-
-    def test_live_reconfiguration(self):
-        "Test that live reconfiguration works"
-        self.worker.hold_jobs_in_build = True
-        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
-        A.addApproval('CRVW', 2)
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-        self.waitUntilSettled()
-
-        self.sched.reconfigure(self.config)
-
-        self.worker.hold_jobs_in_build = False
-        self.worker.release()
-        self.waitUntilSettled()
-        self.assertEqual(self.getJobFromHistory('project-merge').result,
-                         'SUCCESS')
-        self.assertEqual(self.getJobFromHistory('project-test1').result,
-                         'SUCCESS')
-        self.assertEqual(self.getJobFromHistory('project-test2').result,
-                         'SUCCESS')
-        self.assertEqual(A.data['status'], 'MERGED')
-        self.assertEqual(A.reported, 2)
-
-    def test_live_reconfiguration_merge_conflict(self):
-        # A real-world bug: a change in a gate queue has a merge
-        # conflict and a job is added to its project while it's
-        # sitting in the queue.  The job gets added to the change and
-        # enqueued and the change gets stuck.
-        self.worker.registerFunction('build:project-test3')
-        self.worker.hold_jobs_in_build = True
-
-        # This change is fine.  It's here to stop the queue long
-        # enough for the next change to be subject to the
-        # reconfiguration, as well as to provide a conflict for the
-        # next change.  This change will succeed and merge.
-        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
-        A.addPatchset(['conflict'])
-        A.addApproval('CRVW', 2)
-
-        # This change will be in merge conflict.  During the
-        # reconfiguration, we will add a job.  We want to make sure
-        # that doesn't cause it to get stuck.
-        B = self.fake_gerrit.addFakeChange('org/project', 'master', 'B')
-        B.addPatchset(['conflict'])
-        B.addApproval('CRVW', 2)
-
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
-
-        self.waitUntilSettled()
-
-        # No jobs have run yet
-        self.assertEqual(A.data['status'], 'NEW')
-        self.assertEqual(A.reported, 1)
-        self.assertEqual(B.data['status'], 'NEW')
-        self.assertEqual(B.reported, 1)
-        self.assertEqual(len(self.history), 0)
-
-        # Add the "project-test3" job.
-        self.config.set('zuul', 'layout_config',
-                        'tests/fixtures/layout-live-'
-                        'reconfiguration-add-job.yaml')
-        self.sched.reconfigure(self.config)
-        self.waitUntilSettled()
-
-        self.worker.hold_jobs_in_build = False
-        self.worker.release()
-        self.waitUntilSettled()
-
-        self.assertEqual(A.data['status'], 'MERGED')
-        self.assertEqual(A.reported, 2)
-        self.assertEqual(B.data['status'], 'NEW')
-        self.assertEqual(B.reported, 2)
-        self.assertEqual(self.getJobFromHistory('project-merge').result,
-                         'SUCCESS')
-        self.assertEqual(self.getJobFromHistory('project-test1').result,
-                         'SUCCESS')
-        self.assertEqual(self.getJobFromHistory('project-test2').result,
-                         'SUCCESS')
-        self.assertEqual(self.getJobFromHistory('project-test3').result,
-                         'SUCCESS')
-        self.assertEqual(len(self.history), 4)
-
-    def test_live_reconfiguration_failed_root(self):
-        # An extrapolation of test_live_reconfiguration_merge_conflict
-        # that tests a job added to a job tree with a failed root does
-        # not run.
-        self.worker.registerFunction('build:project-test3')
-        self.worker.hold_jobs_in_build = True
-
-        # This change is fine.  It's here to stop the queue long
-        # enough for the next change to be subject to the
-        # reconfiguration.  This change will succeed and merge.
-        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
-        A.addPatchset(['conflict'])
-        A.addApproval('CRVW', 2)
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-        self.waitUntilSettled()
-        self.worker.release('.*-merge')
-        self.waitUntilSettled()
-
-        B = self.fake_gerrit.addFakeChange('org/project', 'master', 'B')
-        self.worker.addFailTest('project-merge', B)
-        B.addApproval('CRVW', 2)
-        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
-        self.waitUntilSettled()
-
-        self.worker.release('.*-merge')
-        self.waitUntilSettled()
-
-        # Both -merge jobs have run, but no others.
-        self.assertEqual(A.data['status'], 'NEW')
-        self.assertEqual(A.reported, 1)
-        self.assertEqual(B.data['status'], 'NEW')
-        self.assertEqual(B.reported, 1)
-        self.assertEqual(self.history[0].result, 'SUCCESS')
-        self.assertEqual(self.history[0].name, 'project-merge')
-        self.assertEqual(self.history[1].result, 'FAILURE')
-        self.assertEqual(self.history[1].name, 'project-merge')
-        self.assertEqual(len(self.history), 2)
-
-        # Add the "project-test3" job.
-        self.config.set('zuul', 'layout_config',
-                        'tests/fixtures/layout-live-'
-                        'reconfiguration-add-job.yaml')
-        self.sched.reconfigure(self.config)
-        self.waitUntilSettled()
-
-        self.worker.hold_jobs_in_build = False
-        self.worker.release()
-        self.waitUntilSettled()
-
-        self.assertEqual(A.data['status'], 'MERGED')
-        self.assertEqual(A.reported, 2)
-        self.assertEqual(B.data['status'], 'NEW')
-        self.assertEqual(B.reported, 2)
-        self.assertEqual(self.history[0].result, 'SUCCESS')
-        self.assertEqual(self.history[0].name, 'project-merge')
-        self.assertEqual(self.history[1].result, 'FAILURE')
-        self.assertEqual(self.history[1].name, 'project-merge')
-        self.assertEqual(self.history[2].result, 'SUCCESS')
-        self.assertEqual(self.history[3].result, 'SUCCESS')
-        self.assertEqual(self.history[4].result, 'SUCCESS')
-        self.assertEqual(len(self.history), 5)
-
-    def test_live_reconfiguration_failed_job(self):
-        # Test that a change with a removed failing job does not
-        # disrupt reconfiguration.  If a change has a failed job and
-        # that job is removed during a reconfiguration, we observed a
-        # bug where the code to re-set build statuses would run on
-        # that build and raise an exception because the job no longer
-        # existed.
-        self.worker.hold_jobs_in_build = True
-
-        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
-
-        # This change will fail and later be removed by the reconfiguration.
-        self.worker.addFailTest('project-test1', A)
-
-        self.fake_gerrit.addEvent(A.getPatchsetCreatedEvent(1))
-        self.waitUntilSettled()
-        self.worker.release('.*-merge')
-        self.waitUntilSettled()
-        self.worker.release('project-test1')
-        self.waitUntilSettled()
-
-        self.assertEqual(A.data['status'], 'NEW')
-        self.assertEqual(A.reported, 0)
-
-        self.assertEqual(self.getJobFromHistory('project-merge').result,
-                         'SUCCESS')
-        self.assertEqual(self.getJobFromHistory('project-test1').result,
-                         'FAILURE')
-        self.assertEqual(len(self.history), 2)
-
-        # Remove the test1 job.
-        self.config.set('zuul', 'layout_config',
-                        'tests/fixtures/layout-live-'
-                        'reconfiguration-failed-job.yaml')
-        self.sched.reconfigure(self.config)
-        self.waitUntilSettled()
-
-        self.worker.hold_jobs_in_build = False
-        self.worker.release()
-        self.waitUntilSettled()
-
-        self.assertEqual(self.getJobFromHistory('project-test2').result,
-                         'SUCCESS')
-        self.assertEqual(self.getJobFromHistory('project-testfile').result,
-                         'SUCCESS')
-        self.assertEqual(len(self.history), 4)
-
-        self.assertEqual(A.data['status'], 'NEW')
-        self.assertEqual(A.reported, 1)
-        self.assertIn('Build succeeded', A.messages[0])
-        # Ensure the removed job was not included in the report.
-        self.assertNotIn('project-test1', A.messages[0])
-
-    def test_live_reconfiguration_shared_queue(self):
-        # Test that a change with a failing job which was removed from
-        # this project but otherwise still exists in the system does
-        # not disrupt reconfiguration.
-
-        self.worker.hold_jobs_in_build = True
-
-        A = self.fake_gerrit.addFakeChange('org/project1', 'master', 'A')
-
-        self.worker.addFailTest('project1-project2-integration', A)
-
-        self.fake_gerrit.addEvent(A.getPatchsetCreatedEvent(1))
-        self.waitUntilSettled()
-        self.worker.release('.*-merge')
-        self.waitUntilSettled()
-        self.worker.release('project1-project2-integration')
-        self.waitUntilSettled()
-
-        self.assertEqual(A.data['status'], 'NEW')
-        self.assertEqual(A.reported, 0)
-
-        self.assertEqual(self.getJobFromHistory('project1-merge').result,
-                         'SUCCESS')
-        self.assertEqual(self.getJobFromHistory(
-            'project1-project2-integration').result, 'FAILURE')
-        self.assertEqual(len(self.history), 2)
-
-        # Remove the integration job.
-        self.config.set('zuul', 'layout_config',
-                        'tests/fixtures/layout-live-'
-                        'reconfiguration-shared-queue.yaml')
-        self.sched.reconfigure(self.config)
-        self.waitUntilSettled()
-
-        self.worker.hold_jobs_in_build = False
-        self.worker.release()
-        self.waitUntilSettled()
-
-        self.assertEqual(self.getJobFromHistory('project1-merge').result,
-                         'SUCCESS')
-        self.assertEqual(self.getJobFromHistory('project1-test1').result,
-                         'SUCCESS')
-        self.assertEqual(self.getJobFromHistory('project1-test2').result,
-                         'SUCCESS')
-        self.assertEqual(self.getJobFromHistory(
-            'project1-project2-integration').result, 'FAILURE')
-        self.assertEqual(len(self.history), 4)
-
-        self.assertEqual(A.data['status'], 'NEW')
-        self.assertEqual(A.reported, 1)
-        self.assertIn('Build succeeded', A.messages[0])
-        # Ensure the removed job was not included in the report.
-        self.assertNotIn('project1-project2-integration', A.messages[0])
-
-    def test_double_live_reconfiguration_shared_queue(self):
-        # This was a real-world regression.  A change is added to
-        # gate; a reconfigure happens, a second change which depends
-        # on the first is added, and a second reconfiguration happens.
-        # Ensure that both changes merge.
-
-        # A failure may indicate incorrect caching or cleaning up of
-        # references during a reconfiguration.
-        self.worker.hold_jobs_in_build = True
-
-        A = self.fake_gerrit.addFakeChange('org/project1', 'master', 'A')
-        B = self.fake_gerrit.addFakeChange('org/project1', 'master', 'B')
-        B.setDependsOn(A, 1)
-        A.addApproval('CRVW', 2)
-        B.addApproval('CRVW', 2)
-
-        # Add the parent change.
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-        self.waitUntilSettled()
-        self.worker.release('.*-merge')
-        self.waitUntilSettled()
-
-        # Reconfigure (with only one change in the pipeline).
-        self.sched.reconfigure(self.config)
-        self.waitUntilSettled()
-
-        # Add the child change.
-        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
-        self.waitUntilSettled()
-        self.worker.release('.*-merge')
-        self.waitUntilSettled()
-
-        # Reconfigure (with both in the pipeline).
-        self.sched.reconfigure(self.config)
-        self.waitUntilSettled()
-
-        self.worker.hold_jobs_in_build = False
-        self.worker.release()
-        self.waitUntilSettled()
-
-        self.assertEqual(len(self.history), 8)
-
-        self.assertEqual(A.data['status'], 'MERGED')
-        self.assertEqual(A.reported, 2)
-        self.assertEqual(B.data['status'], 'MERGED')
-        self.assertEqual(B.reported, 2)
-
-    def test_live_reconfiguration_del_project(self):
-        # Test project deletion from layout
-        # while changes are enqueued
-
-        self.worker.hold_jobs_in_build = True
-        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
-        B = self.fake_gerrit.addFakeChange('org/project1', 'master', 'B')
-        C = self.fake_gerrit.addFakeChange('org/project1', 'master', 'C')
-
-        # A Depends-On: B
-        A.data['commitMessage'] = '%s\n\nDepends-On: %s\n' % (
-            A.subject, B.data['id'])
-        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
-
-        self.fake_gerrit.addEvent(A.getPatchsetCreatedEvent(1))
-        self.fake_gerrit.addEvent(C.getPatchsetCreatedEvent(1))
-        self.waitUntilSettled()
-        self.worker.release('.*-merge')
-        self.waitUntilSettled()
-        self.assertEqual(len(self.builds), 5)
-
-        # This layout defines only org/project, not org/project1
-        self.config.set('zuul', 'layout_config',
-                        'tests/fixtures/layout-live-'
-                        'reconfiguration-del-project.yaml')
-        self.sched.reconfigure(self.config)
-        self.waitUntilSettled()
-
-        # Builds for C aborted, builds for A succeed,
-        # and have change B applied ahead
-        job_c = self.getJobFromHistory('project1-test1')
-        self.assertEqual(job_c.changes, '3,1')
-        self.assertEqual(job_c.result, 'ABORTED')
-
-        self.worker.hold_jobs_in_build = False
-        self.worker.release()
-        self.waitUntilSettled()
-
-        self.assertEqual(self.getJobFromHistory('project-test1').changes,
-                         '2,1 1,1')
-
-        self.assertEqual(A.data['status'], 'NEW')
-        self.assertEqual(B.data['status'], 'NEW')
-        self.assertEqual(C.data['status'], 'NEW')
-        self.assertEqual(A.reported, 1)
-        self.assertEqual(B.reported, 0)
-        self.assertEqual(C.reported, 0)
-
-        self.assertEqual(len(self.sched.layout.pipelines['check'].queues), 0)
-        self.assertIn('Build succeeded', A.messages[0])
-
-    def test_live_reconfiguration_functions(self):
-        "Test live reconfiguration with a custom function"
-        self.worker.registerFunction('build:node-project-test1:debian')
-        self.worker.registerFunction('build:node-project-test1:wheezy')
-        A = self.fake_gerrit.addFakeChange('org/node-project', 'master', 'A')
-        A.addApproval('CRVW', 2)
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-        self.waitUntilSettled()
-
-        self.assertIsNone(self.getJobFromHistory('node-project-merge').node)
-        self.assertEqual(self.getJobFromHistory('node-project-test1').node,
-                         'debian')
-        self.assertIsNone(self.getJobFromHistory('node-project-test2').node)
-
-        self.config.set('zuul', 'layout_config',
-                        'tests/fixtures/layout-live-'
-                        'reconfiguration-functions.yaml')
-        self.sched.reconfigure(self.config)
-        self.worker.build_history = []
-
-        B = self.fake_gerrit.addFakeChange('org/node-project', 'master', 'B')
-        B.addApproval('CRVW', 2)
-        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
-        self.waitUntilSettled()
-
-        self.assertIsNone(self.getJobFromHistory('node-project-merge').node)
-        self.assertEqual(self.getJobFromHistory('node-project-test1').node,
-                         'wheezy')
-        self.assertIsNone(self.getJobFromHistory('node-project-test2').node)
-
-    def test_delayed_repo_init(self):
-        self.config.set('zuul', 'layout_config',
-                        'tests/fixtures/layout-delayed-repo-init.yaml')
-        self.sched.reconfigure(self.config)
-
-        self.init_repo("org/new-project")
-        A = self.fake_gerrit.addFakeChange('org/new-project', 'master', 'A')
-
-        A.addApproval('CRVW', 2)
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-        self.waitUntilSettled()
-        self.assertEqual(self.getJobFromHistory('project-merge').result,
-                         'SUCCESS')
-        self.assertEqual(self.getJobFromHistory('project-test1').result,
-                         'SUCCESS')
-        self.assertEqual(self.getJobFromHistory('project-test2').result,
-                         'SUCCESS')
-        self.assertEqual(A.data['status'], 'MERGED')
-        self.assertEqual(A.reported, 2)
-
-    def test_repo_deleted(self):
-        self.config.set('zuul', 'layout_config',
-                        'tests/fixtures/layout-repo-deleted.yaml')
-        self.sched.reconfigure(self.config)
-
-        self.init_repo("org/delete-project")
-        A = self.fake_gerrit.addFakeChange('org/delete-project', 'master', 'A')
-
-        A.addApproval('CRVW', 2)
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-        self.waitUntilSettled()
-        self.assertEqual(self.getJobFromHistory('project-merge').result,
-                         'SUCCESS')
-        self.assertEqual(self.getJobFromHistory('project-test1').result,
-                         'SUCCESS')
-        self.assertEqual(self.getJobFromHistory('project-test2').result,
-                         'SUCCESS')
-        self.assertEqual(A.data['status'], 'MERGED')
-        self.assertEqual(A.reported, 2)
-
-        # Delete org/new-project zuul repo. Should be recloned.
-        shutil.rmtree(os.path.join(self.git_root, "org/delete-project"))
-
-        B = self.fake_gerrit.addFakeChange('org/delete-project', 'master', 'B')
-
-        B.addApproval('CRVW', 2)
-        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
-        self.waitUntilSettled()
-        self.assertEqual(self.getJobFromHistory('project-merge').result,
-                         'SUCCESS')
-        self.assertEqual(self.getJobFromHistory('project-test1').result,
-                         'SUCCESS')
-        self.assertEqual(self.getJobFromHistory('project-test2').result,
-                         'SUCCESS')
-        self.assertEqual(B.data['status'], 'MERGED')
-        self.assertEqual(B.reported, 2)
-
-    def test_tags(self):
-        "Test job tags"
-        self.config.set('zuul', 'layout_config',
-                        'tests/fixtures/layout-tags.yaml')
-        self.sched.reconfigure(self.config)
-
-        A = self.fake_gerrit.addFakeChange('org/project1', 'master', 'A')
-        B = self.fake_gerrit.addFakeChange('org/project2', 'master', 'B')
-        self.fake_gerrit.addEvent(A.getPatchsetCreatedEvent(1))
-        self.fake_gerrit.addEvent(B.getPatchsetCreatedEvent(1))
-        self.waitUntilSettled()
-
-        results = {'project1-merge': 'extratag merge project1',
-                   'project2-merge': 'merge'}
-
-        for build in self.history:
-            self.assertEqual(results.get(build.name, ''),
-                             build.parameters.get('BUILD_TAGS'))
-
-    def test_timer(self):
-        "Test that a periodic job is triggered"
-        self.worker.hold_jobs_in_build = True
-        self.config.set('zuul', 'layout_config',
-                        'tests/fixtures/layout-timer.yaml')
-        self.sched.reconfigure(self.config)
-        self.registerJobs()
-
-        # The pipeline triggers every second, so we should have seen
-        # several by now.
-        time.sleep(5)
-        self.waitUntilSettled()
-
-        self.assertEqual(len(self.builds), 2)
-
-        port = self.webapp.server.socket.getsockname()[1]
-
-        f = urllib.urlopen("http://localhost:%s/status.json" % port)
-        data = f.read()
-
-        self.worker.hold_jobs_in_build = False
-        # Stop queuing timer triggered jobs so that the assertions
-        # below don't race against more jobs being queued.
-        self.config.set('zuul', 'layout_config',
-                        'tests/fixtures/layout-no-timer.yaml')
-        self.sched.reconfigure(self.config)
-        self.registerJobs()
-        self.worker.release()
-        self.waitUntilSettled()
-
-        self.assertEqual(self.getJobFromHistory(
-            'project-bitrot-stable-old').result, 'SUCCESS')
-        self.assertEqual(self.getJobFromHistory(
-            'project-bitrot-stable-older').result, 'SUCCESS')
-
-        data = json.loads(data)
-        status_jobs = set()
-        for p in data['pipelines']:
-            for q in p['change_queues']:
-                for head in q['heads']:
-                    for change in head:
-                        self.assertEqual(change['id'], None)
-                        for job in change['jobs']:
-                            status_jobs.add(job['name'])
-        self.assertIn('project-bitrot-stable-old', status_jobs)
-        self.assertIn('project-bitrot-stable-older', status_jobs)
-
-    def test_idle(self):
-        "Test that frequent periodic jobs work"
-        self.worker.hold_jobs_in_build = True
-
-        for x in range(1, 3):
-            # Test that timer triggers periodic jobs even across
-            # layout config reloads.
-            # Start timer trigger
-            self.config.set('zuul', 'layout_config',
-                            'tests/fixtures/layout-idle.yaml')
-            self.sched.reconfigure(self.config)
-            self.registerJobs()
-            self.waitUntilSettled()
-
-            # The pipeline triggers every second, so we should have seen
-            # several by now.
-            time.sleep(5)
-
-            # Stop queuing timer triggered jobs so that the assertions
-            # below don't race against more jobs being queued.
-            self.config.set('zuul', 'layout_config',
-                            'tests/fixtures/layout-no-timer.yaml')
-            self.sched.reconfigure(self.config)
-            self.registerJobs()
-            self.waitUntilSettled()
-
-            self.assertEqual(len(self.builds), 2)
-            self.worker.release('.*')
-            self.waitUntilSettled()
-            self.assertEqual(len(self.builds), 0)
-            self.assertEqual(len(self.history), x * 2)
-
-    def test_check_smtp_pool(self):
-        self.config.set('zuul', 'layout_config',
-                        'tests/fixtures/layout-smtp.yaml')
-        self.sched.reconfigure(self.config)
-
-        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
-        self.waitUntilSettled()
-
-        self.fake_gerrit.addEvent(A.getPatchsetCreatedEvent(1))
-        self.waitUntilSettled()
-
-        self.assertEqual(len(self.smtp_messages), 2)
-
-        # A.messages only holds what FakeGerrit places in it. Thus we
-        # work on the knowledge of what the first message should be as
-        # it is only configured to go to SMTP.
-
-        self.assertEqual('zuul@example.com',
-                         self.smtp_messages[0]['from_email'])
-        self.assertEqual(['you@example.com'],
-                         self.smtp_messages[0]['to_email'])
-        self.assertEqual('Starting check jobs.',
-                         self.smtp_messages[0]['body'])
-
-        self.assertEqual('zuul_from@example.com',
-                         self.smtp_messages[1]['from_email'])
-        self.assertEqual(['alternative_me@example.com'],
-                         self.smtp_messages[1]['to_email'])
-        self.assertEqual(A.messages[0],
-                         self.smtp_messages[1]['body'])
-
-    def test_timer_smtp(self):
-        "Test that a periodic job is triggered"
-        self.worker.hold_jobs_in_build = True
-        self.config.set('zuul', 'layout_config',
-                        'tests/fixtures/layout-timer-smtp.yaml')
-        self.sched.reconfigure(self.config)
-        self.registerJobs()
-
-        # The pipeline triggers every second, so we should have seen
-        # several by now.
-        time.sleep(5)
-        self.waitUntilSettled()
-
-        self.assertEqual(len(self.builds), 2)
-        self.worker.release('.*')
-        self.waitUntilSettled()
-        self.assertEqual(len(self.history), 2)
-
-        self.assertEqual(self.getJobFromHistory(
-            'project-bitrot-stable-old').result, 'SUCCESS')
-        self.assertEqual(self.getJobFromHistory(
-            'project-bitrot-stable-older').result, 'SUCCESS')
-
-        self.assertEqual(len(self.smtp_messages), 1)
-
-        # A.messages only holds what FakeGerrit places in it. Thus we
-        # work on the knowledge of what the first message should be as
-        # it is only configured to go to SMTP.
-
-        self.assertEqual('zuul_from@example.com',
-                         self.smtp_messages[0]['from_email'])
-        self.assertEqual(['alternative_me@example.com'],
-                         self.smtp_messages[0]['to_email'])
-        self.assertIn('Subject: Periodic check for org/project succeeded',
-                      self.smtp_messages[0]['headers'])
-
-        # Stop queuing timer triggered jobs and let any that may have
-        # queued through so that end of test assertions pass.
-        self.config.set('zuul', 'layout_config',
-                        'tests/fixtures/layout-no-timer.yaml')
-        self.sched.reconfigure(self.config)
-        self.registerJobs()
-        self.waitUntilSettled()
-        self.worker.release('.*')
-        self.waitUntilSettled()
-
-    def test_client_enqueue_change(self):
-        "Test that the RPC client can enqueue a change"
-        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
-        A.addApproval('CRVW', 2)
-        A.addApproval('APRV', 1)
-
-        client = zuul.rpcclient.RPCClient('127.0.0.1',
-                                          self.gearman_server.port)
-        r = client.enqueue(pipeline='gate',
-                           project='org/project',
-                           trigger='gerrit',
-                           change='1,1')
-        self.waitUntilSettled()
-        self.assertEqual(self.getJobFromHistory('project-merge').result,
-                         'SUCCESS')
-        self.assertEqual(self.getJobFromHistory('project-test1').result,
-                         'SUCCESS')
-        self.assertEqual(self.getJobFromHistory('project-test2').result,
-                         'SUCCESS')
-        self.assertEqual(A.data['status'], 'MERGED')
-        self.assertEqual(A.reported, 2)
-        self.assertEqual(r, True)
-
-    def test_client_enqueue_ref(self):
-        "Test that the RPC client can enqueue a ref"
-
-        client = zuul.rpcclient.RPCClient('127.0.0.1',
-                                          self.gearman_server.port)
-        r = client.enqueue_ref(
-            pipeline='post',
-            project='org/project',
-            trigger='gerrit',
-            ref='master',
-            oldrev='90f173846e3af9154517b88543ffbd1691f31366',
-            newrev='d479a0bfcb34da57a31adb2a595c0cf687812543')
-        self.waitUntilSettled()
-        job_names = [x.name for x in self.history]
-        self.assertEqual(len(self.history), 1)
-        self.assertIn('project-post', job_names)
-        self.assertEqual(r, True)
-
-    def test_client_enqueue_negative(self):
-        "Test that the RPC client returns errors"
-        client = zuul.rpcclient.RPCClient('127.0.0.1',
-                                          self.gearman_server.port)
-        with testtools.ExpectedException(zuul.rpcclient.RPCFailure,
-                                         "Invalid project"):
-            r = client.enqueue(pipeline='gate',
-                               project='project-does-not-exist',
-                               trigger='gerrit',
-                               change='1,1')
-            client.shutdown()
-            self.assertEqual(r, False)
-
-        with testtools.ExpectedException(zuul.rpcclient.RPCFailure,
-                                         "Invalid pipeline"):
-            r = client.enqueue(pipeline='pipeline-does-not-exist',
-                               project='org/project',
-                               trigger='gerrit',
-                               change='1,1')
-            client.shutdown()
-            self.assertEqual(r, False)
-
-        with testtools.ExpectedException(zuul.rpcclient.RPCFailure,
-                                         "Invalid trigger"):
-            r = client.enqueue(pipeline='gate',
-                               project='org/project',
-                               trigger='trigger-does-not-exist',
-                               change='1,1')
-            client.shutdown()
-            self.assertEqual(r, False)
-
-        with testtools.ExpectedException(zuul.rpcclient.RPCFailure,
-                                         "Invalid change"):
-            r = client.enqueue(pipeline='gate',
-                               project='org/project',
-                               trigger='gerrit',
-                               change='1,1')
-            client.shutdown()
-            self.assertEqual(r, False)
-
-        self.waitUntilSettled()
-        self.assertEqual(len(self.history), 0)
-        self.assertEqual(len(self.builds), 0)
-
-    def test_client_promote(self):
-        "Test that the RPC client can promote a change"
-        self.worker.hold_jobs_in_build = True
-        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
-        B = self.fake_gerrit.addFakeChange('org/project', 'master', 'B')
-        C = self.fake_gerrit.addFakeChange('org/project', 'master', 'C')
-        A.addApproval('CRVW', 2)
-        B.addApproval('CRVW', 2)
-        C.addApproval('CRVW', 2)
-
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
-        self.fake_gerrit.addEvent(C.addApproval('APRV', 1))
-
-        self.waitUntilSettled()
-
-        items = self.sched.layout.pipelines['gate'].getAllItems()
-        enqueue_times = {}
-        for item in items:
-            enqueue_times[str(item.change)] = item.enqueue_time
-
-        client = zuul.rpcclient.RPCClient('127.0.0.1',
-                                          self.gearman_server.port)
-        r = client.promote(pipeline='gate',
-                           change_ids=['2,1', '3,1'])
-
-        # ensure that enqueue times are durable
-        items = self.sched.layout.pipelines['gate'].getAllItems()
-        for item in items:
-            self.assertEqual(
-                enqueue_times[str(item.change)], item.enqueue_time)
-
-        self.waitUntilSettled()
-        self.worker.release('.*-merge')
-        self.waitUntilSettled()
-        self.worker.release('.*-merge')
-        self.waitUntilSettled()
-        self.worker.release('.*-merge')
-        self.waitUntilSettled()
-
-        self.assertEqual(len(self.builds), 6)
-        self.assertEqual(self.builds[0].name, 'project-test1')
-        self.assertEqual(self.builds[1].name, 'project-test2')
-        self.assertEqual(self.builds[2].name, 'project-test1')
-        self.assertEqual(self.builds[3].name, 'project-test2')
-        self.assertEqual(self.builds[4].name, 'project-test1')
-        self.assertEqual(self.builds[5].name, 'project-test2')
-
-        self.assertTrue(self.job_has_changes(self.builds[0], B))
-        self.assertFalse(self.job_has_changes(self.builds[0], A))
-        self.assertFalse(self.job_has_changes(self.builds[0], C))
-
-        self.assertTrue(self.job_has_changes(self.builds[2], B))
-        self.assertTrue(self.job_has_changes(self.builds[2], C))
-        self.assertFalse(self.job_has_changes(self.builds[2], A))
-
-        self.assertTrue(self.job_has_changes(self.builds[4], B))
-        self.assertTrue(self.job_has_changes(self.builds[4], C))
-        self.assertTrue(self.job_has_changes(self.builds[4], A))
-
-        self.worker.release()
-        self.waitUntilSettled()
-
-        self.assertEqual(A.data['status'], 'MERGED')
-        self.assertEqual(A.reported, 2)
-        self.assertEqual(B.data['status'], 'MERGED')
-        self.assertEqual(B.reported, 2)
-        self.assertEqual(C.data['status'], 'MERGED')
-        self.assertEqual(C.reported, 2)
-
-        client.shutdown()
-        self.assertEqual(r, True)
-
-    def test_client_promote_dependent(self):
-        "Test that the RPC client can promote a dependent change"
-        # C (depends on B) -> B -> A ; then promote C to get:
-        # A -> C (depends on B) -> B
-        self.worker.hold_jobs_in_build = True
-        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
-        B = self.fake_gerrit.addFakeChange('org/project', 'master', 'B')
-        C = self.fake_gerrit.addFakeChange('org/project', 'master', 'C')
-
-        C.setDependsOn(B, 1)
-
-        A.addApproval('CRVW', 2)
-        B.addApproval('CRVW', 2)
-        C.addApproval('CRVW', 2)
-
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
-        self.fake_gerrit.addEvent(C.addApproval('APRV', 1))
-
-        self.waitUntilSettled()
-
-        client = zuul.rpcclient.RPCClient('127.0.0.1',
-                                          self.gearman_server.port)
-        r = client.promote(pipeline='gate',
-                           change_ids=['3,1'])
-
-        self.waitUntilSettled()
-        self.worker.release('.*-merge')
-        self.waitUntilSettled()
-        self.worker.release('.*-merge')
-        self.waitUntilSettled()
-        self.worker.release('.*-merge')
-        self.waitUntilSettled()
-
-        self.assertEqual(len(self.builds), 6)
-        self.assertEqual(self.builds[0].name, 'project-test1')
-        self.assertEqual(self.builds[1].name, 'project-test2')
-        self.assertEqual(self.builds[2].name, 'project-test1')
-        self.assertEqual(self.builds[3].name, 'project-test2')
-        self.assertEqual(self.builds[4].name, 'project-test1')
-        self.assertEqual(self.builds[5].name, 'project-test2')
-
-        self.assertTrue(self.job_has_changes(self.builds[0], B))
-        self.assertFalse(self.job_has_changes(self.builds[0], A))
-        self.assertFalse(self.job_has_changes(self.builds[0], C))
-
-        self.assertTrue(self.job_has_changes(self.builds[2], B))
-        self.assertTrue(self.job_has_changes(self.builds[2], C))
-        self.assertFalse(self.job_has_changes(self.builds[2], A))
-
-        self.assertTrue(self.job_has_changes(self.builds[4], B))
-        self.assertTrue(self.job_has_changes(self.builds[4], C))
-        self.assertTrue(self.job_has_changes(self.builds[4], A))
-
-        self.worker.release()
-        self.waitUntilSettled()
-
-        self.assertEqual(A.data['status'], 'MERGED')
-        self.assertEqual(A.reported, 2)
-        self.assertEqual(B.data['status'], 'MERGED')
-        self.assertEqual(B.reported, 2)
-        self.assertEqual(C.data['status'], 'MERGED')
-        self.assertEqual(C.reported, 2)
-
-        client.shutdown()
-        self.assertEqual(r, True)
-
-    def test_client_promote_negative(self):
-        "Test that the RPC client returns errors for promotion"
-        self.worker.hold_jobs_in_build = True
-        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
-        A.addApproval('CRVW', 2)
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-        self.waitUntilSettled()
-
-        client = zuul.rpcclient.RPCClient('127.0.0.1',
-                                          self.gearman_server.port)
-
-        with testtools.ExpectedException(zuul.rpcclient.RPCFailure):
-            r = client.promote(pipeline='nonexistent',
-                               change_ids=['2,1', '3,1'])
-            client.shutdown()
-            self.assertEqual(r, False)
-
-        with testtools.ExpectedException(zuul.rpcclient.RPCFailure):
-            r = client.promote(pipeline='gate',
-                               change_ids=['4,1'])
-            client.shutdown()
-            self.assertEqual(r, False)
-
-        self.worker.hold_jobs_in_build = False
-        self.worker.release()
-        self.waitUntilSettled()
-
-    def test_queue_rate_limiting(self):
-        "Test that DependentPipelines are rate limited with dep across window"
-        self.config.set('zuul', 'layout_config',
-                        'tests/fixtures/layout-rate-limit.yaml')
-        self.sched.reconfigure(self.config)
-        self.worker.hold_jobs_in_build = True
-        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
-        B = self.fake_gerrit.addFakeChange('org/project', 'master', 'B')
-        C = self.fake_gerrit.addFakeChange('org/project', 'master', 'C')
-
-        C.setDependsOn(B, 1)
-        self.worker.addFailTest('project-test1', A)
-
-        A.addApproval('CRVW', 2)
-        B.addApproval('CRVW', 2)
-        C.addApproval('CRVW', 2)
-
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
-        self.fake_gerrit.addEvent(C.addApproval('APRV', 1))
-        self.waitUntilSettled()
-
-        # Only A and B will have their merge jobs queued because
-        # window is 2.
-        self.assertEqual(len(self.builds), 2)
-        self.assertEqual(self.builds[0].name, 'project-merge')
-        self.assertEqual(self.builds[1].name, 'project-merge')
-
-        self.worker.release('.*-merge')
-        self.waitUntilSettled()
-        self.worker.release('.*-merge')
-        self.waitUntilSettled()
-
-        # Only A and B will have their test jobs queued because
-        # window is 2.
-        self.assertEqual(len(self.builds), 4)
-        self.assertEqual(self.builds[0].name, 'project-test1')
-        self.assertEqual(self.builds[1].name, 'project-test2')
-        self.assertEqual(self.builds[2].name, 'project-test1')
-        self.assertEqual(self.builds[3].name, 'project-test2')
-
-        self.worker.release('project-.*')
-        self.waitUntilSettled()
-
-        queue = self.sched.layout.pipelines['gate'].queues[0]
-        # A failed so window is reduced by 1 to 1.
-        self.assertEqual(queue.window, 1)
-        self.assertEqual(queue.window_floor, 1)
-        self.assertEqual(A.data['status'], 'NEW')
-
-        # Gate is reset and only B's merge job is queued because
-        # window shrunk to 1.
-        self.assertEqual(len(self.builds), 1)
-        self.assertEqual(self.builds[0].name, 'project-merge')
-
-        self.worker.release('.*-merge')
-        self.waitUntilSettled()
-
-        # Only B's test jobs are queued because window is still 1.
-        self.assertEqual(len(self.builds), 2)
-        self.assertEqual(self.builds[0].name, 'project-test1')
-        self.assertEqual(self.builds[1].name, 'project-test2')
-
-        self.worker.release('project-.*')
-        self.waitUntilSettled()
-
-        # B was successfully merged so window is increased to 2.
-        self.assertEqual(queue.window, 2)
-        self.assertEqual(queue.window_floor, 1)
-        self.assertEqual(B.data['status'], 'MERGED')
-
-        # Only C is left and its merge job is queued.
-        self.assertEqual(len(self.builds), 1)
-        self.assertEqual(self.builds[0].name, 'project-merge')
-
-        self.worker.release('.*-merge')
-        self.waitUntilSettled()
-
-        # After successful merge job the test jobs for C are queued.
-        self.assertEqual(len(self.builds), 2)
-        self.assertEqual(self.builds[0].name, 'project-test1')
-        self.assertEqual(self.builds[1].name, 'project-test2')
-
-        self.worker.release('project-.*')
-        self.waitUntilSettled()
-
-        # C successfully merged so window is bumped to 3.
-        self.assertEqual(queue.window, 3)
-        self.assertEqual(queue.window_floor, 1)
-        self.assertEqual(C.data['status'], 'MERGED')
-
-    def test_queue_rate_limiting_dependent(self):
-        "Test that DependentPipelines are rate limited with dep in window"
-        self.config.set('zuul', 'layout_config',
-                        'tests/fixtures/layout-rate-limit.yaml')
-        self.sched.reconfigure(self.config)
-        self.worker.hold_jobs_in_build = True
-        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
-        B = self.fake_gerrit.addFakeChange('org/project', 'master', 'B')
-        C = self.fake_gerrit.addFakeChange('org/project', 'master', 'C')
-
-        B.setDependsOn(A, 1)
-
-        self.worker.addFailTest('project-test1', A)
-
-        A.addApproval('CRVW', 2)
-        B.addApproval('CRVW', 2)
-        C.addApproval('CRVW', 2)
-
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
-        self.fake_gerrit.addEvent(C.addApproval('APRV', 1))
-        self.waitUntilSettled()
-
-        # Only A and B will have their merge jobs queued because
-        # window is 2.
-        self.assertEqual(len(self.builds), 2)
-        self.assertEqual(self.builds[0].name, 'project-merge')
-        self.assertEqual(self.builds[1].name, 'project-merge')
-
-        self.worker.release('.*-merge')
-        self.waitUntilSettled()
-        self.worker.release('.*-merge')
-        self.waitUntilSettled()
-
-        # Only A and B will have their test jobs queued because
-        # window is 2.
-        self.assertEqual(len(self.builds), 4)
-        self.assertEqual(self.builds[0].name, 'project-test1')
-        self.assertEqual(self.builds[1].name, 'project-test2')
-        self.assertEqual(self.builds[2].name, 'project-test1')
-        self.assertEqual(self.builds[3].name, 'project-test2')
-
-        self.worker.release('project-.*')
-        self.waitUntilSettled()
-
-        queue = self.sched.layout.pipelines['gate'].queues[0]
-        # A failed so window is reduced by 1 to 1.
-        self.assertEqual(queue.window, 1)
-        self.assertEqual(queue.window_floor, 1)
-        self.assertEqual(A.data['status'], 'NEW')
-        self.assertEqual(B.data['status'], 'NEW')
-
-        # Gate is reset and only C's merge job is queued because
-        # window shrunk to 1 and A and B were dequeued.
-        self.assertEqual(len(self.builds), 1)
-        self.assertEqual(self.builds[0].name, 'project-merge')
-
-        self.worker.release('.*-merge')
-        self.waitUntilSettled()
-
-        # Only C's test jobs are queued because window is still 1.
-        self.assertEqual(len(self.builds), 2)
-        self.assertEqual(self.builds[0].name, 'project-test1')
-        self.assertEqual(self.builds[1].name, 'project-test2')
-
-        self.worker.release('project-.*')
-        self.waitUntilSettled()
-
-        # C was successfully merged so window is increased to 2.
-        self.assertEqual(queue.window, 2)
-        self.assertEqual(queue.window_floor, 1)
-        self.assertEqual(C.data['status'], 'MERGED')
-
-    def test_worker_update_metadata(self):
-        "Test if a worker can send back metadata about itself"
-        self.worker.hold_jobs_in_build = True
-
-        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
-        A.addApproval('CRVW', 2)
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-        self.waitUntilSettled()
-
-        self.assertEqual(len(self.launcher.builds), 1)
-
-        self.log.debug('Current builds:')
-        self.log.debug(self.launcher.builds)
-
-        start = time.time()
-        while True:
-            if time.time() - start > 10:
-                raise Exception("Timeout waiting for gearman server to report "
-                                + "back to the client")
-            build = self.launcher.builds.values()[0]
-            if build.worker.name == "My Worker":
-                break
-            else:
-                time.sleep(0)
-
-        self.log.debug(build)
-        self.assertEqual("My Worker", build.worker.name)
-        self.assertEqual("localhost", build.worker.hostname)
-        self.assertEqual(['127.0.0.1', '192.168.1.1'], build.worker.ips)
-        self.assertEqual("zuul.example.org", build.worker.fqdn)
-        self.assertEqual("FakeBuilder", build.worker.program)
-        self.assertEqual("v1.1", build.worker.version)
-        self.assertEqual({'something': 'else'}, build.worker.extra)
-
-        self.worker.hold_jobs_in_build = False
-        self.worker.release()
-        self.waitUntilSettled()
-
-    def test_footer_message(self):
-        "Test a pipeline's footer message is correctly added to the report."
-        self.config.set('zuul', 'layout_config',
-                        'tests/fixtures/layout-footer-message.yaml')
-        self.sched.reconfigure(self.config)
-        self.registerJobs()
-
-        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
-        A.addApproval('CRVW', 2)
-        self.worker.addFailTest('test1', A)
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-        self.waitUntilSettled()
-
-        B = self.fake_gerrit.addFakeChange('org/project', 'master', 'B')
-        B.addApproval('CRVW', 2)
-        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
-        self.waitUntilSettled()
-
-        self.assertEqual(2, len(self.smtp_messages))
-
-        failure_body = """\
-Build failed.  For information on how to proceed, see \
-http://wiki.example.org/Test_Failures
-
-- test1 http://logs.example.com/1/1/gate/test1/0 : FAILURE in 0s
-- test2 http://logs.example.com/1/1/gate/test2/1 : SUCCESS in 0s
-
-For CI problems and help debugging, contact ci@example.org"""
-
-        success_body = """\
-Build succeeded.
-
-- test1 http://logs.example.com/2/1/gate/test1/2 : SUCCESS in 0s
-- test2 http://logs.example.com/2/1/gate/test2/3 : SUCCESS in 0s
-
-For CI problems and help debugging, contact ci@example.org"""
-
-        self.assertEqual(failure_body, self.smtp_messages[0]['body'])
-        self.assertEqual(success_body, self.smtp_messages[1]['body'])
-
-    def test_merge_failure_reporters(self):
-        """Check that the config is set up correctly"""
-
-        self.config.set('zuul', 'layout_config',
-                        'tests/fixtures/layout-merge-failure.yaml')
-        self.sched.reconfigure(self.config)
-        self.registerJobs()
-
-        self.assertEqual(
-            "Merge Failed.\n\nThis change or one of its cross-repo "
-            "dependencies was unable to be automatically merged with the "
-            "current state of its repository. Please rebase the change and "
-            "upload a new patchset.",
-            self.sched.layout.pipelines['check'].merge_failure_message)
-        self.assertEqual(
-            "The merge failed! For more information...",
-            self.sched.layout.pipelines['gate'].merge_failure_message)
-
-        self.assertEqual(
-            len(self.sched.layout.pipelines['check'].merge_failure_actions), 1)
-        self.assertEqual(
-            len(self.sched.layout.pipelines['gate'].merge_failure_actions), 2)
-
-        self.assertTrue(isinstance(
-            self.sched.layout.pipelines['check'].merge_failure_actions[0],
-            zuul.reporter.gerrit.GerritReporter))
-
-        self.assertTrue(
-            (
-                isinstance(self.sched.layout.pipelines['gate'].
-                           merge_failure_actions[0],
-                           zuul.reporter.smtp.SMTPReporter) and
-                isinstance(self.sched.layout.pipelines['gate'].
-                           merge_failure_actions[1],
-                           zuul.reporter.gerrit.GerritReporter)
-            ) or (
-                isinstance(self.sched.layout.pipelines['gate'].
-                           merge_failure_actions[0],
-                           zuul.reporter.gerrit.GerritReporter) and
-                isinstance(self.sched.layout.pipelines['gate'].
-                           merge_failure_actions[1],
-                           zuul.reporter.smtp.SMTPReporter)
-            )
-        )
-
-    def test_merge_failure_reports(self):
-        """Check that when a change fails to merge the correct message is sent
-        to the correct reporter"""
-        self.config.set('zuul', 'layout_config',
-                        'tests/fixtures/layout-merge-failure.yaml')
-        self.sched.reconfigure(self.config)
-        self.registerJobs()
-
-        # Check a test failure isn't reported to SMTP
-        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
-        A.addApproval('CRVW', 2)
-        self.worker.addFailTest('project-test1', A)
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-        self.waitUntilSettled()
-
-        self.assertEqual(3, len(self.history))  # 3 jobs
-        self.assertEqual(0, len(self.smtp_messages))
-
-        # Check a merge failure is reported to SMTP
-        # B should be merged, but C will conflict with B
-        B = self.fake_gerrit.addFakeChange('org/project', 'master', 'B')
-        B.addPatchset(['conflict'])
-        C = self.fake_gerrit.addFakeChange('org/project', 'master', 'C')
-        C.addPatchset(['conflict'])
-        B.addApproval('CRVW', 2)
-        C.addApproval('CRVW', 2)
-        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
-        self.fake_gerrit.addEvent(C.addApproval('APRV', 1))
-        self.waitUntilSettled()
-
-        self.assertEqual(6, len(self.history))  # A and B jobs
-        self.assertEqual(1, len(self.smtp_messages))
-        self.assertEqual('The merge failed! For more information...',
-                         self.smtp_messages[0]['body'])
-
-    def test_default_merge_failure_reports(self):
-        """Check that the default merge failure reports are correct."""
-
-        # A should report success, B should report merge failure.
-        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
-        A.addPatchset(['conflict'])
-        B = self.fake_gerrit.addFakeChange('org/project', 'master', 'B')
-        B.addPatchset(['conflict'])
-        A.addApproval('CRVW', 2)
-        B.addApproval('CRVW', 2)
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
-        self.waitUntilSettled()
-
-        self.assertEqual(3, len(self.history))  # A jobs
-        self.assertEqual(A.reported, 2)
-        self.assertEqual(B.reported, 2)
-        self.assertEqual(A.data['status'], 'MERGED')
-        self.assertEqual(B.data['status'], 'NEW')
-        self.assertIn('Build succeeded', A.messages[1])
-        self.assertIn('Merge Failed', B.messages[1])
-        self.assertIn('automatically merged', B.messages[1])
-        self.assertNotIn('logs.example.com', B.messages[1])
-        self.assertNotIn('SKIPPED', B.messages[1])
-
-    def test_swift_instructions(self):
-        "Test that the correct swift instructions are sent to the workers"
-        self.config.set('zuul', 'layout_config',
-                        'tests/fixtures/layout-swift.yaml')
-        self.sched.reconfigure(self.config)
-        self.registerJobs()
-
-        self.worker.hold_jobs_in_build = True
-        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
-
-        A.addApproval('CRVW', 2)
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-        self.waitUntilSettled()
-
-        self.assertEqual(
-            "https://storage.example.org/V1/AUTH_account/merge_logs/1/1/1/"
-            "gate/test-merge/",
-            self.builds[0].parameters['SWIFT_logs_URL'][:-7])
-        self.assertEqual(5,
-                         len(self.builds[0].parameters['SWIFT_logs_HMAC_BODY'].
-                             split('\n')))
-        self.assertIn('SWIFT_logs_SIGNATURE', self.builds[0].parameters)
-
-        self.assertEqual(
-            "https://storage.example.org/V1/AUTH_account/logs/1/1/1/"
-            "gate/test-test/",
-            self.builds[1].parameters['SWIFT_logs_URL'][:-7])
-        self.assertEqual(5,
-                         len(self.builds[1].parameters['SWIFT_logs_HMAC_BODY'].
-                             split('\n')))
-        self.assertIn('SWIFT_logs_SIGNATURE', self.builds[1].parameters)
-
-        self.assertEqual(
-            "https://storage.example.org/V1/AUTH_account/stash/1/1/1/"
-            "gate/test-test/",
-            self.builds[1].parameters['SWIFT_MOSTLY_URL'][:-7])
-        self.assertEqual(5,
-                         len(self.builds[1].
-                             parameters['SWIFT_MOSTLY_HMAC_BODY'].split('\n')))
-        self.assertIn('SWIFT_MOSTLY_SIGNATURE', self.builds[1].parameters)
-
-        self.worker.hold_jobs_in_build = False
-        self.worker.release()
-        self.waitUntilSettled()
-
-    def test_client_get_running_jobs(self):
-        "Test that the RPC client can get a list of running jobs"
-        self.worker.hold_jobs_in_build = True
-        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
-        A.addApproval('CRVW', 2)
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-        self.waitUntilSettled()
-
-        client = zuul.rpcclient.RPCClient('127.0.0.1',
-                                          self.gearman_server.port)
-
-        # Wait for gearman server to send the initial workData back to zuul
-        start = time.time()
-        while True:
-            if time.time() - start > 10:
-                raise Exception("Timeout waiting for gearman server to report "
-                                + "back to the client")
-            build = self.launcher.builds.values()[0]
-            if build.worker.name == "My Worker":
-                break
-            else:
-                time.sleep(0)
-
-        running_items = client.get_running_jobs()
-
-        self.assertEqual(1, len(running_items))
-        running_item = running_items[0]
-        self.assertEqual([], running_item['failing_reasons'])
-        self.assertEqual([], running_item['items_behind'])
-        self.assertEqual('https://hostname/1', running_item['url'])
-        self.assertEqual(None, running_item['item_ahead'])
-        self.assertEqual('org/project', running_item['project'])
-        self.assertEqual(None, running_item['remaining_time'])
-        self.assertEqual(True, running_item['active'])
-        self.assertEqual('1,1', running_item['id'])
-
-        self.assertEqual(3, len(running_item['jobs']))
-        for job in running_item['jobs']:
-            if job['name'] == 'project-merge':
-                self.assertEqual('project-merge', job['name'])
-                self.assertEqual('gate', job['pipeline'])
-                self.assertEqual(False, job['retry'])
-                self.assertEqual('https://server/job/project-merge/0/',
-                                 job['url'])
-                self.assertEqual(7, len(job['worker']))
-                self.assertEqual(False, job['canceled'])
-                self.assertEqual(True, job['voting'])
-                self.assertEqual(None, job['result'])
-                self.assertEqual('gate', job['pipeline'])
-                break
-
-        self.worker.hold_jobs_in_build = False
-        self.worker.release()
-        self.waitUntilSettled()
-
-        running_items = client.get_running_jobs()
-        self.assertEqual(0, len(running_items))
-
-    def test_nonvoting_pipeline(self):
-        "Test that a nonvoting pipeline (experimental) can still report"
-
-        A = self.fake_gerrit.addFakeChange('org/experimental-project',
-                                           'master', 'A')
-        self.fake_gerrit.addEvent(A.getPatchsetCreatedEvent(1))
-        self.waitUntilSettled()
-        self.assertEqual(
-            self.getJobFromHistory('experimental-project-test').result,
-            'SUCCESS')
-        self.assertEqual(A.reported, 1)
-
-    def test_crd_gate(self):
-        "Test cross-repo dependencies"
-        A = self.fake_gerrit.addFakeChange('org/project1', 'master', 'A')
-        B = self.fake_gerrit.addFakeChange('org/project2', 'master', 'B')
-        A.addApproval('CRVW', 2)
-        B.addApproval('CRVW', 2)
-
-        AM2 = self.fake_gerrit.addFakeChange('org/project1', 'master', 'AM2')
-        AM1 = self.fake_gerrit.addFakeChange('org/project1', 'master', 'AM1')
-        AM2.setMerged()
-        AM1.setMerged()
-
-        BM2 = self.fake_gerrit.addFakeChange('org/project2', 'master', 'BM2')
-        BM1 = self.fake_gerrit.addFakeChange('org/project2', 'master', 'BM1')
-        BM2.setMerged()
-        BM1.setMerged()
-
-        # A -> AM1 -> AM2
-        # B -> BM1 -> BM2
-        # A Depends-On: B
-        # M2 is here to make sure it is never queried.  If it is, it
-        # means zuul is walking down the entire history of merged
-        # changes.
-
-        B.setDependsOn(BM1, 1)
-        BM1.setDependsOn(BM2, 1)
-
-        A.setDependsOn(AM1, 1)
-        AM1.setDependsOn(AM2, 1)
-
-        A.data['commitMessage'] = '%s\n\nDepends-On: %s\n' % (
-            A.subject, B.data['id'])
-
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-        self.waitUntilSettled()
-
-        self.assertEqual(A.data['status'], 'NEW')
-        self.assertEqual(B.data['status'], 'NEW')
-
-        for connection in self.connections.values():
-            connection.maintainCache([])
-
-        self.worker.hold_jobs_in_build = True
-        B.addApproval('APRV', 1)
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-        self.waitUntilSettled()
-
-        self.worker.release('.*-merge')
-        self.waitUntilSettled()
-        self.worker.release('.*-merge')
-        self.waitUntilSettled()
-        self.worker.hold_jobs_in_build = False
-        self.worker.release()
-        self.waitUntilSettled()
-
-        self.assertEqual(AM2.queried, 0)
-        self.assertEqual(BM2.queried, 0)
-        self.assertEqual(A.data['status'], 'MERGED')
-        self.assertEqual(B.data['status'], 'MERGED')
-        self.assertEqual(A.reported, 2)
-        self.assertEqual(B.reported, 2)
-
-        self.assertEqual(self.getJobFromHistory('project1-merge').changes,
-                         '2,1 1,1')
-
-    def test_crd_branch(self):
-        "Test cross-repo dependencies in multiple branches"
-        A = self.fake_gerrit.addFakeChange('org/project1', 'master', 'A')
-        B = self.fake_gerrit.addFakeChange('org/project2', 'master', 'B')
-        C = self.fake_gerrit.addFakeChange('org/project2', 'mp', 'C')
-        C.data['id'] = B.data['id']
-        A.addApproval('CRVW', 2)
-        B.addApproval('CRVW', 2)
-        C.addApproval('CRVW', 2)
-
-        # A Depends-On: B+C
-        A.data['commitMessage'] = '%s\n\nDepends-On: %s\n' % (
-            A.subject, B.data['id'])
-
-        self.worker.hold_jobs_in_build = True
-        B.addApproval('APRV', 1)
-        C.addApproval('APRV', 1)
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-        self.waitUntilSettled()
-
-        self.worker.release('.*-merge')
-        self.waitUntilSettled()
-        self.worker.release('.*-merge')
-        self.waitUntilSettled()
-        self.worker.release('.*-merge')
-        self.waitUntilSettled()
-        self.worker.hold_jobs_in_build = False
-        self.worker.release()
-        self.waitUntilSettled()
-
-        self.assertEqual(A.data['status'], 'MERGED')
-        self.assertEqual(B.data['status'], 'MERGED')
-        self.assertEqual(C.data['status'], 'MERGED')
-        self.assertEqual(A.reported, 2)
-        self.assertEqual(B.reported, 2)
-        self.assertEqual(C.reported, 2)
-
-        self.assertEqual(self.getJobFromHistory('project1-merge').changes,
-                         '2,1 3,1 1,1')
-
-    def test_crd_multiline(self):
-        "Test multiple depends-on lines in commit"
-        A = self.fake_gerrit.addFakeChange('org/project1', 'master', 'A')
-        B = self.fake_gerrit.addFakeChange('org/project2', 'master', 'B')
-        C = self.fake_gerrit.addFakeChange('org/project2', 'master', 'C')
-        A.addApproval('CRVW', 2)
-        B.addApproval('CRVW', 2)
-        C.addApproval('CRVW', 2)
-
-        # A Depends-On: B+C
-        A.data['commitMessage'] = '%s\n\nDepends-On: %s\nDepends-On: %s\n' % (
-            A.subject, B.data['id'], C.data['id'])
-
-        self.worker.hold_jobs_in_build = True
-        B.addApproval('APRV', 1)
-        C.addApproval('APRV', 1)
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-        self.waitUntilSettled()
-
-        self.worker.release('.*-merge')
-        self.waitUntilSettled()
-        self.worker.release('.*-merge')
-        self.waitUntilSettled()
-        self.worker.release('.*-merge')
-        self.waitUntilSettled()
-        self.worker.hold_jobs_in_build = False
-        self.worker.release()
-        self.waitUntilSettled()
-
-        self.assertEqual(A.data['status'], 'MERGED')
-        self.assertEqual(B.data['status'], 'MERGED')
-        self.assertEqual(C.data['status'], 'MERGED')
-        self.assertEqual(A.reported, 2)
-        self.assertEqual(B.reported, 2)
-        self.assertEqual(C.reported, 2)
-
-        self.assertEqual(self.getJobFromHistory('project1-merge').changes,
-                         '2,1 3,1 1,1')
-
-    def test_crd_unshared_gate(self):
-        "Test cross-repo dependencies in unshared gate queues"
-        A = self.fake_gerrit.addFakeChange('org/project1', 'master', 'A')
-        B = self.fake_gerrit.addFakeChange('org/project', 'master', 'B')
-        A.addApproval('CRVW', 2)
-        B.addApproval('CRVW', 2)
-
-        # A Depends-On: B
-        A.data['commitMessage'] = '%s\n\nDepends-On: %s\n' % (
-            A.subject, B.data['id'])
-
-        # A and B do not share a queue, make sure that A is unable to
-        # enqueue B (and therefore, A is unable to be enqueued).
-        B.addApproval('APRV', 1)
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-        self.waitUntilSettled()
-
-        self.assertEqual(A.data['status'], 'NEW')
-        self.assertEqual(B.data['status'], 'NEW')
-        self.assertEqual(A.reported, 0)
-        self.assertEqual(B.reported, 0)
-        self.assertEqual(len(self.history), 0)
-
-        # Enqueue and merge B alone.
-        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
-        self.waitUntilSettled()
-
-        self.assertEqual(B.data['status'], 'MERGED')
-        self.assertEqual(B.reported, 2)
-
-        # Now that B is merged, A should be able to be enqueued and
-        # merged.
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-        self.waitUntilSettled()
-
-        self.assertEqual(A.data['status'], 'MERGED')
-        self.assertEqual(A.reported, 2)
-
-    def test_crd_gate_reverse(self):
-        "Test reverse cross-repo dependencies"
-        A = self.fake_gerrit.addFakeChange('org/project1', 'master', 'A')
-        B = self.fake_gerrit.addFakeChange('org/project2', 'master', 'B')
-        A.addApproval('CRVW', 2)
-        B.addApproval('CRVW', 2)
-
-        # A Depends-On: B
-
-        A.data['commitMessage'] = '%s\n\nDepends-On: %s\n' % (
-            A.subject, B.data['id'])
-
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-        self.waitUntilSettled()
-
-        self.assertEqual(A.data['status'], 'NEW')
-        self.assertEqual(B.data['status'], 'NEW')
-
-        self.worker.hold_jobs_in_build = True
-        A.addApproval('APRV', 1)
-        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
-        self.waitUntilSettled()
-
-        self.worker.release('.*-merge')
-        self.waitUntilSettled()
-        self.worker.release('.*-merge')
-        self.waitUntilSettled()
-        self.worker.hold_jobs_in_build = False
-        self.worker.release()
-        self.waitUntilSettled()
-
-        self.assertEqual(A.data['status'], 'MERGED')
-        self.assertEqual(B.data['status'], 'MERGED')
-        self.assertEqual(A.reported, 2)
-        self.assertEqual(B.reported, 2)
-
-        self.assertEqual(self.getJobFromHistory('project1-merge').changes,
-                         '2,1 1,1')
-
-    def test_crd_cycle(self):
-        "Test cross-repo dependency cycles"
-        A = self.fake_gerrit.addFakeChange('org/project1', 'master', 'A')
-        B = self.fake_gerrit.addFakeChange('org/project2', 'master', 'B')
-        A.addApproval('CRVW', 2)
-        B.addApproval('CRVW', 2)
-
-        # A -> B -> A (via commit-depends)
-
-        A.data['commitMessage'] = '%s\n\nDepends-On: %s\n' % (
-            A.subject, B.data['id'])
-        B.data['commitMessage'] = '%s\n\nDepends-On: %s\n' % (
-            B.subject, A.data['id'])
-
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-        self.waitUntilSettled()
-
-        self.assertEqual(A.reported, 0)
-        self.assertEqual(B.reported, 0)
-        self.assertEqual(A.data['status'], 'NEW')
-        self.assertEqual(B.data['status'], 'NEW')
-
-    def test_crd_gate_unknown(self):
-        "Test unknown projects in dependent pipeline"
-        self.init_repo("org/unknown")
-        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
-        B = self.fake_gerrit.addFakeChange('org/unknown', 'master', 'B')
-        A.addApproval('CRVW', 2)
-        B.addApproval('CRVW', 2)
-
-        # A Depends-On: B
-        A.data['commitMessage'] = '%s\n\nDepends-On: %s\n' % (
-            A.subject, B.data['id'])
-
-        B.addApproval('APRV', 1)
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-        self.waitUntilSettled()
-
-        # Unknown projects cannot share a queue with any other
-        # since they don't have common jobs with any other (they have no jobs).
-        # Changes which depend on unknown project changes
-        # should not be processed in dependent pipeline
-        self.assertEqual(A.data['status'], 'NEW')
-        self.assertEqual(B.data['status'], 'NEW')
-        self.assertEqual(A.reported, 0)
-        self.assertEqual(B.reported, 0)
-        self.assertEqual(len(self.history), 0)
-
-        # Simulate change B being gated outside this layout
-        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
-        B.setMerged()
-        self.waitUntilSettled()
-        self.assertEqual(len(self.history), 0)
-
-        # Now that B is merged, A should be able to be enqueued and
-        # merged.
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-        self.waitUntilSettled()
-
-        self.assertEqual(A.data['status'], 'MERGED')
-        self.assertEqual(A.reported, 2)
-        self.assertEqual(B.data['status'], 'MERGED')
-        self.assertEqual(B.reported, 0)
-
-    def test_crd_check(self):
-        "Test cross-repo dependencies in independent pipelines"
-
-        self.gearman_server.hold_jobs_in_queue = True
-        A = self.fake_gerrit.addFakeChange('org/project1', 'master', 'A')
-        B = self.fake_gerrit.addFakeChange('org/project2', 'master', 'B')
-
-        # A Depends-On: B
-        A.data['commitMessage'] = '%s\n\nDepends-On: %s\n' % (
-            A.subject, B.data['id'])
-
-        self.fake_gerrit.addEvent(A.getPatchsetCreatedEvent(1))
-        self.waitUntilSettled()
-
-        queue = self.gearman_server.getQueue()
-        ref = self.getParameter(queue[-1], 'ZUUL_REF')
-        self.gearman_server.hold_jobs_in_queue = False
-        self.gearman_server.release()
-        self.waitUntilSettled()
-
-        path = os.path.join(self.git_root, "org/project1")
-        repo = git.Repo(path)
-        repo_messages = [c.message.strip() for c in repo.iter_commits(ref)]
-        repo_messages.reverse()
-        correct_messages = ['initial commit', 'A-1']
-        self.assertEqual(repo_messages, correct_messages)
-
-        path = os.path.join(self.git_root, "org/project2")
-        repo = git.Repo(path)
-        repo_messages = [c.message.strip() for c in repo.iter_commits(ref)]
-        repo_messages.reverse()
-        correct_messages = ['initial commit', 'B-1']
-        self.assertEqual(repo_messages, correct_messages)
-
-        self.assertEqual(A.data['status'], 'NEW')
-        self.assertEqual(B.data['status'], 'NEW')
-        self.assertEqual(A.reported, 1)
-        self.assertEqual(B.reported, 0)
-
-        self.assertEqual(self.history[0].changes, '2,1 1,1')
-        self.assertEqual(len(self.sched.layout.pipelines['check'].queues), 0)
-
-    def test_crd_check_git_depends(self):
-        "Test single-repo dependencies in independent pipelines"
-        self.gearman_server.hold_jobs_in_build = True
-        A = self.fake_gerrit.addFakeChange('org/project1', 'master', 'A')
-        B = self.fake_gerrit.addFakeChange('org/project1', 'master', 'B')
-
-        # Add two git-dependent changes and make sure they both report
-        # success.
-        B.setDependsOn(A, 1)
-        self.fake_gerrit.addEvent(A.getPatchsetCreatedEvent(1))
-        self.waitUntilSettled()
-        self.fake_gerrit.addEvent(B.getPatchsetCreatedEvent(1))
-        self.waitUntilSettled()
-
-        self.orderedRelease()
-        self.gearman_server.hold_jobs_in_build = False
-        self.waitUntilSettled()
-
-        self.assertEqual(A.data['status'], 'NEW')
-        self.assertEqual(B.data['status'], 'NEW')
-        self.assertEqual(A.reported, 1)
-        self.assertEqual(B.reported, 1)
-
-        self.assertEqual(self.history[0].changes, '1,1')
-        self.assertEqual(self.history[-1].changes, '1,1 2,1')
-        self.assertEqual(len(self.sched.layout.pipelines['check'].queues), 0)
-
-        self.assertIn('Build succeeded', A.messages[0])
-        self.assertIn('Build succeeded', B.messages[0])
-
-    def test_crd_check_duplicate(self):
-        "Test duplicate check in independent pipelines"
-        self.worker.hold_jobs_in_build = True
-        A = self.fake_gerrit.addFakeChange('org/project1', 'master', 'A')
-        B = self.fake_gerrit.addFakeChange('org/project1', 'master', 'B')
-        check_pipeline = self.sched.layout.pipelines['check']
-
-        # Add two git-dependent changes...
-        B.setDependsOn(A, 1)
-        self.fake_gerrit.addEvent(B.getPatchsetCreatedEvent(1))
-        self.waitUntilSettled()
-        self.assertEqual(len(check_pipeline.getAllItems()), 2)
-
-        # ...make sure the live one is not duplicated...
-        self.fake_gerrit.addEvent(B.getPatchsetCreatedEvent(1))
-        self.waitUntilSettled()
-        self.assertEqual(len(check_pipeline.getAllItems()), 2)
-
-        # ...but the non-live one is able to be.
-        self.fake_gerrit.addEvent(A.getPatchsetCreatedEvent(1))
-        self.waitUntilSettled()
-        self.assertEqual(len(check_pipeline.getAllItems()), 3)
-
-        # Release jobs in order to avoid races with change A jobs
-        # finishing before change B jobs.
-        self.orderedRelease()
-        self.worker.hold_jobs_in_build = False
-        self.worker.release()
-        self.waitUntilSettled()
-
-        self.assertEqual(A.data['status'], 'NEW')
-        self.assertEqual(B.data['status'], 'NEW')
-        self.assertEqual(A.reported, 1)
-        self.assertEqual(B.reported, 1)
-
-        self.assertEqual(self.history[0].changes, '1,1 2,1')
-        self.assertEqual(self.history[1].changes, '1,1')
-        self.assertEqual(len(self.sched.layout.pipelines['check'].queues), 0)
-
-        self.assertIn('Build succeeded', A.messages[0])
-        self.assertIn('Build succeeded', B.messages[0])
-
-    def _test_crd_check_reconfiguration(self, project1, project2):
-        "Test cross-repo dependencies re-enqueued in independent pipelines"
-
-        self.gearman_server.hold_jobs_in_queue = True
-        A = self.fake_gerrit.addFakeChange(project1, 'master', 'A')
-        B = self.fake_gerrit.addFakeChange(project2, 'master', 'B')
-
-        # A Depends-On: B
-        A.data['commitMessage'] = '%s\n\nDepends-On: %s\n' % (
-            A.subject, B.data['id'])
-
-        self.fake_gerrit.addEvent(A.getPatchsetCreatedEvent(1))
-        self.waitUntilSettled()
-
-        self.sched.reconfigure(self.config)
-
-        # Make sure the items still share a change queue, and the
-        # first one is not live.
-        self.assertEqual(len(self.sched.layout.pipelines['check'].queues), 1)
-        queue = self.sched.layout.pipelines['check'].queues[0]
-        first_item = queue.queue[0]
-        for item in queue.queue:
-            self.assertEqual(item.queue, first_item.queue)
-        self.assertFalse(first_item.live)
-        self.assertTrue(queue.queue[1].live)
-
-        self.gearman_server.hold_jobs_in_queue = False
-        self.gearman_server.release()
-        self.waitUntilSettled()
-
-        self.assertEqual(A.data['status'], 'NEW')
-        self.assertEqual(B.data['status'], 'NEW')
-        self.assertEqual(A.reported, 1)
-        self.assertEqual(B.reported, 0)
-
-        self.assertEqual(self.history[0].changes, '2,1 1,1')
-        self.assertEqual(len(self.sched.layout.pipelines['check'].queues), 0)
-
-    def test_crd_check_reconfiguration(self):
-        self._test_crd_check_reconfiguration('org/project1', 'org/project2')
-
-    def test_crd_undefined_project(self):
-        """Test that undefined projects in dependencies are handled for
-        independent pipelines"""
-        # It's a hack for fake gerrit,
-        # as it implies repo creation upon the creation of any change
-        self.init_repo("org/unknown")
-        self._test_crd_check_reconfiguration('org/project1', 'org/unknown')
-
-    def test_crd_check_ignore_dependencies(self):
-        "Test cross-repo dependencies can be ignored"
-        self.config.set('zuul', 'layout_config',
-                        'tests/fixtures/layout-ignore-dependencies.yaml')
-        self.sched.reconfigure(self.config)
-        self.registerJobs()
-
-        self.gearman_server.hold_jobs_in_queue = True
-        A = self.fake_gerrit.addFakeChange('org/project1', 'master', 'A')
-        B = self.fake_gerrit.addFakeChange('org/project2', 'master', 'B')
-        C = self.fake_gerrit.addFakeChange('org/project2', 'master', 'C')
-
-        # A Depends-On: B
-        A.data['commitMessage'] = '%s\n\nDepends-On: %s\n' % (
-            A.subject, B.data['id'])
-        # C git-depends on B
-        C.setDependsOn(B, 1)
-        self.fake_gerrit.addEvent(A.getPatchsetCreatedEvent(1))
-        self.fake_gerrit.addEvent(B.getPatchsetCreatedEvent(1))
-        self.fake_gerrit.addEvent(C.getPatchsetCreatedEvent(1))
-        self.waitUntilSettled()
-
-        # Make sure none of the items share a change queue, and all
-        # are live.
-        check_pipeline = self.sched.layout.pipelines['check']
-        self.assertEqual(len(check_pipeline.queues), 3)
-        self.assertEqual(len(check_pipeline.getAllItems()), 3)
-        for item in check_pipeline.getAllItems():
-            self.assertTrue(item.live)
-
-        self.gearman_server.hold_jobs_in_queue = False
-        self.gearman_server.release()
-        self.waitUntilSettled()
-
-        self.assertEqual(A.data['status'], 'NEW')
-        self.assertEqual(B.data['status'], 'NEW')
-        self.assertEqual(C.data['status'], 'NEW')
-        self.assertEqual(A.reported, 1)
-        self.assertEqual(B.reported, 1)
-        self.assertEqual(C.reported, 1)
-
-        # Each job should have tested exactly one change
-        for job in self.history:
-            self.assertEqual(len(job.changes.split()), 1)
-
-    def test_crd_check_transitive(self):
-        "Test transitive cross-repo dependencies"
-        # Specifically, if A -> B -> C, and C gets a new patchset and
-        # A gets a new patchset, ensure the test of A,2 includes B,1
-        # and C,2 (not C,1 which would indicate stale data in the
-        # cache for B).
-        A = self.fake_gerrit.addFakeChange('org/project1', 'master', 'A')
-        B = self.fake_gerrit.addFakeChange('org/project2', 'master', 'B')
-        C = self.fake_gerrit.addFakeChange('org/project3', 'master', 'C')
-
-        # A Depends-On: B
-        A.data['commitMessage'] = '%s\n\nDepends-On: %s\n' % (
-            A.subject, B.data['id'])
-
-        # B Depends-On: C
-        B.data['commitMessage'] = '%s\n\nDepends-On: %s\n' % (
-            B.subject, C.data['id'])
-
-        self.fake_gerrit.addEvent(A.getPatchsetCreatedEvent(1))
-        self.waitUntilSettled()
-        self.assertEqual(self.history[-1].changes, '3,1 2,1 1,1')
-
-        self.fake_gerrit.addEvent(B.getPatchsetCreatedEvent(1))
-        self.waitUntilSettled()
-        self.assertEqual(self.history[-1].changes, '3,1 2,1')
-
-        self.fake_gerrit.addEvent(C.getPatchsetCreatedEvent(1))
-        self.waitUntilSettled()
-        self.assertEqual(self.history[-1].changes, '3,1')
-
-        C.addPatchset()
-        self.fake_gerrit.addEvent(C.getPatchsetCreatedEvent(2))
-        self.waitUntilSettled()
-        self.assertEqual(self.history[-1].changes, '3,2')
-
-        A.addPatchset()
-        self.fake_gerrit.addEvent(A.getPatchsetCreatedEvent(2))
-        self.waitUntilSettled()
-        self.assertEqual(self.history[-1].changes, '3,2 2,1 1,2')
-
-    def test_crd_cycle_join(self):
-        "Test an updated change creates a cycle"
-        A = self.fake_gerrit.addFakeChange('org/project2', 'master', 'A')
-
-        self.fake_gerrit.addEvent(A.getPatchsetCreatedEvent(1))
-        self.waitUntilSettled()
-
-        # Create B->A
-        B = self.fake_gerrit.addFakeChange('org/project1', 'master', 'B')
-        B.data['commitMessage'] = '%s\n\nDepends-On: %s\n' % (
-            B.subject, A.data['id'])
-        self.fake_gerrit.addEvent(B.getPatchsetCreatedEvent(1))
-        self.waitUntilSettled()
-
-        # Update A to add A->B (a cycle).
-        A.addPatchset()
-        A.data['commitMessage'] = '%s\n\nDepends-On: %s\n' % (
-            A.subject, B.data['id'])
-        # Normally we would submit the patchset-created event for
-        # processing here, however, we have no way of noting whether
-        # the dependency cycle detection correctly raised an
-        # exception, so instead, we reach into the source driver and
-        # call the method that would ultimately be called by the event
-        # processing.
-
-        source = self.sched.layout.pipelines['gate'].source
-        with testtools.ExpectedException(
-            Exception, "Dependency cycle detected"):
-            source._getChange(u'1', u'2', True)
-        self.log.debug("Got expected dependency cycle exception")
-
-        # Now if we update B to remove the depends-on, everything
-        # should be okay.  B; A->B
-
-        B.addPatchset()
-        B.data['commitMessage'] = '%s\n' % (B.subject,)
-        source._getChange(u'1', u'2', True)
-        source._getChange(u'2', u'2', True)
-
-    def test_disable_at(self):
-        "Test a pipeline will only report to the disabled trigger when failing"
-
-        self.config.set('zuul', 'layout_config',
-                        'tests/fixtures/layout-disable-at.yaml')
-        self.sched.reconfigure(self.config)
-
-        self.assertEqual(3, self.sched.layout.pipelines['check'].disable_at)
-        self.assertEqual(
-            0, self.sched.layout.pipelines['check']._consecutive_failures)
-        self.assertFalse(self.sched.layout.pipelines['check']._disabled)
-
-        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
-        B = self.fake_gerrit.addFakeChange('org/project', 'master', 'B')
-        C = self.fake_gerrit.addFakeChange('org/project', 'master', 'C')
-        D = self.fake_gerrit.addFakeChange('org/project', 'master', 'D')
-        E = self.fake_gerrit.addFakeChange('org/project', 'master', 'E')
-        F = self.fake_gerrit.addFakeChange('org/project', 'master', 'F')
-        G = self.fake_gerrit.addFakeChange('org/project', 'master', 'G')
-        H = self.fake_gerrit.addFakeChange('org/project', 'master', 'H')
-        I = self.fake_gerrit.addFakeChange('org/project', 'master', 'I')
-        J = self.fake_gerrit.addFakeChange('org/project', 'master', 'J')
-        K = self.fake_gerrit.addFakeChange('org/project', 'master', 'K')
-
-        self.worker.addFailTest('project-test1', A)
-        self.worker.addFailTest('project-test1', B)
-        # Let C pass, resetting the counter
-        self.worker.addFailTest('project-test1', D)
-        self.worker.addFailTest('project-test1', E)
-        self.worker.addFailTest('project-test1', F)
-        self.worker.addFailTest('project-test1', G)
-        self.worker.addFailTest('project-test1', H)
-        # I also passes but should only report to the disabled reporters
-        self.worker.addFailTest('project-test1', J)
-        self.worker.addFailTest('project-test1', K)
-
-        self.fake_gerrit.addEvent(A.getPatchsetCreatedEvent(1))
-        self.fake_gerrit.addEvent(B.getPatchsetCreatedEvent(1))
-        self.waitUntilSettled()
-
-        self.assertEqual(
-            2, self.sched.layout.pipelines['check']._consecutive_failures)
-        self.assertFalse(self.sched.layout.pipelines['check']._disabled)
-
-        self.fake_gerrit.addEvent(C.getPatchsetCreatedEvent(1))
-        self.waitUntilSettled()
-
-        self.assertEqual(
-            0, self.sched.layout.pipelines['check']._consecutive_failures)
-        self.assertFalse(self.sched.layout.pipelines['check']._disabled)
-
-        self.fake_gerrit.addEvent(D.getPatchsetCreatedEvent(1))
-        self.fake_gerrit.addEvent(E.getPatchsetCreatedEvent(1))
-        self.fake_gerrit.addEvent(F.getPatchsetCreatedEvent(1))
-        self.waitUntilSettled()
-
-        # We should be disabled now
-        self.assertEqual(
-            3, self.sched.layout.pipelines['check']._consecutive_failures)
-        self.assertTrue(self.sched.layout.pipelines['check']._disabled)
-
-        # We need to wait between each of these patches to make sure the
-        # smtp messages come back in an expected order
-        self.fake_gerrit.addEvent(G.getPatchsetCreatedEvent(1))
-        self.waitUntilSettled()
-        self.fake_gerrit.addEvent(H.getPatchsetCreatedEvent(1))
-        self.waitUntilSettled()
-        self.fake_gerrit.addEvent(I.getPatchsetCreatedEvent(1))
-        self.waitUntilSettled()
-
-        # The first 6 (ABCDEF) jobs should have reported back to gerrt thus
-        # leaving a message on each change
-        self.assertEqual(1, len(A.messages))
-        self.assertIn('Build failed.', A.messages[0])
-        self.assertEqual(1, len(B.messages))
-        self.assertIn('Build failed.', B.messages[0])
-        self.assertEqual(1, len(C.messages))
-        self.assertIn('Build succeeded.', C.messages[0])
-        self.assertEqual(1, len(D.messages))
-        self.assertIn('Build failed.', D.messages[0])
-        self.assertEqual(1, len(E.messages))
-        self.assertIn('Build failed.', E.messages[0])
-        self.assertEqual(1, len(F.messages))
-        self.assertIn('Build failed.', F.messages[0])
-
-        # The last 3 (GHI) would have only reported via smtp.
-        self.assertEqual(3, len(self.smtp_messages))
-        self.assertEqual(0, len(G.messages))
-        self.assertIn('Build failed.', self.smtp_messages[0]['body'])
-        self.assertIn('/7/1/check', self.smtp_messages[0]['body'])
-        self.assertEqual(0, len(H.messages))
-        self.assertIn('Build failed.', self.smtp_messages[1]['body'])
-        self.assertIn('/8/1/check', self.smtp_messages[1]['body'])
-        self.assertEqual(0, len(I.messages))
-        self.assertIn('Build succeeded.', self.smtp_messages[2]['body'])
-        self.assertIn('/9/1/check', self.smtp_messages[2]['body'])
-
-        # Now reload the configuration (simulate a HUP) to check the pipeline
-        # comes out of disabled
-        self.sched.reconfigure(self.config)
-
-        self.assertEqual(3, self.sched.layout.pipelines['check'].disable_at)
-        self.assertEqual(
-            0, self.sched.layout.pipelines['check']._consecutive_failures)
-        self.assertFalse(self.sched.layout.pipelines['check']._disabled)
-
-        self.fake_gerrit.addEvent(J.getPatchsetCreatedEvent(1))
-        self.fake_gerrit.addEvent(K.getPatchsetCreatedEvent(1))
-        self.waitUntilSettled()
-
-        self.assertEqual(
-            2, self.sched.layout.pipelines['check']._consecutive_failures)
-        self.assertFalse(self.sched.layout.pipelines['check']._disabled)
-
-        # J and K went back to gerrit
-        self.assertEqual(1, len(J.messages))
-        self.assertIn('Build failed.', J.messages[0])
-        self.assertEqual(1, len(K.messages))
-        self.assertIn('Build failed.', K.messages[0])
-        # No more messages reported via smtp
-        self.assertEqual(3, len(self.smtp_messages))
+#!/usr/bin/env python2.7
+
+# Copyright 2012 Hewlett-Packard Development Company, L.P.
+#
+# Licensed under the Apache License, Version 2.0 (the "License"); you may
+# not use this file except in compliance with the License. You may obtain
+# a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+# License for the specific language governing permissions and limitations
+# under the License.
+
+import json
+import logging
+import os
+import re
+import shutil
+import time
+import urllib
+import urllib2
+import yaml
+
+import git
+import testtools
+
+import zuul.change_matcher
+import zuul.scheduler
+import zuul.rpcclient
+import zuul.reporter.gerrit
+import zuul.reporter.smtp
+
+from tests.base import (
+    BaseTestCase,
+    ZuulTestCase,
+    repack_repo,
+)
+
+logging.basicConfig(level=logging.DEBUG,
+                    format='%(asctime)s %(name)-32s '
+                    '%(levelname)-8s %(message)s')
+
+
+class TestSchedulerConfigParsing(BaseTestCase):
+
+    def test_parse_skip_if(self):
+        job_yaml = """
+jobs:
+  - name: job_name
+    skip-if:
+      - project: ^project_name$
+        branch: ^stable/icehouse$
+        all-files-match-any:
+          - ^filename$
+      - project: ^project2_name$
+        all-files-match-any:
+          - ^filename2$
+    """.strip()
+        data = yaml.load(job_yaml)
+        config_job = data.get('jobs')[0]
+        sched = zuul.scheduler.Scheduler({})
+        cm = zuul.change_matcher
+        expected = cm.MatchAny([
+            cm.MatchAll([
+                cm.ProjectMatcher('^project_name$'),
+                cm.BranchMatcher('^stable/icehouse$'),
+                cm.MatchAllFiles([cm.FileMatcher('^filename$')]),
+            ]),
+            cm.MatchAll([
+                cm.ProjectMatcher('^project2_name$'),
+                cm.MatchAllFiles([cm.FileMatcher('^filename2$')]),
+            ]),
+        ])
+        matcher = sched._parseSkipIf(config_job)
+        self.assertEqual(expected, matcher)
+
+
+class TestScheduler(ZuulTestCase):
+
+    def test_jobs_launched(self):
+        "Test that jobs are launched and a change is merged"
+
+        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
+        A.addApproval('CRVW', 2)
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+        self.waitUntilSettled()
+        self.assertEqual(self.getJobFromHistory('project-merge').result,
+                         'SUCCESS')
+        self.assertEqual(self.getJobFromHistory('project-test1').result,
+                         'SUCCESS')
+        self.assertEqual(self.getJobFromHistory('project-test2').result,
+                         'SUCCESS')
+        self.assertEqual(A.data['status'], 'MERGED')
+        self.assertEqual(A.reported, 2)
+
+        self.assertReportedStat('gerrit.event.comment-added', value='1|c')
+        self.assertReportedStat('zuul.pipeline.gate.current_changes',
+                                value='1|g')
+        self.assertReportedStat('zuul.pipeline.gate.job.project-merge.SUCCESS',
+                                kind='ms')
+        self.assertReportedStat('zuul.pipeline.gate.job.project-merge.SUCCESS',
+                                value='1|c')
+        self.assertReportedStat('zuul.pipeline.gate.resident_time', kind='ms')
+        self.assertReportedStat('zuul.pipeline.gate.total_changes',
+                                value='1|c')
+        self.assertReportedStat(
+            'zuul.pipeline.gate.org.project.resident_time', kind='ms')
+        self.assertReportedStat(
+            'zuul.pipeline.gate.org.project.total_changes', value='1|c')
+
+        for build in self.builds:
+            self.assertEqual(build.parameters['ZUUL_VOTING'], '1')
+
+    def test_initial_pipeline_gauges(self):
+        "Test that each pipeline reported its length on start"
+        pipeline_names = self.sched.layout.pipelines.keys()
+        self.assertNotEqual(len(pipeline_names), 0)
+        for name in pipeline_names:
+            self.assertReportedStat('zuul.pipeline.%s.current_changes' % name,
+                                    value='0|g')
+
+    def test_duplicate_pipelines(self):
+        "Test that a change matching multiple pipelines works"
+
+        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
+        self.fake_gerrit.addEvent(A.getChangeRestoredEvent())
+        self.waitUntilSettled()
+
+        self.assertEqual(len(self.history), 2)
+        self.history[0].name == 'project-test1'
+        self.history[1].name == 'project-test1'
+
+        self.assertEqual(len(A.messages), 2)
+        if 'dup1/project-test1' in A.messages[0]:
+            self.assertIn('dup1/project-test1', A.messages[0])
+            self.assertNotIn('dup2/project-test1', A.messages[0])
+            self.assertNotIn('dup1/project-test1', A.messages[1])
+            self.assertIn('dup2/project-test1', A.messages[1])
+        else:
+            self.assertIn('dup1/project-test1', A.messages[1])
+            self.assertNotIn('dup2/project-test1', A.messages[1])
+            self.assertNotIn('dup1/project-test1', A.messages[0])
+            self.assertIn('dup2/project-test1', A.messages[0])
+
+    def test_parallel_changes(self):
+        "Test that changes are tested in parallel and merged in series"
+
+        self.worker.hold_jobs_in_build = True
+        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
+        B = self.fake_gerrit.addFakeChange('org/project', 'master', 'B')
+        C = self.fake_gerrit.addFakeChange('org/project', 'master', 'C')
+        A.addApproval('CRVW', 2)
+        B.addApproval('CRVW', 2)
+        C.addApproval('CRVW', 2)
+
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
+        self.fake_gerrit.addEvent(C.addApproval('APRV', 1))
+
+        self.waitUntilSettled()
+        self.assertEqual(len(self.builds), 1)
+        self.assertEqual(self.builds[0].name, 'project-merge')
+        self.assertTrue(self.job_has_changes(self.builds[0], A))
+
+        self.worker.release('.*-merge')
+        self.waitUntilSettled()
+        self.assertEqual(len(self.builds), 3)
+        self.assertEqual(self.builds[0].name, 'project-test1')
+        self.assertTrue(self.job_has_changes(self.builds[0], A))
+        self.assertEqual(self.builds[1].name, 'project-test2')
+        self.assertTrue(self.job_has_changes(self.builds[1], A))
+        self.assertEqual(self.builds[2].name, 'project-merge')
+        self.assertTrue(self.job_has_changes(self.builds[2], A, B))
+
+        self.worker.release('.*-merge')
+        self.waitUntilSettled()
+        self.assertEqual(len(self.builds), 5)
+        self.assertEqual(self.builds[0].name, 'project-test1')
+        self.assertTrue(self.job_has_changes(self.builds[0], A))
+        self.assertEqual(self.builds[1].name, 'project-test2')
+        self.assertTrue(self.job_has_changes(self.builds[1], A))
+
+        self.assertEqual(self.builds[2].name, 'project-test1')
+        self.assertTrue(self.job_has_changes(self.builds[2], A, B))
+        self.assertEqual(self.builds[3].name, 'project-test2')
+        self.assertTrue(self.job_has_changes(self.builds[3], A, B))
+
+        self.assertEqual(self.builds[4].name, 'project-merge')
+        self.assertTrue(self.job_has_changes(self.builds[4], A, B, C))
+
+        self.worker.release('.*-merge')
+        self.waitUntilSettled()
+        self.assertEqual(len(self.builds), 6)
+        self.assertEqual(self.builds[0].name, 'project-test1')
+        self.assertTrue(self.job_has_changes(self.builds[0], A))
+        self.assertEqual(self.builds[1].name, 'project-test2')
+        self.assertTrue(self.job_has_changes(self.builds[1], A))
+
+        self.assertEqual(self.builds[2].name, 'project-test1')
+        self.assertTrue(self.job_has_changes(self.builds[2], A, B))
+        self.assertEqual(self.builds[3].name, 'project-test2')
+        self.assertTrue(self.job_has_changes(self.builds[3], A, B))
+
+        self.assertEqual(self.builds[4].name, 'project-test1')
+        self.assertTrue(self.job_has_changes(self.builds[4], A, B, C))
+        self.assertEqual(self.builds[5].name, 'project-test2')
+        self.assertTrue(self.job_has_changes(self.builds[5], A, B, C))
+
+        self.worker.hold_jobs_in_build = False
+        self.worker.release()
+        self.waitUntilSettled()
+        self.assertEqual(len(self.builds), 0)
+
+        self.assertEqual(len(self.history), 9)
+        self.assertEqual(A.data['status'], 'MERGED')
+        self.assertEqual(B.data['status'], 'MERGED')
+        self.assertEqual(C.data['status'], 'MERGED')
+        self.assertEqual(A.reported, 2)
+        self.assertEqual(B.reported, 2)
+        self.assertEqual(C.reported, 2)
+
+    def test_failed_changes(self):
+        "Test that a change behind a failed change is retested"
+        self.worker.hold_jobs_in_build = True
+
+        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
+        B = self.fake_gerrit.addFakeChange('org/project', 'master', 'B')
+        A.addApproval('CRVW', 2)
+        B.addApproval('CRVW', 2)
+
+        self.worker.addFailTest('project-test1', A)
+
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
+        self.waitUntilSettled()
+
+        self.worker.release('.*-merge')
+        self.waitUntilSettled()
+
+        self.worker.hold_jobs_in_build = False
+        self.worker.release()
+
+        self.waitUntilSettled()
+        # It's certain that the merge job for change 2 will run, but
+        # the test1 and test2 jobs may or may not run.
+        self.assertTrue(len(self.history) > 6)
+        self.assertEqual(A.data['status'], 'NEW')
+        self.assertEqual(B.data['status'], 'MERGED')
+        self.assertEqual(A.reported, 2)
+        self.assertEqual(B.reported, 2)
+
+    def test_independent_queues(self):
+        "Test that changes end up in the right queues"
+
+        self.worker.hold_jobs_in_build = True
+        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
+        B = self.fake_gerrit.addFakeChange('org/project1', 'master', 'B')
+        C = self.fake_gerrit.addFakeChange('org/project2', 'master', 'C')
+        A.addApproval('CRVW', 2)
+        B.addApproval('CRVW', 2)
+        C.addApproval('CRVW', 2)
+
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
+        self.fake_gerrit.addEvent(C.addApproval('APRV', 1))
+
+        self.waitUntilSettled()
+
+        # There should be one merge job at the head of each queue running
+        self.assertEqual(len(self.builds), 2)
+        self.assertEqual(self.builds[0].name, 'project-merge')
+        self.assertTrue(self.job_has_changes(self.builds[0], A))
+        self.assertEqual(self.builds[1].name, 'project1-merge')
+        self.assertTrue(self.job_has_changes(self.builds[1], B))
+
+        # Release the current merge builds
+        self.worker.release('.*-merge')
+        self.waitUntilSettled()
+        # Release the merge job for project2 which is behind project1
+        self.worker.release('.*-merge')
+        self.waitUntilSettled()
+
+        # All the test builds should be running:
+        # project1 (3) + project2 (3) + project (2) = 8
+        self.assertEqual(len(self.builds), 8)
+
+        self.worker.release()
+        self.waitUntilSettled()
+        self.assertEqual(len(self.builds), 0)
+
+        self.assertEqual(len(self.history), 11)
+        self.assertEqual(A.data['status'], 'MERGED')
+        self.assertEqual(B.data['status'], 'MERGED')
+        self.assertEqual(C.data['status'], 'MERGED')
+        self.assertEqual(A.reported, 2)
+        self.assertEqual(B.reported, 2)
+        self.assertEqual(C.reported, 2)
+
+    def test_failed_change_at_head(self):
+        "Test that if a change at the head fails, jobs behind it are canceled"
+
+        self.worker.hold_jobs_in_build = True
+        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
+        B = self.fake_gerrit.addFakeChange('org/project', 'master', 'B')
+        C = self.fake_gerrit.addFakeChange('org/project', 'master', 'C')
+        A.addApproval('CRVW', 2)
+        B.addApproval('CRVW', 2)
+        C.addApproval('CRVW', 2)
+
+        self.worker.addFailTest('project-test1', A)
+
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
+        self.fake_gerrit.addEvent(C.addApproval('APRV', 1))
+
+        self.waitUntilSettled()
+
+        self.assertEqual(len(self.builds), 1)
+        self.assertEqual(self.builds[0].name, 'project-merge')
+        self.assertTrue(self.job_has_changes(self.builds[0], A))
+
+        self.worker.release('.*-merge')
+        self.waitUntilSettled()
+        self.worker.release('.*-merge')
+        self.waitUntilSettled()
+        self.worker.release('.*-merge')
+        self.waitUntilSettled()
+
+        self.assertEqual(len(self.builds), 6)
+        self.assertEqual(self.builds[0].name, 'project-test1')
+        self.assertEqual(self.builds[1].name, 'project-test2')
+        self.assertEqual(self.builds[2].name, 'project-test1')
+        self.assertEqual(self.builds[3].name, 'project-test2')
+        self.assertEqual(self.builds[4].name, 'project-test1')
+        self.assertEqual(self.builds[5].name, 'project-test2')
+
+        self.release(self.builds[0])
+        self.waitUntilSettled()
+
+        # project-test2, project-merge for B
+        self.assertEqual(len(self.builds), 2)
+        self.assertEqual(self.countJobResults(self.history, 'ABORTED'), 4)
+
+        self.worker.hold_jobs_in_build = False
+        self.worker.release()
+        self.waitUntilSettled()
+
+        self.assertEqual(len(self.builds), 0)
+        self.assertEqual(len(self.history), 15)
+        self.assertEqual(A.data['status'], 'NEW')
+        self.assertEqual(B.data['status'], 'MERGED')
+        self.assertEqual(C.data['status'], 'MERGED')
+        self.assertEqual(A.reported, 2)
+        self.assertEqual(B.reported, 2)
+        self.assertEqual(C.reported, 2)
+
+    def test_failed_change_in_middle(self):
+        "Test a failed change in the middle of the queue"
+
+        self.worker.hold_jobs_in_build = True
+        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
+        B = self.fake_gerrit.addFakeChange('org/project', 'master', 'B')
+        C = self.fake_gerrit.addFakeChange('org/project', 'master', 'C')
+        A.addApproval('CRVW', 2)
+        B.addApproval('CRVW', 2)
+        C.addApproval('CRVW', 2)
+
+        self.worker.addFailTest('project-test1', B)
+
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
+        self.fake_gerrit.addEvent(C.addApproval('APRV', 1))
+
+        self.waitUntilSettled()
+
+        self.worker.release('.*-merge')
+        self.waitUntilSettled()
+        self.worker.release('.*-merge')
+        self.waitUntilSettled()
+        self.worker.release('.*-merge')
+        self.waitUntilSettled()
+
+        self.assertEqual(len(self.builds), 6)
+        self.assertEqual(self.builds[0].name, 'project-test1')
+        self.assertEqual(self.builds[1].name, 'project-test2')
+        self.assertEqual(self.builds[2].name, 'project-test1')
+        self.assertEqual(self.builds[3].name, 'project-test2')
+        self.assertEqual(self.builds[4].name, 'project-test1')
+        self.assertEqual(self.builds[5].name, 'project-test2')
+
+        self.release(self.builds[2])
+        self.waitUntilSettled()
+
+        # project-test1 and project-test2 for A
+        # project-test2 for B
+        # project-merge for C (without B)
+        self.assertEqual(len(self.builds), 4)
+        self.assertEqual(self.countJobResults(self.history, 'ABORTED'), 2)
+
+        self.worker.release('.*-merge')
+        self.waitUntilSettled()
+
+        # project-test1 and project-test2 for A
+        # project-test2 for B
+        # project-test1 and project-test2 for C
+        self.assertEqual(len(self.builds), 5)
+
+        items = self.sched.layout.pipelines['gate'].getAllItems()
+        builds = items[0].current_build_set.getBuilds()
+        self.assertEqual(self.countJobResults(builds, 'SUCCESS'), 1)
+        self.assertEqual(self.countJobResults(builds, None), 2)
+        builds = items[1].current_build_set.getBuilds()
+        self.assertEqual(self.countJobResults(builds, 'SUCCESS'), 1)
+        self.assertEqual(self.countJobResults(builds, 'FAILURE'), 1)
+        self.assertEqual(self.countJobResults(builds, None), 1)
+        builds = items[2].current_build_set.getBuilds()
+        self.assertEqual(self.countJobResults(builds, 'SUCCESS'), 1)
+        self.assertEqual(self.countJobResults(builds, None), 2)
+
+        self.worker.hold_jobs_in_build = False
+        self.worker.release()
+        self.waitUntilSettled()
+
+        self.assertEqual(len(self.builds), 0)
+        self.assertEqual(len(self.history), 12)
+        self.assertEqual(A.data['status'], 'MERGED')
+        self.assertEqual(B.data['status'], 'NEW')
+        self.assertEqual(C.data['status'], 'MERGED')
+        self.assertEqual(A.reported, 2)
+        self.assertEqual(B.reported, 2)
+        self.assertEqual(C.reported, 2)
+
+    def test_failed_change_at_head_with_queue(self):
+        "Test that if a change at the head fails, queued jobs are canceled"
+
+        self.gearman_server.hold_jobs_in_queue = True
+        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
+        B = self.fake_gerrit.addFakeChange('org/project', 'master', 'B')
+        C = self.fake_gerrit.addFakeChange('org/project', 'master', 'C')
+        A.addApproval('CRVW', 2)
+        B.addApproval('CRVW', 2)
+        C.addApproval('CRVW', 2)
+
+        self.worker.addFailTest('project-test1', A)
+
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
+        self.fake_gerrit.addEvent(C.addApproval('APRV', 1))
+
+        self.waitUntilSettled()
+        queue = self.gearman_server.getQueue()
+        self.assertEqual(len(self.builds), 0)
+        self.assertEqual(len(queue), 1)
+        self.assertEqual(queue[0].name, 'build:project-merge')
+        self.assertTrue(self.job_has_changes(queue[0], A))
+
+        self.gearman_server.release('.*-merge')
+        self.waitUntilSettled()
+        self.gearman_server.release('.*-merge')
+        self.waitUntilSettled()
+        self.gearman_server.release('.*-merge')
+        self.waitUntilSettled()
+        queue = self.gearman_server.getQueue()
+
+        self.assertEqual(len(self.builds), 0)
+        self.assertEqual(len(queue), 6)
+        self.assertEqual(queue[0].name, 'build:project-test1')
+        self.assertEqual(queue[1].name, 'build:project-test2')
+        self.assertEqual(queue[2].name, 'build:project-test1')
+        self.assertEqual(queue[3].name, 'build:project-test2')
+        self.assertEqual(queue[4].name, 'build:project-test1')
+        self.assertEqual(queue[5].name, 'build:project-test2')
+
+        self.release(queue[0])
+        self.waitUntilSettled()
+
+        self.assertEqual(len(self.builds), 0)
+        queue = self.gearman_server.getQueue()
+        self.assertEqual(len(queue), 2)  # project-test2, project-merge for B
+        self.assertEqual(self.countJobResults(self.history, 'ABORTED'), 0)
+
+        self.gearman_server.hold_jobs_in_queue = False
+        self.gearman_server.release()
+        self.waitUntilSettled()
+
+        self.assertEqual(len(self.builds), 0)
+        self.assertEqual(len(self.history), 11)
+        self.assertEqual(A.data['status'], 'NEW')
+        self.assertEqual(B.data['status'], 'MERGED')
+        self.assertEqual(C.data['status'], 'MERGED')
+        self.assertEqual(A.reported, 2)
+        self.assertEqual(B.reported, 2)
+        self.assertEqual(C.reported, 2)
+
+    def test_two_failed_changes_at_head(self):
+        "Test that changes are reparented correctly if 2 fail at head"
+
+        self.worker.hold_jobs_in_build = True
+        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
+        B = self.fake_gerrit.addFakeChange('org/project', 'master', 'B')
+        C = self.fake_gerrit.addFakeChange('org/project', 'master', 'C')
+        A.addApproval('CRVW', 2)
+        B.addApproval('CRVW', 2)
+        C.addApproval('CRVW', 2)
+
+        self.worker.addFailTest('project-test1', A)
+        self.worker.addFailTest('project-test1', B)
+
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
+        self.fake_gerrit.addEvent(C.addApproval('APRV', 1))
+        self.waitUntilSettled()
+
+        self.worker.release('.*-merge')
+        self.waitUntilSettled()
+        self.worker.release('.*-merge')
+        self.waitUntilSettled()
+        self.worker.release('.*-merge')
+        self.waitUntilSettled()
+
+        self.assertEqual(len(self.builds), 6)
+        self.assertEqual(self.builds[0].name, 'project-test1')
+        self.assertEqual(self.builds[1].name, 'project-test2')
+        self.assertEqual(self.builds[2].name, 'project-test1')
+        self.assertEqual(self.builds[3].name, 'project-test2')
+        self.assertEqual(self.builds[4].name, 'project-test1')
+        self.assertEqual(self.builds[5].name, 'project-test2')
+
+        self.assertTrue(self.job_has_changes(self.builds[0], A))
+        self.assertTrue(self.job_has_changes(self.builds[2], A))
+        self.assertTrue(self.job_has_changes(self.builds[2], B))
+        self.assertTrue(self.job_has_changes(self.builds[4], A))
+        self.assertTrue(self.job_has_changes(self.builds[4], B))
+        self.assertTrue(self.job_has_changes(self.builds[4], C))
+
+        # Fail change B first
+        self.release(self.builds[2])
+        self.waitUntilSettled()
+
+        # restart of C after B failure
+        self.worker.release('.*-merge')
+        self.waitUntilSettled()
+
+        self.assertEqual(len(self.builds), 5)
+        self.assertEqual(self.builds[0].name, 'project-test1')
+        self.assertEqual(self.builds[1].name, 'project-test2')
+        self.assertEqual(self.builds[2].name, 'project-test2')
+        self.assertEqual(self.builds[3].name, 'project-test1')
+        self.assertEqual(self.builds[4].name, 'project-test2')
+
+        self.assertTrue(self.job_has_changes(self.builds[1], A))
+        self.assertTrue(self.job_has_changes(self.builds[2], A))
+        self.assertTrue(self.job_has_changes(self.builds[2], B))
+        self.assertTrue(self.job_has_changes(self.builds[4], A))
+        self.assertFalse(self.job_has_changes(self.builds[4], B))
+        self.assertTrue(self.job_has_changes(self.builds[4], C))
+
+        # Finish running all passing jobs for change A
+        self.release(self.builds[1])
+        self.waitUntilSettled()
+        # Fail and report change A
+        self.release(self.builds[0])
+        self.waitUntilSettled()
+
+        # restart of B,C after A failure
+        self.worker.release('.*-merge')
+        self.waitUntilSettled()
+        self.worker.release('.*-merge')
+        self.waitUntilSettled()
+
+        self.assertEqual(len(self.builds), 4)
+        self.assertEqual(self.builds[0].name, 'project-test1')  # B
+        self.assertEqual(self.builds[1].name, 'project-test2')  # B
+        self.assertEqual(self.builds[2].name, 'project-test1')  # C
+        self.assertEqual(self.builds[3].name, 'project-test2')  # C
+
+        self.assertFalse(self.job_has_changes(self.builds[1], A))
+        self.assertTrue(self.job_has_changes(self.builds[1], B))
+        self.assertFalse(self.job_has_changes(self.builds[1], C))
+
+        self.assertFalse(self.job_has_changes(self.builds[2], A))
+        # After A failed and B and C restarted, B should be back in
+        # C's tests because it has not failed yet.
+        self.assertTrue(self.job_has_changes(self.builds[2], B))
+        self.assertTrue(self.job_has_changes(self.builds[2], C))
+
+        self.worker.hold_jobs_in_build = False
+        self.worker.release()
+        self.waitUntilSettled()
+
+        self.assertEqual(len(self.builds), 0)
+        self.assertEqual(len(self.history), 21)
+        self.assertEqual(A.data['status'], 'NEW')
+        self.assertEqual(B.data['status'], 'NEW')
+        self.assertEqual(C.data['status'], 'MERGED')
+        self.assertEqual(A.reported, 2)
+        self.assertEqual(B.reported, 2)
+        self.assertEqual(C.reported, 2)
+
+    def test_patch_order(self):
+        "Test that dependent patches are tested in the right order"
+        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
+        B = self.fake_gerrit.addFakeChange('org/project', 'master', 'B')
+        C = self.fake_gerrit.addFakeChange('org/project', 'master', 'C')
+        A.addApproval('CRVW', 2)
+        B.addApproval('CRVW', 2)
+        C.addApproval('CRVW', 2)
+
+        M2 = self.fake_gerrit.addFakeChange('org/project', 'master', 'M2')
+        M1 = self.fake_gerrit.addFakeChange('org/project', 'master', 'M1')
+        M2.setMerged()
+        M1.setMerged()
+
+        # C -> B -> A -> M1 -> M2
+        # M2 is here to make sure it is never queried.  If it is, it
+        # means zuul is walking down the entire history of merged
+        # changes.
+
+        C.setDependsOn(B, 1)
+        B.setDependsOn(A, 1)
+        A.setDependsOn(M1, 1)
+        M1.setDependsOn(M2, 1)
+
+        self.fake_gerrit.addEvent(C.addApproval('APRV', 1))
+
+        self.waitUntilSettled()
+
+        self.assertEqual(A.data['status'], 'NEW')
+        self.assertEqual(B.data['status'], 'NEW')
+        self.assertEqual(C.data['status'], 'NEW')
+
+        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+
+        self.waitUntilSettled()
+        self.assertEqual(M2.queried, 0)
+        self.assertEqual(A.data['status'], 'MERGED')
+        self.assertEqual(B.data['status'], 'MERGED')
+        self.assertEqual(C.data['status'], 'MERGED')
+        self.assertEqual(A.reported, 2)
+        self.assertEqual(B.reported, 2)
+        self.assertEqual(C.reported, 2)
+
+    def test_needed_changes_enqueue(self):
+        "Test that a needed change is enqueued ahead"
+        #          A      Given a git tree like this, if we enqueue
+        #         / \     change C, we should walk up and down the tree
+        #        B   G    and enqueue changes in the order ABCDEFG.
+        #       /|\       This is also the order that you would get if
+        #     *C E F      you enqueued changes in the order ABCDEFG, so
+        #     /           the ordering is stable across re-enqueue events.
+        #    D
+
+        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
+        B = self.fake_gerrit.addFakeChange('org/project', 'master', 'B')
+        C = self.fake_gerrit.addFakeChange('org/project', 'master', 'C')
+        D = self.fake_gerrit.addFakeChange('org/project', 'master', 'D')
+        E = self.fake_gerrit.addFakeChange('org/project', 'master', 'E')
+        F = self.fake_gerrit.addFakeChange('org/project', 'master', 'F')
+        G = self.fake_gerrit.addFakeChange('org/project', 'master', 'G')
+        B.setDependsOn(A, 1)
+        C.setDependsOn(B, 1)
+        D.setDependsOn(C, 1)
+        E.setDependsOn(B, 1)
+        F.setDependsOn(B, 1)
+        G.setDependsOn(A, 1)
+
+        A.addApproval('CRVW', 2)
+        B.addApproval('CRVW', 2)
+        C.addApproval('CRVW', 2)
+        D.addApproval('CRVW', 2)
+        E.addApproval('CRVW', 2)
+        F.addApproval('CRVW', 2)
+        G.addApproval('CRVW', 2)
+        self.fake_gerrit.addEvent(C.addApproval('APRV', 1))
+
+        self.waitUntilSettled()
+
+        self.assertEqual(A.data['status'], 'NEW')
+        self.assertEqual(B.data['status'], 'NEW')
+        self.assertEqual(C.data['status'], 'NEW')
+        self.assertEqual(D.data['status'], 'NEW')
+        self.assertEqual(E.data['status'], 'NEW')
+        self.assertEqual(F.data['status'], 'NEW')
+        self.assertEqual(G.data['status'], 'NEW')
+
+        # We're about to add approvals to changes without adding the
+        # triggering events to Zuul, so that we can be sure that it is
+        # enqueing the changes based on dependencies, not because of
+        # triggering events.  Since it will have the changes cached
+        # already (without approvals), we need to clear the cache
+        # first.
+        for connection in self.connections.values():
+            connection.maintainCache([])
+
+        self.worker.hold_jobs_in_build = True
+        A.addApproval('APRV', 1)
+        B.addApproval('APRV', 1)
+        D.addApproval('APRV', 1)
+        E.addApproval('APRV', 1)
+        F.addApproval('APRV', 1)
+        G.addApproval('APRV', 1)
+        self.fake_gerrit.addEvent(C.addApproval('APRV', 1))
+
+        for x in range(8):
+            self.worker.release('.*-merge')
+            self.waitUntilSettled()
+        self.worker.hold_jobs_in_build = False
+        self.worker.release()
+        self.waitUntilSettled()
+
+        self.assertEqual(A.data['status'], 'MERGED')
+        self.assertEqual(B.data['status'], 'MERGED')
+        self.assertEqual(C.data['status'], 'MERGED')
+        self.assertEqual(D.data['status'], 'MERGED')
+        self.assertEqual(E.data['status'], 'MERGED')
+        self.assertEqual(F.data['status'], 'MERGED')
+        self.assertEqual(G.data['status'], 'MERGED')
+        self.assertEqual(A.reported, 2)
+        self.assertEqual(B.reported, 2)
+        self.assertEqual(C.reported, 2)
+        self.assertEqual(D.reported, 2)
+        self.assertEqual(E.reported, 2)
+        self.assertEqual(F.reported, 2)
+        self.assertEqual(G.reported, 2)
+        self.assertEqual(self.history[6].changes,
+                         '1,1 2,1 3,1 4,1 5,1 6,1 7,1')
+
+    def test_source_cache(self):
+        "Test that the source cache operates correctly"
+        self.worker.hold_jobs_in_build = True
+
+        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
+        B = self.fake_gerrit.addFakeChange('org/project', 'master', 'B')
+        X = self.fake_gerrit.addFakeChange('org/project', 'master', 'X')
+        A.addApproval('CRVW', 2)
+        B.addApproval('CRVW', 2)
+
+        M1 = self.fake_gerrit.addFakeChange('org/project', 'master', 'M1')
+        M1.setMerged()
+
+        B.setDependsOn(A, 1)
+        A.setDependsOn(M1, 1)
+
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+        self.fake_gerrit.addEvent(X.getPatchsetCreatedEvent(1))
+
+        self.waitUntilSettled()
+
+        for build in self.builds:
+            if build.parameters['ZUUL_PIPELINE'] == 'check':
+                build.release()
+        self.waitUntilSettled()
+        for build in self.builds:
+            if build.parameters['ZUUL_PIPELINE'] == 'check':
+                build.release()
+        self.waitUntilSettled()
+
+        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
+        self.waitUntilSettled()
+
+        self.log.debug("len %s" % self.fake_gerrit._change_cache.keys())
+        # there should still be changes in the cache
+        self.assertNotEqual(len(self.fake_gerrit._change_cache.keys()), 0)
+
+        self.worker.hold_jobs_in_build = False
+        self.worker.release()
+        self.waitUntilSettled()
+
+        self.assertEqual(A.data['status'], 'MERGED')
+        self.assertEqual(B.data['status'], 'MERGED')
+        self.assertEqual(A.queried, 2)  # Initial and isMerged
+        self.assertEqual(B.queried, 3)  # Initial A, refresh from B, isMerged
+
+    def test_can_merge(self):
+        "Test whether a change is ready to merge"
+        # TODO: move to test_gerrit (this is a unit test!)
+        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
+        source = self.sched.layout.pipelines['gate'].source
+        a = source._getChange(1, 2)
+        mgr = self.sched.layout.pipelines['gate'].manager
+        self.assertFalse(source.canMerge(a, mgr.getSubmitAllowNeeds()))
+
+        A.addApproval('CRVW', 2)
+        a = source._getChange(1, 2, refresh=True)
+        self.assertFalse(source.canMerge(a, mgr.getSubmitAllowNeeds()))
+
+        A.addApproval('APRV', 1)
+        a = source._getChange(1, 2, refresh=True)
+        self.assertTrue(source.canMerge(a, mgr.getSubmitAllowNeeds()))
+
+    def test_build_configuration(self):
+        "Test that zuul merges the right commits for testing"
+
+        self.gearman_server.hold_jobs_in_queue = True
+        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
+        B = self.fake_gerrit.addFakeChange('org/project', 'master', 'B')
+        C = self.fake_gerrit.addFakeChange('org/project', 'master', 'C')
+        A.addApproval('CRVW', 2)
+        B.addApproval('CRVW', 2)
+        C.addApproval('CRVW', 2)
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
+        self.fake_gerrit.addEvent(C.addApproval('APRV', 1))
+        self.waitUntilSettled()
+
+        self.gearman_server.release('.*-merge')
+        self.waitUntilSettled()
+        self.gearman_server.release('.*-merge')
+        self.waitUntilSettled()
+        self.gearman_server.release('.*-merge')
+        self.waitUntilSettled()
+        queue = self.gearman_server.getQueue()
+        ref = self.getParameter(queue[-1], 'ZUUL_REF')
+        self.gearman_server.hold_jobs_in_queue = False
+        self.gearman_server.release()
+        self.waitUntilSettled()
+
+        path = os.path.join(self.git_root, "org/project")
+        repo = git.Repo(path)
+        repo_messages = [c.message.strip() for c in repo.iter_commits(ref)]
+        repo_messages.reverse()
+        correct_messages = ['initial commit', 'A-1', 'B-1', 'C-1']
+        self.assertEqual(repo_messages, correct_messages)
+
+    def test_build_configuration_conflict(self):
+        "Test that merge conflicts are handled"
+
+        self.gearman_server.hold_jobs_in_queue = True
+        A = self.fake_gerrit.addFakeChange('org/conflict-project',
+                                           'master', 'A')
+        A.addPatchset(['conflict'])
+        B = self.fake_gerrit.addFakeChange('org/conflict-project',
+                                           'master', 'B')
+        B.addPatchset(['conflict'])
+        C = self.fake_gerrit.addFakeChange('org/conflict-project',
+                                           'master', 'C')
+        A.addApproval('CRVW', 2)
+        B.addApproval('CRVW', 2)
+        C.addApproval('CRVW', 2)
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
+        self.fake_gerrit.addEvent(C.addApproval('APRV', 1))
+        self.waitUntilSettled()
+
+        self.assertEqual(A.reported, 1)
+        self.assertEqual(B.reported, 1)
+        self.assertEqual(C.reported, 1)
+
+        self.gearman_server.release('.*-merge')
+        self.waitUntilSettled()
+        self.gearman_server.release('.*-merge')
+        self.waitUntilSettled()
+        self.gearman_server.release('.*-merge')
+        self.waitUntilSettled()
+
+        self.assertEqual(len(self.history), 2)  # A and C merge jobs
+
+        self.gearman_server.hold_jobs_in_queue = False
+        self.gearman_server.release()
+        self.waitUntilSettled()
+
+        self.assertEqual(A.data['status'], 'MERGED')
+        self.assertEqual(B.data['status'], 'NEW')
+        self.assertEqual(C.data['status'], 'MERGED')
+        self.assertEqual(A.reported, 2)
+        self.assertEqual(B.reported, 2)
+        self.assertEqual(C.reported, 2)
+        self.assertEqual(len(self.history), 6)
+
+    def test_post(self):
+        "Test that post jobs run"
+
+        e = {
+            "type": "ref-updated",
+            "submitter": {
+                "name": "User Name",
+            },
+            "refUpdate": {
+                "oldRev": "90f173846e3af9154517b88543ffbd1691f31366",
+                "newRev": "d479a0bfcb34da57a31adb2a595c0cf687812543",
+                "refName": "master",
+                "project": "org/project",
+            }
+        }
+        self.fake_gerrit.addEvent(e)
+        self.waitUntilSettled()
+
+        job_names = [x.name for x in self.history]
+        self.assertEqual(len(self.history), 1)
+        self.assertIn('project-post', job_names)
+
+    def test_post_ignore_deletes(self):
+        "Test that deleting refs does not trigger post jobs"
+
+        e = {
+            "type": "ref-updated",
+            "submitter": {
+                "name": "User Name",
+            },
+            "refUpdate": {
+                "oldRev": "90f173846e3af9154517b88543ffbd1691f31366",
+                "newRev": "0000000000000000000000000000000000000000",
+                "refName": "master",
+                "project": "org/project",
+            }
+        }
+        self.fake_gerrit.addEvent(e)
+        self.waitUntilSettled()
+
+        job_names = [x.name for x in self.history]
+        self.assertEqual(len(self.history), 0)
+        self.assertNotIn('project-post', job_names)
+
+    def test_post_ignore_deletes_negative(self):
+        "Test that deleting refs does trigger post jobs"
+
+        self.config.set('zuul', 'layout_config',
+                        'tests/fixtures/layout-dont-ignore-deletes.yaml')
+        self.sched.reconfigure(self.config)
+
+        e = {
+            "type": "ref-updated",
+            "submitter": {
+                "name": "User Name",
+            },
+            "refUpdate": {
+                "oldRev": "90f173846e3af9154517b88543ffbd1691f31366",
+                "newRev": "0000000000000000000000000000000000000000",
+                "refName": "master",
+                "project": "org/project",
+            }
+        }
+        self.fake_gerrit.addEvent(e)
+        self.waitUntilSettled()
+
+        job_names = [x.name for x in self.history]
+        self.assertEqual(len(self.history), 1)
+        self.assertIn('project-post', job_names)
+
+    def test_build_configuration_branch(self):
+        "Test that the right commits are on alternate branches"
+
+        self.gearman_server.hold_jobs_in_queue = True
+        A = self.fake_gerrit.addFakeChange('org/project', 'mp', 'A')
+        B = self.fake_gerrit.addFakeChange('org/project', 'mp', 'B')
+        C = self.fake_gerrit.addFakeChange('org/project', 'mp', 'C')
+        A.addApproval('CRVW', 2)
+        B.addApproval('CRVW', 2)
+        C.addApproval('CRVW', 2)
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
+        self.fake_gerrit.addEvent(C.addApproval('APRV', 1))
+        self.waitUntilSettled()
+
+        self.gearman_server.release('.*-merge')
+        self.waitUntilSettled()
+        self.gearman_server.release('.*-merge')
+        self.waitUntilSettled()
+        self.gearman_server.release('.*-merge')
+        self.waitUntilSettled()
+        queue = self.gearman_server.getQueue()
+        ref = self.getParameter(queue[-1], 'ZUUL_REF')
+        self.gearman_server.hold_jobs_in_queue = False
+        self.gearman_server.release()
+        self.waitUntilSettled()
+
+        path = os.path.join(self.git_root, "org/project")
+        repo = git.Repo(path)
+        repo_messages = [c.message.strip() for c in repo.iter_commits(ref)]
+        repo_messages.reverse()
+        correct_messages = ['initial commit', 'mp commit', 'A-1', 'B-1', 'C-1']
+        self.assertEqual(repo_messages, correct_messages)
+
+    def test_build_configuration_branch_interaction(self):
+        "Test that switching between branches works"
+        self.test_build_configuration()
+        self.test_build_configuration_branch()
+        # C has been merged, undo that
+        path = os.path.join(self.upstream_root, "org/project")
+        repo = git.Repo(path)
+        repo.heads.master.commit = repo.commit('init')
+        self.test_build_configuration()
+
+    def test_build_configuration_multi_branch(self):
+        "Test that dependent changes on multiple branches are merged"
+
+        self.gearman_server.hold_jobs_in_queue = True
+        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
+        B = self.fake_gerrit.addFakeChange('org/project', 'mp', 'B')
+        C = self.fake_gerrit.addFakeChange('org/project', 'master', 'C')
+        A.addApproval('CRVW', 2)
+        B.addApproval('CRVW', 2)
+        C.addApproval('CRVW', 2)
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
+        self.fake_gerrit.addEvent(C.addApproval('APRV', 1))
+        self.waitUntilSettled()
+        queue = self.gearman_server.getQueue()
+        job_A = None
+        for job in queue:
+            if 'project-merge' in job.name:
+                job_A = job
+        ref_A = self.getParameter(job_A, 'ZUUL_REF')
+        commit_A = self.getParameter(job_A, 'ZUUL_COMMIT')
+        self.log.debug("Got Zuul ref for change A: %s" % ref_A)
+        self.log.debug("Got Zuul commit for change A: %s" % commit_A)
+
+        self.gearman_server.release('.*-merge')
+        self.waitUntilSettled()
+        queue = self.gearman_server.getQueue()
+        job_B = None
+        for job in queue:
+            if 'project-merge' in job.name:
+                job_B = job
+        ref_B = self.getParameter(job_B, 'ZUUL_REF')
+        commit_B = self.getParameter(job_B, 'ZUUL_COMMIT')
+        self.log.debug("Got Zuul ref for change B: %s" % ref_B)
+        self.log.debug("Got Zuul commit for change B: %s" % commit_B)
+
+        self.gearman_server.release('.*-merge')
+        self.waitUntilSettled()
+        queue = self.gearman_server.getQueue()
+        for job in queue:
+            if 'project-merge' in job.name:
+                job_C = job
+        ref_C = self.getParameter(job_C, 'ZUUL_REF')
+        commit_C = self.getParameter(job_C, 'ZUUL_COMMIT')
+        self.log.debug("Got Zuul ref for change C: %s" % ref_C)
+        self.log.debug("Got Zuul commit for change C: %s" % commit_C)
+        self.gearman_server.hold_jobs_in_queue = False
+        self.gearman_server.release()
+        self.waitUntilSettled()
+
+        path = os.path.join(self.git_root, "org/project")
+        repo = git.Repo(path)
+
+        repo_messages = [c.message.strip()
+                         for c in repo.iter_commits(ref_C)]
+        repo_shas = [c.hexsha for c in repo.iter_commits(ref_C)]
+        repo_messages.reverse()
+        correct_messages = ['initial commit', 'A-1', 'C-1']
+        # Ensure the right commits are in the history for this ref
+        self.assertEqual(repo_messages, correct_messages)
+        # Ensure ZUUL_REF -> ZUUL_COMMIT
+        self.assertEqual(repo_shas[0], commit_C)
+
+        repo_messages = [c.message.strip()
+                         for c in repo.iter_commits(ref_B)]
+        repo_shas = [c.hexsha for c in repo.iter_commits(ref_B)]
+        repo_messages.reverse()
+        correct_messages = ['initial commit', 'mp commit', 'B-1']
+        self.assertEqual(repo_messages, correct_messages)
+        self.assertEqual(repo_shas[0], commit_B)
+
+        repo_messages = [c.message.strip()
+                         for c in repo.iter_commits(ref_A)]
+        repo_shas = [c.hexsha for c in repo.iter_commits(ref_A)]
+        repo_messages.reverse()
+        correct_messages = ['initial commit', 'A-1']
+        self.assertEqual(repo_messages, correct_messages)
+        self.assertEqual(repo_shas[0], commit_A)
+
+        self.assertNotEqual(ref_A, ref_B, ref_C)
+        self.assertNotEqual(commit_A, commit_B, commit_C)
+
+    def test_one_job_project(self):
+        "Test that queueing works with one job"
+        A = self.fake_gerrit.addFakeChange('org/one-job-project',
+                                           'master', 'A')
+        B = self.fake_gerrit.addFakeChange('org/one-job-project',
+                                           'master', 'B')
+        A.addApproval('CRVW', 2)
+        B.addApproval('CRVW', 2)
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
+        self.waitUntilSettled()
+
+        self.assertEqual(A.data['status'], 'MERGED')
+        self.assertEqual(A.reported, 2)
+        self.assertEqual(B.data['status'], 'MERGED')
+        self.assertEqual(B.reported, 2)
+
+    def test_job_from_templates_launched(self):
+        "Test whether a job generated via a template can be launched"
+
+        A = self.fake_gerrit.addFakeChange(
+            'org/templated-project', 'master', 'A')
+        self.fake_gerrit.addEvent(A.getPatchsetCreatedEvent(1))
+        self.waitUntilSettled()
+
+        self.assertEqual(self.getJobFromHistory('project-test1').result,
+                         'SUCCESS')
+        self.assertEqual(self.getJobFromHistory('project-test2').result,
+                         'SUCCESS')
+
+    def test_layered_templates(self):
+        "Test whether a job generated via a template can be launched"
+
+        A = self.fake_gerrit.addFakeChange(
+            'org/layered-project', 'master', 'A')
+        self.fake_gerrit.addEvent(A.getPatchsetCreatedEvent(1))
+        self.waitUntilSettled()
+
+        self.assertEqual(self.getJobFromHistory('project-test1').result,
+                         'SUCCESS')
+        self.assertEqual(self.getJobFromHistory('project-test2').result,
+                         'SUCCESS')
+        self.assertEqual(self.getJobFromHistory('layered-project-test3'
+                                                ).result, 'SUCCESS')
+        self.assertEqual(self.getJobFromHistory('layered-project-test4'
+                                                ).result, 'SUCCESS')
+        self.assertEqual(self.getJobFromHistory('layered-project-foo-test5'
+                                                ).result, 'SUCCESS')
+        self.assertEqual(self.getJobFromHistory('project-test6').result,
+                         'SUCCESS')
+
+    def test_dependent_changes_dequeue(self):
+        "Test that dependent patches are not needlessly tested"
+
+        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
+        B = self.fake_gerrit.addFakeChange('org/project', 'master', 'B')
+        C = self.fake_gerrit.addFakeChange('org/project', 'master', 'C')
+        A.addApproval('CRVW', 2)
+        B.addApproval('CRVW', 2)
+        C.addApproval('CRVW', 2)
+
+        M1 = self.fake_gerrit.addFakeChange('org/project', 'master', 'M1')
+        M1.setMerged()
+
+        # C -> B -> A -> M1
+
+        C.setDependsOn(B, 1)
+        B.setDependsOn(A, 1)
+        A.setDependsOn(M1, 1)
+
+        self.worker.addFailTest('project-merge', A)
+
+        self.fake_gerrit.addEvent(C.addApproval('APRV', 1))
+        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+
+        self.waitUntilSettled()
+
+        self.assertEqual(A.data['status'], 'NEW')
+        self.assertEqual(A.reported, 2)
+        self.assertEqual(B.data['status'], 'NEW')
+        self.assertEqual(B.reported, 2)
+        self.assertEqual(C.data['status'], 'NEW')
+        self.assertEqual(C.reported, 2)
+        self.assertEqual(len(self.history), 1)
+
+    def test_failing_dependent_changes(self):
+        "Test that failing dependent patches are taken out of stream"
+        self.worker.hold_jobs_in_build = True
+        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
+        B = self.fake_gerrit.addFakeChange('org/project', 'master', 'B')
+        C = self.fake_gerrit.addFakeChange('org/project', 'master', 'C')
+        D = self.fake_gerrit.addFakeChange('org/project', 'master', 'D')
+        E = self.fake_gerrit.addFakeChange('org/project', 'master', 'E')
+        A.addApproval('CRVW', 2)
+        B.addApproval('CRVW', 2)
+        C.addApproval('CRVW', 2)
+        D.addApproval('CRVW', 2)
+        E.addApproval('CRVW', 2)
+
+        # E, D -> C -> B, A
+
+        D.setDependsOn(C, 1)
+        C.setDependsOn(B, 1)
+
+        self.worker.addFailTest('project-test1', B)
+
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+        self.fake_gerrit.addEvent(D.addApproval('APRV', 1))
+        self.fake_gerrit.addEvent(C.addApproval('APRV', 1))
+        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
+        self.fake_gerrit.addEvent(E.addApproval('APRV', 1))
+
+        self.waitUntilSettled()
+        self.worker.release('.*-merge')
+        self.waitUntilSettled()
+        self.worker.release('.*-merge')
+        self.waitUntilSettled()
+        self.worker.release('.*-merge')
+        self.waitUntilSettled()
+        self.worker.release('.*-merge')
+        self.waitUntilSettled()
+        self.worker.release('.*-merge')
+        self.waitUntilSettled()
+
+        self.worker.hold_jobs_in_build = False
+        for build in self.builds:
+            if build.parameters['ZUUL_CHANGE'] != '1':
+                build.release()
+                self.waitUntilSettled()
+
+        self.worker.release()
+        self.waitUntilSettled()
+
+        self.assertEqual(A.data['status'], 'MERGED')
+        self.assertEqual(A.reported, 2)
+        self.assertIn('Build succeeded', A.messages[1])
+        self.assertEqual(B.data['status'], 'NEW')
+        self.assertEqual(B.reported, 2)
+        self.assertIn('Build failed', B.messages[1])
+        self.assertEqual(C.data['status'], 'NEW')
+        self.assertEqual(C.reported, 2)
+        self.assertIn('depends on a change', C.messages[1])
+        self.assertEqual(D.data['status'], 'NEW')
+        self.assertEqual(D.reported, 2)
+        self.assertIn('depends on a change', D.messages[1])
+        self.assertEqual(E.data['status'], 'MERGED')
+        self.assertEqual(E.reported, 2)
+        self.assertIn('Build succeeded', E.messages[1])
+        self.assertEqual(len(self.history), 18)
+
+    def test_head_is_dequeued_once(self):
+        "Test that if a change at the head fails it is dequeued only once"
+        # If it's dequeued more than once, we should see extra
+        # aborted jobs.
+
+        self.worker.hold_jobs_in_build = True
+        A = self.fake_gerrit.addFakeChange('org/project1', 'master', 'A')
+        B = self.fake_gerrit.addFakeChange('org/project1', 'master', 'B')
+        C = self.fake_gerrit.addFakeChange('org/project1', 'master', 'C')
+        A.addApproval('CRVW', 2)
+        B.addApproval('CRVW', 2)
+        C.addApproval('CRVW', 2)
+
+        self.worker.addFailTest('project1-test1', A)
+        self.worker.addFailTest('project1-test2', A)
+        self.worker.addFailTest('project1-project2-integration', A)
+
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
+        self.fake_gerrit.addEvent(C.addApproval('APRV', 1))
+
+        self.waitUntilSettled()
+
+        self.assertEqual(len(self.builds), 1)
+        self.assertEqual(self.builds[0].name, 'project1-merge')
+        self.assertTrue(self.job_has_changes(self.builds[0], A))
+
+        self.worker.release('.*-merge')
+        self.waitUntilSettled()
+        self.worker.release('.*-merge')
+        self.waitUntilSettled()
+        self.worker.release('.*-merge')
+        self.waitUntilSettled()
+
+        self.assertEqual(len(self.builds), 9)
+        self.assertEqual(self.builds[0].name, 'project1-test1')
+        self.assertEqual(self.builds[1].name, 'project1-test2')
+        self.assertEqual(self.builds[2].name, 'project1-project2-integration')
+        self.assertEqual(self.builds[3].name, 'project1-test1')
+        self.assertEqual(self.builds[4].name, 'project1-test2')
+        self.assertEqual(self.builds[5].name, 'project1-project2-integration')
+        self.assertEqual(self.builds[6].name, 'project1-test1')
+        self.assertEqual(self.builds[7].name, 'project1-test2')
+        self.assertEqual(self.builds[8].name, 'project1-project2-integration')
+
+        self.release(self.builds[0])
+        self.waitUntilSettled()
+
+        self.assertEqual(len(self.builds), 3)  # test2,integration, merge for B
+        self.assertEqual(self.countJobResults(self.history, 'ABORTED'), 6)
+
+        self.worker.hold_jobs_in_build = False
+        self.worker.release()
+        self.waitUntilSettled()
+
+        self.assertEqual(len(self.builds), 0)
+        self.assertEqual(len(self.history), 20)
+
+        self.assertEqual(A.data['status'], 'NEW')
+        self.assertEqual(B.data['status'], 'MERGED')
+        self.assertEqual(C.data['status'], 'MERGED')
+        self.assertEqual(A.reported, 2)
+        self.assertEqual(B.reported, 2)
+        self.assertEqual(C.reported, 2)
+
+    def test_nonvoting_job(self):
+        "Test that non-voting jobs don't vote."
+
+        A = self.fake_gerrit.addFakeChange('org/nonvoting-project',
+                                           'master', 'A')
+        A.addApproval('CRVW', 2)
+        self.worker.addFailTest('nonvoting-project-test2', A)
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+
+        self.waitUntilSettled()
+
+        self.assertEqual(A.data['status'], 'MERGED')
+        self.assertEqual(A.reported, 2)
+        self.assertEqual(
+            self.getJobFromHistory('nonvoting-project-merge').result,
+            'SUCCESS')
+        self.assertEqual(
+            self.getJobFromHistory('nonvoting-project-test1').result,
+            'SUCCESS')
+        self.assertEqual(
+            self.getJobFromHistory('nonvoting-project-test2').result,
+            'FAILURE')
+
+        for build in self.builds:
+            self.assertEqual(build.parameters['ZUUL_VOTING'], '0')
+
+    def test_check_queue_success(self):
+        "Test successful check queue jobs."
+
+        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
+        self.fake_gerrit.addEvent(A.getPatchsetCreatedEvent(1))
+
+        self.waitUntilSettled()
+
+        self.assertEqual(A.data['status'], 'NEW')
+        self.assertEqual(A.reported, 1)
+        self.assertEqual(self.getJobFromHistory('project-merge').result,
+                         'SUCCESS')
+        self.assertEqual(self.getJobFromHistory('project-test1').result,
+                         'SUCCESS')
+        self.assertEqual(self.getJobFromHistory('project-test2').result,
+                         'SUCCESS')
+
+    def test_check_queue_failure(self):
+        "Test failed check queue jobs."
+
+        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
+        self.worker.addFailTest('project-test2', A)
+        self.fake_gerrit.addEvent(A.getPatchsetCreatedEvent(1))
+
+        self.waitUntilSettled()
+
+        self.assertEqual(A.data['status'], 'NEW')
+        self.assertEqual(A.reported, 1)
+        self.assertEqual(self.getJobFromHistory('project-merge').result,
+                         'SUCCESS')
+        self.assertEqual(self.getJobFromHistory('project-test1').result,
+                         'SUCCESS')
+        self.assertEqual(self.getJobFromHistory('project-test2').result,
+                         'FAILURE')
+
+    def test_dependent_behind_dequeue(self):
+        "test that dependent changes behind dequeued changes work"
+        # This complicated test is a reproduction of a real life bug
+        self.sched.reconfigure(self.config)
+
+        self.worker.hold_jobs_in_build = True
+        A = self.fake_gerrit.addFakeChange('org/project1', 'master', 'A')
+        B = self.fake_gerrit.addFakeChange('org/project1', 'master', 'B')
+        C = self.fake_gerrit.addFakeChange('org/project2', 'master', 'C')
+        D = self.fake_gerrit.addFakeChange('org/project2', 'master', 'D')
+        E = self.fake_gerrit.addFakeChange('org/project2', 'master', 'E')
+        F = self.fake_gerrit.addFakeChange('org/project3', 'master', 'F')
+        D.setDependsOn(C, 1)
+        E.setDependsOn(D, 1)
+        A.addApproval('CRVW', 2)
+        B.addApproval('CRVW', 2)
+        C.addApproval('CRVW', 2)
+        D.addApproval('CRVW', 2)
+        E.addApproval('CRVW', 2)
+        F.addApproval('CRVW', 2)
+
+        A.fail_merge = True
+
+        # Change object re-use in the gerrit trigger is hidden if
+        # changes are added in quick succession; waiting makes it more
+        # like real life.
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+        self.waitUntilSettled()
+        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
+        self.waitUntilSettled()
+
+        self.worker.release('.*-merge')
+        self.waitUntilSettled()
+        self.worker.release('.*-merge')
+        self.waitUntilSettled()
+
+        self.fake_gerrit.addEvent(C.addApproval('APRV', 1))
+        self.waitUntilSettled()
+        self.fake_gerrit.addEvent(D.addApproval('APRV', 1))
+        self.waitUntilSettled()
+        self.fake_gerrit.addEvent(E.addApproval('APRV', 1))
+        self.waitUntilSettled()
+        self.fake_gerrit.addEvent(F.addApproval('APRV', 1))
+        self.waitUntilSettled()
+
+        self.worker.release('.*-merge')
+        self.waitUntilSettled()
+        self.worker.release('.*-merge')
+        self.waitUntilSettled()
+        self.worker.release('.*-merge')
+        self.waitUntilSettled()
+        self.worker.release('.*-merge')
+        self.waitUntilSettled()
+
+        # all jobs running
+
+        # Grab pointers to the jobs we want to release before
+        # releasing any, because list indexes may change as
+        # the jobs complete.
+        a, b, c = self.builds[:3]
+        a.release()
+        b.release()
+        c.release()
+        self.waitUntilSettled()
+
+        self.worker.hold_jobs_in_build = False
+        self.worker.release()
+        self.waitUntilSettled()
+
+        self.assertEqual(A.data['status'], 'NEW')
+        self.assertEqual(B.data['status'], 'MERGED')
+        self.assertEqual(C.data['status'], 'MERGED')
+        self.assertEqual(D.data['status'], 'MERGED')
+        self.assertEqual(E.data['status'], 'MERGED')
+        self.assertEqual(F.data['status'], 'MERGED')
+
+        self.assertEqual(A.reported, 2)
+        self.assertEqual(B.reported, 2)
+        self.assertEqual(C.reported, 2)
+        self.assertEqual(D.reported, 2)
+        self.assertEqual(E.reported, 2)
+        self.assertEqual(F.reported, 2)
+
+        self.assertEqual(self.countJobResults(self.history, 'ABORTED'), 15)
+        self.assertEqual(len(self.history), 44)
+
+    def test_merger_repack(self):
+        "Test that the merger works after a repack"
+
+        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
+        A.addApproval('CRVW', 2)
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+        self.waitUntilSettled()
+        self.assertEqual(self.getJobFromHistory('project-merge').result,
+                         'SUCCESS')
+        self.assertEqual(self.getJobFromHistory('project-test1').result,
+                         'SUCCESS')
+        self.assertEqual(self.getJobFromHistory('project-test2').result,
+                         'SUCCESS')
+        self.assertEqual(A.data['status'], 'MERGED')
+        self.assertEqual(A.reported, 2)
+        self.assertEmptyQueues()
+        self.worker.build_history = []
+
+        path = os.path.join(self.git_root, "org/project")
+        print repack_repo(path)
+
+        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
+        A.addApproval('CRVW', 2)
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+        self.waitUntilSettled()
+        self.assertEqual(self.getJobFromHistory('project-merge').result,
+                         'SUCCESS')
+        self.assertEqual(self.getJobFromHistory('project-test1').result,
+                         'SUCCESS')
+        self.assertEqual(self.getJobFromHistory('project-test2').result,
+                         'SUCCESS')
+        self.assertEqual(A.data['status'], 'MERGED')
+        self.assertEqual(A.reported, 2)
+
+    def test_merger_repack_large_change(self):
+        "Test that the merger works with large changes after a repack"
+        # https://bugs.launchpad.net/zuul/+bug/1078946
+        # This test assumes the repo is already cloned; make sure it is
+        url = self.fake_gerrit.getGitUrl(
+            self.sched.layout.projects['org/project1'])
+        self.merge_server.merger.addProject('org/project1', url)
+        A = self.fake_gerrit.addFakeChange('org/project1', 'master', 'A')
+        A.addPatchset(large=True)
+        path = os.path.join(self.upstream_root, "org/project1")
+        print repack_repo(path)
+        path = os.path.join(self.git_root, "org/project1")
+        print repack_repo(path)
+
+        A.addApproval('CRVW', 2)
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+        self.waitUntilSettled()
+        self.assertEqual(self.getJobFromHistory('project1-merge').result,
+                         'SUCCESS')
+        self.assertEqual(self.getJobFromHistory('project1-test1').result,
+                         'SUCCESS')
+        self.assertEqual(self.getJobFromHistory('project1-test2').result,
+                         'SUCCESS')
+        self.assertEqual(A.data['status'], 'MERGED')
+        self.assertEqual(A.reported, 2)
+
+    def test_nonexistent_job(self):
+        "Test launching a job that doesn't exist"
+        # Set to the state immediately after a restart
+        self.resetGearmanServer()
+        self.launcher.negative_function_cache_ttl = 0
+
+        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
+        A.addApproval('CRVW', 2)
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+        # There may be a thread about to report a lost change
+        while A.reported < 2:
+            self.waitUntilSettled()
+        job_names = [x.name for x in self.history]
+        self.assertFalse(job_names)
+        self.assertEqual(A.data['status'], 'NEW')
+        self.assertEqual(A.reported, 2)
+        self.assertEmptyQueues()
+
+        # Make sure things still work:
+        self.registerJobs()
+        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
+        A.addApproval('CRVW', 2)
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+        self.waitUntilSettled()
+        self.assertEqual(self.getJobFromHistory('project-merge').result,
+                         'SUCCESS')
+        self.assertEqual(self.getJobFromHistory('project-test1').result,
+                         'SUCCESS')
+        self.assertEqual(self.getJobFromHistory('project-test2').result,
+                         'SUCCESS')
+        self.assertEqual(A.data['status'], 'MERGED')
+        self.assertEqual(A.reported, 2)
+
+    def test_single_nonexistent_post_job(self):
+        "Test launching a single post job that doesn't exist"
+        e = {
+            "type": "ref-updated",
+            "submitter": {
+                "name": "User Name",
+            },
+            "refUpdate": {
+                "oldRev": "90f173846e3af9154517b88543ffbd1691f31366",
+                "newRev": "d479a0bfcb34da57a31adb2a595c0cf687812543",
+                "refName": "master",
+                "project": "org/project",
+            }
+        }
+        # Set to the state immediately after a restart
+        self.resetGearmanServer()
+        self.launcher.negative_function_cache_ttl = 0
+
+        self.fake_gerrit.addEvent(e)
+        self.waitUntilSettled()
+
+        self.assertEqual(len(self.history), 0)
+
+    def test_new_patchset_dequeues_old(self):
+        "Test that a new patchset causes the old to be dequeued"
+        # D -> C (depends on B) -> B (depends on A) -> A -> M
+        self.worker.hold_jobs_in_build = True
+        M = self.fake_gerrit.addFakeChange('org/project', 'master', 'M')
+        M.setMerged()
+
+        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
+        B = self.fake_gerrit.addFakeChange('org/project', 'master', 'B')
+        C = self.fake_gerrit.addFakeChange('org/project', 'master', 'C')
+        D = self.fake_gerrit.addFakeChange('org/project', 'master', 'D')
+        A.addApproval('CRVW', 2)
+        B.addApproval('CRVW', 2)
+        C.addApproval('CRVW', 2)
+        D.addApproval('CRVW', 2)
+
+        C.setDependsOn(B, 1)
+        B.setDependsOn(A, 1)
+        A.setDependsOn(M, 1)
+
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
+        self.fake_gerrit.addEvent(C.addApproval('APRV', 1))
+        self.fake_gerrit.addEvent(D.addApproval('APRV', 1))
+        self.waitUntilSettled()
+
+        B.addPatchset()
+        self.fake_gerrit.addEvent(B.getPatchsetCreatedEvent(2))
+        self.waitUntilSettled()
+
+        self.worker.hold_jobs_in_build = False
+        self.worker.release()
+        self.waitUntilSettled()
+
+        self.assertEqual(A.data['status'], 'MERGED')
+        self.assertEqual(A.reported, 2)
+        self.assertEqual(B.data['status'], 'NEW')
+        self.assertEqual(B.reported, 2)
+        self.assertEqual(C.data['status'], 'NEW')
+        self.assertEqual(C.reported, 2)
+        self.assertEqual(D.data['status'], 'MERGED')
+        self.assertEqual(D.reported, 2)
+        self.assertEqual(len(self.history), 9)  # 3 each for A, B, D.
+
+    def test_new_patchset_check(self):
+        "Test a new patchset in check"
+
+        self.worker.hold_jobs_in_build = True
+
+        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
+        B = self.fake_gerrit.addFakeChange('org/project', 'master', 'B')
+        check_pipeline = self.sched.layout.pipelines['check']
+
+        # Add two git-dependent changes
+        B.setDependsOn(A, 1)
+        self.fake_gerrit.addEvent(B.getPatchsetCreatedEvent(1))
+        self.waitUntilSettled()
+        self.fake_gerrit.addEvent(A.getPatchsetCreatedEvent(1))
+        self.waitUntilSettled()
+
+        # A live item, and a non-live/live pair
+        items = check_pipeline.getAllItems()
+        self.assertEqual(len(items), 3)
+
+        self.assertEqual(items[0].change.number, '1')
+        self.assertEqual(items[0].change.patchset, '1')
+        self.assertFalse(items[0].live)
+
+        self.assertEqual(items[1].change.number, '2')
+        self.assertEqual(items[1].change.patchset, '1')
+        self.assertTrue(items[1].live)
+
+        self.assertEqual(items[2].change.number, '1')
+        self.assertEqual(items[2].change.patchset, '1')
+        self.assertTrue(items[2].live)
+
+        # Add a new patchset to A
+        A.addPatchset()
+        self.fake_gerrit.addEvent(A.getPatchsetCreatedEvent(2))
+        self.waitUntilSettled()
+
+        # The live copy of A,1 should be gone, but the non-live and B
+        # should continue, and we should have a new A,2
+        items = check_pipeline.getAllItems()
+        self.assertEqual(len(items), 3)
+
+        self.assertEqual(items[0].change.number, '1')
+        self.assertEqual(items[0].change.patchset, '1')
+        self.assertFalse(items[0].live)
+
+        self.assertEqual(items[1].change.number, '2')
+        self.assertEqual(items[1].change.patchset, '1')
+        self.assertTrue(items[1].live)
+
+        self.assertEqual(items[2].change.number, '1')
+        self.assertEqual(items[2].change.patchset, '2')
+        self.assertTrue(items[2].live)
+
+        # Add a new patchset to B
+        B.addPatchset()
+        self.fake_gerrit.addEvent(B.getPatchsetCreatedEvent(2))
+        self.waitUntilSettled()
+
+        # The live copy of B,1 should be gone, and it's non-live copy of A,1
+        # but we should have a new B,2 (still based on A,1)
+        items = check_pipeline.getAllItems()
+        self.assertEqual(len(items), 3)
+
+        self.assertEqual(items[0].change.number, '1')
+        self.assertEqual(items[0].change.patchset, '2')
+        self.assertTrue(items[0].live)
+
+        self.assertEqual(items[1].change.number, '1')
+        self.assertEqual(items[1].change.patchset, '1')
+        self.assertFalse(items[1].live)
+
+        self.assertEqual(items[2].change.number, '2')
+        self.assertEqual(items[2].change.patchset, '2')
+        self.assertTrue(items[2].live)
+
+        self.builds[0].release()
+        self.waitUntilSettled()
+        self.builds[0].release()
+        self.waitUntilSettled()
+        self.worker.hold_jobs_in_build = False
+        self.worker.release()
+        self.waitUntilSettled()
+
+        self.assertEqual(A.reported, 1)
+        self.assertEqual(B.reported, 1)
+        self.assertEqual(self.history[0].result, 'ABORTED')
+        self.assertEqual(self.history[0].changes, '1,1')
+        self.assertEqual(self.history[1].result, 'ABORTED')
+        self.assertEqual(self.history[1].changes, '1,1 2,1')
+        self.assertEqual(self.history[2].result, 'SUCCESS')
+        self.assertEqual(self.history[2].changes, '1,2')
+        self.assertEqual(self.history[3].result, 'SUCCESS')
+        self.assertEqual(self.history[3].changes, '1,1 2,2')
+
+    def test_abandoned_gate(self):
+        "Test that an abandoned change is dequeued from gate"
+
+        self.worker.hold_jobs_in_build = True
+
+        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
+        A.addApproval('CRVW', 2)
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+        self.waitUntilSettled()
+        self.assertEqual(len(self.builds), 1, "One job being built (on hold)")
+        self.assertEqual(self.builds[0].name, 'project-merge')
+
+        self.fake_gerrit.addEvent(A.getChangeAbandonedEvent())
+        self.waitUntilSettled()
+
+        self.worker.release('.*-merge')
+        self.waitUntilSettled()
+
+        self.assertEqual(len(self.builds), 0, "No job running")
+        self.assertEqual(len(self.history), 1, "Only one build in history")
+        self.assertEqual(self.history[0].result, 'ABORTED',
+                         "Build should have been aborted")
+        self.assertEqual(A.reported, 1,
+                         "Abandoned gate change should report only start")
+
+    def test_abandoned_check(self):
+        "Test that an abandoned change is dequeued from check"
+
+        self.worker.hold_jobs_in_build = True
+
+        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
+        B = self.fake_gerrit.addFakeChange('org/project', 'master', 'B')
+        check_pipeline = self.sched.layout.pipelines['check']
+
+        # Add two git-dependent changes
+        B.setDependsOn(A, 1)
+        self.fake_gerrit.addEvent(B.getPatchsetCreatedEvent(1))
+        self.waitUntilSettled()
+        self.fake_gerrit.addEvent(A.getPatchsetCreatedEvent(1))
+        self.waitUntilSettled()
+        # A live item, and a non-live/live pair
+        items = check_pipeline.getAllItems()
+        self.assertEqual(len(items), 3)
+
+        self.assertEqual(items[0].change.number, '1')
+        self.assertFalse(items[0].live)
+
+        self.assertEqual(items[1].change.number, '2')
+        self.assertTrue(items[1].live)
+
+        self.assertEqual(items[2].change.number, '1')
+        self.assertTrue(items[2].live)
+
+        # Abandon A
+        self.fake_gerrit.addEvent(A.getChangeAbandonedEvent())
+        self.waitUntilSettled()
+
+        # The live copy of A should be gone, but the non-live and B
+        # should continue
+        items = check_pipeline.getAllItems()
+        self.assertEqual(len(items), 2)
+
+        self.assertEqual(items[0].change.number, '1')
+        self.assertFalse(items[0].live)
+
+        self.assertEqual(items[1].change.number, '2')
+        self.assertTrue(items[1].live)
+
+        self.worker.hold_jobs_in_build = False
+        self.worker.release()
+        self.waitUntilSettled()
+
+        self.assertEqual(len(self.history), 4)
+        self.assertEqual(self.history[0].result, 'ABORTED',
+                         'Build should have been aborted')
+        self.assertEqual(A.reported, 0, "Abandoned change should not report")
+        self.assertEqual(B.reported, 1, "Change should report")
+
+    def test_abandoned_not_timer(self):
+        "Test that an abandoned change does not cancel timer jobs"
+
+        self.worker.hold_jobs_in_build = True
+
+        # Start timer trigger - also org/project
+        self.config.set('zuul', 'layout_config',
+                        'tests/fixtures/layout-idle.yaml')
+        self.sched.reconfigure(self.config)
+        self.registerJobs()
+        # The pipeline triggers every second, so we should have seen
+        # several by now.
+        time.sleep(5)
+        self.waitUntilSettled()
+        # Stop queuing timer triggered jobs so that the assertions
+        # below don't race against more jobs being queued.
+        self.config.set('zuul', 'layout_config',
+                        'tests/fixtures/layout-no-timer.yaml')
+        self.sched.reconfigure(self.config)
+        self.registerJobs()
+        self.assertEqual(len(self.builds), 2, "Two timer jobs")
+
+        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
+        self.fake_gerrit.addEvent(A.getPatchsetCreatedEvent(1))
+        self.waitUntilSettled()
+        self.assertEqual(len(self.builds), 3, "One change plus two timer jobs")
+
+        self.fake_gerrit.addEvent(A.getChangeAbandonedEvent())
+        self.waitUntilSettled()
+
+        self.assertEqual(len(self.builds), 2, "Two timer jobs remain")
+
+        self.worker.release()
+        self.waitUntilSettled()
+
+    def test_zuul_url_return(self):
+        "Test if ZUUL_URL is returning when zuul_url is set in zuul.conf"
+        self.assertTrue(self.sched.config.has_option('merger', 'zuul_url'))
+        self.worker.hold_jobs_in_build = True
+
+        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
+        A.addApproval('CRVW', 2)
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+        self.waitUntilSettled()
+
+        self.assertEqual(len(self.builds), 1)
+        for build in self.builds:
+            self.assertTrue('ZUUL_URL' in build.parameters)
+
+        self.worker.hold_jobs_in_build = False
+        self.worker.release()
+        self.waitUntilSettled()
+
+    def test_new_patchset_dequeues_old_on_head(self):
+        "Test that a new patchset causes the old to be dequeued (at head)"
+        # D -> C (depends on B) -> B (depends on A) -> A -> M
+        self.worker.hold_jobs_in_build = True
+        M = self.fake_gerrit.addFakeChange('org/project', 'master', 'M')
+        M.setMerged()
+        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
+        B = self.fake_gerrit.addFakeChange('org/project', 'master', 'B')
+        C = self.fake_gerrit.addFakeChange('org/project', 'master', 'C')
+        D = self.fake_gerrit.addFakeChange('org/project', 'master', 'D')
+        A.addApproval('CRVW', 2)
+        B.addApproval('CRVW', 2)
+        C.addApproval('CRVW', 2)
+        D.addApproval('CRVW', 2)
+
+        C.setDependsOn(B, 1)
+        B.setDependsOn(A, 1)
+        A.setDependsOn(M, 1)
+
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
+        self.fake_gerrit.addEvent(C.addApproval('APRV', 1))
+        self.fake_gerrit.addEvent(D.addApproval('APRV', 1))
+        self.waitUntilSettled()
+
+        A.addPatchset()
+        self.fake_gerrit.addEvent(A.getPatchsetCreatedEvent(2))
+        self.waitUntilSettled()
+
+        self.worker.hold_jobs_in_build = False
+        self.worker.release()
+        self.waitUntilSettled()
+
+        self.assertEqual(A.data['status'], 'NEW')
+        self.assertEqual(A.reported, 2)
+        self.assertEqual(B.data['status'], 'NEW')
+        self.assertEqual(B.reported, 2)
+        self.assertEqual(C.data['status'], 'NEW')
+        self.assertEqual(C.reported, 2)
+        self.assertEqual(D.data['status'], 'MERGED')
+        self.assertEqual(D.reported, 2)
+        self.assertEqual(len(self.history), 7)
+
+    def test_new_patchset_dequeues_old_without_dependents(self):
+        "Test that a new patchset causes only the old to be dequeued"
+        self.worker.hold_jobs_in_build = True
+        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
+        B = self.fake_gerrit.addFakeChange('org/project', 'master', 'B')
+        C = self.fake_gerrit.addFakeChange('org/project', 'master', 'C')
+        A.addApproval('CRVW', 2)
+        B.addApproval('CRVW', 2)
+        C.addApproval('CRVW', 2)
+
+        self.fake_gerrit.addEvent(C.addApproval('APRV', 1))
+        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+        self.waitUntilSettled()
+
+        B.addPatchset()
+        self.fake_gerrit.addEvent(B.getPatchsetCreatedEvent(2))
+        self.waitUntilSettled()
+
+        self.worker.hold_jobs_in_build = False
+        self.worker.release()
+        self.waitUntilSettled()
+
+        self.assertEqual(A.data['status'], 'MERGED')
+        self.assertEqual(A.reported, 2)
+        self.assertEqual(B.data['status'], 'NEW')
+        self.assertEqual(B.reported, 2)
+        self.assertEqual(C.data['status'], 'MERGED')
+        self.assertEqual(C.reported, 2)
+        self.assertEqual(len(self.history), 9)
+
+    def test_new_patchset_dequeues_old_independent_queue(self):
+        "Test that a new patchset causes the old to be dequeued (independent)"
+        self.worker.hold_jobs_in_build = True
+        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
+        B = self.fake_gerrit.addFakeChange('org/project', 'master', 'B')
+        C = self.fake_gerrit.addFakeChange('org/project', 'master', 'C')
+        self.fake_gerrit.addEvent(A.getPatchsetCreatedEvent(1))
+        self.fake_gerrit.addEvent(B.getPatchsetCreatedEvent(1))
+        self.fake_gerrit.addEvent(C.getPatchsetCreatedEvent(1))
+        self.waitUntilSettled()
+
+        B.addPatchset()
+        self.fake_gerrit.addEvent(B.getPatchsetCreatedEvent(2))
+        self.waitUntilSettled()
+
+        self.worker.hold_jobs_in_build = False
+        self.worker.release()
+        self.waitUntilSettled()
+
+        self.assertEqual(A.data['status'], 'NEW')
+        self.assertEqual(A.reported, 1)
+        self.assertEqual(B.data['status'], 'NEW')
+        self.assertEqual(B.reported, 1)
+        self.assertEqual(C.data['status'], 'NEW')
+        self.assertEqual(C.reported, 1)
+        self.assertEqual(len(self.history), 10)
+        self.assertEqual(self.countJobResults(self.history, 'ABORTED'), 1)
+
+    def test_noop_job(self):
+        "Test that the internal noop job works"
+        A = self.fake_gerrit.addFakeChange('org/noop-project', 'master', 'A')
+        A.addApproval('CRVW', 2)
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+        self.waitUntilSettled()
+
+        self.assertEqual(len(self.gearman_server.getQueue()), 0)
+        self.assertTrue(self.sched._areAllBuildsComplete())
+        self.assertEqual(len(self.history), 0)
+        self.assertEqual(A.data['status'], 'MERGED')
+        self.assertEqual(A.reported, 2)
+
+    def test_no_job_project(self):
+        "Test that reports with no jobs don't get sent"
+        A = self.fake_gerrit.addFakeChange('org/no-jobs-project',
+                                           'master', 'A')
+        self.fake_gerrit.addEvent(A.getPatchsetCreatedEvent(1))
+        self.waitUntilSettled()
+
+        # Change wasn't reported to
+        self.assertEqual(A.reported, False)
+
+        # Check queue is empty afterwards
+        check_pipeline = self.sched.layout.pipelines['check']
+        items = check_pipeline.getAllItems()
+        self.assertEqual(len(items), 0)
+
+        self.assertEqual(len(self.history), 0)
+
+    def test_zuul_refs(self):
+        "Test that zuul refs exist and have the right changes"
+        self.worker.hold_jobs_in_build = True
+        M1 = self.fake_gerrit.addFakeChange('org/project1', 'master', 'M1')
+        M1.setMerged()
+        M2 = self.fake_gerrit.addFakeChange('org/project2', 'master', 'M2')
+        M2.setMerged()
+
+        A = self.fake_gerrit.addFakeChange('org/project1', 'master', 'A')
+        B = self.fake_gerrit.addFakeChange('org/project1', 'master', 'B')
+        C = self.fake_gerrit.addFakeChange('org/project2', 'master', 'C')
+        D = self.fake_gerrit.addFakeChange('org/project2', 'master', 'D')
+        A.addApproval('CRVW', 2)
+        B.addApproval('CRVW', 2)
+        C.addApproval('CRVW', 2)
+        D.addApproval('CRVW', 2)
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
+        self.fake_gerrit.addEvent(C.addApproval('APRV', 1))
+        self.fake_gerrit.addEvent(D.addApproval('APRV', 1))
+
+        self.waitUntilSettled()
+        self.worker.release('.*-merge')
+        self.waitUntilSettled()
+        self.worker.release('.*-merge')
+        self.waitUntilSettled()
+        self.worker.release('.*-merge')
+        self.waitUntilSettled()
+        self.worker.release('.*-merge')
+        self.waitUntilSettled()
+
+        a_zref = b_zref = c_zref = d_zref = None
+        for x in self.builds:
+            if x.parameters['ZUUL_CHANGE'] == '3':
+                a_zref = x.parameters['ZUUL_REF']
+            if x.parameters['ZUUL_CHANGE'] == '4':
+                b_zref = x.parameters['ZUUL_REF']
+            if x.parameters['ZUUL_CHANGE'] == '5':
+                c_zref = x.parameters['ZUUL_REF']
+            if x.parameters['ZUUL_CHANGE'] == '6':
+                d_zref = x.parameters['ZUUL_REF']
+
+        # There are... four... refs.
+        self.assertIsNotNone(a_zref)
+        self.assertIsNotNone(b_zref)
+        self.assertIsNotNone(c_zref)
+        self.assertIsNotNone(d_zref)
+
+        # And they should all be different
+        refs = set([a_zref, b_zref, c_zref, d_zref])
+        self.assertEqual(len(refs), 4)
+
+        # a ref should have a, not b, and should not be in project2
+        self.assertTrue(self.ref_has_change(a_zref, A))
+        self.assertFalse(self.ref_has_change(a_zref, B))
+        self.assertFalse(self.ref_has_change(a_zref, M2))
+
+        # b ref should have a and b, and should not be in project2
+        self.assertTrue(self.ref_has_change(b_zref, A))
+        self.assertTrue(self.ref_has_change(b_zref, B))
+        self.assertFalse(self.ref_has_change(b_zref, M2))
+
+        # c ref should have a and b in 1, c in 2
+        self.assertTrue(self.ref_has_change(c_zref, A))
+        self.assertTrue(self.ref_has_change(c_zref, B))
+        self.assertTrue(self.ref_has_change(c_zref, C))
+        self.assertFalse(self.ref_has_change(c_zref, D))
+
+        # d ref should have a and b in 1, c and d in 2
+        self.assertTrue(self.ref_has_change(d_zref, A))
+        self.assertTrue(self.ref_has_change(d_zref, B))
+        self.assertTrue(self.ref_has_change(d_zref, C))
+        self.assertTrue(self.ref_has_change(d_zref, D))
+
+        self.worker.hold_jobs_in_build = False
+        self.worker.release()
+        self.waitUntilSettled()
+
+        self.assertEqual(A.data['status'], 'MERGED')
+        self.assertEqual(A.reported, 2)
+        self.assertEqual(B.data['status'], 'MERGED')
+        self.assertEqual(B.reported, 2)
+        self.assertEqual(C.data['status'], 'MERGED')
+        self.assertEqual(C.reported, 2)
+        self.assertEqual(D.data['status'], 'MERGED')
+        self.assertEqual(D.reported, 2)
+
+    def test_rerun_on_error(self):
+        "Test that if a worker fails to run a job, it is run again"
+        self.worker.hold_jobs_in_build = True
+        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
+        A.addApproval('CRVW', 2)
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+        self.waitUntilSettled()
+
+        self.builds[0].run_error = True
+        self.worker.hold_jobs_in_build = False
+        self.worker.release()
+        self.waitUntilSettled()
+        self.assertEqual(self.countJobResults(self.history, 'RUN_ERROR'), 1)
+        self.assertEqual(self.countJobResults(self.history, 'SUCCESS'), 3)
+
+    def test_statsd(self):
+        "Test each of the statsd methods used in the scheduler"
+        import extras
+        statsd = extras.try_import('statsd.statsd')
+        statsd.incr('test-incr')
+        statsd.timing('test-timing', 3)
+        statsd.gauge('test-gauge', 12)
+        self.assertReportedStat('test-incr', '1|c')
+        self.assertReportedStat('test-timing', '3|ms')
+        self.assertReportedStat('test-gauge', '12|g')
+
+    def test_stuck_job_cleanup(self):
+        "Test that pending jobs are cleaned up if removed from layout"
+        # This job won't be registered at startup because it is not in
+        # the standard layout, but we need it to already be registerd
+        # for when we reconfigure, as that is when Zuul will attempt
+        # to run the new job.
+        self.worker.registerFunction('build:gate-noop')
+        self.gearman_server.hold_jobs_in_queue = True
+        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
+        A.addApproval('CRVW', 2)
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+        self.waitUntilSettled()
+        self.assertEqual(len(self.gearman_server.getQueue()), 1)
+
+        self.config.set('zuul', 'layout_config',
+                        'tests/fixtures/layout-no-jobs.yaml')
+        self.sched.reconfigure(self.config)
+        self.waitUntilSettled()
+
+        self.gearman_server.release('gate-noop')
+        self.waitUntilSettled()
+        self.assertEqual(len(self.gearman_server.getQueue()), 0)
+        self.assertTrue(self.sched._areAllBuildsComplete())
+
+        self.assertEqual(len(self.history), 1)
+        self.assertEqual(self.history[0].name, 'gate-noop')
+        self.assertEqual(self.history[0].result, 'SUCCESS')
+
+    def test_file_head(self):
+        # This is a regression test for an observed bug.  A change
+        # with a file named "HEAD" in the root directory of the repo
+        # was processed by a merger.  It then was unable to reset the
+        # repo because of:
+        #   GitCommandError: 'git reset --hard HEAD' returned
+        #       with exit code 128
+        #   stderr: 'fatal: ambiguous argument 'HEAD': both revision
+        #       and filename
+        #   Use '--' to separate filenames from revisions'
+
+        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
+        A.addPatchset(['HEAD'])
+        B = self.fake_gerrit.addFakeChange('org/project', 'master', 'B')
+
+        self.fake_gerrit.addEvent(A.getPatchsetCreatedEvent(2))
+        self.waitUntilSettled()
+
+        self.fake_gerrit.addEvent(B.getPatchsetCreatedEvent(1))
+        self.waitUntilSettled()
+
+        self.assertIn('Build succeeded', A.messages[0])
+        self.assertIn('Build succeeded', B.messages[0])
+
+    def test_file_jobs(self):
+        "Test that file jobs run only when appropriate"
+        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
+        A.addPatchset(['pip-requires'])
+        B = self.fake_gerrit.addFakeChange('org/project', 'master', 'B')
+        A.addApproval('CRVW', 2)
+        B.addApproval('CRVW', 2)
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
+        self.waitUntilSettled()
+
+        testfile_jobs = [x for x in self.history
+                         if x.name == 'project-testfile']
+
+        self.assertEqual(len(testfile_jobs), 1)
+        self.assertEqual(testfile_jobs[0].changes, '1,2')
+        self.assertEqual(A.data['status'], 'MERGED')
+        self.assertEqual(A.reported, 2)
+        self.assertEqual(B.data['status'], 'MERGED')
+        self.assertEqual(B.reported, 2)
+
+    def _test_skip_if_jobs(self, branch, should_skip):
+        "Test that jobs with a skip-if filter run only when appropriate"
+        self.config.set('zuul', 'layout_config',
+                        'tests/fixtures/layout-skip-if.yaml')
+        self.sched.reconfigure(self.config)
+        self.registerJobs()
+
+        change = self.fake_gerrit.addFakeChange('org/project',
+                                                branch,
+                                                'test skip-if')
+        self.fake_gerrit.addEvent(change.getPatchsetCreatedEvent(1))
+        self.waitUntilSettled()
+
+        tested_change_ids = [x.changes[0] for x in self.history
+                             if x.name == 'project-test-skip-if']
+
+        if should_skip:
+            self.assertEqual([], tested_change_ids)
+        else:
+            self.assertIn(change.data['number'], tested_change_ids)
+
+    def test_skip_if_match_skips_job(self):
+        self._test_skip_if_jobs(branch='master', should_skip=True)
+
+    def test_skip_if_no_match_runs_job(self):
+        self._test_skip_if_jobs(branch='mp', should_skip=False)
+
+    def test_test_config(self):
+        "Test that we can test the config"
+        self.sched.testConfig(self.config.get('zuul', 'layout_config'),
+                              self.connections)
+
+    def test_build_description(self):
+        "Test that build descriptions update"
+        self.worker.registerFunction('set_description:' +
+                                     self.worker.worker_id)
+
+        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
+        A.addApproval('CRVW', 2)
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+        self.waitUntilSettled()
+        desc = self.history[0].description
+        self.log.debug("Description: %s" % desc)
+        self.assertTrue(re.search("Branch.*master", desc))
+        self.assertTrue(re.search("Pipeline.*gate", desc))
+        self.assertTrue(re.search("project-merge.*SUCCESS", desc))
+        self.assertTrue(re.search("project-test1.*SUCCESS", desc))
+        self.assertTrue(re.search("project-test2.*SUCCESS", desc))
+        self.assertTrue(re.search("Reported result.*SUCCESS", desc))
+
+    def test_queue_names(self):
+        "Test shared change queue names"
+        project1 = self.sched.layout.projects['org/project1']
+        project2 = self.sched.layout.projects['org/project2']
+        q1 = self.sched.layout.pipelines['gate'].getQueue(project1)
+        q2 = self.sched.layout.pipelines['gate'].getQueue(project2)
+        self.assertEqual(q1.name, 'integration')
+        self.assertEqual(q2.name, 'integration')
+
+        self.config.set('zuul', 'layout_config',
+                        'tests/fixtures/layout-bad-queue.yaml')
+        with testtools.ExpectedException(
+            Exception, "More than one name assigned to change queue"):
+            self.sched.reconfigure(self.config)
+
+    def test_queue_precedence(self):
+        "Test that queue precedence works"
+
+        self.gearman_server.hold_jobs_in_queue = True
+        self.worker.hold_jobs_in_build = True
+        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
+        self.fake_gerrit.addEvent(A.getPatchsetCreatedEvent(1))
+        A.addApproval('CRVW', 2)
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+
+        self.waitUntilSettled()
+        self.gearman_server.hold_jobs_in_queue = False
+        self.gearman_server.release()
+        self.waitUntilSettled()
+
+        # Run one build at a time to ensure non-race order:
+        self.orderedRelease()
+        self.worker.hold_jobs_in_build = False
+        self.waitUntilSettled()
+
+        self.log.debug(self.history)
+        self.assertEqual(self.history[0].pipeline, 'gate')
+        self.assertEqual(self.history[1].pipeline, 'check')
+        self.assertEqual(self.history[2].pipeline, 'gate')
+        self.assertEqual(self.history[3].pipeline, 'gate')
+        self.assertEqual(self.history[4].pipeline, 'check')
+        self.assertEqual(self.history[5].pipeline, 'check')
+
+    def test_json_status(self):
+        "Test that we can retrieve JSON status info"
+        self.worker.hold_jobs_in_build = True
+        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
+        A.addApproval('CRVW', 2)
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+        self.waitUntilSettled()
+
+        self.worker.release('project-merge')
+        self.waitUntilSettled()
+
+        port = self.webapp.server.socket.getsockname()[1]
+
+        req = urllib2.Request("http://localhost:%s/status.json" % port)
+        f = urllib2.urlopen(req)
+        headers = f.info()
+        self.assertIn('Content-Length', headers)
+        self.assertIn('Content-Type', headers)
+        self.assertIsNotNone(re.match('^application/json(; charset=UTF-8)?$',
+                                      headers['Content-Type']))
+        self.assertIn('Access-Control-Allow-Origin', headers)
+        self.assertIn('Cache-Control', headers)
+        self.assertIn('Last-Modified', headers)
+        self.assertIn('Expires', headers)
+        data = f.read()
+
+        self.worker.hold_jobs_in_build = False
+        self.worker.release()
+        self.waitUntilSettled()
+
+        data = json.loads(data)
+        status_jobs = []
+        for p in data['pipelines']:
+            for q in p['change_queues']:
+                if p['name'] in ['gate', 'conflict']:
+                    self.assertEqual(q['window'], 20)
+                else:
+                    self.assertEqual(q['window'], 0)
+                for head in q['heads']:
+                    for change in head:
+                        self.assertTrue(change['active'])
+                        self.assertEqual(change['id'], '1,1')
+                        for job in change['jobs']:
+                            status_jobs.append(job)
+        self.assertEqual('project-merge', status_jobs[0]['name'])
+        self.assertEqual('https://server/job/project-merge/0/',
+                         status_jobs[0]['url'])
+        self.assertEqual('http://logs.example.com/1/1/gate/project-merge/0',
+                         status_jobs[0]['report_url'])
+
+        self.assertEqual('project-test1', status_jobs[1]['name'])
+        self.assertEqual('https://server/job/project-test1/1/',
+                         status_jobs[1]['url'])
+        self.assertEqual('http://logs.example.com/1/1/gate/project-test1/1',
+                         status_jobs[1]['report_url'])
+
+        self.assertEqual('project-test2', status_jobs[2]['name'])
+        self.assertEqual('https://server/job/project-test2/2/',
+                         status_jobs[2]['url'])
+        self.assertEqual('http://logs.example.com/1/1/gate/project-test2/2',
+                         status_jobs[2]['report_url'])
+
+    def test_merging_queues(self):
+        "Test that transitively-connected change queues are merged"
+        self.config.set('zuul', 'layout_config',
+                        'tests/fixtures/layout-merge-queues.yaml')
+        self.sched.reconfigure(self.config)
+        self.assertEqual(len(self.sched.layout.pipelines['gate'].queues), 1)
+
+    def test_mutex(self):
+        "Test job mutexes"
+        self.config.set('zuul', 'layout_config',
+                        'tests/fixtures/layout-mutex.yaml')
+        self.sched.reconfigure(self.config)
+
+        self.worker.hold_jobs_in_build = True
+        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
+        B = self.fake_gerrit.addFakeChange('org/project', 'master', 'B')
+        self.assertFalse('test-mutex' in self.sched.mutex.mutexes)
+
+        self.fake_gerrit.addEvent(A.getPatchsetCreatedEvent(1))
+        self.fake_gerrit.addEvent(B.getPatchsetCreatedEvent(1))
+        self.waitUntilSettled()
+        self.assertEqual(len(self.builds), 3)
+        self.assertEqual(self.builds[0].name, 'project-test1')
+        self.assertEqual(self.builds[1].name, 'mutex-one')
+        self.assertEqual(self.builds[2].name, 'project-test1')
+
+        self.worker.release('mutex-one')
+        self.waitUntilSettled()
+
+        self.assertEqual(len(self.builds), 3)
+        self.assertEqual(self.builds[0].name, 'project-test1')
+        self.assertEqual(self.builds[1].name, 'project-test1')
+        self.assertEqual(self.builds[2].name, 'mutex-two')
+        self.assertTrue('test-mutex' in self.sched.mutex.mutexes)
+
+        self.worker.release('mutex-two')
+        self.waitUntilSettled()
+
+        self.assertEqual(len(self.builds), 3)
+        self.assertEqual(self.builds[0].name, 'project-test1')
+        self.assertEqual(self.builds[1].name, 'project-test1')
+        self.assertEqual(self.builds[2].name, 'mutex-one')
+        self.assertTrue('test-mutex' in self.sched.mutex.mutexes)
+
+        self.worker.release('mutex-one')
+        self.waitUntilSettled()
+
+        self.assertEqual(len(self.builds), 3)
+        self.assertEqual(self.builds[0].name, 'project-test1')
+        self.assertEqual(self.builds[1].name, 'project-test1')
+        self.assertEqual(self.builds[2].name, 'mutex-two')
+        self.assertTrue('test-mutex' in self.sched.mutex.mutexes)
+
+        self.worker.release('mutex-two')
+        self.waitUntilSettled()
+
+        self.assertEqual(len(self.builds), 2)
+        self.assertEqual(self.builds[0].name, 'project-test1')
+        self.assertEqual(self.builds[1].name, 'project-test1')
+        self.assertFalse('test-mutex' in self.sched.mutex.mutexes)
+
+        self.worker.hold_jobs_in_build = False
+        self.worker.release()
+
+        self.waitUntilSettled()
+        self.assertEqual(len(self.builds), 0)
+
+        self.assertEqual(A.reported, 1)
+        self.assertEqual(B.reported, 1)
+        self.assertFalse('test-mutex' in self.sched.mutex.mutexes)
+
+    def test_node_label(self):
+        "Test that a job runs on a specific node label"
+        self.worker.registerFunction('build:node-project-test1:debian')
+
+        A = self.fake_gerrit.addFakeChange('org/node-project', 'master', 'A')
+        A.addApproval('CRVW', 2)
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+        self.waitUntilSettled()
+
+        self.assertIsNone(self.getJobFromHistory('node-project-merge').node)
+        self.assertEqual(self.getJobFromHistory('node-project-test1').node,
+                         'debian')
+        self.assertIsNone(self.getJobFromHistory('node-project-test2').node)
+
+    def test_live_reconfiguration(self):
+        "Test that live reconfiguration works"
+        self.worker.hold_jobs_in_build = True
+        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
+        A.addApproval('CRVW', 2)
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+        self.waitUntilSettled()
+
+        self.sched.reconfigure(self.config)
+
+        self.worker.hold_jobs_in_build = False
+        self.worker.release()
+        self.waitUntilSettled()
+        self.assertEqual(self.getJobFromHistory('project-merge').result,
+                         'SUCCESS')
+        self.assertEqual(self.getJobFromHistory('project-test1').result,
+                         'SUCCESS')
+        self.assertEqual(self.getJobFromHistory('project-test2').result,
+                         'SUCCESS')
+        self.assertEqual(A.data['status'], 'MERGED')
+        self.assertEqual(A.reported, 2)
+
+    def test_live_reconfiguration_merge_conflict(self):
+        # A real-world bug: a change in a gate queue has a merge
+        # conflict and a job is added to its project while it's
+        # sitting in the queue.  The job gets added to the change and
+        # enqueued and the change gets stuck.
+        self.worker.registerFunction('build:project-test3')
+        self.worker.hold_jobs_in_build = True
+
+        # This change is fine.  It's here to stop the queue long
+        # enough for the next change to be subject to the
+        # reconfiguration, as well as to provide a conflict for the
+        # next change.  This change will succeed and merge.
+        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
+        A.addPatchset(['conflict'])
+        A.addApproval('CRVW', 2)
+
+        # This change will be in merge conflict.  During the
+        # reconfiguration, we will add a job.  We want to make sure
+        # that doesn't cause it to get stuck.
+        B = self.fake_gerrit.addFakeChange('org/project', 'master', 'B')
+        B.addPatchset(['conflict'])
+        B.addApproval('CRVW', 2)
+
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
+
+        self.waitUntilSettled()
+
+        # No jobs have run yet
+        self.assertEqual(A.data['status'], 'NEW')
+        self.assertEqual(A.reported, 1)
+        self.assertEqual(B.data['status'], 'NEW')
+        self.assertEqual(B.reported, 1)
+        self.assertEqual(len(self.history), 0)
+
+        # Add the "project-test3" job.
+        self.config.set('zuul', 'layout_config',
+                        'tests/fixtures/layout-live-'
+                        'reconfiguration-add-job.yaml')
+        self.sched.reconfigure(self.config)
+        self.waitUntilSettled()
+
+        self.worker.hold_jobs_in_build = False
+        self.worker.release()
+        self.waitUntilSettled()
+
+        self.assertEqual(A.data['status'], 'MERGED')
+        self.assertEqual(A.reported, 2)
+        self.assertEqual(B.data['status'], 'NEW')
+        self.assertEqual(B.reported, 2)
+        self.assertEqual(self.getJobFromHistory('project-merge').result,
+                         'SUCCESS')
+        self.assertEqual(self.getJobFromHistory('project-test1').result,
+                         'SUCCESS')
+        self.assertEqual(self.getJobFromHistory('project-test2').result,
+                         'SUCCESS')
+        self.assertEqual(self.getJobFromHistory('project-test3').result,
+                         'SUCCESS')
+        self.assertEqual(len(self.history), 4)
+
+    def test_live_reconfiguration_failed_root(self):
+        # An extrapolation of test_live_reconfiguration_merge_conflict
+        # that tests a job added to a job tree with a failed root does
+        # not run.
+        self.worker.registerFunction('build:project-test3')
+        self.worker.hold_jobs_in_build = True
+
+        # This change is fine.  It's here to stop the queue long
+        # enough for the next change to be subject to the
+        # reconfiguration.  This change will succeed and merge.
+        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
+        A.addPatchset(['conflict'])
+        A.addApproval('CRVW', 2)
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+        self.waitUntilSettled()
+        self.worker.release('.*-merge')
+        self.waitUntilSettled()
+
+        B = self.fake_gerrit.addFakeChange('org/project', 'master', 'B')
+        self.worker.addFailTest('project-merge', B)
+        B.addApproval('CRVW', 2)
+        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
+        self.waitUntilSettled()
+
+        self.worker.release('.*-merge')
+        self.waitUntilSettled()
+
+        # Both -merge jobs have run, but no others.
+        self.assertEqual(A.data['status'], 'NEW')
+        self.assertEqual(A.reported, 1)
+        self.assertEqual(B.data['status'], 'NEW')
+        self.assertEqual(B.reported, 1)
+        self.assertEqual(self.history[0].result, 'SUCCESS')
+        self.assertEqual(self.history[0].name, 'project-merge')
+        self.assertEqual(self.history[1].result, 'FAILURE')
+        self.assertEqual(self.history[1].name, 'project-merge')
+        self.assertEqual(len(self.history), 2)
+
+        # Add the "project-test3" job.
+        self.config.set('zuul', 'layout_config',
+                        'tests/fixtures/layout-live-'
+                        'reconfiguration-add-job.yaml')
+        self.sched.reconfigure(self.config)
+        self.waitUntilSettled()
+
+        self.worker.hold_jobs_in_build = False
+        self.worker.release()
+        self.waitUntilSettled()
+
+        self.assertEqual(A.data['status'], 'MERGED')
+        self.assertEqual(A.reported, 2)
+        self.assertEqual(B.data['status'], 'NEW')
+        self.assertEqual(B.reported, 2)
+        self.assertEqual(self.history[0].result, 'SUCCESS')
+        self.assertEqual(self.history[0].name, 'project-merge')
+        self.assertEqual(self.history[1].result, 'FAILURE')
+        self.assertEqual(self.history[1].name, 'project-merge')
+        self.assertEqual(self.history[2].result, 'SUCCESS')
+        self.assertEqual(self.history[3].result, 'SUCCESS')
+        self.assertEqual(self.history[4].result, 'SUCCESS')
+        self.assertEqual(len(self.history), 5)
+
+    def test_live_reconfiguration_failed_job(self):
+        # Test that a change with a removed failing job does not
+        # disrupt reconfiguration.  If a change has a failed job and
+        # that job is removed during a reconfiguration, we observed a
+        # bug where the code to re-set build statuses would run on
+        # that build and raise an exception because the job no longer
+        # existed.
+        self.worker.hold_jobs_in_build = True
+
+        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
+
+        # This change will fail and later be removed by the reconfiguration.
+        self.worker.addFailTest('project-test1', A)
+
+        self.fake_gerrit.addEvent(A.getPatchsetCreatedEvent(1))
+        self.waitUntilSettled()
+        self.worker.release('.*-merge')
+        self.waitUntilSettled()
+        self.worker.release('project-test1')
+        self.waitUntilSettled()
+
+        self.assertEqual(A.data['status'], 'NEW')
+        self.assertEqual(A.reported, 0)
+
+        self.assertEqual(self.getJobFromHistory('project-merge').result,
+                         'SUCCESS')
+        self.assertEqual(self.getJobFromHistory('project-test1').result,
+                         'FAILURE')
+        self.assertEqual(len(self.history), 2)
+
+        # Remove the test1 job.
+        self.config.set('zuul', 'layout_config',
+                        'tests/fixtures/layout-live-'
+                        'reconfiguration-failed-job.yaml')
+        self.sched.reconfigure(self.config)
+        self.waitUntilSettled()
+
+        self.worker.hold_jobs_in_build = False
+        self.worker.release()
+        self.waitUntilSettled()
+
+        self.assertEqual(self.getJobFromHistory('project-test2').result,
+                         'SUCCESS')
+        self.assertEqual(self.getJobFromHistory('project-testfile').result,
+                         'SUCCESS')
+        self.assertEqual(len(self.history), 4)
+
+        self.assertEqual(A.data['status'], 'NEW')
+        self.assertEqual(A.reported, 1)
+        self.assertIn('Build succeeded', A.messages[0])
+        # Ensure the removed job was not included in the report.
+        self.assertNotIn('project-test1', A.messages[0])
+
+    def test_live_reconfiguration_shared_queue(self):
+        # Test that a change with a failing job which was removed from
+        # this project but otherwise still exists in the system does
+        # not disrupt reconfiguration.
+
+        self.worker.hold_jobs_in_build = True
+
+        A = self.fake_gerrit.addFakeChange('org/project1', 'master', 'A')
+
+        self.worker.addFailTest('project1-project2-integration', A)
+
+        self.fake_gerrit.addEvent(A.getPatchsetCreatedEvent(1))
+        self.waitUntilSettled()
+        self.worker.release('.*-merge')
+        self.waitUntilSettled()
+        self.worker.release('project1-project2-integration')
+        self.waitUntilSettled()
+
+        self.assertEqual(A.data['status'], 'NEW')
+        self.assertEqual(A.reported, 0)
+
+        self.assertEqual(self.getJobFromHistory('project1-merge').result,
+                         'SUCCESS')
+        self.assertEqual(self.getJobFromHistory(
+            'project1-project2-integration').result, 'FAILURE')
+        self.assertEqual(len(self.history), 2)
+
+        # Remove the integration job.
+        self.config.set('zuul', 'layout_config',
+                        'tests/fixtures/layout-live-'
+                        'reconfiguration-shared-queue.yaml')
+        self.sched.reconfigure(self.config)
+        self.waitUntilSettled()
+
+        self.worker.hold_jobs_in_build = False
+        self.worker.release()
+        self.waitUntilSettled()
+
+        self.assertEqual(self.getJobFromHistory('project1-merge').result,
+                         'SUCCESS')
+        self.assertEqual(self.getJobFromHistory('project1-test1').result,
+                         'SUCCESS')
+        self.assertEqual(self.getJobFromHistory('project1-test2').result,
+                         'SUCCESS')
+        self.assertEqual(self.getJobFromHistory(
+            'project1-project2-integration').result, 'FAILURE')
+        self.assertEqual(len(self.history), 4)
+
+        self.assertEqual(A.data['status'], 'NEW')
+        self.assertEqual(A.reported, 1)
+        self.assertIn('Build succeeded', A.messages[0])
+        # Ensure the removed job was not included in the report.
+        self.assertNotIn('project1-project2-integration', A.messages[0])
+
+    def test_double_live_reconfiguration_shared_queue(self):
+        # This was a real-world regression.  A change is added to
+        # gate; a reconfigure happens, a second change which depends
+        # on the first is added, and a second reconfiguration happens.
+        # Ensure that both changes merge.
+
+        # A failure may indicate incorrect caching or cleaning up of
+        # references during a reconfiguration.
+        self.worker.hold_jobs_in_build = True
+
+        A = self.fake_gerrit.addFakeChange('org/project1', 'master', 'A')
+        B = self.fake_gerrit.addFakeChange('org/project1', 'master', 'B')
+        B.setDependsOn(A, 1)
+        A.addApproval('CRVW', 2)
+        B.addApproval('CRVW', 2)
+
+        # Add the parent change.
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+        self.waitUntilSettled()
+        self.worker.release('.*-merge')
+        self.waitUntilSettled()
+
+        # Reconfigure (with only one change in the pipeline).
+        self.sched.reconfigure(self.config)
+        self.waitUntilSettled()
+
+        # Add the child change.
+        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
+        self.waitUntilSettled()
+        self.worker.release('.*-merge')
+        self.waitUntilSettled()
+
+        # Reconfigure (with both in the pipeline).
+        self.sched.reconfigure(self.config)
+        self.waitUntilSettled()
+
+        self.worker.hold_jobs_in_build = False
+        self.worker.release()
+        self.waitUntilSettled()
+
+        self.assertEqual(len(self.history), 8)
+
+        self.assertEqual(A.data['status'], 'MERGED')
+        self.assertEqual(A.reported, 2)
+        self.assertEqual(B.data['status'], 'MERGED')
+        self.assertEqual(B.reported, 2)
+
+    def test_live_reconfiguration_del_project(self):
+        # Test project deletion from layout
+        # while changes are enqueued
+
+        self.worker.hold_jobs_in_build = True
+        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
+        B = self.fake_gerrit.addFakeChange('org/project1', 'master', 'B')
+        C = self.fake_gerrit.addFakeChange('org/project1', 'master', 'C')
+
+        # A Depends-On: B
+        A.data['commitMessage'] = '%s\n\nDepends-On: %s\n' % (
+            A.subject, B.data['id'])
+        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
+
+        self.fake_gerrit.addEvent(A.getPatchsetCreatedEvent(1))
+        self.fake_gerrit.addEvent(C.getPatchsetCreatedEvent(1))
+        self.waitUntilSettled()
+        self.worker.release('.*-merge')
+        self.waitUntilSettled()
+        self.assertEqual(len(self.builds), 5)
+
+        # This layout defines only org/project, not org/project1
+        self.config.set('zuul', 'layout_config',
+                        'tests/fixtures/layout-live-'
+                        'reconfiguration-del-project.yaml')
+        self.sched.reconfigure(self.config)
+        self.waitUntilSettled()
+
+        # Builds for C aborted, builds for A succeed,
+        # and have change B applied ahead
+        job_c = self.getJobFromHistory('project1-test1')
+        self.assertEqual(job_c.changes, '3,1')
+        self.assertEqual(job_c.result, 'ABORTED')
+
+        self.worker.hold_jobs_in_build = False
+        self.worker.release()
+        self.waitUntilSettled()
+
+        self.assertEqual(self.getJobFromHistory('project-test1').changes,
+                         '2,1 1,1')
+
+        self.assertEqual(A.data['status'], 'NEW')
+        self.assertEqual(B.data['status'], 'NEW')
+        self.assertEqual(C.data['status'], 'NEW')
+        self.assertEqual(A.reported, 1)
+        self.assertEqual(B.reported, 0)
+        self.assertEqual(C.reported, 0)
+
+        self.assertEqual(len(self.sched.layout.pipelines['check'].queues), 0)
+        self.assertIn('Build succeeded', A.messages[0])
+
+    def test_live_reconfiguration_functions(self):
+        "Test live reconfiguration with a custom function"
+        self.worker.registerFunction('build:node-project-test1:debian')
+        self.worker.registerFunction('build:node-project-test1:wheezy')
+        A = self.fake_gerrit.addFakeChange('org/node-project', 'master', 'A')
+        A.addApproval('CRVW', 2)
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+        self.waitUntilSettled()
+
+        self.assertIsNone(self.getJobFromHistory('node-project-merge').node)
+        self.assertEqual(self.getJobFromHistory('node-project-test1').node,
+                         'debian')
+        self.assertIsNone(self.getJobFromHistory('node-project-test2').node)
+
+        self.config.set('zuul', 'layout_config',
+                        'tests/fixtures/layout-live-'
+                        'reconfiguration-functions.yaml')
+        self.sched.reconfigure(self.config)
+        self.worker.build_history = []
+
+        B = self.fake_gerrit.addFakeChange('org/node-project', 'master', 'B')
+        B.addApproval('CRVW', 2)
+        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
+        self.waitUntilSettled()
+
+        self.assertIsNone(self.getJobFromHistory('node-project-merge').node)
+        self.assertEqual(self.getJobFromHistory('node-project-test1').node,
+                         'wheezy')
+        self.assertIsNone(self.getJobFromHistory('node-project-test2').node)
+
+    def test_delayed_repo_init(self):
+        self.config.set('zuul', 'layout_config',
+                        'tests/fixtures/layout-delayed-repo-init.yaml')
+        self.sched.reconfigure(self.config)
+
+        self.init_repo("org/new-project")
+        A = self.fake_gerrit.addFakeChange('org/new-project', 'master', 'A')
+
+        A.addApproval('CRVW', 2)
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+        self.waitUntilSettled()
+        self.assertEqual(self.getJobFromHistory('project-merge').result,
+                         'SUCCESS')
+        self.assertEqual(self.getJobFromHistory('project-test1').result,
+                         'SUCCESS')
+        self.assertEqual(self.getJobFromHistory('project-test2').result,
+                         'SUCCESS')
+        self.assertEqual(A.data['status'], 'MERGED')
+        self.assertEqual(A.reported, 2)
+
+    def test_repo_deleted(self):
+        self.config.set('zuul', 'layout_config',
+                        'tests/fixtures/layout-repo-deleted.yaml')
+        self.sched.reconfigure(self.config)
+
+        self.init_repo("org/delete-project")
+        A = self.fake_gerrit.addFakeChange('org/delete-project', 'master', 'A')
+
+        A.addApproval('CRVW', 2)
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+        self.waitUntilSettled()
+        self.assertEqual(self.getJobFromHistory('project-merge').result,
+                         'SUCCESS')
+        self.assertEqual(self.getJobFromHistory('project-test1').result,
+                         'SUCCESS')
+        self.assertEqual(self.getJobFromHistory('project-test2').result,
+                         'SUCCESS')
+        self.assertEqual(A.data['status'], 'MERGED')
+        self.assertEqual(A.reported, 2)
+
+        # Delete org/new-project zuul repo. Should be recloned.
+        shutil.rmtree(os.path.join(self.git_root, "org/delete-project"))
+
+        B = self.fake_gerrit.addFakeChange('org/delete-project', 'master', 'B')
+
+        B.addApproval('CRVW', 2)
+        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
+        self.waitUntilSettled()
+        self.assertEqual(self.getJobFromHistory('project-merge').result,
+                         'SUCCESS')
+        self.assertEqual(self.getJobFromHistory('project-test1').result,
+                         'SUCCESS')
+        self.assertEqual(self.getJobFromHistory('project-test2').result,
+                         'SUCCESS')
+        self.assertEqual(B.data['status'], 'MERGED')
+        self.assertEqual(B.reported, 2)
+
+    def test_tags(self):
+        "Test job tags"
+        self.config.set('zuul', 'layout_config',
+                        'tests/fixtures/layout-tags.yaml')
+        self.sched.reconfigure(self.config)
+
+        A = self.fake_gerrit.addFakeChange('org/project1', 'master', 'A')
+        B = self.fake_gerrit.addFakeChange('org/project2', 'master', 'B')
+        self.fake_gerrit.addEvent(A.getPatchsetCreatedEvent(1))
+        self.fake_gerrit.addEvent(B.getPatchsetCreatedEvent(1))
+        self.waitUntilSettled()
+
+        results = {'project1-merge': 'extratag merge project1',
+                   'project2-merge': 'merge'}
+
+        for build in self.history:
+            self.assertEqual(results.get(build.name, ''),
+                             build.parameters.get('BUILD_TAGS'))
+
+    def test_timer(self):
+        "Test that a periodic job is triggered"
+        self.worker.hold_jobs_in_build = True
+        self.config.set('zuul', 'layout_config',
+                        'tests/fixtures/layout-timer.yaml')
+        self.sched.reconfigure(self.config)
+        self.registerJobs()
+
+        # The pipeline triggers every second, so we should have seen
+        # several by now.
+        time.sleep(5)
+        self.waitUntilSettled()
+
+        self.assertEqual(len(self.builds), 2)
+
+        port = self.webapp.server.socket.getsockname()[1]
+
+        f = urllib.urlopen("http://localhost:%s/status.json" % port)
+        data = f.read()
+
+        self.worker.hold_jobs_in_build = False
+        # Stop queuing timer triggered jobs so that the assertions
+        # below don't race against more jobs being queued.
+        self.config.set('zuul', 'layout_config',
+                        'tests/fixtures/layout-no-timer.yaml')
+        self.sched.reconfigure(self.config)
+        self.registerJobs()
+        self.worker.release()
+        self.waitUntilSettled()
+
+        self.assertEqual(self.getJobFromHistory(
+            'project-bitrot-stable-old').result, 'SUCCESS')
+        self.assertEqual(self.getJobFromHistory(
+            'project-bitrot-stable-older').result, 'SUCCESS')
+
+        data = json.loads(data)
+        status_jobs = set()
+        for p in data['pipelines']:
+            for q in p['change_queues']:
+                for head in q['heads']:
+                    for change in head:
+                        self.assertEqual(change['id'], None)
+                        for job in change['jobs']:
+                            status_jobs.add(job['name'])
+        self.assertIn('project-bitrot-stable-old', status_jobs)
+        self.assertIn('project-bitrot-stable-older', status_jobs)
+
+    def test_idle(self):
+        "Test that frequent periodic jobs work"
+        self.worker.hold_jobs_in_build = True
+
+        for x in range(1, 3):
+            # Test that timer triggers periodic jobs even across
+            # layout config reloads.
+            # Start timer trigger
+            self.config.set('zuul', 'layout_config',
+                            'tests/fixtures/layout-idle.yaml')
+            self.sched.reconfigure(self.config)
+            self.registerJobs()
+            self.waitUntilSettled()
+
+            # The pipeline triggers every second, so we should have seen
+            # several by now.
+            time.sleep(5)
+
+            # Stop queuing timer triggered jobs so that the assertions
+            # below don't race against more jobs being queued.
+            self.config.set('zuul', 'layout_config',
+                            'tests/fixtures/layout-no-timer.yaml')
+            self.sched.reconfigure(self.config)
+            self.registerJobs()
+            self.waitUntilSettled()
+
+            self.assertEqual(len(self.builds), 2)
+            self.worker.release('.*')
+            self.waitUntilSettled()
+            self.assertEqual(len(self.builds), 0)
+            self.assertEqual(len(self.history), x * 2)
+
+    def test_check_smtp_pool(self):
+        self.config.set('zuul', 'layout_config',
+                        'tests/fixtures/layout-smtp.yaml')
+        self.sched.reconfigure(self.config)
+
+        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
+        self.waitUntilSettled()
+
+        self.fake_gerrit.addEvent(A.getPatchsetCreatedEvent(1))
+        self.waitUntilSettled()
+
+        self.assertEqual(len(self.smtp_messages), 2)
+
+        # A.messages only holds what FakeGerrit places in it. Thus we
+        # work on the knowledge of what the first message should be as
+        # it is only configured to go to SMTP.
+
+        self.assertEqual('zuul@example.com',
+                         self.smtp_messages[0]['from_email'])
+        self.assertEqual(['you@example.com'],
+                         self.smtp_messages[0]['to_email'])
+        self.assertEqual('Starting check jobs.',
+                         self.smtp_messages[0]['body'])
+
+        self.assertEqual('zuul_from@example.com',
+                         self.smtp_messages[1]['from_email'])
+        self.assertEqual(['alternative_me@example.com'],
+                         self.smtp_messages[1]['to_email'])
+        self.assertEqual(A.messages[0],
+                         self.smtp_messages[1]['body'])
+
+    def test_timer_smtp(self):
+        "Test that a periodic job is triggered"
+        self.worker.hold_jobs_in_build = True
+        self.config.set('zuul', 'layout_config',
+                        'tests/fixtures/layout-timer-smtp.yaml')
+        self.sched.reconfigure(self.config)
+        self.registerJobs()
+
+        # The pipeline triggers every second, so we should have seen
+        # several by now.
+        time.sleep(5)
+        self.waitUntilSettled()
+
+        self.assertEqual(len(self.builds), 2)
+        self.worker.release('.*')
+        self.waitUntilSettled()
+        self.assertEqual(len(self.history), 2)
+
+        self.assertEqual(self.getJobFromHistory(
+            'project-bitrot-stable-old').result, 'SUCCESS')
+        self.assertEqual(self.getJobFromHistory(
+            'project-bitrot-stable-older').result, 'SUCCESS')
+
+        self.assertEqual(len(self.smtp_messages), 1)
+
+        # A.messages only holds what FakeGerrit places in it. Thus we
+        # work on the knowledge of what the first message should be as
+        # it is only configured to go to SMTP.
+
+        self.assertEqual('zuul_from@example.com',
+                         self.smtp_messages[0]['from_email'])
+        self.assertEqual(['alternative_me@example.com'],
+                         self.smtp_messages[0]['to_email'])
+        self.assertIn('Subject: Periodic check for org/project succeeded',
+                      self.smtp_messages[0]['headers'])
+
+        # Stop queuing timer triggered jobs and let any that may have
+        # queued through so that end of test assertions pass.
+        self.config.set('zuul', 'layout_config',
+                        'tests/fixtures/layout-no-timer.yaml')
+        self.sched.reconfigure(self.config)
+        self.registerJobs()
+        self.waitUntilSettled()
+        self.worker.release('.*')
+        self.waitUntilSettled()
+
+    def test_client_enqueue_change(self):
+        "Test that the RPC client can enqueue a change"
+        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
+        A.addApproval('CRVW', 2)
+        A.addApproval('APRV', 1)
+
+        client = zuul.rpcclient.RPCClient('127.0.0.1',
+                                          self.gearman_server.port)
+        r = client.enqueue(pipeline='gate',
+                           project='org/project',
+                           trigger='gerrit',
+                           change='1,1')
+        self.waitUntilSettled()
+        self.assertEqual(self.getJobFromHistory('project-merge').result,
+                         'SUCCESS')
+        self.assertEqual(self.getJobFromHistory('project-test1').result,
+                         'SUCCESS')
+        self.assertEqual(self.getJobFromHistory('project-test2').result,
+                         'SUCCESS')
+        self.assertEqual(A.data['status'], 'MERGED')
+        self.assertEqual(A.reported, 2)
+        self.assertEqual(r, True)
+
+    def test_client_enqueue_ref(self):
+        "Test that the RPC client can enqueue a ref"
+
+        client = zuul.rpcclient.RPCClient('127.0.0.1',
+                                          self.gearman_server.port)
+        r = client.enqueue_ref(
+            pipeline='post',
+            project='org/project',
+            trigger='gerrit',
+            ref='master',
+            oldrev='90f173846e3af9154517b88543ffbd1691f31366',
+            newrev='d479a0bfcb34da57a31adb2a595c0cf687812543')
+        self.waitUntilSettled()
+        job_names = [x.name for x in self.history]
+        self.assertEqual(len(self.history), 1)
+        self.assertIn('project-post', job_names)
+        self.assertEqual(r, True)
+
+    def test_client_enqueue_negative(self):
+        "Test that the RPC client returns errors"
+        client = zuul.rpcclient.RPCClient('127.0.0.1',
+                                          self.gearman_server.port)
+        with testtools.ExpectedException(zuul.rpcclient.RPCFailure,
+                                         "Invalid project"):
+            r = client.enqueue(pipeline='gate',
+                               project='project-does-not-exist',
+                               trigger='gerrit',
+                               change='1,1')
+            client.shutdown()
+            self.assertEqual(r, False)
+
+        with testtools.ExpectedException(zuul.rpcclient.RPCFailure,
+                                         "Invalid pipeline"):
+            r = client.enqueue(pipeline='pipeline-does-not-exist',
+                               project='org/project',
+                               trigger='gerrit',
+                               change='1,1')
+            client.shutdown()
+            self.assertEqual(r, False)
+
+        with testtools.ExpectedException(zuul.rpcclient.RPCFailure,
+                                         "Invalid trigger"):
+            r = client.enqueue(pipeline='gate',
+                               project='org/project',
+                               trigger='trigger-does-not-exist',
+                               change='1,1')
+            client.shutdown()
+            self.assertEqual(r, False)
+
+        with testtools.ExpectedException(zuul.rpcclient.RPCFailure,
+                                         "Invalid change"):
+            r = client.enqueue(pipeline='gate',
+                               project='org/project',
+                               trigger='gerrit',
+                               change='1,1')
+            client.shutdown()
+            self.assertEqual(r, False)
+
+        self.waitUntilSettled()
+        self.assertEqual(len(self.history), 0)
+        self.assertEqual(len(self.builds), 0)
+
+    def test_client_promote(self):
+        "Test that the RPC client can promote a change"
+        self.worker.hold_jobs_in_build = True
+        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
+        B = self.fake_gerrit.addFakeChange('org/project', 'master', 'B')
+        C = self.fake_gerrit.addFakeChange('org/project', 'master', 'C')
+        A.addApproval('CRVW', 2)
+        B.addApproval('CRVW', 2)
+        C.addApproval('CRVW', 2)
+
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
+        self.fake_gerrit.addEvent(C.addApproval('APRV', 1))
+
+        self.waitUntilSettled()
+
+        items = self.sched.layout.pipelines['gate'].getAllItems()
+        enqueue_times = {}
+        for item in items:
+            enqueue_times[str(item.change)] = item.enqueue_time
+
+        client = zuul.rpcclient.RPCClient('127.0.0.1',
+                                          self.gearman_server.port)
+        r = client.promote(pipeline='gate',
+                           change_ids=['2,1', '3,1'])
+
+        # ensure that enqueue times are durable
+        items = self.sched.layout.pipelines['gate'].getAllItems()
+        for item in items:
+            self.assertEqual(
+                enqueue_times[str(item.change)], item.enqueue_time)
+
+        self.waitUntilSettled()
+        self.worker.release('.*-merge')
+        self.waitUntilSettled()
+        self.worker.release('.*-merge')
+        self.waitUntilSettled()
+        self.worker.release('.*-merge')
+        self.waitUntilSettled()
+
+        self.assertEqual(len(self.builds), 6)
+        self.assertEqual(self.builds[0].name, 'project-test1')
+        self.assertEqual(self.builds[1].name, 'project-test2')
+        self.assertEqual(self.builds[2].name, 'project-test1')
+        self.assertEqual(self.builds[3].name, 'project-test2')
+        self.assertEqual(self.builds[4].name, 'project-test1')
+        self.assertEqual(self.builds[5].name, 'project-test2')
+
+        self.assertTrue(self.job_has_changes(self.builds[0], B))
+        self.assertFalse(self.job_has_changes(self.builds[0], A))
+        self.assertFalse(self.job_has_changes(self.builds[0], C))
+
+        self.assertTrue(self.job_has_changes(self.builds[2], B))
+        self.assertTrue(self.job_has_changes(self.builds[2], C))
+        self.assertFalse(self.job_has_changes(self.builds[2], A))
+
+        self.assertTrue(self.job_has_changes(self.builds[4], B))
+        self.assertTrue(self.job_has_changes(self.builds[4], C))
+        self.assertTrue(self.job_has_changes(self.builds[4], A))
+
+        self.worker.release()
+        self.waitUntilSettled()
+
+        self.assertEqual(A.data['status'], 'MERGED')
+        self.assertEqual(A.reported, 2)
+        self.assertEqual(B.data['status'], 'MERGED')
+        self.assertEqual(B.reported, 2)
+        self.assertEqual(C.data['status'], 'MERGED')
+        self.assertEqual(C.reported, 2)
+
+        client.shutdown()
+        self.assertEqual(r, True)
+
+    def test_client_promote_dependent(self):
+        "Test that the RPC client can promote a dependent change"
+        # C (depends on B) -> B -> A ; then promote C to get:
+        # A -> C (depends on B) -> B
+        self.worker.hold_jobs_in_build = True
+        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
+        B = self.fake_gerrit.addFakeChange('org/project', 'master', 'B')
+        C = self.fake_gerrit.addFakeChange('org/project', 'master', 'C')
+
+        C.setDependsOn(B, 1)
+
+        A.addApproval('CRVW', 2)
+        B.addApproval('CRVW', 2)
+        C.addApproval('CRVW', 2)
+
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
+        self.fake_gerrit.addEvent(C.addApproval('APRV', 1))
+
+        self.waitUntilSettled()
+
+        client = zuul.rpcclient.RPCClient('127.0.0.1',
+                                          self.gearman_server.port)
+        r = client.promote(pipeline='gate',
+                           change_ids=['3,1'])
+
+        self.waitUntilSettled()
+        self.worker.release('.*-merge')
+        self.waitUntilSettled()
+        self.worker.release('.*-merge')
+        self.waitUntilSettled()
+        self.worker.release('.*-merge')
+        self.waitUntilSettled()
+
+        self.assertEqual(len(self.builds), 6)
+        self.assertEqual(self.builds[0].name, 'project-test1')
+        self.assertEqual(self.builds[1].name, 'project-test2')
+        self.assertEqual(self.builds[2].name, 'project-test1')
+        self.assertEqual(self.builds[3].name, 'project-test2')
+        self.assertEqual(self.builds[4].name, 'project-test1')
+        self.assertEqual(self.builds[5].name, 'project-test2')
+
+        self.assertTrue(self.job_has_changes(self.builds[0], B))
+        self.assertFalse(self.job_has_changes(self.builds[0], A))
+        self.assertFalse(self.job_has_changes(self.builds[0], C))
+
+        self.assertTrue(self.job_has_changes(self.builds[2], B))
+        self.assertTrue(self.job_has_changes(self.builds[2], C))
+        self.assertFalse(self.job_has_changes(self.builds[2], A))
+
+        self.assertTrue(self.job_has_changes(self.builds[4], B))
+        self.assertTrue(self.job_has_changes(self.builds[4], C))
+        self.assertTrue(self.job_has_changes(self.builds[4], A))
+
+        self.worker.release()
+        self.waitUntilSettled()
+
+        self.assertEqual(A.data['status'], 'MERGED')
+        self.assertEqual(A.reported, 2)
+        self.assertEqual(B.data['status'], 'MERGED')
+        self.assertEqual(B.reported, 2)
+        self.assertEqual(C.data['status'], 'MERGED')
+        self.assertEqual(C.reported, 2)
+
+        client.shutdown()
+        self.assertEqual(r, True)
+
+    def test_client_promote_negative(self):
+        "Test that the RPC client returns errors for promotion"
+        self.worker.hold_jobs_in_build = True
+        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
+        A.addApproval('CRVW', 2)
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+        self.waitUntilSettled()
+
+        client = zuul.rpcclient.RPCClient('127.0.0.1',
+                                          self.gearman_server.port)
+
+        with testtools.ExpectedException(zuul.rpcclient.RPCFailure):
+            r = client.promote(pipeline='nonexistent',
+                               change_ids=['2,1', '3,1'])
+            client.shutdown()
+            self.assertEqual(r, False)
+
+        with testtools.ExpectedException(zuul.rpcclient.RPCFailure):
+            r = client.promote(pipeline='gate',
+                               change_ids=['4,1'])
+            client.shutdown()
+            self.assertEqual(r, False)
+
+        self.worker.hold_jobs_in_build = False
+        self.worker.release()
+        self.waitUntilSettled()
+
+    def test_queue_rate_limiting(self):
+        "Test that DependentPipelines are rate limited with dep across window"
+        self.config.set('zuul', 'layout_config',
+                        'tests/fixtures/layout-rate-limit.yaml')
+        self.sched.reconfigure(self.config)
+        self.worker.hold_jobs_in_build = True
+        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
+        B = self.fake_gerrit.addFakeChange('org/project', 'master', 'B')
+        C = self.fake_gerrit.addFakeChange('org/project', 'master', 'C')
+
+        C.setDependsOn(B, 1)
+        self.worker.addFailTest('project-test1', A)
+
+        A.addApproval('CRVW', 2)
+        B.addApproval('CRVW', 2)
+        C.addApproval('CRVW', 2)
+
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
+        self.fake_gerrit.addEvent(C.addApproval('APRV', 1))
+        self.waitUntilSettled()
+
+        # Only A and B will have their merge jobs queued because
+        # window is 2.
+        self.assertEqual(len(self.builds), 2)
+        self.assertEqual(self.builds[0].name, 'project-merge')
+        self.assertEqual(self.builds[1].name, 'project-merge')
+
+        self.worker.release('.*-merge')
+        self.waitUntilSettled()
+        self.worker.release('.*-merge')
+        self.waitUntilSettled()
+
+        # Only A and B will have their test jobs queued because
+        # window is 2.
+        self.assertEqual(len(self.builds), 4)
+        self.assertEqual(self.builds[0].name, 'project-test1')
+        self.assertEqual(self.builds[1].name, 'project-test2')
+        self.assertEqual(self.builds[2].name, 'project-test1')
+        self.assertEqual(self.builds[3].name, 'project-test2')
+
+        self.worker.release('project-.*')
+        self.waitUntilSettled()
+
+        queue = self.sched.layout.pipelines['gate'].queues[0]
+        # A failed so window is reduced by 1 to 1.
+        self.assertEqual(queue.window, 1)
+        self.assertEqual(queue.window_floor, 1)
+        self.assertEqual(A.data['status'], 'NEW')
+
+        # Gate is reset and only B's merge job is queued because
+        # window shrunk to 1.
+        self.assertEqual(len(self.builds), 1)
+        self.assertEqual(self.builds[0].name, 'project-merge')
+
+        self.worker.release('.*-merge')
+        self.waitUntilSettled()
+
+        # Only B's test jobs are queued because window is still 1.
+        self.assertEqual(len(self.builds), 2)
+        self.assertEqual(self.builds[0].name, 'project-test1')
+        self.assertEqual(self.builds[1].name, 'project-test2')
+
+        self.worker.release('project-.*')
+        self.waitUntilSettled()
+
+        # B was successfully merged so window is increased to 2.
+        self.assertEqual(queue.window, 2)
+        self.assertEqual(queue.window_floor, 1)
+        self.assertEqual(B.data['status'], 'MERGED')
+
+        # Only C is left and its merge job is queued.
+        self.assertEqual(len(self.builds), 1)
+        self.assertEqual(self.builds[0].name, 'project-merge')
+
+        self.worker.release('.*-merge')
+        self.waitUntilSettled()
+
+        # After successful merge job the test jobs for C are queued.
+        self.assertEqual(len(self.builds), 2)
+        self.assertEqual(self.builds[0].name, 'project-test1')
+        self.assertEqual(self.builds[1].name, 'project-test2')
+
+        self.worker.release('project-.*')
+        self.waitUntilSettled()
+
+        # C successfully merged so window is bumped to 3.
+        self.assertEqual(queue.window, 3)
+        self.assertEqual(queue.window_floor, 1)
+        self.assertEqual(C.data['status'], 'MERGED')
+
+    def test_queue_rate_limiting_dependent(self):
+        "Test that DependentPipelines are rate limited with dep in window"
+        self.config.set('zuul', 'layout_config',
+                        'tests/fixtures/layout-rate-limit.yaml')
+        self.sched.reconfigure(self.config)
+        self.worker.hold_jobs_in_build = True
+        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
+        B = self.fake_gerrit.addFakeChange('org/project', 'master', 'B')
+        C = self.fake_gerrit.addFakeChange('org/project', 'master', 'C')
+
+        B.setDependsOn(A, 1)
+
+        self.worker.addFailTest('project-test1', A)
+
+        A.addApproval('CRVW', 2)
+        B.addApproval('CRVW', 2)
+        C.addApproval('CRVW', 2)
+
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
+        self.fake_gerrit.addEvent(C.addApproval('APRV', 1))
+        self.waitUntilSettled()
+
+        # Only A and B will have their merge jobs queued because
+        # window is 2.
+        self.assertEqual(len(self.builds), 2)
+        self.assertEqual(self.builds[0].name, 'project-merge')
+        self.assertEqual(self.builds[1].name, 'project-merge')
+
+        self.worker.release('.*-merge')
+        self.waitUntilSettled()
+        self.worker.release('.*-merge')
+        self.waitUntilSettled()
+
+        # Only A and B will have their test jobs queued because
+        # window is 2.
+        self.assertEqual(len(self.builds), 4)
+        self.assertEqual(self.builds[0].name, 'project-test1')
+        self.assertEqual(self.builds[1].name, 'project-test2')
+        self.assertEqual(self.builds[2].name, 'project-test1')
+        self.assertEqual(self.builds[3].name, 'project-test2')
+
+        self.worker.release('project-.*')
+        self.waitUntilSettled()
+
+        queue = self.sched.layout.pipelines['gate'].queues[0]
+        # A failed so window is reduced by 1 to 1.
+        self.assertEqual(queue.window, 1)
+        self.assertEqual(queue.window_floor, 1)
+        self.assertEqual(A.data['status'], 'NEW')
+        self.assertEqual(B.data['status'], 'NEW')
+
+        # Gate is reset and only C's merge job is queued because
+        # window shrunk to 1 and A and B were dequeued.
+        self.assertEqual(len(self.builds), 1)
+        self.assertEqual(self.builds[0].name, 'project-merge')
+
+        self.worker.release('.*-merge')
+        self.waitUntilSettled()
+
+        # Only C's test jobs are queued because window is still 1.
+        self.assertEqual(len(self.builds), 2)
+        self.assertEqual(self.builds[0].name, 'project-test1')
+        self.assertEqual(self.builds[1].name, 'project-test2')
+
+        self.worker.release('project-.*')
+        self.waitUntilSettled()
+
+        # C was successfully merged so window is increased to 2.
+        self.assertEqual(queue.window, 2)
+        self.assertEqual(queue.window_floor, 1)
+        self.assertEqual(C.data['status'], 'MERGED')
+
+    def test_worker_update_metadata(self):
+        "Test if a worker can send back metadata about itself"
+        self.worker.hold_jobs_in_build = True
+
+        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
+        A.addApproval('CRVW', 2)
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+        self.waitUntilSettled()
+
+        self.assertEqual(len(self.launcher.builds), 1)
+
+        self.log.debug('Current builds:')
+        self.log.debug(self.launcher.builds)
+
+        start = time.time()
+        while True:
+            if time.time() - start > 10:
+                raise Exception("Timeout waiting for gearman server to report "
+                                + "back to the client")
+            build = self.launcher.builds.values()[0]
+            if build.worker.name == "My Worker":
+                break
+            else:
+                time.sleep(0)
+
+        self.log.debug(build)
+        self.assertEqual("My Worker", build.worker.name)
+        self.assertEqual("localhost", build.worker.hostname)
+        self.assertEqual(['127.0.0.1', '192.168.1.1'], build.worker.ips)
+        self.assertEqual("zuul.example.org", build.worker.fqdn)
+        self.assertEqual("FakeBuilder", build.worker.program)
+        self.assertEqual("v1.1", build.worker.version)
+        self.assertEqual({'something': 'else'}, build.worker.extra)
+
+        self.worker.hold_jobs_in_build = False
+        self.worker.release()
+        self.waitUntilSettled()
+
+    def test_footer_message(self):
+        "Test a pipeline's footer message is correctly added to the report."
+        self.config.set('zuul', 'layout_config',
+                        'tests/fixtures/layout-footer-message.yaml')
+        self.sched.reconfigure(self.config)
+        self.registerJobs()
+
+        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
+        A.addApproval('CRVW', 2)
+        self.worker.addFailTest('test1', A)
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+        self.waitUntilSettled()
+
+        B = self.fake_gerrit.addFakeChange('org/project', 'master', 'B')
+        B.addApproval('CRVW', 2)
+        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
+        self.waitUntilSettled()
+
+        self.assertEqual(2, len(self.smtp_messages))
+
+        failure_body = """\
+Build failed.  For information on how to proceed, see \
+http://wiki.example.org/Test_Failures
+
+- test1 http://logs.example.com/1/1/gate/test1/0 : FAILURE in 0s
+- test2 http://logs.example.com/1/1/gate/test2/1 : SUCCESS in 0s
+
+For CI problems and help debugging, contact ci@example.org"""
+
+        success_body = """\
+Build succeeded.
+
+- test1 http://logs.example.com/2/1/gate/test1/2 : SUCCESS in 0s
+- test2 http://logs.example.com/2/1/gate/test2/3 : SUCCESS in 0s
+
+For CI problems and help debugging, contact ci@example.org"""
+
+        self.assertEqual(failure_body, self.smtp_messages[0]['body'])
+        self.assertEqual(success_body, self.smtp_messages[1]['body'])
+
+    def test_merge_failure_reporters(self):
+        """Check that the config is set up correctly"""
+
+        self.config.set('zuul', 'layout_config',
+                        'tests/fixtures/layout-merge-failure.yaml')
+        self.sched.reconfigure(self.config)
+        self.registerJobs()
+
+        self.assertEqual(
+            "Merge Failed.\n\nThis change or one of its cross-repo "
+            "dependencies was unable to be automatically merged with the "
+            "current state of its repository. Please rebase the change and "
+            "upload a new patchset.",
+            self.sched.layout.pipelines['check'].merge_failure_message)
+        self.assertEqual(
+            "The merge failed! For more information...",
+            self.sched.layout.pipelines['gate'].merge_failure_message)
+
+        self.assertEqual(
+            len(self.sched.layout.pipelines['check'].merge_failure_actions), 1)
+        self.assertEqual(
+            len(self.sched.layout.pipelines['gate'].merge_failure_actions), 2)
+
+        self.assertTrue(isinstance(
+            self.sched.layout.pipelines['check'].merge_failure_actions[0],
+            zuul.reporter.gerrit.GerritReporter))
+
+        self.assertTrue(
+            (
+                isinstance(self.sched.layout.pipelines['gate'].
+                           merge_failure_actions[0],
+                           zuul.reporter.smtp.SMTPReporter) and
+                isinstance(self.sched.layout.pipelines['gate'].
+                           merge_failure_actions[1],
+                           zuul.reporter.gerrit.GerritReporter)
+            ) or (
+                isinstance(self.sched.layout.pipelines['gate'].
+                           merge_failure_actions[0],
+                           zuul.reporter.gerrit.GerritReporter) and
+                isinstance(self.sched.layout.pipelines['gate'].
+                           merge_failure_actions[1],
+                           zuul.reporter.smtp.SMTPReporter)
+            )
+        )
+
+    def test_merge_failure_reports(self):
+        """Check that when a change fails to merge the correct message is sent
+        to the correct reporter"""
+        self.config.set('zuul', 'layout_config',
+                        'tests/fixtures/layout-merge-failure.yaml')
+        self.sched.reconfigure(self.config)
+        self.registerJobs()
+
+        # Check a test failure isn't reported to SMTP
+        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
+        A.addApproval('CRVW', 2)
+        self.worker.addFailTest('project-test1', A)
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+        self.waitUntilSettled()
+
+        self.assertEqual(3, len(self.history))  # 3 jobs
+        self.assertEqual(0, len(self.smtp_messages))
+
+        # Check a merge failure is reported to SMTP
+        # B should be merged, but C will conflict with B
+        B = self.fake_gerrit.addFakeChange('org/project', 'master', 'B')
+        B.addPatchset(['conflict'])
+        C = self.fake_gerrit.addFakeChange('org/project', 'master', 'C')
+        C.addPatchset(['conflict'])
+        B.addApproval('CRVW', 2)
+        C.addApproval('CRVW', 2)
+        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
+        self.fake_gerrit.addEvent(C.addApproval('APRV', 1))
+        self.waitUntilSettled()
+
+        self.assertEqual(6, len(self.history))  # A and B jobs
+        self.assertEqual(1, len(self.smtp_messages))
+        self.assertEqual('The merge failed! For more information...',
+                         self.smtp_messages[0]['body'])
+
+    def test_default_merge_failure_reports(self):
+        """Check that the default merge failure reports are correct."""
+
+        # A should report success, B should report merge failure.
+        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
+        A.addPatchset(['conflict'])
+        B = self.fake_gerrit.addFakeChange('org/project', 'master', 'B')
+        B.addPatchset(['conflict'])
+        A.addApproval('CRVW', 2)
+        B.addApproval('CRVW', 2)
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
+        self.waitUntilSettled()
+
+        self.assertEqual(3, len(self.history))  # A jobs
+        self.assertEqual(A.reported, 2)
+        self.assertEqual(B.reported, 2)
+        self.assertEqual(A.data['status'], 'MERGED')
+        self.assertEqual(B.data['status'], 'NEW')
+        self.assertIn('Build succeeded', A.messages[1])
+        self.assertIn('Merge Failed', B.messages[1])
+        self.assertIn('automatically merged', B.messages[1])
+        self.assertNotIn('logs.example.com', B.messages[1])
+        self.assertNotIn('SKIPPED', B.messages[1])
+
+    def test_swift_instructions(self):
+        "Test that the correct swift instructions are sent to the workers"
+        self.config.set('zuul', 'layout_config',
+                        'tests/fixtures/layout-swift.yaml')
+        self.sched.reconfigure(self.config)
+        self.registerJobs()
+
+        self.worker.hold_jobs_in_build = True
+        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
+
+        A.addApproval('CRVW', 2)
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+        self.waitUntilSettled()
+
+        self.assertEqual(
+            "https://storage.example.org/V1/AUTH_account/merge_logs/1/1/1/"
+            "gate/test-merge/",
+            self.builds[0].parameters['SWIFT_logs_URL'][:-7])
+        self.assertEqual(5,
+                         len(self.builds[0].parameters['SWIFT_logs_HMAC_BODY'].
+                             split('\n')))
+        self.assertIn('SWIFT_logs_SIGNATURE', self.builds[0].parameters)
+
+        self.assertEqual(
+            "https://storage.example.org/V1/AUTH_account/logs/1/1/1/"
+            "gate/test-test/",
+            self.builds[1].parameters['SWIFT_logs_URL'][:-7])
+        self.assertEqual(5,
+                         len(self.builds[1].parameters['SWIFT_logs_HMAC_BODY'].
+                             split('\n')))
+        self.assertIn('SWIFT_logs_SIGNATURE', self.builds[1].parameters)
+
+        self.assertEqual(
+            "https://storage.example.org/V1/AUTH_account/stash/1/1/1/"
+            "gate/test-test/",
+            self.builds[1].parameters['SWIFT_MOSTLY_URL'][:-7])
+        self.assertEqual(5,
+                         len(self.builds[1].
+                             parameters['SWIFT_MOSTLY_HMAC_BODY'].split('\n')))
+        self.assertIn('SWIFT_MOSTLY_SIGNATURE', self.builds[1].parameters)
+
+        self.worker.hold_jobs_in_build = False
+        self.worker.release()
+        self.waitUntilSettled()
+
+    def test_client_get_running_jobs(self):
+        "Test that the RPC client can get a list of running jobs"
+        self.worker.hold_jobs_in_build = True
+        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
+        A.addApproval('CRVW', 2)
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+        self.waitUntilSettled()
+
+        client = zuul.rpcclient.RPCClient('127.0.0.1',
+                                          self.gearman_server.port)
+
+        # Wait for gearman server to send the initial workData back to zuul
+        start = time.time()
+        while True:
+            if time.time() - start > 10:
+                raise Exception("Timeout waiting for gearman server to report "
+                                + "back to the client")
+            build = self.launcher.builds.values()[0]
+            if build.worker.name == "My Worker":
+                break
+            else:
+                time.sleep(0)
+
+        running_items = client.get_running_jobs()
+
+        self.assertEqual(1, len(running_items))
+        running_item = running_items[0]
+        self.assertEqual([], running_item['failing_reasons'])
+        self.assertEqual([], running_item['items_behind'])
+        self.assertEqual('https://hostname/1', running_item['url'])
+        self.assertEqual(None, running_item['item_ahead'])
+        self.assertEqual('org/project', running_item['project'])
+        self.assertEqual(None, running_item['remaining_time'])
+        self.assertEqual(True, running_item['active'])
+        self.assertEqual('1,1', running_item['id'])
+
+        self.assertEqual(3, len(running_item['jobs']))
+        for job in running_item['jobs']:
+            if job['name'] == 'project-merge':
+                self.assertEqual('project-merge', job['name'])
+                self.assertEqual('gate', job['pipeline'])
+                self.assertEqual(False, job['retry'])
+                self.assertEqual('https://server/job/project-merge/0/',
+                                 job['url'])
+                self.assertEqual(7, len(job['worker']))
+                self.assertEqual(False, job['canceled'])
+                self.assertEqual(True, job['voting'])
+                self.assertEqual(None, job['result'])
+                self.assertEqual('gate', job['pipeline'])
+                break
+
+        self.worker.hold_jobs_in_build = False
+        self.worker.release()
+        self.waitUntilSettled()
+
+        running_items = client.get_running_jobs()
+        self.assertEqual(0, len(running_items))
+
+    def test_nonvoting_pipeline(self):
+        "Test that a nonvoting pipeline (experimental) can still report"
+
+        A = self.fake_gerrit.addFakeChange('org/experimental-project',
+                                           'master', 'A')
+        self.fake_gerrit.addEvent(A.getPatchsetCreatedEvent(1))
+        self.waitUntilSettled()
+        self.assertEqual(
+            self.getJobFromHistory('experimental-project-test').result,
+            'SUCCESS')
+        self.assertEqual(A.reported, 1)
+
+    def test_crd_gate(self):
+        "Test cross-repo dependencies"
+        A = self.fake_gerrit.addFakeChange('org/project1', 'master', 'A')
+        B = self.fake_gerrit.addFakeChange('org/project2', 'master', 'B')
+        A.addApproval('CRVW', 2)
+        B.addApproval('CRVW', 2)
+
+        AM2 = self.fake_gerrit.addFakeChange('org/project1', 'master', 'AM2')
+        AM1 = self.fake_gerrit.addFakeChange('org/project1', 'master', 'AM1')
+        AM2.setMerged()
+        AM1.setMerged()
+
+        BM2 = self.fake_gerrit.addFakeChange('org/project2', 'master', 'BM2')
+        BM1 = self.fake_gerrit.addFakeChange('org/project2', 'master', 'BM1')
+        BM2.setMerged()
+        BM1.setMerged()
+
+        # A -> AM1 -> AM2
+        # B -> BM1 -> BM2
+        # A Depends-On: B
+        # M2 is here to make sure it is never queried.  If it is, it
+        # means zuul is walking down the entire history of merged
+        # changes.
+
+        B.setDependsOn(BM1, 1)
+        BM1.setDependsOn(BM2, 1)
+
+        A.setDependsOn(AM1, 1)
+        AM1.setDependsOn(AM2, 1)
+
+        A.data['commitMessage'] = '%s\n\nDepends-On: %s\n' % (
+            A.subject, B.data['id'])
+
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+        self.waitUntilSettled()
+
+        self.assertEqual(A.data['status'], 'NEW')
+        self.assertEqual(B.data['status'], 'NEW')
+
+        for connection in self.connections.values():
+            connection.maintainCache([])
+
+        self.worker.hold_jobs_in_build = True
+        B.addApproval('APRV', 1)
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+        self.waitUntilSettled()
+
+        self.worker.release('.*-merge')
+        self.waitUntilSettled()
+        self.worker.release('.*-merge')
+        self.waitUntilSettled()
+        self.worker.hold_jobs_in_build = False
+        self.worker.release()
+        self.waitUntilSettled()
+
+        self.assertEqual(AM2.queried, 0)
+        self.assertEqual(BM2.queried, 0)
+        self.assertEqual(A.data['status'], 'MERGED')
+        self.assertEqual(B.data['status'], 'MERGED')
+        self.assertEqual(A.reported, 2)
+        self.assertEqual(B.reported, 2)
+
+        self.assertEqual(self.getJobFromHistory('project1-merge').changes,
+                         '2,1 1,1')
+
+    def test_crd_branch(self):
+        "Test cross-repo dependencies in multiple branches"
+        A = self.fake_gerrit.addFakeChange('org/project1', 'master', 'A')
+        B = self.fake_gerrit.addFakeChange('org/project2', 'master', 'B')
+        C = self.fake_gerrit.addFakeChange('org/project2', 'mp', 'C')
+        C.data['id'] = B.data['id']
+        A.addApproval('CRVW', 2)
+        B.addApproval('CRVW', 2)
+        C.addApproval('CRVW', 2)
+
+        # A Depends-On: B+C
+        A.data['commitMessage'] = '%s\n\nDepends-On: %s\n' % (
+            A.subject, B.data['id'])
+
+        self.worker.hold_jobs_in_build = True
+        B.addApproval('APRV', 1)
+        C.addApproval('APRV', 1)
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+        self.waitUntilSettled()
+
+        self.worker.release('.*-merge')
+        self.waitUntilSettled()
+        self.worker.release('.*-merge')
+        self.waitUntilSettled()
+        self.worker.release('.*-merge')
+        self.waitUntilSettled()
+        self.worker.hold_jobs_in_build = False
+        self.worker.release()
+        self.waitUntilSettled()
+
+        self.assertEqual(A.data['status'], 'MERGED')
+        self.assertEqual(B.data['status'], 'MERGED')
+        self.assertEqual(C.data['status'], 'MERGED')
+        self.assertEqual(A.reported, 2)
+        self.assertEqual(B.reported, 2)
+        self.assertEqual(C.reported, 2)
+
+        self.assertEqual(self.getJobFromHistory('project1-merge').changes,
+                         '2,1 3,1 1,1')
+
+    def test_crd_multiline(self):
+        "Test multiple depends-on lines in commit"
+        A = self.fake_gerrit.addFakeChange('org/project1', 'master', 'A')
+        B = self.fake_gerrit.addFakeChange('org/project2', 'master', 'B')
+        C = self.fake_gerrit.addFakeChange('org/project2', 'master', 'C')
+        A.addApproval('CRVW', 2)
+        B.addApproval('CRVW', 2)
+        C.addApproval('CRVW', 2)
+
+        # A Depends-On: B+C
+        A.data['commitMessage'] = '%s\n\nDepends-On: %s\nDepends-On: %s\n' % (
+            A.subject, B.data['id'], C.data['id'])
+
+        self.worker.hold_jobs_in_build = True
+        B.addApproval('APRV', 1)
+        C.addApproval('APRV', 1)
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+        self.waitUntilSettled()
+
+        self.worker.release('.*-merge')
+        self.waitUntilSettled()
+        self.worker.release('.*-merge')
+        self.waitUntilSettled()
+        self.worker.release('.*-merge')
+        self.waitUntilSettled()
+        self.worker.hold_jobs_in_build = False
+        self.worker.release()
+        self.waitUntilSettled()
+
+        self.assertEqual(A.data['status'], 'MERGED')
+        self.assertEqual(B.data['status'], 'MERGED')
+        self.assertEqual(C.data['status'], 'MERGED')
+        self.assertEqual(A.reported, 2)
+        self.assertEqual(B.reported, 2)
+        self.assertEqual(C.reported, 2)
+
+        self.assertEqual(self.getJobFromHistory('project1-merge').changes,
+                         '2,1 3,1 1,1')
+
+    def test_crd_unshared_gate(self):
+        "Test cross-repo dependencies in unshared gate queues"
+        A = self.fake_gerrit.addFakeChange('org/project1', 'master', 'A')
+        B = self.fake_gerrit.addFakeChange('org/project', 'master', 'B')
+        A.addApproval('CRVW', 2)
+        B.addApproval('CRVW', 2)
+
+        # A Depends-On: B
+        A.data['commitMessage'] = '%s\n\nDepends-On: %s\n' % (
+            A.subject, B.data['id'])
+
+        # A and B do not share a queue, make sure that A is unable to
+        # enqueue B (and therefore, A is unable to be enqueued).
+        B.addApproval('APRV', 1)
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+        self.waitUntilSettled()
+
+        self.assertEqual(A.data['status'], 'NEW')
+        self.assertEqual(B.data['status'], 'NEW')
+        self.assertEqual(A.reported, 0)
+        self.assertEqual(B.reported, 0)
+        self.assertEqual(len(self.history), 0)
+
+        # Enqueue and merge B alone.
+        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
+        self.waitUntilSettled()
+
+        self.assertEqual(B.data['status'], 'MERGED')
+        self.assertEqual(B.reported, 2)
+
+        # Now that B is merged, A should be able to be enqueued and
+        # merged.
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+        self.waitUntilSettled()
+
+        self.assertEqual(A.data['status'], 'MERGED')
+        self.assertEqual(A.reported, 2)
+
+    def test_crd_gate_reverse(self):
+        "Test reverse cross-repo dependencies"
+        A = self.fake_gerrit.addFakeChange('org/project1', 'master', 'A')
+        B = self.fake_gerrit.addFakeChange('org/project2', 'master', 'B')
+        A.addApproval('CRVW', 2)
+        B.addApproval('CRVW', 2)
+
+        # A Depends-On: B
+
+        A.data['commitMessage'] = '%s\n\nDepends-On: %s\n' % (
+            A.subject, B.data['id'])
+
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+        self.waitUntilSettled()
+
+        self.assertEqual(A.data['status'], 'NEW')
+        self.assertEqual(B.data['status'], 'NEW')
+
+        self.worker.hold_jobs_in_build = True
+        A.addApproval('APRV', 1)
+        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
+        self.waitUntilSettled()
+
+        self.worker.release('.*-merge')
+        self.waitUntilSettled()
+        self.worker.release('.*-merge')
+        self.waitUntilSettled()
+        self.worker.hold_jobs_in_build = False
+        self.worker.release()
+        self.waitUntilSettled()
+
+        self.assertEqual(A.data['status'], 'MERGED')
+        self.assertEqual(B.data['status'], 'MERGED')
+        self.assertEqual(A.reported, 2)
+        self.assertEqual(B.reported, 2)
+
+        self.assertEqual(self.getJobFromHistory('project1-merge').changes,
+                         '2,1 1,1')
+
+    def test_crd_cycle(self):
+        "Test cross-repo dependency cycles"
+        A = self.fake_gerrit.addFakeChange('org/project1', 'master', 'A')
+        B = self.fake_gerrit.addFakeChange('org/project2', 'master', 'B')
+        A.addApproval('CRVW', 2)
+        B.addApproval('CRVW', 2)
+
+        # A -> B -> A (via commit-depends)
+
+        A.data['commitMessage'] = '%s\n\nDepends-On: %s\n' % (
+            A.subject, B.data['id'])
+        B.data['commitMessage'] = '%s\n\nDepends-On: %s\n' % (
+            B.subject, A.data['id'])
+
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+        self.waitUntilSettled()
+
+        self.assertEqual(A.reported, 0)
+        self.assertEqual(B.reported, 0)
+        self.assertEqual(A.data['status'], 'NEW')
+        self.assertEqual(B.data['status'], 'NEW')
+
+    def test_crd_gate_unknown(self):
+        "Test unknown projects in dependent pipeline"
+        self.init_repo("org/unknown")
+        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
+        B = self.fake_gerrit.addFakeChange('org/unknown', 'master', 'B')
+        A.addApproval('CRVW', 2)
+        B.addApproval('CRVW', 2)
+
+        # A Depends-On: B
+        A.data['commitMessage'] = '%s\n\nDepends-On: %s\n' % (
+            A.subject, B.data['id'])
+
+        B.addApproval('APRV', 1)
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+        self.waitUntilSettled()
+
+        # Unknown projects cannot share a queue with any other
+        # since they don't have common jobs with any other (they have no jobs).
+        # Changes which depend on unknown project changes
+        # should not be processed in dependent pipeline
+        self.assertEqual(A.data['status'], 'NEW')
+        self.assertEqual(B.data['status'], 'NEW')
+        self.assertEqual(A.reported, 0)
+        self.assertEqual(B.reported, 0)
+        self.assertEqual(len(self.history), 0)
+
+        # Simulate change B being gated outside this layout
+        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
+        B.setMerged()
+        self.waitUntilSettled()
+        self.assertEqual(len(self.history), 0)
+
+        # Now that B is merged, A should be able to be enqueued and
+        # merged.
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+        self.waitUntilSettled()
+
+        self.assertEqual(A.data['status'], 'MERGED')
+        self.assertEqual(A.reported, 2)
+        self.assertEqual(B.data['status'], 'MERGED')
+        self.assertEqual(B.reported, 0)
+
+    def test_crd_check(self):
+        "Test cross-repo dependencies in independent pipelines"
+
+        self.gearman_server.hold_jobs_in_queue = True
+        A = self.fake_gerrit.addFakeChange('org/project1', 'master', 'A')
+        B = self.fake_gerrit.addFakeChange('org/project2', 'master', 'B')
+
+        # A Depends-On: B
+        A.data['commitMessage'] = '%s\n\nDepends-On: %s\n' % (
+            A.subject, B.data['id'])
+
+        self.fake_gerrit.addEvent(A.getPatchsetCreatedEvent(1))
+        self.waitUntilSettled()
+
+        queue = self.gearman_server.getQueue()
+        ref = self.getParameter(queue[-1], 'ZUUL_REF')
+        self.gearman_server.hold_jobs_in_queue = False
+        self.gearman_server.release()
+        self.waitUntilSettled()
+
+        path = os.path.join(self.git_root, "org/project1")
+        repo = git.Repo(path)
+        repo_messages = [c.message.strip() for c in repo.iter_commits(ref)]
+        repo_messages.reverse()
+        correct_messages = ['initial commit', 'A-1']
+        self.assertEqual(repo_messages, correct_messages)
+
+        path = os.path.join(self.git_root, "org/project2")
+        repo = git.Repo(path)
+        repo_messages = [c.message.strip() for c in repo.iter_commits(ref)]
+        repo_messages.reverse()
+        correct_messages = ['initial commit', 'B-1']
+        self.assertEqual(repo_messages, correct_messages)
+
+        self.assertEqual(A.data['status'], 'NEW')
+        self.assertEqual(B.data['status'], 'NEW')
+        self.assertEqual(A.reported, 1)
+        self.assertEqual(B.reported, 0)
+
+        self.assertEqual(self.history[0].changes, '2,1 1,1')
+        self.assertEqual(len(self.sched.layout.pipelines['check'].queues), 0)
+
+    def test_crd_check_git_depends(self):
+        "Test single-repo dependencies in independent pipelines"
+        self.gearman_server.hold_jobs_in_build = True
+        A = self.fake_gerrit.addFakeChange('org/project1', 'master', 'A')
+        B = self.fake_gerrit.addFakeChange('org/project1', 'master', 'B')
+
+        # Add two git-dependent changes and make sure they both report
+        # success.
+        B.setDependsOn(A, 1)
+        self.fake_gerrit.addEvent(A.getPatchsetCreatedEvent(1))
+        self.waitUntilSettled()
+        self.fake_gerrit.addEvent(B.getPatchsetCreatedEvent(1))
+        self.waitUntilSettled()
+
+        self.orderedRelease()
+        self.gearman_server.hold_jobs_in_build = False
+        self.waitUntilSettled()
+
+        self.assertEqual(A.data['status'], 'NEW')
+        self.assertEqual(B.data['status'], 'NEW')
+        self.assertEqual(A.reported, 1)
+        self.assertEqual(B.reported, 1)
+
+        self.assertEqual(self.history[0].changes, '1,1')
+        self.assertEqual(self.history[-1].changes, '1,1 2,1')
+        self.assertEqual(len(self.sched.layout.pipelines['check'].queues), 0)
+
+        self.assertIn('Build succeeded', A.messages[0])
+        self.assertIn('Build succeeded', B.messages[0])
+
+    def test_crd_check_duplicate(self):
+        "Test duplicate check in independent pipelines"
+        self.worker.hold_jobs_in_build = True
+        A = self.fake_gerrit.addFakeChange('org/project1', 'master', 'A')
+        B = self.fake_gerrit.addFakeChange('org/project1', 'master', 'B')
+        check_pipeline = self.sched.layout.pipelines['check']
+
+        # Add two git-dependent changes...
+        B.setDependsOn(A, 1)
+        self.fake_gerrit.addEvent(B.getPatchsetCreatedEvent(1))
+        self.waitUntilSettled()
+        self.assertEqual(len(check_pipeline.getAllItems()), 2)
+
+        # ...make sure the live one is not duplicated...
+        self.fake_gerrit.addEvent(B.getPatchsetCreatedEvent(1))
+        self.waitUntilSettled()
+        self.assertEqual(len(check_pipeline.getAllItems()), 2)
+
+        # ...but the non-live one is able to be.
+        self.fake_gerrit.addEvent(A.getPatchsetCreatedEvent(1))
+        self.waitUntilSettled()
+        self.assertEqual(len(check_pipeline.getAllItems()), 3)
+
+        # Release jobs in order to avoid races with change A jobs
+        # finishing before change B jobs.
+        self.orderedRelease()
+        self.worker.hold_jobs_in_build = False
+        self.worker.release()
+        self.waitUntilSettled()
+
+        self.assertEqual(A.data['status'], 'NEW')
+        self.assertEqual(B.data['status'], 'NEW')
+        self.assertEqual(A.reported, 1)
+        self.assertEqual(B.reported, 1)
+
+        self.assertEqual(self.history[0].changes, '1,1 2,1')
+        self.assertEqual(self.history[1].changes, '1,1')
+        self.assertEqual(len(self.sched.layout.pipelines['check'].queues), 0)
+
+        self.assertIn('Build succeeded', A.messages[0])
+        self.assertIn('Build succeeded', B.messages[0])
+
+    def _test_crd_check_reconfiguration(self, project1, project2):
+        "Test cross-repo dependencies re-enqueued in independent pipelines"
+
+        self.gearman_server.hold_jobs_in_queue = True
+        A = self.fake_gerrit.addFakeChange(project1, 'master', 'A')
+        B = self.fake_gerrit.addFakeChange(project2, 'master', 'B')
+
+        # A Depends-On: B
+        A.data['commitMessage'] = '%s\n\nDepends-On: %s\n' % (
+            A.subject, B.data['id'])
+
+        self.fake_gerrit.addEvent(A.getPatchsetCreatedEvent(1))
+        self.waitUntilSettled()
+
+        self.sched.reconfigure(self.config)
+
+        # Make sure the items still share a change queue, and the
+        # first one is not live.
+        self.assertEqual(len(self.sched.layout.pipelines['check'].queues), 1)
+        queue = self.sched.layout.pipelines['check'].queues[0]
+        first_item = queue.queue[0]
+        for item in queue.queue:
+            self.assertEqual(item.queue, first_item.queue)
+        self.assertFalse(first_item.live)
+        self.assertTrue(queue.queue[1].live)
+
+        self.gearman_server.hold_jobs_in_queue = False
+        self.gearman_server.release()
+        self.waitUntilSettled()
+
+        self.assertEqual(A.data['status'], 'NEW')
+        self.assertEqual(B.data['status'], 'NEW')
+        self.assertEqual(A.reported, 1)
+        self.assertEqual(B.reported, 0)
+
+        self.assertEqual(self.history[0].changes, '2,1 1,1')
+        self.assertEqual(len(self.sched.layout.pipelines['check'].queues), 0)
+
+    def test_crd_check_reconfiguration(self):
+        self._test_crd_check_reconfiguration('org/project1', 'org/project2')
+
+    def test_crd_undefined_project(self):
+        """Test that undefined projects in dependencies are handled for
+        independent pipelines"""
+        # It's a hack for fake gerrit,
+        # as it implies repo creation upon the creation of any change
+        self.init_repo("org/unknown")
+        self._test_crd_check_reconfiguration('org/project1', 'org/unknown')
+
+    def test_crd_check_ignore_dependencies(self):
+        "Test cross-repo dependencies can be ignored"
+        self.config.set('zuul', 'layout_config',
+                        'tests/fixtures/layout-ignore-dependencies.yaml')
+        self.sched.reconfigure(self.config)
+        self.registerJobs()
+
+        self.gearman_server.hold_jobs_in_queue = True
+        A = self.fake_gerrit.addFakeChange('org/project1', 'master', 'A')
+        B = self.fake_gerrit.addFakeChange('org/project2', 'master', 'B')
+        C = self.fake_gerrit.addFakeChange('org/project2', 'master', 'C')
+
+        # A Depends-On: B
+        A.data['commitMessage'] = '%s\n\nDepends-On: %s\n' % (
+            A.subject, B.data['id'])
+        # C git-depends on B
+        C.setDependsOn(B, 1)
+        self.fake_gerrit.addEvent(A.getPatchsetCreatedEvent(1))
+        self.fake_gerrit.addEvent(B.getPatchsetCreatedEvent(1))
+        self.fake_gerrit.addEvent(C.getPatchsetCreatedEvent(1))
+        self.waitUntilSettled()
+
+        # Make sure none of the items share a change queue, and all
+        # are live.
+        check_pipeline = self.sched.layout.pipelines['check']
+        self.assertEqual(len(check_pipeline.queues), 3)
+        self.assertEqual(len(check_pipeline.getAllItems()), 3)
+        for item in check_pipeline.getAllItems():
+            self.assertTrue(item.live)
+
+        self.gearman_server.hold_jobs_in_queue = False
+        self.gearman_server.release()
+        self.waitUntilSettled()
+
+        self.assertEqual(A.data['status'], 'NEW')
+        self.assertEqual(B.data['status'], 'NEW')
+        self.assertEqual(C.data['status'], 'NEW')
+        self.assertEqual(A.reported, 1)
+        self.assertEqual(B.reported, 1)
+        self.assertEqual(C.reported, 1)
+
+        # Each job should have tested exactly one change
+        for job in self.history:
+            self.assertEqual(len(job.changes.split()), 1)
+
+    def test_crd_check_transitive(self):
+        "Test transitive cross-repo dependencies"
+        # Specifically, if A -> B -> C, and C gets a new patchset and
+        # A gets a new patchset, ensure the test of A,2 includes B,1
+        # and C,2 (not C,1 which would indicate stale data in the
+        # cache for B).
+        A = self.fake_gerrit.addFakeChange('org/project1', 'master', 'A')
+        B = self.fake_gerrit.addFakeChange('org/project2', 'master', 'B')
+        C = self.fake_gerrit.addFakeChange('org/project3', 'master', 'C')
+
+        # A Depends-On: B
+        A.data['commitMessage'] = '%s\n\nDepends-On: %s\n' % (
+            A.subject, B.data['id'])
+
+        # B Depends-On: C
+        B.data['commitMessage'] = '%s\n\nDepends-On: %s\n' % (
+            B.subject, C.data['id'])
+
+        self.fake_gerrit.addEvent(A.getPatchsetCreatedEvent(1))
+        self.waitUntilSettled()
+        self.assertEqual(self.history[-1].changes, '3,1 2,1 1,1')
+
+        self.fake_gerrit.addEvent(B.getPatchsetCreatedEvent(1))
+        self.waitUntilSettled()
+        self.assertEqual(self.history[-1].changes, '3,1 2,1')
+
+        self.fake_gerrit.addEvent(C.getPatchsetCreatedEvent(1))
+        self.waitUntilSettled()
+        self.assertEqual(self.history[-1].changes, '3,1')
+
+        C.addPatchset()
+        self.fake_gerrit.addEvent(C.getPatchsetCreatedEvent(2))
+        self.waitUntilSettled()
+        self.assertEqual(self.history[-1].changes, '3,2')
+
+        A.addPatchset()
+        self.fake_gerrit.addEvent(A.getPatchsetCreatedEvent(2))
+        self.waitUntilSettled()
+        self.assertEqual(self.history[-1].changes, '3,2 2,1 1,2')
+
+    def test_crd_cycle_join(self):
+        "Test an updated change creates a cycle"
+        A = self.fake_gerrit.addFakeChange('org/project2', 'master', 'A')
+
+        self.fake_gerrit.addEvent(A.getPatchsetCreatedEvent(1))
+        self.waitUntilSettled()
+
+        # Create B->A
+        B = self.fake_gerrit.addFakeChange('org/project1', 'master', 'B')
+        B.data['commitMessage'] = '%s\n\nDepends-On: %s\n' % (
+            B.subject, A.data['id'])
+        self.fake_gerrit.addEvent(B.getPatchsetCreatedEvent(1))
+        self.waitUntilSettled()
+
+        # Update A to add A->B (a cycle).
+        A.addPatchset()
+        A.data['commitMessage'] = '%s\n\nDepends-On: %s\n' % (
+            A.subject, B.data['id'])
+        # Normally we would submit the patchset-created event for
+        # processing here, however, we have no way of noting whether
+        # the dependency cycle detection correctly raised an
+        # exception, so instead, we reach into the source driver and
+        # call the method that would ultimately be called by the event
+        # processing.
+
+        source = self.sched.layout.pipelines['gate'].source
+        with testtools.ExpectedException(
+            Exception, "Dependency cycle detected"):
+            source._getChange(u'1', u'2', True)
+        self.log.debug("Got expected dependency cycle exception")
+
+        # Now if we update B to remove the depends-on, everything
+        # should be okay.  B; A->B
+
+        B.addPatchset()
+        B.data['commitMessage'] = '%s\n' % (B.subject,)
+        source._getChange(u'1', u'2', True)
+        source._getChange(u'2', u'2', True)
+
+    def test_disable_at(self):
+        "Test a pipeline will only report to the disabled trigger when failing"
+
+        self.config.set('zuul', 'layout_config',
+                        'tests/fixtures/layout-disable-at.yaml')
+        self.sched.reconfigure(self.config)
+
+        self.assertEqual(3, self.sched.layout.pipelines['check'].disable_at)
+        self.assertEqual(
+            0, self.sched.layout.pipelines['check']._consecutive_failures)
+        self.assertFalse(self.sched.layout.pipelines['check']._disabled)
+
+        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
+        B = self.fake_gerrit.addFakeChange('org/project', 'master', 'B')
+        C = self.fake_gerrit.addFakeChange('org/project', 'master', 'C')
+        D = self.fake_gerrit.addFakeChange('org/project', 'master', 'D')
+        E = self.fake_gerrit.addFakeChange('org/project', 'master', 'E')
+        F = self.fake_gerrit.addFakeChange('org/project', 'master', 'F')
+        G = self.fake_gerrit.addFakeChange('org/project', 'master', 'G')
+        H = self.fake_gerrit.addFakeChange('org/project', 'master', 'H')
+        I = self.fake_gerrit.addFakeChange('org/project', 'master', 'I')
+        J = self.fake_gerrit.addFakeChange('org/project', 'master', 'J')
+        K = self.fake_gerrit.addFakeChange('org/project', 'master', 'K')
+
+        self.worker.addFailTest('project-test1', A)
+        self.worker.addFailTest('project-test1', B)
+        # Let C pass, resetting the counter
+        self.worker.addFailTest('project-test1', D)
+        self.worker.addFailTest('project-test1', E)
+        self.worker.addFailTest('project-test1', F)
+        self.worker.addFailTest('project-test1', G)
+        self.worker.addFailTest('project-test1', H)
+        # I also passes but should only report to the disabled reporters
+        self.worker.addFailTest('project-test1', J)
+        self.worker.addFailTest('project-test1', K)
+
+        self.fake_gerrit.addEvent(A.getPatchsetCreatedEvent(1))
+        self.fake_gerrit.addEvent(B.getPatchsetCreatedEvent(1))
+        self.waitUntilSettled()
+
+        self.assertEqual(
+            2, self.sched.layout.pipelines['check']._consecutive_failures)
+        self.assertFalse(self.sched.layout.pipelines['check']._disabled)
+
+        self.fake_gerrit.addEvent(C.getPatchsetCreatedEvent(1))
+        self.waitUntilSettled()
+
+        self.assertEqual(
+            0, self.sched.layout.pipelines['check']._consecutive_failures)
+        self.assertFalse(self.sched.layout.pipelines['check']._disabled)
+
+        self.fake_gerrit.addEvent(D.getPatchsetCreatedEvent(1))
+        self.fake_gerrit.addEvent(E.getPatchsetCreatedEvent(1))
+        self.fake_gerrit.addEvent(F.getPatchsetCreatedEvent(1))
+        self.waitUntilSettled()
+
+        # We should be disabled now
+        self.assertEqual(
+            3, self.sched.layout.pipelines['check']._consecutive_failures)
+        self.assertTrue(self.sched.layout.pipelines['check']._disabled)
+
+        # We need to wait between each of these patches to make sure the
+        # smtp messages come back in an expected order
+        self.fake_gerrit.addEvent(G.getPatchsetCreatedEvent(1))
+        self.waitUntilSettled()
+        self.fake_gerrit.addEvent(H.getPatchsetCreatedEvent(1))
+        self.waitUntilSettled()
+        self.fake_gerrit.addEvent(I.getPatchsetCreatedEvent(1))
+        self.waitUntilSettled()
+
+        # The first 6 (ABCDEF) jobs should have reported back to gerrt thus
+        # leaving a message on each change
+        self.assertEqual(1, len(A.messages))
+        self.assertIn('Build failed.', A.messages[0])
+        self.assertEqual(1, len(B.messages))
+        self.assertIn('Build failed.', B.messages[0])
+        self.assertEqual(1, len(C.messages))
+        self.assertIn('Build succeeded.', C.messages[0])
+        self.assertEqual(1, len(D.messages))
+        self.assertIn('Build failed.', D.messages[0])
+        self.assertEqual(1, len(E.messages))
+        self.assertIn('Build failed.', E.messages[0])
+        self.assertEqual(1, len(F.messages))
+        self.assertIn('Build failed.', F.messages[0])
+
+        # The last 3 (GHI) would have only reported via smtp.
+        self.assertEqual(3, len(self.smtp_messages))
+        self.assertEqual(0, len(G.messages))
+        self.assertIn('Build failed.', self.smtp_messages[0]['body'])
+        self.assertIn('/7/1/check', self.smtp_messages[0]['body'])
+        self.assertEqual(0, len(H.messages))
+        self.assertIn('Build failed.', self.smtp_messages[1]['body'])
+        self.assertIn('/8/1/check', self.smtp_messages[1]['body'])
+        self.assertEqual(0, len(I.messages))
+        self.assertIn('Build succeeded.', self.smtp_messages[2]['body'])
+        self.assertIn('/9/1/check', self.smtp_messages[2]['body'])
+
+        # Now reload the configuration (simulate a HUP) to check the pipeline
+        # comes out of disabled
+        self.sched.reconfigure(self.config)
+
+        self.assertEqual(3, self.sched.layout.pipelines['check'].disable_at)
+        self.assertEqual(
+            0, self.sched.layout.pipelines['check']._consecutive_failures)
+        self.assertFalse(self.sched.layout.pipelines['check']._disabled)
+
+        self.fake_gerrit.addEvent(J.getPatchsetCreatedEvent(1))
+        self.fake_gerrit.addEvent(K.getPatchsetCreatedEvent(1))
+        self.waitUntilSettled()
+
+        self.assertEqual(
+            2, self.sched.layout.pipelines['check']._consecutive_failures)
+        self.assertFalse(self.sched.layout.pipelines['check']._disabled)
+
+        # J and K went back to gerrit
+        self.assertEqual(1, len(J.messages))
+        self.assertIn('Build failed.', J.messages[0])
+        self.assertEqual(1, len(K.messages))
+        self.assertIn('Build failed.', K.messages[0])
+        # No more messages reported via smtp
+        self.assertEqual(3, len(self.smtp_messages))
diff --git a/tests/test_source.py b/tests/test_source.py
index 8a3e7d5..d14bc8f 100644
--- a/tests/test_source.py
+++ b/tests/test_source.py
@@ -1,25 +1,25 @@
-# Copyright 2014 Rackspace Australia
-#
-# Licensed under the Apache License, Version 2.0 (the "License"); you may
-# not use this file except in compliance with the License. You may obtain
-# a copy of the License at
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-# License for the specific language governing permissions and limitations
-# under the License.
-
-import logging
-import testtools
-
-import zuul.source
-
-
-class TestGerritSource(testtools.TestCase):
-    log = logging.getLogger("zuul.test_source")
-
-    def test_source_name(self):
-        self.assertEqual('gerrit', zuul.source.gerrit.GerritSource.name)
+# Copyright 2014 Rackspace Australia
+#
+# Licensed under the Apache License, Version 2.0 (the "License"); you may
+# not use this file except in compliance with the License. You may obtain
+# a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+# License for the specific language governing permissions and limitations
+# under the License.
+
+import logging
+import testtools
+
+import zuul.source
+
+
+class TestGerritSource(testtools.TestCase):
+    log = logging.getLogger("zuul.test_source")
+
+    def test_source_name(self):
+        self.assertEqual('gerrit', zuul.source.gerrit.GerritSource.name)
diff --git a/tests/test_stack_dump.py b/tests/test_stack_dump.py
index 824e04c..86b930a 100644
--- a/tests/test_stack_dump.py
+++ b/tests/test_stack_dump.py
@@ -1,34 +1,34 @@
-# Copyright 2013 Hewlett-Packard Development Company, L.P.
-#
-# Licensed under the Apache License, Version 2.0 (the "License"); you may
-# not use this file except in compliance with the License. You may obtain
-# a copy of the License at
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-# License for the specific language governing permissions and limitations
-# under the License.
-
-import fixtures
-import logging
-import signal
-import testtools
-
-import zuul.cmd
-
-
-class TestStackDump(testtools.TestCase):
-    def setUp(self):
-        super(TestStackDump, self).setUp()
-        self.log_fixture = self.useFixture(
-            fixtures.FakeLogger(level=logging.DEBUG))
-
-    def test_stack_dump_logs(self):
-        "Test that stack dumps end up in logs."
-
-        zuul.cmd.stack_dump_handler(signal.SIGUSR2, None)
-        self.assertIn("Thread", self.log_fixture.output)
-        self.assertIn("test_stack_dump_logs", self.log_fixture.output)
+# Copyright 2013 Hewlett-Packard Development Company, L.P.
+#
+# Licensed under the Apache License, Version 2.0 (the "License"); you may
+# not use this file except in compliance with the License. You may obtain
+# a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+# License for the specific language governing permissions and limitations
+# under the License.
+
+import fixtures
+import logging
+import signal
+import testtools
+
+import zuul.cmd
+
+
+class TestStackDump(testtools.TestCase):
+    def setUp(self):
+        super(TestStackDump, self).setUp()
+        self.log_fixture = self.useFixture(
+            fixtures.FakeLogger(level=logging.DEBUG))
+
+    def test_stack_dump_logs(self):
+        "Test that stack dumps end up in logs."
+
+        zuul.cmd.stack_dump_handler(signal.SIGUSR2, None)
+        self.assertIn("Thread", self.log_fixture.output)
+        self.assertIn("test_stack_dump_logs", self.log_fixture.output)
diff --git a/tests/test_trigger.py b/tests/test_trigger.py
index 7eb1b69..7d3a77d 100644
--- a/tests/test_trigger.py
+++ b/tests/test_trigger.py
@@ -1,51 +1,51 @@
-# Copyright 2014 Rackspace Australia
-#
-# Licensed under the Apache License, Version 2.0 (the "License"); you may
-# not use this file except in compliance with the License. You may obtain
-# a copy of the License at
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-# License for the specific language governing permissions and limitations
-# under the License.
-
-import logging
-import testtools
-
-import zuul.trigger
-
-
-class TestGerritTrigger(testtools.TestCase):
-    log = logging.getLogger("zuul.test_trigger")
-
-    def test_trigger_abc(self):
-        # We only need to instantiate a class for this
-        zuul.trigger.gerrit.GerritTrigger({})
-
-    def test_trigger_name(self):
-        self.assertEqual('gerrit', zuul.trigger.gerrit.GerritTrigger.name)
-
-
-class TestTimerTrigger(testtools.TestCase):
-    log = logging.getLogger("zuul.test_trigger")
-
-    def test_trigger_abc(self):
-        # We only need to instantiate a class for this
-        zuul.trigger.timer.TimerTrigger({})
-
-    def test_trigger_name(self):
-        self.assertEqual('timer', zuul.trigger.timer.TimerTrigger.name)
-
-
-class TestZuulTrigger(testtools.TestCase):
-    log = logging.getLogger("zuul.test_trigger")
-
-    def test_trigger_abc(self):
-        # We only need to instantiate a class for this
-        zuul.trigger.zuultrigger.ZuulTrigger({})
-
-    def test_trigger_name(self):
-        self.assertEqual('zuul', zuul.trigger.zuultrigger.ZuulTrigger.name)
+# Copyright 2014 Rackspace Australia
+#
+# Licensed under the Apache License, Version 2.0 (the "License"); you may
+# not use this file except in compliance with the License. You may obtain
+# a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+# License for the specific language governing permissions and limitations
+# under the License.
+
+import logging
+import testtools
+
+import zuul.trigger
+
+
+class TestGerritTrigger(testtools.TestCase):
+    log = logging.getLogger("zuul.test_trigger")
+
+    def test_trigger_abc(self):
+        # We only need to instantiate a class for this
+        zuul.trigger.gerrit.GerritTrigger({})
+
+    def test_trigger_name(self):
+        self.assertEqual('gerrit', zuul.trigger.gerrit.GerritTrigger.name)
+
+
+class TestTimerTrigger(testtools.TestCase):
+    log = logging.getLogger("zuul.test_trigger")
+
+    def test_trigger_abc(self):
+        # We only need to instantiate a class for this
+        zuul.trigger.timer.TimerTrigger({})
+
+    def test_trigger_name(self):
+        self.assertEqual('timer', zuul.trigger.timer.TimerTrigger.name)
+
+
+class TestZuulTrigger(testtools.TestCase):
+    log = logging.getLogger("zuul.test_trigger")
+
+    def test_trigger_abc(self):
+        # We only need to instantiate a class for this
+        zuul.trigger.zuultrigger.ZuulTrigger({})
+
+    def test_trigger_name(self):
+        self.assertEqual('zuul', zuul.trigger.zuultrigger.ZuulTrigger.name)
diff --git a/tests/test_webapp.py b/tests/test_webapp.py
index 8a88261..29c8d90 100644
--- a/tests/test_webapp.py
+++ b/tests/test_webapp.py
@@ -1,85 +1,85 @@
-#!/usr/bin/env python2.7
-
-# Copyright 2014 Hewlett-Packard Development Company, L.P.
-# Copyright 2014 Rackspace Australia
-#
-# Licensed under the Apache License, Version 2.0 (the "License"); you may
-# not use this file except in compliance with the License. You may obtain
-# a copy of the License at
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-# License for the specific language governing permissions and limitations
-# under the License.
-
-import json
-import urllib2
-
-from tests.base import ZuulTestCase
-
-
-class TestWebapp(ZuulTestCase):
-
-    def _cleanup(self):
-        self.worker.hold_jobs_in_build = False
-        self.worker.release()
-        self.waitUntilSettled()
-
-    def setUp(self):
-        super(TestWebapp, self).setUp()
-        self.addCleanup(self._cleanup)
-        self.worker.hold_jobs_in_build = True
-        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
-        A.addApproval('CRVW', 2)
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-        B = self.fake_gerrit.addFakeChange('org/project1', 'master', 'B')
-        B.addApproval('CRVW', 2)
-        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
-        self.waitUntilSettled()
-        self.port = self.webapp.server.socket.getsockname()[1]
-
-    def test_webapp_status(self):
-        "Test that we can filter to only certain changes in the webapp."
-
-        req = urllib2.Request(
-            "http://localhost:%s/status" % self.port)
-        f = urllib2.urlopen(req)
-        data = json.loads(f.read())
-
-        self.assertIn('pipelines', data)
-
-    def test_webapp_status_compat(self):
-        # testing compat with status.json
-        req = urllib2.Request(
-            "http://localhost:%s/status.json" % self.port)
-        f = urllib2.urlopen(req)
-        data = json.loads(f.read())
-
-        self.assertIn('pipelines', data)
-
-    def test_webapp_bad_url(self):
-        # do we 404 correctly
-        req = urllib2.Request(
-            "http://localhost:%s/status/foo" % self.port)
-        self.assertRaises(urllib2.HTTPError, urllib2.urlopen, req)
-
-    def test_webapp_find_change(self):
-        # can we filter by change id
-        req = urllib2.Request(
-            "http://localhost:%s/status/change/1,1" % self.port)
-        f = urllib2.urlopen(req)
-        data = json.loads(f.read())
-
-        self.assertEqual(1, len(data), data)
-        self.assertEqual("org/project", data[0]['project'])
-
-        req = urllib2.Request(
-            "http://localhost:%s/status/change/2,1" % self.port)
-        f = urllib2.urlopen(req)
-        data = json.loads(f.read())
-
-        self.assertEqual(1, len(data), data)
-        self.assertEqual("org/project1", data[0]['project'], data)
+#!/usr/bin/env python2.7
+
+# Copyright 2014 Hewlett-Packard Development Company, L.P.
+# Copyright 2014 Rackspace Australia
+#
+# Licensed under the Apache License, Version 2.0 (the "License"); you may
+# not use this file except in compliance with the License. You may obtain
+# a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+# License for the specific language governing permissions and limitations
+# under the License.
+
+import json
+import urllib2
+
+from tests.base import ZuulTestCase
+
+
+class TestWebapp(ZuulTestCase):
+
+    def _cleanup(self):
+        self.worker.hold_jobs_in_build = False
+        self.worker.release()
+        self.waitUntilSettled()
+
+    def setUp(self):
+        super(TestWebapp, self).setUp()
+        self.addCleanup(self._cleanup)
+        self.worker.hold_jobs_in_build = True
+        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
+        A.addApproval('CRVW', 2)
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+        B = self.fake_gerrit.addFakeChange('org/project1', 'master', 'B')
+        B.addApproval('CRVW', 2)
+        self.fake_gerrit.addEvent(B.addApproval('APRV', 1))
+        self.waitUntilSettled()
+        self.port = self.webapp.server.socket.getsockname()[1]
+
+    def test_webapp_status(self):
+        "Test that we can filter to only certain changes in the webapp."
+
+        req = urllib2.Request(
+            "http://localhost:%s/status" % self.port)
+        f = urllib2.urlopen(req)
+        data = json.loads(f.read())
+
+        self.assertIn('pipelines', data)
+
+    def test_webapp_status_compat(self):
+        # testing compat with status.json
+        req = urllib2.Request(
+            "http://localhost:%s/status.json" % self.port)
+        f = urllib2.urlopen(req)
+        data = json.loads(f.read())
+
+        self.assertIn('pipelines', data)
+
+    def test_webapp_bad_url(self):
+        # do we 404 correctly
+        req = urllib2.Request(
+            "http://localhost:%s/status/foo" % self.port)
+        self.assertRaises(urllib2.HTTPError, urllib2.urlopen, req)
+
+    def test_webapp_find_change(self):
+        # can we filter by change id
+        req = urllib2.Request(
+            "http://localhost:%s/status/change/1,1" % self.port)
+        f = urllib2.urlopen(req)
+        data = json.loads(f.read())
+
+        self.assertEqual(1, len(data), data)
+        self.assertEqual("org/project", data[0]['project'])
+
+        req = urllib2.Request(
+            "http://localhost:%s/status/change/2,1" % self.port)
+        f = urllib2.urlopen(req)
+        data = json.loads(f.read())
+
+        self.assertEqual(1, len(data), data)
+        self.assertEqual("org/project1", data[0]['project'], data)
diff --git a/tests/test_zuultrigger.py b/tests/test_zuultrigger.py
index 49b79ad..999be61 100644
--- a/tests/test_zuultrigger.py
+++ b/tests/test_zuultrigger.py
@@ -1,142 +1,142 @@
-#!/usr/bin/env python2.7
-
-# Copyright 2014 Hewlett-Packard Development Company, L.P.
-#
-# Licensed under the Apache License, Version 2.0 (the "License"); you may
-# not use this file except in compliance with the License. You may obtain
-# a copy of the License at
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-# License for the specific language governing permissions and limitations
-# under the License.
-
-import logging
-
-from tests.base import ZuulTestCase
-
-logging.basicConfig(level=logging.DEBUG,
-                    format='%(asctime)s %(name)-32s '
-                    '%(levelname)-8s %(message)s')
-
-
-class TestZuulTrigger(ZuulTestCase):
-    """Test Zuul Trigger"""
-
-    def test_zuul_trigger_parent_change_enqueued(self):
-        "Test Zuul trigger event: parent-change-enqueued"
-        self.config.set('zuul', 'layout_config',
-                        'tests/fixtures/layout-zuultrigger-enqueued.yaml')
-        self.sched.reconfigure(self.config)
-        self.registerJobs()
-
-        # This test has the following three changes:
-        # B1 -> A; B2 -> A
-        # When A is enqueued in the gate, B1 and B2 should both attempt
-        # to be enqueued in both pipelines.  B1 should end up in check
-        # and B2 in gate because of differing pipeline requirements.
-        self.worker.hold_jobs_in_build = True
-        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
-        B1 = self.fake_gerrit.addFakeChange('org/project', 'master', 'B1')
-        B2 = self.fake_gerrit.addFakeChange('org/project', 'master', 'B2')
-        A.addApproval('CRVW', 2)
-        B1.addApproval('CRVW', 2)
-        B2.addApproval('CRVW', 2)
-        A.addApproval('VRFY', 1)    # required by gate
-        B1.addApproval('VRFY', -1)  # should go to check
-        B2.addApproval('VRFY', 1)   # should go to gate
-        B1.addApproval('APRV', 1)
-        B2.addApproval('APRV', 1)
-        B1.setDependsOn(A, 1)
-        B2.setDependsOn(A, 1)
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-        # Jobs are being held in build to make sure that 3,1 has time
-        # to enqueue behind 1,1 so that the test is more
-        # deterministic.
-        self.waitUntilSettled()
-        self.worker.hold_jobs_in_build = False
-        self.worker.release()
-        self.waitUntilSettled()
-
-        self.assertEqual(len(self.history), 3)
-        for job in self.history:
-            if job.changes == '1,1':
-                self.assertEqual(job.name, 'project-gate')
-            elif job.changes == '1,1 2,1':
-                self.assertEqual(job.name, 'project-check')
-            elif job.changes == '1,1 3,1':
-                self.assertEqual(job.name, 'project-gate')
-            else:
-                raise Exception("Unknown job")
-
-    def test_zuul_trigger_project_change_merged(self):
-        "Test Zuul trigger event: project-change-merged"
-        self.config.set('zuul', 'layout_config',
-                        'tests/fixtures/layout-zuultrigger-merged.yaml')
-        self.sched.reconfigure(self.config)
-        self.registerJobs()
-
-        # This test has the following three changes:
-        # A, B, C;  B conflicts with A, but C does not.
-        # When A is merged, B and C should be checked for conflicts,
-        # and B should receive a -1.
-        # D and E are used to repeat the test in the second part, but
-        # are defined here to that they end up in the trigger cache.
-        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
-        B = self.fake_gerrit.addFakeChange('org/project', 'master', 'B')
-        C = self.fake_gerrit.addFakeChange('org/project', 'master', 'C')
-        D = self.fake_gerrit.addFakeChange('org/project', 'master', 'D')
-        E = self.fake_gerrit.addFakeChange('org/project', 'master', 'E')
-        A.addPatchset(['conflict'])
-        B.addPatchset(['conflict'])
-        D.addPatchset(['conflict2'])
-        E.addPatchset(['conflict2'])
-        A.addApproval('CRVW', 2)
-        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
-        self.waitUntilSettled()
-
-        self.assertEqual(len(self.history), 1)
-        self.assertEqual(self.history[0].name, 'project-gate')
-        self.assertEqual(A.reported, 2)
-        self.assertEqual(B.reported, 1)
-        self.assertEqual(C.reported, 0)
-        self.assertEqual(D.reported, 0)
-        self.assertEqual(E.reported, 0)
-        self.assertEqual(
-            B.messages[0],
-            "Merge Failed.\n\nThis change or one of its cross-repo "
-            "dependencies was unable to be automatically merged with the "
-            "current state of its repository. Please rebase the change and "
-            "upload a new patchset.")
-
-        self.assertTrue("project:org/project status:open" in
-                        self.fake_gerrit.queries)
-
-        # Reconfigure and run the test again.  This is a regression
-        # check to make sure that we don't end up with a stale trigger
-        # cache that has references to projects from the old
-        # configuration.
-        self.sched.reconfigure(self.config)
-
-        D.addApproval('CRVW', 2)
-        self.fake_gerrit.addEvent(D.addApproval('APRV', 1))
-        self.waitUntilSettled()
-
-        self.assertEqual(len(self.history), 2)
-        self.assertEqual(self.history[1].name, 'project-gate')
-        self.assertEqual(A.reported, 2)
-        self.assertEqual(B.reported, 1)
-        self.assertEqual(C.reported, 0)
-        self.assertEqual(D.reported, 2)
-        self.assertEqual(E.reported, 1)
-        self.assertEqual(
-            E.messages[0],
-            "Merge Failed.\n\nThis change or one of its cross-repo "
-            "dependencies was unable to be automatically merged with the "
-            "current state of its repository. Please rebase the change and "
-            "upload a new patchset.")
-        self.assertEqual(self.fake_gerrit.queries[1],
-                         "project:org/project status:open")
+#!/usr/bin/env python2.7
+
+# Copyright 2014 Hewlett-Packard Development Company, L.P.
+#
+# Licensed under the Apache License, Version 2.0 (the "License"); you may
+# not use this file except in compliance with the License. You may obtain
+# a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+# License for the specific language governing permissions and limitations
+# under the License.
+
+import logging
+
+from tests.base import ZuulTestCase
+
+logging.basicConfig(level=logging.DEBUG,
+                    format='%(asctime)s %(name)-32s '
+                    '%(levelname)-8s %(message)s')
+
+
+class TestZuulTrigger(ZuulTestCase):
+    """Test Zuul Trigger"""
+
+    def test_zuul_trigger_parent_change_enqueued(self):
+        "Test Zuul trigger event: parent-change-enqueued"
+        self.config.set('zuul', 'layout_config',
+                        'tests/fixtures/layout-zuultrigger-enqueued.yaml')
+        self.sched.reconfigure(self.config)
+        self.registerJobs()
+
+        # This test has the following three changes:
+        # B1 -> A; B2 -> A
+        # When A is enqueued in the gate, B1 and B2 should both attempt
+        # to be enqueued in both pipelines.  B1 should end up in check
+        # and B2 in gate because of differing pipeline requirements.
+        self.worker.hold_jobs_in_build = True
+        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
+        B1 = self.fake_gerrit.addFakeChange('org/project', 'master', 'B1')
+        B2 = self.fake_gerrit.addFakeChange('org/project', 'master', 'B2')
+        A.addApproval('CRVW', 2)
+        B1.addApproval('CRVW', 2)
+        B2.addApproval('CRVW', 2)
+        A.addApproval('VRFY', 1)    # required by gate
+        B1.addApproval('VRFY', -1)  # should go to check
+        B2.addApproval('VRFY', 1)   # should go to gate
+        B1.addApproval('APRV', 1)
+        B2.addApproval('APRV', 1)
+        B1.setDependsOn(A, 1)
+        B2.setDependsOn(A, 1)
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+        # Jobs are being held in build to make sure that 3,1 has time
+        # to enqueue behind 1,1 so that the test is more
+        # deterministic.
+        self.waitUntilSettled()
+        self.worker.hold_jobs_in_build = False
+        self.worker.release()
+        self.waitUntilSettled()
+
+        self.assertEqual(len(self.history), 3)
+        for job in self.history:
+            if job.changes == '1,1':
+                self.assertEqual(job.name, 'project-gate')
+            elif job.changes == '1,1 2,1':
+                self.assertEqual(job.name, 'project-check')
+            elif job.changes == '1,1 3,1':
+                self.assertEqual(job.name, 'project-gate')
+            else:
+                raise Exception("Unknown job")
+
+    def test_zuul_trigger_project_change_merged(self):
+        "Test Zuul trigger event: project-change-merged"
+        self.config.set('zuul', 'layout_config',
+                        'tests/fixtures/layout-zuultrigger-merged.yaml')
+        self.sched.reconfigure(self.config)
+        self.registerJobs()
+
+        # This test has the following three changes:
+        # A, B, C;  B conflicts with A, but C does not.
+        # When A is merged, B and C should be checked for conflicts,
+        # and B should receive a -1.
+        # D and E are used to repeat the test in the second part, but
+        # are defined here to that they end up in the trigger cache.
+        A = self.fake_gerrit.addFakeChange('org/project', 'master', 'A')
+        B = self.fake_gerrit.addFakeChange('org/project', 'master', 'B')
+        C = self.fake_gerrit.addFakeChange('org/project', 'master', 'C')
+        D = self.fake_gerrit.addFakeChange('org/project', 'master', 'D')
+        E = self.fake_gerrit.addFakeChange('org/project', 'master', 'E')
+        A.addPatchset(['conflict'])
+        B.addPatchset(['conflict'])
+        D.addPatchset(['conflict2'])
+        E.addPatchset(['conflict2'])
+        A.addApproval('CRVW', 2)
+        self.fake_gerrit.addEvent(A.addApproval('APRV', 1))
+        self.waitUntilSettled()
+
+        self.assertEqual(len(self.history), 1)
+        self.assertEqual(self.history[0].name, 'project-gate')
+        self.assertEqual(A.reported, 2)
+        self.assertEqual(B.reported, 1)
+        self.assertEqual(C.reported, 0)
+        self.assertEqual(D.reported, 0)
+        self.assertEqual(E.reported, 0)
+        self.assertEqual(
+            B.messages[0],
+            "Merge Failed.\n\nThis change or one of its cross-repo "
+            "dependencies was unable to be automatically merged with the "
+            "current state of its repository. Please rebase the change and "
+            "upload a new patchset.")
+
+        self.assertTrue("project:org/project status:open" in
+                        self.fake_gerrit.queries)
+
+        # Reconfigure and run the test again.  This is a regression
+        # check to make sure that we don't end up with a stale trigger
+        # cache that has references to projects from the old
+        # configuration.
+        self.sched.reconfigure(self.config)
+
+        D.addApproval('CRVW', 2)
+        self.fake_gerrit.addEvent(D.addApproval('APRV', 1))
+        self.waitUntilSettled()
+
+        self.assertEqual(len(self.history), 2)
+        self.assertEqual(self.history[1].name, 'project-gate')
+        self.assertEqual(A.reported, 2)
+        self.assertEqual(B.reported, 1)
+        self.assertEqual(C.reported, 0)
+        self.assertEqual(D.reported, 2)
+        self.assertEqual(E.reported, 1)
+        self.assertEqual(
+            E.messages[0],
+            "Merge Failed.\n\nThis change or one of its cross-repo "
+            "dependencies was unable to be automatically merged with the "
+            "current state of its repository. Please rebase the change and "
+            "upload a new patchset.")
+        self.assertEqual(self.fake_gerrit.queries[1],
+                         "project:org/project status:open")
diff --git a/tools/trigger-job.py b/tools/trigger-job.py
index 4651d7d..732a5ea 100755
--- a/tools/trigger-job.py
+++ b/tools/trigger-job.py
@@ -1,77 +1,77 @@
-#!/usr/bin/env python2.7
-# Copyright 2013 OpenStack Foundation
-#
-# Licensed under the Apache License, Version 2.0 (the "License"); you may
-# not use this file except in compliance with the License. You may obtain
-# a copy of the License at
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-# License for the specific language governing permissions and limitations
-# under the License.
-
-# This script can be used to manually trigger a job in the same way that
-# Zuul does.  At the moment, it only supports the post set of Zuul
-# parameters.
-
-import argparse
-import time
-import json
-from uuid import uuid4
-
-import gear
-
-
-def main():
-    c = gear.Client()
-
-    parser = argparse.ArgumentParser(description='Trigger a Zuul job.')
-    parser.add_argument('--job', dest='job', required=True,
-                        help='Job Name')
-    parser.add_argument('--project', dest='project', required=True,
-                        help='Project name')
-    parser.add_argument('--pipeline', dest='pipeline', default='release',
-                        help='Zuul pipeline')
-    parser.add_argument('--refname', dest='refname',
-                        help='Ref name')
-    parser.add_argument('--oldrev', dest='oldrev',
-                        default='0000000000000000000000000000000000000000',
-                        help='Old revision (SHA)')
-    parser.add_argument('--newrev', dest='newrev',
-                        help='New revision (SHA)')
-    parser.add_argument('--url', dest='url',
-                        default='http://zuul.openstack.org/p', help='Zuul URL')
-    parser.add_argument('--logpath', dest='logpath', required=True,
-                        help='Path for log files.')
-    args = parser.parse_args()
-
-    data = {'ZUUL_PIPELINE': args.pipeline,
-            'ZUUL_PROJECT': args.project,
-            'ZUUL_UUID': str(uuid4().hex),
-            'ZUUL_REF': args.refname,
-            'ZUUL_REFNAME': args.refname,
-            'ZUUL_OLDREV': args.oldrev,
-            'ZUUL_NEWREV': args.newrev,
-            'ZUUL_SHORT_OLDREV': args.oldrev[:7],
-            'ZUUL_SHORT_NEWREV': args.newrev[:7],
-            'ZUUL_COMMIT': args.newrev,
-            'ZUUL_URL': args.url,
-            'LOG_PATH': args.logpath,
-            }
-
-    c.addServer('127.0.0.1', 4730)
-    c.waitForServer()
-
-    job = gear.Job("build:%s" % args.job,
-                   json.dumps(data),
-                   unique=data['ZUUL_UUID'])
-    c.submitJob(job)
-
-    while not job.complete:
-        time.sleep(1)
-
-if __name__ == '__main__':
-    main()
+#!/usr/bin/env python2.7
+# Copyright 2013 OpenStack Foundation
+#
+# Licensed under the Apache License, Version 2.0 (the "License"); you may
+# not use this file except in compliance with the License. You may obtain
+# a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+# License for the specific language governing permissions and limitations
+# under the License.
+
+# This script can be used to manually trigger a job in the same way that
+# Zuul does.  At the moment, it only supports the post set of Zuul
+# parameters.
+
+import argparse
+import time
+import json
+from uuid import uuid4
+
+import gear
+
+
+def main():
+    c = gear.Client()
+
+    parser = argparse.ArgumentParser(description='Trigger a Zuul job.')
+    parser.add_argument('--job', dest='job', required=True,
+                        help='Job Name')
+    parser.add_argument('--project', dest='project', required=True,
+                        help='Project name')
+    parser.add_argument('--pipeline', dest='pipeline', default='release',
+                        help='Zuul pipeline')
+    parser.add_argument('--refname', dest='refname',
+                        help='Ref name')
+    parser.add_argument('--oldrev', dest='oldrev',
+                        default='0000000000000000000000000000000000000000',
+                        help='Old revision (SHA)')
+    parser.add_argument('--newrev', dest='newrev',
+                        help='New revision (SHA)')
+    parser.add_argument('--url', dest='url',
+                        default='http://zuul.openstack.org/p', help='Zuul URL')
+    parser.add_argument('--logpath', dest='logpath', required=True,
+                        help='Path for log files.')
+    args = parser.parse_args()
+
+    data = {'ZUUL_PIPELINE': args.pipeline,
+            'ZUUL_PROJECT': args.project,
+            'ZUUL_UUID': str(uuid4().hex),
+            'ZUUL_REF': args.refname,
+            'ZUUL_REFNAME': args.refname,
+            'ZUUL_OLDREV': args.oldrev,
+            'ZUUL_NEWREV': args.newrev,
+            'ZUUL_SHORT_OLDREV': args.oldrev[:7],
+            'ZUUL_SHORT_NEWREV': args.newrev[:7],
+            'ZUUL_COMMIT': args.newrev,
+            'ZUUL_URL': args.url,
+            'LOG_PATH': args.logpath,
+            }
+
+    c.addServer('127.0.0.1', 4730)
+    c.waitForServer()
+
+    job = gear.Job("build:%s" % args.job,
+                   json.dumps(data),
+                   unique=data['ZUUL_UUID'])
+    c.submitJob(job)
+
+    while not job.complete:
+        time.sleep(1)
+
+if __name__ == '__main__':
+    main()
diff --git a/tools/zuul-changes.py b/tools/zuul-changes.py
index d825ef1..e9c0d28 100755
--- a/tools/zuul-changes.py
+++ b/tools/zuul-changes.py
@@ -1,44 +1,44 @@
-#!/usr/bin/env python2.7
-# Copyright 2013 OpenStack Foundation
-# Copyright 2015 Hewlett-Packard Development Company, L.P.
-#
-# Licensed under the Apache License, Version 2.0 (the "License"); you may
-# not use this file except in compliance with the License. You may obtain
-# a copy of the License at
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-# License for the specific language governing permissions and limitations
-# under the License.
-
-import urllib2
-import json
-import argparse
-
-parser = argparse.ArgumentParser()
-parser.add_argument('url', help='The URL of the running Zuul instance')
-parser.add_argument('pipeline_name', help='The name of the Zuul pipeline')
-options = parser.parse_args()
-
-data = urllib2.urlopen('%s/status.json' % options.url).read()
-data = json.loads(data)
-
-for pipeline in data['pipelines']:
-    if pipeline['name'] != options.pipeline_name:
-        continue
-    for queue in pipeline['change_queues']:
-        for head in queue['heads']:
-            for change in head:
-                if not change['live']:
-                    continue
-                cid, cps = change['id'].split(',')
-                print (
-                    "zuul enqueue --trigger gerrit --pipeline %s "
-                    "--project %s --change %s,%s" % (
-                        options.pipeline_name,
-                        change['project'],
-                        cid, cps)
-                )
+#!/usr/bin/env python2.7
+# Copyright 2013 OpenStack Foundation
+# Copyright 2015 Hewlett-Packard Development Company, L.P.
+#
+# Licensed under the Apache License, Version 2.0 (the "License"); you may
+# not use this file except in compliance with the License. You may obtain
+# a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+# License for the specific language governing permissions and limitations
+# under the License.
+
+import urllib2
+import json
+import argparse
+
+parser = argparse.ArgumentParser()
+parser.add_argument('url', help='The URL of the running Zuul instance')
+parser.add_argument('pipeline_name', help='The name of the Zuul pipeline')
+options = parser.parse_args()
+
+data = urllib2.urlopen('%s/status.json' % options.url).read()
+data = json.loads(data)
+
+for pipeline in data['pipelines']:
+    if pipeline['name'] != options.pipeline_name:
+        continue
+    for queue in pipeline['change_queues']:
+        for head in queue['heads']:
+            for change in head:
+                if not change['live']:
+                    continue
+                cid, cps = change['id'].split(',')
+                print (
+                    "zuul enqueue --trigger gerrit --pipeline %s "
+                    "--project %s --change %s,%s" % (
+                        options.pipeline_name,
+                        change['project'],
+                        cid, cps)
+                )
diff --git a/tox.ini b/tox.ini
index 79ea939..c10f2ac 100644
--- a/tox.ini
+++ b/tox.ini
@@ -1,41 +1,41 @@
-[tox]
-minversion = 1.6
-skipsdist = True
-envlist = pep8, py27
-
-[testenv]
-# Set STATSD env variables so that statsd code paths are tested.
-setenv = STATSD_HOST=127.0.0.1
-         STATSD_PORT=8125
-         VIRTUAL_ENV={envdir}
-         OS_TEST_TIMEOUT=30
-passenv = ZUUL_TEST_ROOT
-usedevelop = True
-install_command = pip install {opts} {packages}
-deps = -r{toxinidir}/requirements.txt
-       -r{toxinidir}/test-requirements.txt
-commands =
-  python setup.py testr --slowest --testr-args='{posargs}'
-
-[testenv:pep8]
-commands = flake8 {posargs}
-
-[testenv:cover]
-commands =
-  python setup.py testr --coverage
-
-[testenv:docs]
-commands = python setup.py build_sphinx
-
-[testenv:venv]
-commands = {posargs}
-
-[testenv:validate-layout]
-commands = zuul-server -c etc/zuul.conf-sample -t -l {posargs}
-
-[flake8]
-# These are ignored intentionally in openstack-infra projects;
-# please don't submit patches that solely correct them or enable them.
-ignore = E125,E129,H
-show-source = True
-exclude = .venv,.tox,dist,doc,build,*.egg
+[tox]
+minversion = 1.6
+skipsdist = True
+envlist = pep8, py27
+
+[testenv]
+# Set STATSD env variables so that statsd code paths are tested.
+setenv = STATSD_HOST=127.0.0.1
+         STATSD_PORT=8125
+         VIRTUAL_ENV={envdir}
+         OS_TEST_TIMEOUT=30
+passenv = ZUUL_TEST_ROOT
+usedevelop = True
+install_command = pip install {opts} {packages}
+deps = -r{toxinidir}/requirements.txt
+       -r{toxinidir}/test-requirements.txt
+commands =
+  python setup.py testr --slowest --testr-args='{posargs}'
+
+[testenv:pep8]
+commands = flake8 {posargs}
+
+[testenv:cover]
+commands =
+  python setup.py testr --coverage
+
+[testenv:docs]
+commands = python setup.py build_sphinx
+
+[testenv:venv]
+commands = {posargs}
+
+[testenv:validate-layout]
+commands = zuul-server -c etc/zuul.conf-sample -t -l {posargs}
+
+[flake8]
+# These are ignored intentionally in openstack-infra projects;
+# please don't submit patches that solely correct them or enable them.
+ignore = E125,E129,H
+show-source = True
+exclude = .venv,.tox,dist,doc,build,*.egg
diff --git a/zuul/change_matcher.py b/zuul/change_matcher.py
index ed380f0..3909103 100644
--- a/zuul/change_matcher.py
+++ b/zuul/change_matcher.py
@@ -1,132 +1,132 @@
-# Copyright 2015 Red Hat, Inc.
-#
-# Licensed under the Apache License, Version 2.0 (the "License"); you may
-# not use this file except in compliance with the License. You may obtain
-# a copy of the License at
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-# License for the specific language governing permissions and limitations
-# under the License.
-
-"""
-This module defines classes used in matching changes based on job
-configuration.
-"""
-
-import re
-
-
-class AbstractChangeMatcher(object):
-
-    def __init__(self, regex):
-        self._regex = regex
-        self.regex = re.compile(regex)
-
-    def matches(self, change):
-        """Return a boolean indication of whether change matches
-        implementation-specific criteria.
-        """
-        raise NotImplementedError()
-
-    def copy(self):
-        return self.__class__(self._regex)
-
-    def __eq__(self, other):
-        return str(self) == str(other)
-
-    def __str__(self):
-        return '{%s:%s}' % (self.__class__.__name__, self._regex)
-
-    def __repr__(self):
-        return '<%s %s>' % (self.__class__.__name__, self._regex)
-
-
-class ProjectMatcher(AbstractChangeMatcher):
-
-    def matches(self, change):
-        return self.regex.match(str(change.project))
-
-
-class BranchMatcher(AbstractChangeMatcher):
-
-    def matches(self, change):
-        return (
-            (hasattr(change, 'branch') and self.regex.match(change.branch)) or
-            (hasattr(change, 'ref') and self.regex.match(change.ref))
-        )
-
-
-class FileMatcher(AbstractChangeMatcher):
-
-    def matches(self, change):
-        if not hasattr(change, 'files'):
-            return False
-        for file_ in change.files:
-            if self.regex.match(file_):
-                return True
-        return False
-
-
-class AbstractMatcherCollection(AbstractChangeMatcher):
-
-    def __init__(self, matchers):
-        self.matchers = matchers
-
-    def __eq__(self, other):
-        return str(self) == str(other)
-
-    def __str__(self):
-        return '{%s:%s}' % (self.__class__.__name__,
-                            ','.join([str(x) for x in self.matchers]))
-
-    def __repr__(self):
-        return '<%s>' % self.__class__.__name__
-
-    def copy(self):
-        return self.__class__(self.matchers[:])
-
-
-class MatchAllFiles(AbstractMatcherCollection):
-
-    commit_regex = re.compile('^/COMMIT_MSG$')
-
-    @property
-    def regexes(self):
-        for matcher in self.matchers:
-            yield matcher.regex
-        yield self.commit_regex
-
-    def matches(self, change):
-        if not (hasattr(change, 'files') and change.files):
-            return False
-        for file_ in change.files:
-            matched_file = False
-            for regex in self.regexes:
-                if regex.match(file_):
-                    matched_file = True
-                    break
-            if not matched_file:
-                return False
-        return True
-
-
-class MatchAll(AbstractMatcherCollection):
-
-    def matches(self, change):
-        for matcher in self.matchers:
-            if not matcher.matches(change):
-                return False
-        return True
-
-
-class MatchAny(AbstractMatcherCollection):
-
-    def matches(self, change):
-        for matcher in self.matchers:
-            if matcher.matches(change):
-                return True
-        return False
+# Copyright 2015 Red Hat, Inc.
+#
+# Licensed under the Apache License, Version 2.0 (the "License"); you may
+# not use this file except in compliance with the License. You may obtain
+# a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+# License for the specific language governing permissions and limitations
+# under the License.
+
+"""
+This module defines classes used in matching changes based on job
+configuration.
+"""
+
+import re
+
+
+class AbstractChangeMatcher(object):
+
+    def __init__(self, regex):
+        self._regex = regex
+        self.regex = re.compile(regex)
+
+    def matches(self, change):
+        """Return a boolean indication of whether change matches
+        implementation-specific criteria.
+        """
+        raise NotImplementedError()
+
+    def copy(self):
+        return self.__class__(self._regex)
+
+    def __eq__(self, other):
+        return str(self) == str(other)
+
+    def __str__(self):
+        return '{%s:%s}' % (self.__class__.__name__, self._regex)
+
+    def __repr__(self):
+        return '<%s %s>' % (self.__class__.__name__, self._regex)
+
+
+class ProjectMatcher(AbstractChangeMatcher):
+
+    def matches(self, change):
+        return self.regex.match(str(change.project))
+
+
+class BranchMatcher(AbstractChangeMatcher):
+
+    def matches(self, change):
+        return (
+            (hasattr(change, 'branch') and self.regex.match(change.branch)) or
+            (hasattr(change, 'ref') and self.regex.match(change.ref))
+        )
+
+
+class FileMatcher(AbstractChangeMatcher):
+
+    def matches(self, change):
+        if not hasattr(change, 'files'):
+            return False
+        for file_ in change.files:
+            if self.regex.match(file_):
+                return True
+        return False
+
+
+class AbstractMatcherCollection(AbstractChangeMatcher):
+
+    def __init__(self, matchers):
+        self.matchers = matchers
+
+    def __eq__(self, other):
+        return str(self) == str(other)
+
+    def __str__(self):
+        return '{%s:%s}' % (self.__class__.__name__,
+                            ','.join([str(x) for x in self.matchers]))
+
+    def __repr__(self):
+        return '<%s>' % self.__class__.__name__
+
+    def copy(self):
+        return self.__class__(self.matchers[:])
+
+
+class MatchAllFiles(AbstractMatcherCollection):
+
+    commit_regex = re.compile('^/COMMIT_MSG$')
+
+    @property
+    def regexes(self):
+        for matcher in self.matchers:
+            yield matcher.regex
+        yield self.commit_regex
+
+    def matches(self, change):
+        if not (hasattr(change, 'files') and change.files):
+            return False
+        for file_ in change.files:
+            matched_file = False
+            for regex in self.regexes:
+                if regex.match(file_):
+                    matched_file = True
+                    break
+            if not matched_file:
+                return False
+        return True
+
+
+class MatchAll(AbstractMatcherCollection):
+
+    def matches(self, change):
+        for matcher in self.matchers:
+            if not matcher.matches(change):
+                return False
+        return True
+
+
+class MatchAny(AbstractMatcherCollection):
+
+    def matches(self, change):
+        for matcher in self.matchers:
+            if matcher.matches(change):
+                return True
+        return False
diff --git a/zuul/cmd/__init__.py b/zuul/cmd/__init__.py
index 966d1f7..f2180d0 100644
--- a/zuul/cmd/__init__.py
+++ b/zuul/cmd/__init__.py
@@ -1,96 +1,96 @@
-#!/usr/bin/env python2.7
-# Copyright 2012 Hewlett-Packard Development Company, L.P.
-# Copyright 2013 OpenStack Foundation
-#
-# Licensed under the Apache License, Version 2.0 (the "License"); you may
-# not use this file except in compliance with the License. You may obtain
-# a copy of the License at
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-# License for the specific language governing permissions and limitations
-# under the License.
-
-from six.moves import configparser as ConfigParser
-import cStringIO
-import extras
-import logging
-import logging.config
-import os
-import signal
-import sys
-import traceback
-
-yappi = extras.try_import('yappi')
-
-# Do not import modules that will pull in paramiko which must not be
-# imported until after the daemonization.
-# https://github.com/paramiko/paramiko/issues/59
-# Similar situation with gear and statsd.
-
-
-def stack_dump_handler(signum, frame):
-    signal.signal(signal.SIGUSR2, signal.SIG_IGN)
-    log_str = ""
-    for thread_id, stack_frame in sys._current_frames().items():
-        log_str += "Thread: %s\n" % thread_id
-        log_str += "".join(traceback.format_stack(stack_frame))
-    log = logging.getLogger("zuul.stack_dump")
-    log.debug(log_str)
-    if yappi:
-        if not yappi.is_running():
-            yappi.start()
-        else:
-            yappi.stop()
-            yappi_out = cStringIO.StringIO()
-            yappi.get_func_stats().print_all(out=yappi_out)
-            yappi.get_thread_stats().print_all(out=yappi_out)
-            log.debug(yappi_out.getvalue())
-            yappi_out.close()
-            yappi.clear_stats()
-    signal.signal(signal.SIGUSR2, stack_dump_handler)
-
-
-class ZuulApp(object):
-
-    def __init__(self):
-        self.args = None
-        self.config = None
-        self.connections = {}
-
-    def _get_version(self):
-        from zuul.version import version_info as zuul_version_info
-        return "Zuul version: %s" % zuul_version_info.release_string()
-
-    def read_config(self):
-        self.config = ConfigParser.ConfigParser()
-        if self.args.config:
-            locations = [self.args.config]
-        else:
-            locations = ['/etc/zuul/zuul.conf',
-                         '~/zuul.conf']
-        for fp in locations:
-            if os.path.exists(os.path.expanduser(fp)):
-                self.config.read(os.path.expanduser(fp))
-                return
-        raise Exception("Unable to locate config file in %s" % locations)
-
-    def setup_logging(self, section, parameter):
-        if self.config.has_option(section, parameter):
-            fp = os.path.expanduser(self.config.get(section, parameter))
-            if not os.path.exists(fp):
-                raise Exception("Unable to read logging config file at %s" %
-                                fp)
-            logging.config.fileConfig(fp)
-        else:
-            logging.basicConfig(level=logging.DEBUG)
-
-    def configure_connections(self):
-        # See comment at top of file about zuul imports
-        import zuul.lib.connections
-
-        self.connections = zuul.lib.connections.configure_connections(
-            self.config)
+#!/usr/bin/env python2.7
+# Copyright 2012 Hewlett-Packard Development Company, L.P.
+# Copyright 2013 OpenStack Foundation
+#
+# Licensed under the Apache License, Version 2.0 (the "License"); you may
+# not use this file except in compliance with the License. You may obtain
+# a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+# License for the specific language governing permissions and limitations
+# under the License.
+
+from six.moves import configparser as ConfigParser
+import cStringIO
+import extras
+import logging
+import logging.config
+import os
+import signal
+import sys
+import traceback
+
+yappi = extras.try_import('yappi')
+
+# Do not import modules that will pull in paramiko which must not be
+# imported until after the daemonization.
+# https://github.com/paramiko/paramiko/issues/59
+# Similar situation with gear and statsd.
+
+
+def stack_dump_handler(signum, frame):
+    signal.signal(signal.SIGUSR2, signal.SIG_IGN)
+    log_str = ""
+    for thread_id, stack_frame in sys._current_frames().items():
+        log_str += "Thread: %s\n" % thread_id
+        log_str += "".join(traceback.format_stack(stack_frame))
+    log = logging.getLogger("zuul.stack_dump")
+    log.debug(log_str)
+    if yappi:
+        if not yappi.is_running():
+            yappi.start()
+        else:
+            yappi.stop()
+            yappi_out = cStringIO.StringIO()
+            yappi.get_func_stats().print_all(out=yappi_out)
+            yappi.get_thread_stats().print_all(out=yappi_out)
+            log.debug(yappi_out.getvalue())
+            yappi_out.close()
+            yappi.clear_stats()
+    signal.signal(signal.SIGUSR2, stack_dump_handler)
+
+
+class ZuulApp(object):
+
+    def __init__(self):
+        self.args = None
+        self.config = None
+        self.connections = {}
+
+    def _get_version(self):
+        from zuul.version import version_info as zuul_version_info
+        return "Zuul version: %s" % zuul_version_info.release_string()
+
+    def read_config(self):
+        self.config = ConfigParser.ConfigParser()
+        if self.args.config:
+            locations = [self.args.config]
+        else:
+            locations = ['/etc/zuul/zuul.conf',
+                         '~/zuul.conf']
+        for fp in locations:
+            if os.path.exists(os.path.expanduser(fp)):
+                self.config.read(os.path.expanduser(fp))
+                return
+        raise Exception("Unable to locate config file in %s" % locations)
+
+    def setup_logging(self, section, parameter):
+        if self.config.has_option(section, parameter):
+            fp = os.path.expanduser(self.config.get(section, parameter))
+            if not os.path.exists(fp):
+                raise Exception("Unable to read logging config file at %s" %
+                                fp)
+            logging.config.fileConfig(fp)
+        else:
+            logging.basicConfig(level=logging.DEBUG)
+
+    def configure_connections(self):
+        # See comment at top of file about zuul imports
+        import zuul.lib.connections
+
+        self.connections = zuul.lib.connections.configure_connections(
+            self.config)
diff --git a/zuul/cmd/client.py b/zuul/cmd/client.py
index e31f467..ef73103 100644
--- a/zuul/cmd/client.py
+++ b/zuul/cmd/client.py
@@ -1,304 +1,304 @@
-#!/usr/bin/env python2.7
-# Copyright 2012 Hewlett-Packard Development Company, L.P.
-# Copyright 2013 OpenStack Foundation
-#
-# Licensed under the Apache License, Version 2.0 (the "License"); you may
-# not use this file except in compliance with the License. You may obtain
-# a copy of the License at
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-# License for the specific language governing permissions and limitations
-# under the License.
-
-import argparse
-import babel.dates
-import datetime
-import logging
-import prettytable
-import sys
-import time
-
-
-import zuul.rpcclient
-import zuul.cmd
-
-
-class Client(zuul.cmd.ZuulApp):
-    log = logging.getLogger("zuul.Client")
-
-    def parse_arguments(self):
-        parser = argparse.ArgumentParser(
-            description='Zuul Project Gating System Client.')
-        parser.add_argument('-c', dest='config',
-                            help='specify the config file')
-        parser.add_argument('-v', dest='verbose', action='store_true',
-                            help='verbose output')
-        parser.add_argument('--version', dest='version', action='version',
-                            version=self._get_version(),
-                            help='show zuul version')
-
-        subparsers = parser.add_subparsers(title='commands',
-                                           description='valid commands',
-                                           help='additional help')
-
-        cmd_enqueue = subparsers.add_parser('enqueue', help='enqueue a change')
-        cmd_enqueue.add_argument('--trigger', help='trigger name',
-                                 required=True)
-        cmd_enqueue.add_argument('--pipeline', help='pipeline name',
-                                 required=True)
-        cmd_enqueue.add_argument('--project', help='project name',
-                                 required=True)
-        cmd_enqueue.add_argument('--change', help='change id',
-                                 required=True)
-        cmd_enqueue.set_defaults(func=self.enqueue)
-
-        cmd_enqueue = subparsers.add_parser('enqueue-ref',
-                                            help='enqueue a ref')
-        cmd_enqueue.add_argument('--trigger', help='trigger name',
-                                 required=True)
-        cmd_enqueue.add_argument('--pipeline', help='pipeline name',
-                                 required=True)
-        cmd_enqueue.add_argument('--project', help='project name',
-                                 required=True)
-        cmd_enqueue.add_argument('--ref', help='ref name',
-                                 required=True)
-        cmd_enqueue.add_argument(
-            '--oldrev', help='old revision',
-            default='0000000000000000000000000000000000000000')
-        cmd_enqueue.add_argument(
-            '--newrev', help='new revision',
-            default='0000000000000000000000000000000000000000')
-        cmd_enqueue.set_defaults(func=self.enqueue_ref)
-
-        cmd_promote = subparsers.add_parser('promote',
-                                            help='promote one or more changes')
-        cmd_promote.add_argument('--pipeline', help='pipeline name',
-                                 required=True)
-        cmd_promote.add_argument('--changes', help='change ids',
-                                 required=True, nargs='+')
-        cmd_promote.set_defaults(func=self.promote)
-
-        cmd_show = subparsers.add_parser('show',
-                                         help='valid show subcommands')
-        show_subparsers = cmd_show.add_subparsers(title='show')
-        show_running_jobs = show_subparsers.add_parser(
-            'running-jobs',
-            help='show the running jobs'
-        )
-        show_running_jobs.add_argument(
-            '--columns',
-            help="comma separated list of columns to display (or 'ALL')",
-            choices=self._show_running_jobs_columns().keys().append('ALL'),
-            default='name, worker.name, start_time, result'
-        )
-
-        # TODO: add filters such as queue, project, changeid etc
-        show_running_jobs.set_defaults(func=self.show_running_jobs)
-
-        self.args = parser.parse_args()
-        if self.args.func == self.enqueue_ref:
-            if self.args.oldrev == self.args.newrev:
-                parser.error("The old and new revisions must not be the same.")
-
-    def setup_logging(self):
-        """Client logging does not rely on conf file"""
-        if self.args.verbose:
-            logging.basicConfig(level=logging.DEBUG)
-
-    def main(self):
-        self.parse_arguments()
-        self.read_config()
-        self.setup_logging()
-
-        self.server = self.config.get('gearman', 'server')
-        if self.config.has_option('gearman', 'port'):
-            self.port = self.config.get('gearman', 'port')
-        else:
-            self.port = 4730
-
-        if self.args.func():
-            sys.exit(0)
-        else:
-            sys.exit(1)
-
-    def enqueue(self):
-        client = zuul.rpcclient.RPCClient(self.server, self.port)
-        r = client.enqueue(pipeline=self.args.pipeline,
-                           project=self.args.project,
-                           trigger=self.args.trigger,
-                           change=self.args.change)
-        return r
-
-    def enqueue_ref(self):
-        client = zuul.rpcclient.RPCClient(self.server, self.port)
-        r = client.enqueue_ref(pipeline=self.args.pipeline,
-                               project=self.args.project,
-                               trigger=self.args.trigger,
-                               ref=self.args.ref,
-                               oldrev=self.args.oldrev,
-                               newrev=self.args.newrev)
-        return r
-
-    def promote(self):
-        client = zuul.rpcclient.RPCClient(self.server, self.port)
-        r = client.promote(pipeline=self.args.pipeline,
-                           change_ids=self.args.changes)
-        return r
-
-    def show_running_jobs(self):
-        client = zuul.rpcclient.RPCClient(self.server, self.port)
-        running_items = client.get_running_jobs()
-
-        if len(running_items) == 0:
-            print "No jobs currently running"
-            return True
-
-        all_fields = self._show_running_jobs_columns()
-        if self.args.columns.upper() == 'ALL':
-            fields = all_fields.keys()
-        else:
-            fields = [f.strip().lower() for f in self.args.columns.split(',')
-                      if f.strip().lower() in all_fields.keys()]
-
-        table = prettytable.PrettyTable(
-            field_names=[all_fields[f]['title'] for f in fields])
-        for item in running_items:
-            for job in item['jobs']:
-                values = []
-                for f in fields:
-                    v = job
-                    for part in f.split('.'):
-                        if hasattr(v, 'get'):
-                            v = v.get(part, '')
-                    if ('transform' in all_fields[f]
-                        and callable(all_fields[f]['transform'])):
-                        v = all_fields[f]['transform'](v)
-                    if 'append' in all_fields[f]:
-                        v += all_fields[f]['append']
-                    values.append(v)
-                table.add_row(values)
-        print table
-        return True
-
-    def _epoch_to_relative_time(self, epoch):
-        if epoch:
-            delta = datetime.timedelta(seconds=(time.time() - int(epoch)))
-            return babel.dates.format_timedelta(delta, locale='en_US')
-        else:
-            return "Unknown"
-
-    def _boolean_to_yes_no(self, value):
-        return 'Yes' if value else 'No'
-
-    def _boolean_to_pass_fail(self, value):
-        return 'Pass' if value else 'Fail'
-
-    def _format_list(self, l):
-        return ', '.join(l) if isinstance(l, list) else ''
-
-    def _show_running_jobs_columns(self):
-        """A helper function to get the list of available columns for
-        `zuul show running-jobs`. Also describes how to convert particular
-        values (for example epoch to time string)"""
-
-        return {
-            'name': {
-                'title': 'Job Name',
-            },
-            'elapsed_time': {
-                'title': 'Elapsed Time',
-                'transform': self._epoch_to_relative_time
-            },
-            'remaining_time': {
-                'title': 'Remaining Time',
-                'transform': self._epoch_to_relative_time
-            },
-            'url': {
-                'title': 'URL'
-            },
-            'result': {
-                'title': 'Result'
-            },
-            'voting': {
-                'title': 'Voting',
-                'transform': self._boolean_to_yes_no
-            },
-            'uuid': {
-                'title': 'UUID'
-            },
-            'launch_time': {
-                'title': 'Launch Time',
-                'transform': self._epoch_to_relative_time,
-                'append': ' ago'
-            },
-            'start_time': {
-                'title': 'Start Time',
-                'transform': self._epoch_to_relative_time,
-                'append': ' ago'
-            },
-            'end_time': {
-                'title': 'End Time',
-                'transform': self._epoch_to_relative_time,
-                'append': ' ago'
-            },
-            'estimated_time': {
-                'title': 'Estimated Time',
-                'transform': self._epoch_to_relative_time,
-                'append': ' to go'
-            },
-            'pipeline': {
-                'title': 'Pipeline'
-            },
-            'canceled': {
-                'title': 'Canceled',
-                'transform': self._boolean_to_yes_no
-            },
-            'retry': {
-                'title': 'Retry'
-            },
-            'number': {
-                'title': 'Number'
-            },
-            'node_labels': {
-                'title': 'Node Labels'
-            },
-            'node_name': {
-                'title': 'Node Name'
-            },
-            'worker.name': {
-                'title': 'Worker'
-            },
-            'worker.hostname': {
-                'title': 'Worker Hostname'
-            },
-            'worker.ips': {
-                'title': 'Worker IPs',
-                'transform': self._format_list
-            },
-            'worker.fqdn': {
-                'title': 'Worker Domain'
-            },
-            'worker.program': {
-                'title': 'Worker Program'
-            },
-            'worker.version': {
-                'title': 'Worker Version'
-            },
-            'worker.extra': {
-                'title': 'Worker Extra'
-            },
-        }
-
-
-def main():
-    client = Client()
-    client.main()
-
-
-if __name__ == "__main__":
-    sys.path.insert(0, '.')
-    main()
+#!/usr/bin/env python2.7
+# Copyright 2012 Hewlett-Packard Development Company, L.P.
+# Copyright 2013 OpenStack Foundation
+#
+# Licensed under the Apache License, Version 2.0 (the "License"); you may
+# not use this file except in compliance with the License. You may obtain
+# a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+# License for the specific language governing permissions and limitations
+# under the License.
+
+import argparse
+import babel.dates
+import datetime
+import logging
+import prettytable
+import sys
+import time
+
+
+import zuul.rpcclient
+import zuul.cmd
+
+
+class Client(zuul.cmd.ZuulApp):
+    log = logging.getLogger("zuul.Client")
+
+    def parse_arguments(self):
+        parser = argparse.ArgumentParser(
+            description='Zuul Project Gating System Client.')
+        parser.add_argument('-c', dest='config',
+                            help='specify the config file')
+        parser.add_argument('-v', dest='verbose', action='store_true',
+                            help='verbose output')
+        parser.add_argument('--version', dest='version', action='version',
+                            version=self._get_version(),
+                            help='show zuul version')
+
+        subparsers = parser.add_subparsers(title='commands',
+                                           description='valid commands',
+                                           help='additional help')
+
+        cmd_enqueue = subparsers.add_parser('enqueue', help='enqueue a change')
+        cmd_enqueue.add_argument('--trigger', help='trigger name',
+                                 required=True)
+        cmd_enqueue.add_argument('--pipeline', help='pipeline name',
+                                 required=True)
+        cmd_enqueue.add_argument('--project', help='project name',
+                                 required=True)
+        cmd_enqueue.add_argument('--change', help='change id',
+                                 required=True)
+        cmd_enqueue.set_defaults(func=self.enqueue)
+
+        cmd_enqueue = subparsers.add_parser('enqueue-ref',
+                                            help='enqueue a ref')
+        cmd_enqueue.add_argument('--trigger', help='trigger name',
+                                 required=True)
+        cmd_enqueue.add_argument('--pipeline', help='pipeline name',
+                                 required=True)
+        cmd_enqueue.add_argument('--project', help='project name',
+                                 required=True)
+        cmd_enqueue.add_argument('--ref', help='ref name',
+                                 required=True)
+        cmd_enqueue.add_argument(
+            '--oldrev', help='old revision',
+            default='0000000000000000000000000000000000000000')
+        cmd_enqueue.add_argument(
+            '--newrev', help='new revision',
+            default='0000000000000000000000000000000000000000')
+        cmd_enqueue.set_defaults(func=self.enqueue_ref)
+
+        cmd_promote = subparsers.add_parser('promote',
+                                            help='promote one or more changes')
+        cmd_promote.add_argument('--pipeline', help='pipeline name',
+                                 required=True)
+        cmd_promote.add_argument('--changes', help='change ids',
+                                 required=True, nargs='+')
+        cmd_promote.set_defaults(func=self.promote)
+
+        cmd_show = subparsers.add_parser('show',
+                                         help='valid show subcommands')
+        show_subparsers = cmd_show.add_subparsers(title='show')
+        show_running_jobs = show_subparsers.add_parser(
+            'running-jobs',
+            help='show the running jobs'
+        )
+        show_running_jobs.add_argument(
+            '--columns',
+            help="comma separated list of columns to display (or 'ALL')",
+            choices=self._show_running_jobs_columns().keys().append('ALL'),
+            default='name, worker.name, start_time, result'
+        )
+
+        # TODO: add filters such as queue, project, changeid etc
+        show_running_jobs.set_defaults(func=self.show_running_jobs)
+
+        self.args = parser.parse_args()
+        if self.args.func == self.enqueue_ref:
+            if self.args.oldrev == self.args.newrev:
+                parser.error("The old and new revisions must not be the same.")
+
+    def setup_logging(self):
+        """Client logging does not rely on conf file"""
+        if self.args.verbose:
+            logging.basicConfig(level=logging.DEBUG)
+
+    def main(self):
+        self.parse_arguments()
+        self.read_config()
+        self.setup_logging()
+
+        self.server = self.config.get('gearman', 'server')
+        if self.config.has_option('gearman', 'port'):
+            self.port = self.config.get('gearman', 'port')
+        else:
+            self.port = 4730
+
+        if self.args.func():
+            sys.exit(0)
+        else:
+            sys.exit(1)
+
+    def enqueue(self):
+        client = zuul.rpcclient.RPCClient(self.server, self.port)
+        r = client.enqueue(pipeline=self.args.pipeline,
+                           project=self.args.project,
+                           trigger=self.args.trigger,
+                           change=self.args.change)
+        return r
+
+    def enqueue_ref(self):
+        client = zuul.rpcclient.RPCClient(self.server, self.port)
+        r = client.enqueue_ref(pipeline=self.args.pipeline,
+                               project=self.args.project,
+                               trigger=self.args.trigger,
+                               ref=self.args.ref,
+                               oldrev=self.args.oldrev,
+                               newrev=self.args.newrev)
+        return r
+
+    def promote(self):
+        client = zuul.rpcclient.RPCClient(self.server, self.port)
+        r = client.promote(pipeline=self.args.pipeline,
+                           change_ids=self.args.changes)
+        return r
+
+    def show_running_jobs(self):
+        client = zuul.rpcclient.RPCClient(self.server, self.port)
+        running_items = client.get_running_jobs()
+
+        if len(running_items) == 0:
+            print "No jobs currently running"
+            return True
+
+        all_fields = self._show_running_jobs_columns()
+        if self.args.columns.upper() == 'ALL':
+            fields = all_fields.keys()
+        else:
+            fields = [f.strip().lower() for f in self.args.columns.split(',')
+                      if f.strip().lower() in all_fields.keys()]
+
+        table = prettytable.PrettyTable(
+            field_names=[all_fields[f]['title'] for f in fields])
+        for item in running_items:
+            for job in item['jobs']:
+                values = []
+                for f in fields:
+                    v = job
+                    for part in f.split('.'):
+                        if hasattr(v, 'get'):
+                            v = v.get(part, '')
+                    if ('transform' in all_fields[f]
+                        and callable(all_fields[f]['transform'])):
+                        v = all_fields[f]['transform'](v)
+                    if 'append' in all_fields[f]:
+                        v += all_fields[f]['append']
+                    values.append(v)
+                table.add_row(values)
+        print table
+        return True
+
+    def _epoch_to_relative_time(self, epoch):
+        if epoch:
+            delta = datetime.timedelta(seconds=(time.time() - int(epoch)))
+            return babel.dates.format_timedelta(delta, locale='en_US')
+        else:
+            return "Unknown"
+
+    def _boolean_to_yes_no(self, value):
+        return 'Yes' if value else 'No'
+
+    def _boolean_to_pass_fail(self, value):
+        return 'Pass' if value else 'Fail'
+
+    def _format_list(self, l):
+        return ', '.join(l) if isinstance(l, list) else ''
+
+    def _show_running_jobs_columns(self):
+        """A helper function to get the list of available columns for
+        `zuul show running-jobs`. Also describes how to convert particular
+        values (for example epoch to time string)"""
+
+        return {
+            'name': {
+                'title': 'Job Name',
+            },
+            'elapsed_time': {
+                'title': 'Elapsed Time',
+                'transform': self._epoch_to_relative_time
+            },
+            'remaining_time': {
+                'title': 'Remaining Time',
+                'transform': self._epoch_to_relative_time
+            },
+            'url': {
+                'title': 'URL'
+            },
+            'result': {
+                'title': 'Result'
+            },
+            'voting': {
+                'title': 'Voting',
+                'transform': self._boolean_to_yes_no
+            },
+            'uuid': {
+                'title': 'UUID'
+            },
+            'launch_time': {
+                'title': 'Launch Time',
+                'transform': self._epoch_to_relative_time,
+                'append': ' ago'
+            },
+            'start_time': {
+                'title': 'Start Time',
+                'transform': self._epoch_to_relative_time,
+                'append': ' ago'
+            },
+            'end_time': {
+                'title': 'End Time',
+                'transform': self._epoch_to_relative_time,
+                'append': ' ago'
+            },
+            'estimated_time': {
+                'title': 'Estimated Time',
+                'transform': self._epoch_to_relative_time,
+                'append': ' to go'
+            },
+            'pipeline': {
+                'title': 'Pipeline'
+            },
+            'canceled': {
+                'title': 'Canceled',
+                'transform': self._boolean_to_yes_no
+            },
+            'retry': {
+                'title': 'Retry'
+            },
+            'number': {
+                'title': 'Number'
+            },
+            'node_labels': {
+                'title': 'Node Labels'
+            },
+            'node_name': {
+                'title': 'Node Name'
+            },
+            'worker.name': {
+                'title': 'Worker'
+            },
+            'worker.hostname': {
+                'title': 'Worker Hostname'
+            },
+            'worker.ips': {
+                'title': 'Worker IPs',
+                'transform': self._format_list
+            },
+            'worker.fqdn': {
+                'title': 'Worker Domain'
+            },
+            'worker.program': {
+                'title': 'Worker Program'
+            },
+            'worker.version': {
+                'title': 'Worker Version'
+            },
+            'worker.extra': {
+                'title': 'Worker Extra'
+            },
+        }
+
+
+def main():
+    client = Client()
+    client.main()
+
+
+if __name__ == "__main__":
+    sys.path.insert(0, '.')
+    main()
diff --git a/zuul/cmd/cloner.py b/zuul/cmd/cloner.py
index 63825e4..73f715d 100755
--- a/zuul/cmd/cloner.py
+++ b/zuul/cmd/cloner.py
@@ -1,164 +1,164 @@
-#!/usr/bin/env python2.7
-#
-# Copyright 2014 Antoine "hashar" Musso
-# Copyright 2014 Wikimedia Foundation Inc.
-#
-# Licensed under the Apache License, Version 2.0 (the "License"); you may
-# not use this file except in compliance with the License. You may obtain
-# a copy of the License at
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-# License for the specific language governing permissions and limitations
-# under the License.
-
-import argparse
-import logging
-import os
-import sys
-
-import zuul.cmd
-import zuul.lib.cloner
-
-ZUUL_ENV_SUFFIXES = (
-    'branch',
-    'ref',
-    'url',
-)
-
-
-class Cloner(zuul.cmd.ZuulApp):
-    log = logging.getLogger("zuul.Cloner")
-
-    def parse_arguments(self, args=sys.argv[1:]):
-        """Parse command line arguments and returns argparse structure"""
-        parser = argparse.ArgumentParser(
-            description='Zuul Project Gating System Cloner.')
-        parser.add_argument('-m', '--map', dest='clone_map_file',
-                            help='specifiy clone map file')
-        parser.add_argument('--workspace', dest='workspace',
-                            default=os.getcwd(),
-                            help='where to clone repositories too')
-        parser.add_argument('-v', '--verbose', dest='verbose',
-                            action='store_true',
-                            help='verbose output')
-        parser.add_argument('--color', dest='color', action='store_true',
-                            help='use color output')
-        parser.add_argument('--version', dest='version', action='version',
-                            version=self._get_version(),
-                            help='show zuul version')
-        parser.add_argument('--cache-dir', dest='cache_dir',
-                            default=os.environ.get('ZUUL_CACHE_DIR'),
-                            help=('a directory that holds cached copies of '
-                                  'repos from which to make an initial clone. '
-                                  'Can also be set via ZUUL_CACHE_DIR '
-                                  'environment variable.'
-                                  ))
-        parser.add_argument('--cache-no-hardlinks', dest='cache_no_hardlinks',
-                            action='store_true',
-                            help=('force git-clone to never use hardlinks when'
-                                  'fetching from the cache directory.'))
-        parser.add_argument('git_base_url',
-                            help='reference repo to clone from')
-        parser.add_argument('projects', nargs='+',
-                            help='list of Gerrit projects to clone')
-
-        project_env = parser.add_argument_group(
-            'project tuning'
-        )
-        project_env.add_argument(
-            '--branch',
-            help=('branch to checkout instead of Zuul selected branch, '
-                  'for example to specify an alternate branch to test '
-                  'client library compatibility.')
-        )
-        project_env.add_argument(
-            '--project-branch', nargs=1, action='append',
-            metavar='PROJECT=BRANCH',
-            help=('project-specific branch to checkout which takes precedence '
-                  'over --branch if it is provided; may be specified multiple '
-                  'times.')
-        )
-
-        zuul_env = parser.add_argument_group(
-            'zuul environment',
-            'Let you override $ZUUL_* environment variables.'
-        )
-        for zuul_suffix in ZUUL_ENV_SUFFIXES:
-            env_name = 'ZUUL_%s' % zuul_suffix.upper()
-            zuul_env.add_argument(
-                '--zuul-%s' % zuul_suffix, metavar='$' + env_name,
-                default=os.environ.get(env_name)
-            )
-
-        args = parser.parse_args(args)
-        # Validate ZUUL_* arguments. If ref is provided then URL is required.
-        zuul_args = [zuul_opt for zuul_opt, val in vars(args).items()
-                     if zuul_opt.startswith('zuul') and val is not None]
-        if 'zuul_ref' in zuul_args and 'zuul_url' not in zuul_args:
-            parser.error("Specifying a Zuul ref requires a Zuul url. "
-                         "Define Zuul arguments either via environment "
-                         "variables or using options above.")
-
-        self.args = args
-
-    def setup_logging(self, color=False, verbose=False):
-        """Cloner logging does not rely on conf file"""
-        if verbose:
-            logging.basicConfig(level=logging.DEBUG)
-        else:
-            logging.basicConfig(level=logging.INFO)
-
-        if color:
-            # Color codes http://www.tldp.org/HOWTO/Bash-Prompt-HOWTO/x329.html
-            logging.addLevelName(  # cyan
-                logging.DEBUG, "\033[36m%s\033[0m" %
-                logging.getLevelName(logging.DEBUG))
-            logging.addLevelName(  # green
-                logging.INFO, "\033[32m%s\033[0m" %
-                logging.getLevelName(logging.INFO))
-            logging.addLevelName(  # yellow
-                logging.WARNING, "\033[33m%s\033[0m" %
-                logging.getLevelName(logging.WARNING))
-            logging.addLevelName(  # red
-                logging.ERROR, "\033[31m%s\033[0m" %
-                logging.getLevelName(logging.ERROR))
-            logging.addLevelName(  # red background
-                logging.CRITICAL, "\033[41m%s\033[0m" %
-                logging.getLevelName(logging.CRITICAL))
-
-    def main(self):
-        self.parse_arguments()
-        self.setup_logging(color=self.args.color, verbose=self.args.verbose)
-        project_branches = {}
-        if self.args.project_branch:
-            for x in self.args.project_branch:
-                project, branch = x[0].split('=')
-                project_branches[project] = branch
-        cloner = zuul.lib.cloner.Cloner(
-            git_base_url=self.args.git_base_url,
-            projects=self.args.projects,
-            workspace=self.args.workspace,
-            zuul_branch=self.args.zuul_branch,
-            zuul_ref=self.args.zuul_ref,
-            zuul_url=self.args.zuul_url,
-            branch=self.args.branch,
-            clone_map_file=self.args.clone_map_file,
-            project_branches=project_branches,
-            cache_dir=self.args.cache_dir,
-            cache_no_hardlinks=self.args.cache_no_hardlinks,
-        )
-        cloner.execute()
-
-
-def main():
-    cloner = Cloner()
-    cloner.main()
-
-
-if __name__ == "__main__":
-    sys.path.insert(0, '.')
-    main()
+#!/usr/bin/env python2.7
+#
+# Copyright 2014 Antoine "hashar" Musso
+# Copyright 2014 Wikimedia Foundation Inc.
+#
+# Licensed under the Apache License, Version 2.0 (the "License"); you may
+# not use this file except in compliance with the License. You may obtain
+# a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+# License for the specific language governing permissions and limitations
+# under the License.
+
+import argparse
+import logging
+import os
+import sys
+
+import zuul.cmd
+import zuul.lib.cloner
+
+ZUUL_ENV_SUFFIXES = (
+    'branch',
+    'ref',
+    'url',
+)
+
+
+class Cloner(zuul.cmd.ZuulApp):
+    log = logging.getLogger("zuul.Cloner")
+
+    def parse_arguments(self, args=sys.argv[1:]):
+        """Parse command line arguments and returns argparse structure"""
+        parser = argparse.ArgumentParser(
+            description='Zuul Project Gating System Cloner.')
+        parser.add_argument('-m', '--map', dest='clone_map_file',
+                            help='specifiy clone map file')
+        parser.add_argument('--workspace', dest='workspace',
+                            default=os.getcwd(),
+                            help='where to clone repositories too')
+        parser.add_argument('-v', '--verbose', dest='verbose',
+                            action='store_true',
+                            help='verbose output')
+        parser.add_argument('--color', dest='color', action='store_true',
+                            help='use color output')
+        parser.add_argument('--version', dest='version', action='version',
+                            version=self._get_version(),
+                            help='show zuul version')
+        parser.add_argument('--cache-dir', dest='cache_dir',
+                            default=os.environ.get('ZUUL_CACHE_DIR'),
+                            help=('a directory that holds cached copies of '
+                                  'repos from which to make an initial clone. '
+                                  'Can also be set via ZUUL_CACHE_DIR '
+                                  'environment variable.'
+                                  ))
+        parser.add_argument('--cache-no-hardlinks', dest='cache_no_hardlinks',
+                            action='store_true',
+                            help=('force git-clone to never use hardlinks when'
+                                  'fetching from the cache directory.'))
+        parser.add_argument('git_base_url',
+                            help='reference repo to clone from')
+        parser.add_argument('projects', nargs='+',
+                            help='list of Gerrit projects to clone')
+
+        project_env = parser.add_argument_group(
+            'project tuning'
+        )
+        project_env.add_argument(
+            '--branch',
+            help=('branch to checkout instead of Zuul selected branch, '
+                  'for example to specify an alternate branch to test '
+                  'client library compatibility.')
+        )
+        project_env.add_argument(
+            '--project-branch', nargs=1, action='append',
+            metavar='PROJECT=BRANCH',
+            help=('project-specific branch to checkout which takes precedence '
+                  'over --branch if it is provided; may be specified multiple '
+                  'times.')
+        )
+
+        zuul_env = parser.add_argument_group(
+            'zuul environment',
+            'Let you override $ZUUL_* environment variables.'
+        )
+        for zuul_suffix in ZUUL_ENV_SUFFIXES:
+            env_name = 'ZUUL_%s' % zuul_suffix.upper()
+            zuul_env.add_argument(
+                '--zuul-%s' % zuul_suffix, metavar='$' + env_name,
+                default=os.environ.get(env_name)
+            )
+
+        args = parser.parse_args(args)
+        # Validate ZUUL_* arguments. If ref is provided then URL is required.
+        zuul_args = [zuul_opt for zuul_opt, val in vars(args).items()
+                     if zuul_opt.startswith('zuul') and val is not None]
+        if 'zuul_ref' in zuul_args and 'zuul_url' not in zuul_args:
+            parser.error("Specifying a Zuul ref requires a Zuul url. "
+                         "Define Zuul arguments either via environment "
+                         "variables or using options above.")
+
+        self.args = args
+
+    def setup_logging(self, color=False, verbose=False):
+        """Cloner logging does not rely on conf file"""
+        if verbose:
+            logging.basicConfig(level=logging.DEBUG)
+        else:
+            logging.basicConfig(level=logging.INFO)
+
+        if color:
+            # Color codes http://www.tldp.org/HOWTO/Bash-Prompt-HOWTO/x329.html
+            logging.addLevelName(  # cyan
+                logging.DEBUG, "\033[36m%s\033[0m" %
+                logging.getLevelName(logging.DEBUG))
+            logging.addLevelName(  # green
+                logging.INFO, "\033[32m%s\033[0m" %
+                logging.getLevelName(logging.INFO))
+            logging.addLevelName(  # yellow
+                logging.WARNING, "\033[33m%s\033[0m" %
+                logging.getLevelName(logging.WARNING))
+            logging.addLevelName(  # red
+                logging.ERROR, "\033[31m%s\033[0m" %
+                logging.getLevelName(logging.ERROR))
+            logging.addLevelName(  # red background
+                logging.CRITICAL, "\033[41m%s\033[0m" %
+                logging.getLevelName(logging.CRITICAL))
+
+    def main(self):
+        self.parse_arguments()
+        self.setup_logging(color=self.args.color, verbose=self.args.verbose)
+        project_branches = {}
+        if self.args.project_branch:
+            for x in self.args.project_branch:
+                project, branch = x[0].split('=')
+                project_branches[project] = branch
+        cloner = zuul.lib.cloner.Cloner(
+            git_base_url=self.args.git_base_url,
+            projects=self.args.projects,
+            workspace=self.args.workspace,
+            zuul_branch=self.args.zuul_branch,
+            zuul_ref=self.args.zuul_ref,
+            zuul_url=self.args.zuul_url,
+            branch=self.args.branch,
+            clone_map_file=self.args.clone_map_file,
+            project_branches=project_branches,
+            cache_dir=self.args.cache_dir,
+            cache_no_hardlinks=self.args.cache_no_hardlinks,
+        )
+        cloner.execute()
+
+
+def main():
+    cloner = Cloner()
+    cloner.main()
+
+
+if __name__ == "__main__":
+    sys.path.insert(0, '.')
+    main()
diff --git a/zuul/cmd/merger.py b/zuul/cmd/merger.py
index 5f51ee6..09d80ed 100644
--- a/zuul/cmd/merger.py
+++ b/zuul/cmd/merger.py
@@ -1,112 +1,112 @@
-#!/usr/bin/env python2.7
-# Copyright 2012 Hewlett-Packard Development Company, L.P.
-# Copyright 2013-2014 OpenStack Foundation
-#
-# Licensed under the Apache License, Version 2.0 (the "License"); you may
-# not use this file except in compliance with the License. You may obtain
-# a copy of the License at
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-# License for the specific language governing permissions and limitations
-# under the License.
-
-import argparse
-import daemon
-import extras
-
-# as of python-daemon 1.6 it doesn't bundle pidlockfile anymore
-# instead it depends on lockfile-0.9.1 which uses pidfile.
-pid_file_module = extras.try_imports(['daemon.pidlockfile', 'daemon.pidfile'])
-
-import os
-import sys
-import signal
-
-import zuul.cmd
-
-# No zuul imports here because they pull in paramiko which must not be
-# imported until after the daemonization.
-# https://github.com/paramiko/paramiko/issues/59
-# Similar situation with gear and statsd.
-
-
-class Merger(zuul.cmd.ZuulApp):
-
-    def parse_arguments(self):
-        parser = argparse.ArgumentParser(description='Zuul merge worker.')
-        parser.add_argument('-c', dest='config',
-                            help='specify the config file')
-        parser.add_argument('-d', dest='nodaemon', action='store_true',
-                            help='do not run as a daemon')
-        parser.add_argument('--version', dest='version', action='version',
-                            version=self._get_version(),
-                            help='show zuul version')
-        self.args = parser.parse_args()
-
-    def exit_handler(self, signum, frame):
-        signal.signal(signal.SIGUSR1, signal.SIG_IGN)
-        self.merger.stop()
-        self.merger.join()
-
-    def main(self):
-        # See comment at top of file about zuul imports
-        import zuul.merger.server
-
-        self.setup_logging('merger', 'log_config')
-
-        self.merger = zuul.merger.server.MergeServer(self.config,
-                                                     self.connections)
-        self.merger.start()
-
-        signal.signal(signal.SIGUSR1, self.exit_handler)
-        signal.signal(signal.SIGUSR2, zuul.cmd.stack_dump_handler)
-        while True:
-            try:
-                signal.pause()
-            except KeyboardInterrupt:
-                print "Ctrl + C: asking merger to exit nicely...\n"
-                self.exit_handler(signal.SIGINT, None)
-
-
-def main():
-    server = Merger()
-    server.parse_arguments()
-
-    server.read_config()
-    server.configure_connections()
-
-    if server.config.has_option('zuul', 'state_dir'):
-        state_dir = os.path.expanduser(server.config.get('zuul', 'state_dir'))
-    else:
-        state_dir = '/var/lib/zuul'
-    test_fn = os.path.join(state_dir, 'test')
-    try:
-        f = open(test_fn, 'w')
-        f.close()
-        os.unlink(test_fn)
-    except Exception:
-        print
-        print "Unable to write to state directory: %s" % state_dir
-        print
-        raise
-
-    if server.config.has_option('merger', 'pidfile'):
-        pid_fn = os.path.expanduser(server.config.get('merger', 'pidfile'))
-    else:
-        pid_fn = '/var/run/zuul-merger/zuul-merger.pid'
-    pid = pid_file_module.TimeoutPIDLockFile(pid_fn, 10)
-
-    if server.args.nodaemon:
-        server.main()
-    else:
-        with daemon.DaemonContext(pidfile=pid):
-            server.main()
-
-
-if __name__ == "__main__":
-    sys.path.insert(0, '.')
-    main()
+#!/usr/bin/env python2.7
+# Copyright 2012 Hewlett-Packard Development Company, L.P.
+# Copyright 2013-2014 OpenStack Foundation
+#
+# Licensed under the Apache License, Version 2.0 (the "License"); you may
+# not use this file except in compliance with the License. You may obtain
+# a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+# License for the specific language governing permissions and limitations
+# under the License.
+
+import argparse
+import daemon
+import extras
+
+# as of python-daemon 1.6 it doesn't bundle pidlockfile anymore
+# instead it depends on lockfile-0.9.1 which uses pidfile.
+pid_file_module = extras.try_imports(['daemon.pidlockfile', 'daemon.pidfile'])
+
+import os
+import sys
+import signal
+
+import zuul.cmd
+
+# No zuul imports here because they pull in paramiko which must not be
+# imported until after the daemonization.
+# https://github.com/paramiko/paramiko/issues/59
+# Similar situation with gear and statsd.
+
+
+class Merger(zuul.cmd.ZuulApp):
+
+    def parse_arguments(self):
+        parser = argparse.ArgumentParser(description='Zuul merge worker.')
+        parser.add_argument('-c', dest='config',
+                            help='specify the config file')
+        parser.add_argument('-d', dest='nodaemon', action='store_true',
+                            help='do not run as a daemon')
+        parser.add_argument('--version', dest='version', action='version',
+                            version=self._get_version(),
+                            help='show zuul version')
+        self.args = parser.parse_args()
+
+    def exit_handler(self, signum, frame):
+        signal.signal(signal.SIGUSR1, signal.SIG_IGN)
+        self.merger.stop()
+        self.merger.join()
+
+    def main(self):
+        # See comment at top of file about zuul imports
+        import zuul.merger.server
+
+        self.setup_logging('merger', 'log_config')
+
+        self.merger = zuul.merger.server.MergeServer(self.config,
+                                                     self.connections)
+        self.merger.start()
+
+        signal.signal(signal.SIGUSR1, self.exit_handler)
+        signal.signal(signal.SIGUSR2, zuul.cmd.stack_dump_handler)
+        while True:
+            try:
+                signal.pause()
+            except KeyboardInterrupt:
+                print "Ctrl + C: asking merger to exit nicely...\n"
+                self.exit_handler(signal.SIGINT, None)
+
+
+def main():
+    server = Merger()
+    server.parse_arguments()
+
+    server.read_config()
+    server.configure_connections()
+
+    if server.config.has_option('zuul', 'state_dir'):
+        state_dir = os.path.expanduser(server.config.get('zuul', 'state_dir'))
+    else:
+        state_dir = '/var/lib/zuul'
+    test_fn = os.path.join(state_dir, 'test')
+    try:
+        f = open(test_fn, 'w')
+        f.close()
+        os.unlink(test_fn)
+    except Exception:
+        print
+        print "Unable to write to state directory: %s" % state_dir
+        print
+        raise
+
+    if server.config.has_option('merger', 'pidfile'):
+        pid_fn = os.path.expanduser(server.config.get('merger', 'pidfile'))
+    else:
+        pid_fn = '/var/run/zuul-merger/zuul-merger.pid'
+    pid = pid_file_module.TimeoutPIDLockFile(pid_fn, 10)
+
+    if server.args.nodaemon:
+        server.main()
+    else:
+        with daemon.DaemonContext(pidfile=pid):
+            server.main()
+
+
+if __name__ == "__main__":
+    sys.path.insert(0, '.')
+    main()
diff --git a/zuul/cmd/server.py b/zuul/cmd/server.py
index e713a52..b37cb88 100755
--- a/zuul/cmd/server.py
+++ b/zuul/cmd/server.py
@@ -1,233 +1,233 @@
-#!/usr/bin/env python2.7
-# Copyright 2012 Hewlett-Packard Development Company, L.P.
-# Copyright 2013 OpenStack Foundation
-#
-# Licensed under the Apache License, Version 2.0 (the "License"); you may
-# not use this file except in compliance with the License. You may obtain
-# a copy of the License at
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-# License for the specific language governing permissions and limitations
-# under the License.
-
-import argparse
-import daemon
-import extras
-
-# as of python-daemon 1.6 it doesn't bundle pidlockfile anymore
-# instead it depends on lockfile-0.9.1 which uses pidfile.
-pid_file_module = extras.try_imports(['daemon.pidlockfile', 'daemon.pidfile'])
-
-import logging
-import os
-import sys
-import signal
-
-import zuul.cmd
-
-# No zuul imports here because they pull in paramiko which must not be
-# imported until after the daemonization.
-# https://github.com/paramiko/paramiko/issues/59
-# Similar situation with gear and statsd.
-
-
-class Server(zuul.cmd.ZuulApp):
-    def __init__(self):
-        super(Server, self).__init__()
-        self.gear_server_pid = None
-
-    def parse_arguments(self):
-        parser = argparse.ArgumentParser(description='Project gating system.')
-        parser.add_argument('-c', dest='config',
-                            help='specify the config file')
-        parser.add_argument('-l', dest='layout',
-                            help='specify the layout file')
-        parser.add_argument('-d', dest='nodaemon', action='store_true',
-                            help='do not run as a daemon')
-        parser.add_argument('-t', dest='validate', nargs='?', const=True,
-                            metavar='JOB_LIST',
-                            help='validate layout file syntax (optionally '
-                            'providing the path to a file with a list of '
-                            'available job names)')
-        parser.add_argument('--version', dest='version', action='version',
-                            version=self._get_version(),
-                            help='show zuul version')
-        self.args = parser.parse_args()
-
-    def reconfigure_handler(self, signum, frame):
-        signal.signal(signal.SIGHUP, signal.SIG_IGN)
-        self.log.debug("Reconfiguration triggered")
-        self.read_config()
-        self.setup_logging('zuul', 'log_config')
-        try:
-            self.sched.reconfigure(self.config)
-        except Exception:
-            self.log.exception("Reconfiguration failed:")
-        signal.signal(signal.SIGHUP, self.reconfigure_handler)
-
-    def exit_handler(self, signum, frame):
-        signal.signal(signal.SIGUSR1, signal.SIG_IGN)
-        self.sched.exit()
-        self.sched.join()
-        self.stop_gear_server()
-
-    def term_handler(self, signum, frame):
-        self.stop_gear_server()
-        os._exit(0)
-
-    def test_config(self, job_list_path):
-        # See comment at top of file about zuul imports
-        import zuul.scheduler
-        import zuul.launcher.gearman
-        import zuul.trigger.gerrit
-
-        logging.basicConfig(level=logging.DEBUG)
-        self.sched = zuul.scheduler.Scheduler(self.config)
-        self.configure_connections()
-        self.sched.registerConnections(self.connections, load=False)
-        layout = self.sched.testConfig(self.config.get('zuul',
-                                                       'layout_config'),
-                                       self.connections)
-        if not job_list_path:
-            return False
-
-        failure = False
-        path = os.path.expanduser(job_list_path)
-        if not os.path.exists(path):
-            raise Exception("Unable to find job list: %s" % path)
-        jobs = set()
-        jobs.add('noop')
-        for line in open(path):
-            v = line.strip()
-            if v:
-                jobs.add(v)
-        for job in sorted(layout.jobs):
-            if job not in jobs:
-                print "Job %s not defined" % job
-                failure = True
-        return failure
-
-    def start_gear_server(self):
-        pipe_read, pipe_write = os.pipe()
-        child_pid = os.fork()
-        if child_pid == 0:
-            os.close(pipe_write)
-            self.setup_logging('gearman_server', 'log_config')
-            import gear
-            statsd_host = os.environ.get('STATSD_HOST')
-            statsd_port = int(os.environ.get('STATSD_PORT', 8125))
-            if self.config.has_option('gearman_server', 'listen_address'):
-                host = self.config.get('gearman_server', 'listen_address')
-            else:
-                host = None
-            gear.Server(4730,
-                        host=host,
-                        statsd_host=statsd_host,
-                        statsd_port=statsd_port,
-                        statsd_prefix='zuul.geard')
-
-            # Keep running until the parent dies:
-            pipe_read = os.fdopen(pipe_read)
-            pipe_read.read()
-            os._exit(0)
-        else:
-            os.close(pipe_read)
-            self.gear_server_pid = child_pid
-            self.gear_pipe_write = pipe_write
-
-    def stop_gear_server(self):
-        if self.gear_server_pid:
-            os.kill(self.gear_server_pid, signal.SIGKILL)
-
-    def main(self):
-        # See comment at top of file about zuul imports
-        import zuul.scheduler
-        import zuul.launcher.gearman
-        import zuul.merger.client
-        import zuul.lib.swift
-        import zuul.webapp
-        import zuul.rpclistener
-
-        signal.signal(signal.SIGUSR2, zuul.cmd.stack_dump_handler)
-        if (self.config.has_option('gearman_server', 'start') and
-            self.config.getboolean('gearman_server', 'start')):
-            self.start_gear_server()
-
-        self.setup_logging('zuul', 'log_config')
-        self.log = logging.getLogger("zuul.Server")
-
-        self.sched = zuul.scheduler.Scheduler(self.config)
-        # TODO(jhesketh): Move swift into a connection?
-        self.swift = zuul.lib.swift.Swift(self.config)
-
-        gearman = zuul.launcher.gearman.Gearman(self.config, self.sched,
-                                                self.swift)
-        merger = zuul.merger.client.MergeClient(self.config, self.sched)
-
-        if self.config.has_option('zuul', 'status_expiry'):
-            cache_expiry = self.config.getint('zuul', 'status_expiry')
-        else:
-            cache_expiry = 1
-        webapp = zuul.webapp.WebApp(self.sched, cache_expiry=cache_expiry)
-        rpc = zuul.rpclistener.RPCListener(self.config, self.sched)
-
-        self.configure_connections()
-        self.sched.setLauncher(gearman)
-        self.sched.setMerger(merger)
-
-        self.log.info('Starting scheduler')
-        self.sched.start()
-        self.sched.registerConnections(self.connections)
-        self.sched.reconfigure(self.config)
-        self.sched.resume()
-        self.log.info('Starting Webapp')
-        webapp.start()
-        self.log.info('Starting RPC')
-        rpc.start()
-
-        signal.signal(signal.SIGHUP, self.reconfigure_handler)
-        signal.signal(signal.SIGUSR1, self.exit_handler)
-        signal.signal(signal.SIGTERM, self.term_handler)
-        while True:
-            try:
-                signal.pause()
-            except KeyboardInterrupt:
-                print "Ctrl + C: asking scheduler to exit nicely...\n"
-                self.exit_handler(signal.SIGINT, None)
-
-
-def main():
-    server = Server()
-    server.parse_arguments()
-
-    server.read_config()
-
-    if server.args.layout:
-        server.config.set('zuul', 'layout_config', server.args.layout)
-
-    if server.args.validate:
-        path = server.args.validate
-        if path is True:
-            path = None
-        sys.exit(server.test_config(path))
-
-    if server.config.has_option('zuul', 'pidfile'):
-        pid_fn = os.path.expanduser(server.config.get('zuul', 'pidfile'))
-    else:
-        pid_fn = '/var/run/zuul/zuul.pid'
-    pid = pid_file_module.TimeoutPIDLockFile(pid_fn, 10)
-
-    if server.args.nodaemon:
-        server.main()
-    else:
-        with daemon.DaemonContext(pidfile=pid):
-            server.main()
-
-
-if __name__ == "__main__":
-    sys.path.insert(0, '.')
-    main()
+#!/usr/bin/env python2.7
+# Copyright 2012 Hewlett-Packard Development Company, L.P.
+# Copyright 2013 OpenStack Foundation
+#
+# Licensed under the Apache License, Version 2.0 (the "License"); you may
+# not use this file except in compliance with the License. You may obtain
+# a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+# License for the specific language governing permissions and limitations
+# under the License.
+
+import argparse
+import daemon
+import extras
+
+# as of python-daemon 1.6 it doesn't bundle pidlockfile anymore
+# instead it depends on lockfile-0.9.1 which uses pidfile.
+pid_file_module = extras.try_imports(['daemon.pidlockfile', 'daemon.pidfile'])
+
+import logging
+import os
+import sys
+import signal
+
+import zuul.cmd
+
+# No zuul imports here because they pull in paramiko which must not be
+# imported until after the daemonization.
+# https://github.com/paramiko/paramiko/issues/59
+# Similar situation with gear and statsd.
+
+
+class Server(zuul.cmd.ZuulApp):
+    def __init__(self):
+        super(Server, self).__init__()
+        self.gear_server_pid = None
+
+    def parse_arguments(self):
+        parser = argparse.ArgumentParser(description='Project gating system.')
+        parser.add_argument('-c', dest='config',
+                            help='specify the config file')
+        parser.add_argument('-l', dest='layout',
+                            help='specify the layout file')
+        parser.add_argument('-d', dest='nodaemon', action='store_true',
+                            help='do not run as a daemon')
+        parser.add_argument('-t', dest='validate', nargs='?', const=True,
+                            metavar='JOB_LIST',
+                            help='validate layout file syntax (optionally '
+                            'providing the path to a file with a list of '
+                            'available job names)')
+        parser.add_argument('--version', dest='version', action='version',
+                            version=self._get_version(),
+                            help='show zuul version')
+        self.args = parser.parse_args()
+
+    def reconfigure_handler(self, signum, frame):
+        signal.signal(signal.SIGHUP, signal.SIG_IGN)
+        self.log.debug("Reconfiguration triggered")
+        self.read_config()
+        self.setup_logging('zuul', 'log_config')
+        try:
+            self.sched.reconfigure(self.config)
+        except Exception:
+            self.log.exception("Reconfiguration failed:")
+        signal.signal(signal.SIGHUP, self.reconfigure_handler)
+
+    def exit_handler(self, signum, frame):
+        signal.signal(signal.SIGUSR1, signal.SIG_IGN)
+        self.sched.exit()
+        self.sched.join()
+        self.stop_gear_server()
+
+    def term_handler(self, signum, frame):
+        self.stop_gear_server()
+        os._exit(0)
+
+    def test_config(self, job_list_path):
+        # See comment at top of file about zuul imports
+        import zuul.scheduler
+        import zuul.launcher.gearman
+        import zuul.trigger.gerrit
+
+        logging.basicConfig(level=logging.DEBUG)
+        self.sched = zuul.scheduler.Scheduler(self.config)
+        self.configure_connections()
+        self.sched.registerConnections(self.connections, load=False)
+        layout = self.sched.testConfig(self.config.get('zuul',
+                                                       'layout_config'),
+                                       self.connections)
+        if not job_list_path:
+            return False
+
+        failure = False
+        path = os.path.expanduser(job_list_path)
+        if not os.path.exists(path):
+            raise Exception("Unable to find job list: %s" % path)
+        jobs = set()
+        jobs.add('noop')
+        for line in open(path):
+            v = line.strip()
+            if v:
+                jobs.add(v)
+        for job in sorted(layout.jobs):
+            if job not in jobs:
+                print "Job %s not defined" % job
+                failure = True
+        return failure
+
+    def start_gear_server(self):
+        pipe_read, pipe_write = os.pipe()
+        child_pid = os.fork()
+        if child_pid == 0:
+            os.close(pipe_write)
+            self.setup_logging('gearman_server', 'log_config')
+            import gear
+            statsd_host = os.environ.get('STATSD_HOST')
+            statsd_port = int(os.environ.get('STATSD_PORT', 8125))
+            if self.config.has_option('gearman_server', 'listen_address'):
+                host = self.config.get('gearman_server', 'listen_address')
+            else:
+                host = None
+            gear.Server(4730,
+                        host=host,
+                        statsd_host=statsd_host,
+                        statsd_port=statsd_port,
+                        statsd_prefix='zuul.geard')
+
+            # Keep running until the parent dies:
+            pipe_read = os.fdopen(pipe_read)
+            pipe_read.read()
+            os._exit(0)
+        else:
+            os.close(pipe_read)
+            self.gear_server_pid = child_pid
+            self.gear_pipe_write = pipe_write
+
+    def stop_gear_server(self):
+        if self.gear_server_pid:
+            os.kill(self.gear_server_pid, signal.SIGKILL)
+
+    def main(self):
+        # See comment at top of file about zuul imports
+        import zuul.scheduler
+        import zuul.launcher.gearman
+        import zuul.merger.client
+        import zuul.lib.swift
+        import zuul.webapp
+        import zuul.rpclistener
+
+        signal.signal(signal.SIGUSR2, zuul.cmd.stack_dump_handler)
+        if (self.config.has_option('gearman_server', 'start') and
+            self.config.getboolean('gearman_server', 'start')):
+            self.start_gear_server()
+
+        self.setup_logging('zuul', 'log_config')
+        self.log = logging.getLogger("zuul.Server")
+
+        self.sched = zuul.scheduler.Scheduler(self.config)
+        # TODO(jhesketh): Move swift into a connection?
+        self.swift = zuul.lib.swift.Swift(self.config)
+
+        gearman = zuul.launcher.gearman.Gearman(self.config, self.sched,
+                                                self.swift)
+        merger = zuul.merger.client.MergeClient(self.config, self.sched)
+
+        if self.config.has_option('zuul', 'status_expiry'):
+            cache_expiry = self.config.getint('zuul', 'status_expiry')
+        else:
+            cache_expiry = 1
+        webapp = zuul.webapp.WebApp(self.sched, cache_expiry=cache_expiry)
+        rpc = zuul.rpclistener.RPCListener(self.config, self.sched)
+
+        self.configure_connections()
+        self.sched.setLauncher(gearman)
+        self.sched.setMerger(merger)
+
+        self.log.info('Starting scheduler')
+        self.sched.start()
+        self.sched.registerConnections(self.connections)
+        self.sched.reconfigure(self.config)
+        self.sched.resume()
+        self.log.info('Starting Webapp')
+        webapp.start()
+        self.log.info('Starting RPC')
+        rpc.start()
+
+        signal.signal(signal.SIGHUP, self.reconfigure_handler)
+        signal.signal(signal.SIGUSR1, self.exit_handler)
+        signal.signal(signal.SIGTERM, self.term_handler)
+        while True:
+            try:
+                signal.pause()
+            except KeyboardInterrupt:
+                print "Ctrl + C: asking scheduler to exit nicely...\n"
+                self.exit_handler(signal.SIGINT, None)
+
+
+def main():
+    server = Server()
+    server.parse_arguments()
+
+    server.read_config()
+
+    if server.args.layout:
+        server.config.set('zuul', 'layout_config', server.args.layout)
+
+    if server.args.validate:
+        path = server.args.validate
+        if path is True:
+            path = None
+        sys.exit(server.test_config(path))
+
+    if server.config.has_option('zuul', 'pidfile'):
+        pid_fn = os.path.expanduser(server.config.get('zuul', 'pidfile'))
+    else:
+        pid_fn = '/var/run/zuul/zuul.pid'
+    pid = pid_file_module.TimeoutPIDLockFile(pid_fn, 10)
+
+    if server.args.nodaemon:
+        server.main()
+    else:
+        with daemon.DaemonContext(pidfile=pid):
+            server.main()
+
+
+if __name__ == "__main__":
+    sys.path.insert(0, '.')
+    main()
diff --git a/zuul/connection/__init__.py b/zuul/connection/__init__.py
index 066b4db..7f765d0 100644
--- a/zuul/connection/__init__.py
+++ b/zuul/connection/__init__.py
@@ -1,71 +1,71 @@
-# Copyright 2014 Rackspace Australia
-#
-# Licensed under the Apache License, Version 2.0 (the "License"); you may
-# not use this file except in compliance with the License. You may obtain
-# a copy of the License at
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-# License for the specific language governing permissions and limitations
-# under the License.
-
-import abc
-
-import six
-
-
-@six.add_metaclass(abc.ABCMeta)
-class BaseConnection(object):
-    """Base class for connections.
-
-    A connection is a shared object that sources, triggers and reporters can
-    use to speak with a remote API without needing to establish a new
-    connection each time or without having to authenticate each time.
-
-    Multiple instances of the same connection may exist with different
-    credentials, for example, thus allowing for different pipelines to operate
-    on different Gerrit installations or post back as a different user etc.
-
-    Connections can implement their own public methods. Required connection
-    methods are validated by the {trigger, source, reporter} they are loaded
-    into. For example, a trigger will likely require some kind of query method
-    while a reporter may need a review method."""
-
-    def __init__(self, connection_name, connection_config):
-        # connection_name is the name given to this connection in zuul.ini
-        # connection_config is a dictionary of config_section from zuul.ini for
-        # this connection.
-        # __init__ shouldn't make the actual connection in case this connection
-        # isn't used in the layout.
-        self.connection_name = connection_name
-        self.connection_config = connection_config
-
-        # Keep track of the sources, triggers and reporters using this
-        # connection
-        self.attached_to = {
-            'source': [],
-            'trigger': [],
-            'reporter': [],
-        }
-
-    def onLoad(self):
-        pass
-
-    def onStop(self):
-        pass
-
-    def registerScheduler(self, sched):
-        self.sched = sched
-
-    def registerUse(self, what, instance):
-        self.attached_to[what].append(instance)
-
-    def maintainCache(self, relevant):
-        """Make cache contain relevant changes.
-
-        This lets the user supply a list of change objects that are
-        still in use.  Anything in our cache that isn't in the supplied
-        list should be safe to remove from the cache."""
+# Copyright 2014 Rackspace Australia
+#
+# Licensed under the Apache License, Version 2.0 (the "License"); you may
+# not use this file except in compliance with the License. You may obtain
+# a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+# License for the specific language governing permissions and limitations
+# under the License.
+
+import abc
+
+import six
+
+
+@six.add_metaclass(abc.ABCMeta)
+class BaseConnection(object):
+    """Base class for connections.
+
+    A connection is a shared object that sources, triggers and reporters can
+    use to speak with a remote API without needing to establish a new
+    connection each time or without having to authenticate each time.
+
+    Multiple instances of the same connection may exist with different
+    credentials, for example, thus allowing for different pipelines to operate
+    on different Gerrit installations or post back as a different user etc.
+
+    Connections can implement their own public methods. Required connection
+    methods are validated by the {trigger, source, reporter} they are loaded
+    into. For example, a trigger will likely require some kind of query method
+    while a reporter may need a review method."""
+
+    def __init__(self, connection_name, connection_config):
+        # connection_name is the name given to this connection in zuul.ini
+        # connection_config is a dictionary of config_section from zuul.ini for
+        # this connection.
+        # __init__ shouldn't make the actual connection in case this connection
+        # isn't used in the layout.
+        self.connection_name = connection_name
+        self.connection_config = connection_config
+
+        # Keep track of the sources, triggers and reporters using this
+        # connection
+        self.attached_to = {
+            'source': [],
+            'trigger': [],
+            'reporter': [],
+        }
+
+    def onLoad(self):
+        pass
+
+    def onStop(self):
+        pass
+
+    def registerScheduler(self, sched):
+        self.sched = sched
+
+    def registerUse(self, what, instance):
+        self.attached_to[what].append(instance)
+
+    def maintainCache(self, relevant):
+        """Make cache contain relevant changes.
+
+        This lets the user supply a list of change objects that are
+        still in use.  Anything in our cache that isn't in the supplied
+        list should be safe to remove from the cache."""
diff --git a/zuul/connection/gerrit.py b/zuul/connection/gerrit.py
index 231ae30..4461482 100644
--- a/zuul/connection/gerrit.py
+++ b/zuul/connection/gerrit.py
@@ -1,487 +1,487 @@
-# Copyright 2011 OpenStack, LLC.
-# Copyright 2012 Hewlett-Packard Development Company, L.P.
-#
-# Licensed under the Apache License, Version 2.0 (the "License"); you may
-# not use this file except in compliance with the License. You may obtain
-# a copy of the License at
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-# License for the specific language governing permissions and limitations
-# under the License.
-
-import threading
-import select
-import json
-import time
-from six.moves import queue as Queue
-import paramiko
-import logging
-import pprint
-import voluptuous as v
-import urllib2
-
-from zuul.connection import BaseConnection
-from zuul.model import TriggerEvent
-
-
-class GerritEventConnector(threading.Thread):
-    """Move events from Gerrit to the scheduler."""
-
-    log = logging.getLogger("zuul.GerritEventConnector")
-<<<<<<< HEAD
-    delay = 5.0
-=======
->>>>>>> 12d23bf... Gerrit trailing delay is now configurable
-
-    def __init__(self, connection, delay=10):
-        super(GerritEventConnector, self).__init__()
-        self.daemon = True
-        self.connection = connection
-        self._stopped = False
-        self.delay = delay
-
-    def stop(self):
-        self._stopped = True
-        self.connection.addEvent(None)
-
-    def _handleEvent(self):
-        ts, data = self.connection.getEvent()
-        if self._stopped:
-            return
-        # Gerrit can produce inconsistent data immediately after an
-        # event, So ensure that we do not deliver the event to Zuul
-        # until at least a certain amount of time has passed.  Note
-        # that if we receive several events in succession, we will
-        # only need to delay for the first event.  In essence, Zuul
-        # should always be a constant number of seconds behind Gerrit.
-        #
-        # Can be configured via the Gerrit driver setting 'event_delay'.
-        now = time.time()
-        time.sleep(max((ts + self.delay) - now, 0.0))
-        event = TriggerEvent()
-        event.type = data.get('type')
-        event.trigger_name = 'gerrit'
-        change = data.get('change')
-        if change:
-            event.project_name = change.get('project')
-            event.branch = change.get('branch')
-            event.change_number = change.get('number')
-            event.change_url = change.get('url')
-            patchset = data.get('patchSet')
-            if patchset:
-                event.patch_number = patchset.get('number')
-                event.refspec = patchset.get('ref')
-            event.approvals = data.get('approvals', [])
-            event.comment = data.get('comment')
-        refupdate = data.get('refUpdate')
-        if refupdate:
-            event.project_name = refupdate.get('project')
-            event.ref = refupdate.get('refName')
-            event.oldrev = refupdate.get('oldRev')
-            event.newrev = refupdate.get('newRev')
-        # Map the event types to a field name holding a Gerrit
-        # account attribute. See Gerrit stream-event documentation
-        # in cmd-stream-events.html
-        accountfield_from_type = {
-            'patchset-created': 'uploader',
-            'draft-published': 'uploader',  # Gerrit 2.5/2.6
-            'change-abandoned': 'abandoner',
-            'change-restored': 'restorer',
-            'change-merged': 'submitter',
-            'merge-failed': 'submitter',  # Gerrit 2.5/2.6
-            'comment-added': 'author',
-            'ref-updated': 'submitter',
-            'reviewer-added': 'reviewer',  # Gerrit 2.5/2.6
-        }
-        try:
-            event.account = data.get(accountfield_from_type[event.type])
-        except KeyError:
-            self.log.warning("Received unrecognized event type '%s' from Gerrit.\
-                    Can not get account information." % event.type)
-            event.account = None
-
-        if (event.change_number and
-            self.connection.sched.getProject(event.project_name)):
-            # Call _getChange for the side effect of updating the
-            # cache.  Note that this modifies Change objects outside
-            # the main thread.
-            # NOTE(jhesketh): Ideally we'd just remove the change from the
-            # cache to denote that it needs updating. However the change
-            # object is already used by Item's and hence BuildSet's etc. and
-            # we need to update those objects by reference so that they have
-            # the correct/new information and also avoid hitting gerrit
-            # multiple times.
-            if self.connection.attached_to['source']:
-                self.connection.attached_to['source'][0]._getChange(
-                    event.change_number, event.patch_number, refresh=True)
-                # We only need to do this once since the connection maintains
-                # the cache (which is shared between all the sources)
-                # NOTE(jhesketh): We may couple sources and connections again
-                # at which point this becomes more sensible.
-        self.connection.sched.addEvent(event)
-
-    def run(self):
-        while True:
-            if self._stopped:
-                return
-            try:
-                self._handleEvent()
-            except:
-                self.log.exception("Exception moving Gerrit event:")
-            finally:
-                self.connection.eventDone()
-
-
-class GerritWatcher(threading.Thread):
-    log = logging.getLogger("gerrit.GerritWatcher")
-    poll_timeout = 500
-
-    def __init__(self, gerrit_connection, username, hostname, port=29418,
-                 keyfile=None):
-        threading.Thread.__init__(self)
-        self.username = username
-        self.keyfile = keyfile
-        self.hostname = hostname
-        self.port = port
-        self.gerrit_connection = gerrit_connection
-        self._stopped = False
-
-    def _read(self, fd):
-        l = fd.readline()
-        data = json.loads(l)
-        self.log.debug("Received data from Gerrit event stream: \n%s" %
-                       pprint.pformat(data))
-        self.gerrit_connection.addEvent(data)
-
-    def _listen(self, stdout, stderr):
-        poll = select.poll()
-        poll.register(stdout.channel)
-        while not self._stopped:
-            ret = poll.poll(self.poll_timeout)
-            for (fd, event) in ret:
-                if fd == stdout.channel.fileno():
-                    if event == select.POLLIN:
-                        self._read(stdout)
-                    else:
-                        raise Exception("event on ssh connection")
-
-    def _run(self):
-        try:
-            client = paramiko.SSHClient()
-            client.load_system_host_keys()
-            client.set_missing_host_key_policy(paramiko.WarningPolicy())
-            client.connect(self.hostname,
-                           username=self.username,
-                           port=self.port,
-                           key_filename=self.keyfile)
-
-            stdin, stdout, stderr = client.exec_command("gerrit stream-events")
-
-            self._listen(stdout, stderr)
-
-            if not stdout.channel.exit_status_ready():
-                # The stream-event is still running but we are done polling
-                # on stdout most likely due to being asked to stop.
-                # Try to stop the stream-events command sending Ctrl-C
-                stdin.write("\x03")
-                time.sleep(.2)
-                if not stdout.channel.exit_status_ready():
-                    # we're still not ready to exit, lets force the channel
-                    # closed now.
-                    stdout.channel.close()
-            ret = stdout.channel.recv_exit_status()
-            self.log.debug("SSH exit status: %s" % ret)
-            client.close()
-
-            if ret and ret not in [-1, 130]:
-                raise Exception("Gerrit error executing stream-events")
-        except:
-            self.log.exception("Exception on ssh event stream:")
-            time.sleep(5)
-
-    def run(self):
-        while not self._stopped:
-            self._run()
-
-    def stop(self):
-        self.log.debug("Stopping watcher")
-        self._stopped = True
-
-
-class GerritConnection(BaseConnection):
-    driver_name = 'gerrit'
-    log = logging.getLogger("connection.gerrit")
-
-    def __init__(self, connection_name, connection_config):
-        super(GerritConnection, self).__init__(connection_name,
-                                               connection_config)
-        if 'server' not in self.connection_config:
-            raise Exception('server is required for gerrit connections in '
-                            '%s' % self.connection_name)
-        if 'user' not in self.connection_config:
-            raise Exception('user is required for gerrit connections in '
-                            '%s' % self.connection_name)
-
-        self.user = self.connection_config.get('user')
-        self.server = self.connection_config.get('server')
-        self.port = int(self.connection_config.get('port', 29418))
-        self.keyfile = self.connection_config.get('sshkey', None)
-        self.watcher_thread = None
-        self.event_queue = None
-        self.event_delay = int(self.connection_config.get('event_delay', 10))
-        self.client = None
-
-        self.baseurl = self.connection_config.get('baseurl',
-                                                  'https://%s' % self.server)
-
-        self._change_cache = {}
-        self.gerrit_event_connector = None
-
-    def getCachedChange(self, key):
-        if key in self._change_cache:
-            return self._change_cache.get(key)
-        return None
-
-    def updateChangeCache(self, key, value):
-        self._change_cache[key] = value
-
-    def deleteCachedChange(self, key):
-        if key in self._change_cache:
-            del self._change_cache[key]
-
-    def maintainCache(self, relevant):
-        # This lets the user supply a list of change objects that are
-        # still in use.  Anything in our cache that isn't in the supplied
-        # list should be safe to remove from the cache.
-        remove = []
-        for key, change in self._change_cache.items():
-            if change not in relevant:
-                remove.append(key)
-        for key in remove:
-            del self._change_cache[key]
-
-    def addEvent(self, data):
-        return self.event_queue.put((time.time(), data))
-
-    def getEvent(self):
-        return self.event_queue.get()
-
-    def eventDone(self):
-        self.event_queue.task_done()
-
-    def review(self, project, change, message, action={}):
-        cmd = 'gerrit review --project %s' % project
-        if message:
-            cmd += ' --message "%s"' % message
-        for key, val in action.items():
-            if val is True:
-                cmd += ' --%s' % key
-            else:
-                cmd += ' --%s %s' % (key, val)
-        cmd += ' %s' % change
-        out, err = self._ssh(cmd)
-        return err
-
-    def query(self, query):
-        args = '--all-approvals --comments --commit-message'
-        args += ' --current-patch-set --dependencies --files'
-        args += ' --patch-sets --submit-records'
-        cmd = 'gerrit query --format json %s %s' % (
-            args, query)
-        out, err = self._ssh(cmd)
-        if not out:
-            return False
-        lines = out.split('\n')
-        if not lines:
-            return False
-        data = json.loads(lines[0])
-        if not data:
-            return False
-        self.log.debug("Received data from Gerrit query: \n%s" %
-                       (pprint.pformat(data)))
-        return data
-
-    def simpleQuery(self, query):
-        def _query_chunk(query):
-            args = '--commit-message --current-patch-set'
-
-            cmd = 'gerrit query --format json %s %s' % (
-                args, query)
-            out, err = self._ssh(cmd)
-            if not out:
-                return False
-            lines = out.split('\n')
-            if not lines:
-                return False
-
-            # filter out blank lines
-            data = [json.loads(line) for line in lines
-                    if line.startswith('{')]
-
-            # check last entry for more changes
-            more_changes = None
-            if 'moreChanges' in data[-1]:
-                more_changes = data[-1]['moreChanges']
-
-            # we have to remove the statistics line
-            del data[-1]
-
-            if not data:
-                return False, more_changes
-            self.log.debug("Received data from Gerrit query: \n%s" %
-                           (pprint.pformat(data)))
-            return data, more_changes
-
-        # gerrit returns 500 results by default, so implement paging
-        # for large projects like nova
-        alldata = []
-        chunk, more_changes = _query_chunk(query)
-        while(chunk):
-            alldata.extend(chunk)
-            if more_changes is None:
-                # continue sortKey based (before Gerrit 2.9)
-                resume = "resume_sortkey:'%s'" % chunk[-1]["sortKey"]
-            elif more_changes:
-                # continue moreChanges based (since Gerrit 2.9)
-                resume = "-S %d" % len(alldata)
-            else:
-                # no more changes
-                break
-
-            chunk, more_changes = _query_chunk("%s %s" % (query, resume))
-        return alldata
-
-    def _open(self):
-        client = paramiko.SSHClient()
-        client.load_system_host_keys()
-        client.set_missing_host_key_policy(paramiko.WarningPolicy())
-        client.connect(self.server,
-                       username=self.user,
-                       port=self.port,
-                       key_filename=self.keyfile)
-        self.client = client
-
-    def _ssh(self, command, stdin_data=None):
-        if not self.client:
-            self._open()
-
-        try:
-            self.log.debug("SSH command:\n%s" % command)
-            stdin, stdout, stderr = self.client.exec_command(command)
-        except:
-            self._open()
-            stdin, stdout, stderr = self.client.exec_command(command)
-
-        if stdin_data:
-            stdin.write(stdin_data)
-
-        out = stdout.read()
-        self.log.debug("SSH received stdout:\n%s" % out)
-
-        ret = stdout.channel.recv_exit_status()
-        self.log.debug("SSH exit status: %s" % ret)
-
-        err = stderr.read()
-        self.log.debug("SSH received stderr:\n%s" % err)
-        if ret:
-            raise Exception("Gerrit error executing %s" % command)
-        return (out, err)
-
-    def getInfoRefs(self, project):
-        url = "%s/p/%s/info/refs?service=git-upload-pack" % (
-            self.baseurl, project)
-        try:
-            data = urllib2.urlopen(url).read()
-        except:
-            self.log.error("Cannot get references from %s" % url)
-            raise  # keeps urllib2 error informations
-        ret = {}
-        read_headers = False
-        read_advertisement = False
-        if data[4] != '#':
-            raise Exception("Gerrit repository does not support "
-                            "git-upload-pack")
-        i = 0
-        while i < len(data):
-            if len(data) - i < 4:
-                raise Exception("Invalid length in info/refs")
-            plen = int(data[i:i + 4], 16)
-            i += 4
-            # It's the length of the packet, including the 4 bytes of the
-            # length itself, unless it's null, in which case the length is
-            # not included.
-            if plen > 0:
-                plen -= 4
-            if len(data) - i < plen:
-                raise Exception("Invalid data in info/refs")
-            line = data[i:i + plen]
-            i += plen
-            if not read_headers:
-                if plen == 0:
-                    read_headers = True
-                continue
-            if not read_advertisement:
-                read_advertisement = True
-                continue
-            if plen == 0:
-                # The terminating null
-                continue
-            line = line.strip()
-            revision, ref = line.split()
-            ret[ref] = revision
-        return ret
-
-    def getGitUrl(self, project):
-        url = 'ssh://%s@%s:%s/%s' % (self.user, self.server, self.port,
-                                     project.name)
-        return url
-
-    def getGitwebUrl(self, project, sha=None):
-        url = '%s/gitweb?p=%s.git' % (self.baseurl, project)
-        if sha:
-            url += ';a=commitdiff;h=' + sha
-        return url
-
-    def onLoad(self):
-        self.log.debug("Starting Gerrit Connection/Watchers")
-        self._start_watcher_thread()
-        self._start_event_connector()
-
-    def onStop(self):
-        self.log.debug("Stopping Gerrit Conncetion/Watchers")
-        self._stop_watcher_thread()
-        self._stop_event_connector()
-
-    def _stop_watcher_thread(self):
-        if self.watcher_thread:
-            self.watcher_thread.stop()
-            self.watcher_thread.join()
-
-    def _start_watcher_thread(self):
-        self.event_queue = Queue.Queue()
-        self.watcher_thread = GerritWatcher(
-            self,
-            self.user,
-            self.server,
-            self.port,
-            keyfile=self.keyfile)
-        self.watcher_thread.start()
-
-    def _stop_event_connector(self):
-        if self.gerrit_event_connector:
-            self.gerrit_event_connector.stop()
-            self.gerrit_event_connector.join()
-
-    def _start_event_connector(self):
-        self.gerrit_event_connector = GerritEventConnector(
-            self, delay=self.event_delay)
-        self.gerrit_event_connector.start()
-
-
-def getSchema():
-    gerrit_connection = v.Any(str, v.Schema({}, extra=True))
-    return gerrit_connection
+# Copyright 2011 OpenStack, LLC.
+# Copyright 2012 Hewlett-Packard Development Company, L.P.
+#
+# Licensed under the Apache License, Version 2.0 (the "License"); you may
+# not use this file except in compliance with the License. You may obtain
+# a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+# License for the specific language governing permissions and limitations
+# under the License.
+
+import threading
+import select
+import json
+import time
+from six.moves import queue as Queue
+import paramiko
+import logging
+import pprint
+import voluptuous as v
+import urllib2
+
+from zuul.connection import BaseConnection
+from zuul.model import TriggerEvent
+
+
+class GerritEventConnector(threading.Thread):
+    """Move events from Gerrit to the scheduler."""
+
+    log = logging.getLogger("zuul.GerritEventConnector")
+<<<<<<< HEAD
+    delay = 5.0
+=======
+>>>>>>> 12d23bf... Gerrit trailing delay is now configurable
+
+    def __init__(self, connection, delay=10):
+        super(GerritEventConnector, self).__init__()
+        self.daemon = True
+        self.connection = connection
+        self._stopped = False
+        self.delay = delay
+
+    def stop(self):
+        self._stopped = True
+        self.connection.addEvent(None)
+
+    def _handleEvent(self):
+        ts, data = self.connection.getEvent()
+        if self._stopped:
+            return
+        # Gerrit can produce inconsistent data immediately after an
+        # event, So ensure that we do not deliver the event to Zuul
+        # until at least a certain amount of time has passed.  Note
+        # that if we receive several events in succession, we will
+        # only need to delay for the first event.  In essence, Zuul
+        # should always be a constant number of seconds behind Gerrit.
+        #
+        # Can be configured via the Gerrit driver setting 'event_delay'.
+        now = time.time()
+        time.sleep(max((ts + self.delay) - now, 0.0))
+        event = TriggerEvent()
+        event.type = data.get('type')
+        event.trigger_name = 'gerrit'
+        change = data.get('change')
+        if change:
+            event.project_name = change.get('project')
+            event.branch = change.get('branch')
+            event.change_number = change.get('number')
+            event.change_url = change.get('url')
+            patchset = data.get('patchSet')
+            if patchset:
+                event.patch_number = patchset.get('number')
+                event.refspec = patchset.get('ref')
+            event.approvals = data.get('approvals', [])
+            event.comment = data.get('comment')
+        refupdate = data.get('refUpdate')
+        if refupdate:
+            event.project_name = refupdate.get('project')
+            event.ref = refupdate.get('refName')
+            event.oldrev = refupdate.get('oldRev')
+            event.newrev = refupdate.get('newRev')
+        # Map the event types to a field name holding a Gerrit
+        # account attribute. See Gerrit stream-event documentation
+        # in cmd-stream-events.html
+        accountfield_from_type = {
+            'patchset-created': 'uploader',
+            'draft-published': 'uploader',  # Gerrit 2.5/2.6
+            'change-abandoned': 'abandoner',
+            'change-restored': 'restorer',
+            'change-merged': 'submitter',
+            'merge-failed': 'submitter',  # Gerrit 2.5/2.6
+            'comment-added': 'author',
+            'ref-updated': 'submitter',
+            'reviewer-added': 'reviewer',  # Gerrit 2.5/2.6
+        }
+        try:
+            event.account = data.get(accountfield_from_type[event.type])
+        except KeyError:
+            self.log.warning("Received unrecognized event type '%s' from Gerrit.\
+                    Can not get account information." % event.type)
+            event.account = None
+
+        if (event.change_number and
+            self.connection.sched.getProject(event.project_name)):
+            # Call _getChange for the side effect of updating the
+            # cache.  Note that this modifies Change objects outside
+            # the main thread.
+            # NOTE(jhesketh): Ideally we'd just remove the change from the
+            # cache to denote that it needs updating. However the change
+            # object is already used by Item's and hence BuildSet's etc. and
+            # we need to update those objects by reference so that they have
+            # the correct/new information and also avoid hitting gerrit
+            # multiple times.
+            if self.connection.attached_to['source']:
+                self.connection.attached_to['source'][0]._getChange(
+                    event.change_number, event.patch_number, refresh=True)
+                # We only need to do this once since the connection maintains
+                # the cache (which is shared between all the sources)
+                # NOTE(jhesketh): We may couple sources and connections again
+                # at which point this becomes more sensible.
+        self.connection.sched.addEvent(event)
+
+    def run(self):
+        while True:
+            if self._stopped:
+                return
+            try:
+                self._handleEvent()
+            except:
+                self.log.exception("Exception moving Gerrit event:")
+            finally:
+                self.connection.eventDone()
+
+
+class GerritWatcher(threading.Thread):
+    log = logging.getLogger("gerrit.GerritWatcher")
+    poll_timeout = 500
+
+    def __init__(self, gerrit_connection, username, hostname, port=29418,
+                 keyfile=None):
+        threading.Thread.__init__(self)
+        self.username = username
+        self.keyfile = keyfile
+        self.hostname = hostname
+        self.port = port
+        self.gerrit_connection = gerrit_connection
+        self._stopped = False
+
+    def _read(self, fd):
+        l = fd.readline()
+        data = json.loads(l)
+        self.log.debug("Received data from Gerrit event stream: \n%s" %
+                       pprint.pformat(data))
+        self.gerrit_connection.addEvent(data)
+
+    def _listen(self, stdout, stderr):
+        poll = select.poll()
+        poll.register(stdout.channel)
+        while not self._stopped:
+            ret = poll.poll(self.poll_timeout)
+            for (fd, event) in ret:
+                if fd == stdout.channel.fileno():
+                    if event == select.POLLIN:
+                        self._read(stdout)
+                    else:
+                        raise Exception("event on ssh connection")
+
+    def _run(self):
+        try:
+            client = paramiko.SSHClient()
+            client.load_system_host_keys()
+            client.set_missing_host_key_policy(paramiko.WarningPolicy())
+            client.connect(self.hostname,
+                           username=self.username,
+                           port=self.port,
+                           key_filename=self.keyfile)
+
+            stdin, stdout, stderr = client.exec_command("gerrit stream-events")
+
+            self._listen(stdout, stderr)
+
+            if not stdout.channel.exit_status_ready():
+                # The stream-event is still running but we are done polling
+                # on stdout most likely due to being asked to stop.
+                # Try to stop the stream-events command sending Ctrl-C
+                stdin.write("\x03")
+                time.sleep(.2)
+                if not stdout.channel.exit_status_ready():
+                    # we're still not ready to exit, lets force the channel
+                    # closed now.
+                    stdout.channel.close()
+            ret = stdout.channel.recv_exit_status()
+            self.log.debug("SSH exit status: %s" % ret)
+            client.close()
+
+            if ret and ret not in [-1, 130]:
+                raise Exception("Gerrit error executing stream-events")
+        except:
+            self.log.exception("Exception on ssh event stream:")
+            time.sleep(5)
+
+    def run(self):
+        while not self._stopped:
+            self._run()
+
+    def stop(self):
+        self.log.debug("Stopping watcher")
+        self._stopped = True
+
+
+class GerritConnection(BaseConnection):
+    driver_name = 'gerrit'
+    log = logging.getLogger("connection.gerrit")
+
+    def __init__(self, connection_name, connection_config):
+        super(GerritConnection, self).__init__(connection_name,
+                                               connection_config)
+        if 'server' not in self.connection_config:
+            raise Exception('server is required for gerrit connections in '
+                            '%s' % self.connection_name)
+        if 'user' not in self.connection_config:
+            raise Exception('user is required for gerrit connections in '
+                            '%s' % self.connection_name)
+
+        self.user = self.connection_config.get('user')
+        self.server = self.connection_config.get('server')
+        self.port = int(self.connection_config.get('port', 29418))
+        self.keyfile = self.connection_config.get('sshkey', None)
+        self.watcher_thread = None
+        self.event_queue = None
+        self.event_delay = int(self.connection_config.get('event_delay', 10))
+        self.client = None
+
+        self.baseurl = self.connection_config.get('baseurl',
+                                                  'https://%s' % self.server)
+
+        self._change_cache = {}
+        self.gerrit_event_connector = None
+
+    def getCachedChange(self, key):
+        if key in self._change_cache:
+            return self._change_cache.get(key)
+        return None
+
+    def updateChangeCache(self, key, value):
+        self._change_cache[key] = value
+
+    def deleteCachedChange(self, key):
+        if key in self._change_cache:
+            del self._change_cache[key]
+
+    def maintainCache(self, relevant):
+        # This lets the user supply a list of change objects that are
+        # still in use.  Anything in our cache that isn't in the supplied
+        # list should be safe to remove from the cache.
+        remove = []
+        for key, change in self._change_cache.items():
+            if change not in relevant:
+                remove.append(key)
+        for key in remove:
+            del self._change_cache[key]
+
+    def addEvent(self, data):
+        return self.event_queue.put((time.time(), data))
+
+    def getEvent(self):
+        return self.event_queue.get()
+
+    def eventDone(self):
+        self.event_queue.task_done()
+
+    def review(self, project, change, message, action={}):
+        cmd = 'gerrit review --project %s' % project
+        if message:
+            cmd += ' --message "%s"' % message
+        for key, val in action.items():
+            if val is True:
+                cmd += ' --%s' % key
+            else:
+                cmd += ' --%s %s' % (key, val)
+        cmd += ' %s' % change
+        out, err = self._ssh(cmd)
+        return err
+
+    def query(self, query):
+        args = '--all-approvals --comments --commit-message'
+        args += ' --current-patch-set --dependencies --files'
+        args += ' --patch-sets --submit-records'
+        cmd = 'gerrit query --format json %s %s' % (
+            args, query)
+        out, err = self._ssh(cmd)
+        if not out:
+            return False
+        lines = out.split('\n')
+        if not lines:
+            return False
+        data = json.loads(lines[0])
+        if not data:
+            return False
+        self.log.debug("Received data from Gerrit query: \n%s" %
+                       (pprint.pformat(data)))
+        return data
+
+    def simpleQuery(self, query):
+        def _query_chunk(query):
+            args = '--commit-message --current-patch-set'
+
+            cmd = 'gerrit query --format json %s %s' % (
+                args, query)
+            out, err = self._ssh(cmd)
+            if not out:
+                return False
+            lines = out.split('\n')
+            if not lines:
+                return False
+
+            # filter out blank lines
+            data = [json.loads(line) for line in lines
+                    if line.startswith('{')]
+
+            # check last entry for more changes
+            more_changes = None
+            if 'moreChanges' in data[-1]:
+                more_changes = data[-1]['moreChanges']
+
+            # we have to remove the statistics line
+            del data[-1]
+
+            if not data:
+                return False, more_changes
+            self.log.debug("Received data from Gerrit query: \n%s" %
+                           (pprint.pformat(data)))
+            return data, more_changes
+
+        # gerrit returns 500 results by default, so implement paging
+        # for large projects like nova
+        alldata = []
+        chunk, more_changes = _query_chunk(query)
+        while(chunk):
+            alldata.extend(chunk)
+            if more_changes is None:
+                # continue sortKey based (before Gerrit 2.9)
+                resume = "resume_sortkey:'%s'" % chunk[-1]["sortKey"]
+            elif more_changes:
+                # continue moreChanges based (since Gerrit 2.9)
+                resume = "-S %d" % len(alldata)
+            else:
+                # no more changes
+                break
+
+            chunk, more_changes = _query_chunk("%s %s" % (query, resume))
+        return alldata
+
+    def _open(self):
+        client = paramiko.SSHClient()
+        client.load_system_host_keys()
+        client.set_missing_host_key_policy(paramiko.WarningPolicy())
+        client.connect(self.server,
+                       username=self.user,
+                       port=self.port,
+                       key_filename=self.keyfile)
+        self.client = client
+
+    def _ssh(self, command, stdin_data=None):
+        if not self.client:
+            self._open()
+
+        try:
+            self.log.debug("SSH command:\n%s" % command)
+            stdin, stdout, stderr = self.client.exec_command(command)
+        except:
+            self._open()
+            stdin, stdout, stderr = self.client.exec_command(command)
+
+        if stdin_data:
+            stdin.write(stdin_data)
+
+        out = stdout.read()
+        self.log.debug("SSH received stdout:\n%s" % out)
+
+        ret = stdout.channel.recv_exit_status()
+        self.log.debug("SSH exit status: %s" % ret)
+
+        err = stderr.read()
+        self.log.debug("SSH received stderr:\n%s" % err)
+        if ret:
+            raise Exception("Gerrit error executing %s" % command)
+        return (out, err)
+
+    def getInfoRefs(self, project):
+        url = "%s/p/%s/info/refs?service=git-upload-pack" % (
+            self.baseurl, project)
+        try:
+            data = urllib2.urlopen(url).read()
+        except:
+            self.log.error("Cannot get references from %s" % url)
+            raise  # keeps urllib2 error informations
+        ret = {}
+        read_headers = False
+        read_advertisement = False
+        if data[4] != '#':
+            raise Exception("Gerrit repository does not support "
+                            "git-upload-pack")
+        i = 0
+        while i < len(data):
+            if len(data) - i < 4:
+                raise Exception("Invalid length in info/refs")
+            plen = int(data[i:i + 4], 16)
+            i += 4
+            # It's the length of the packet, including the 4 bytes of the
+            # length itself, unless it's null, in which case the length is
+            # not included.
+            if plen > 0:
+                plen -= 4
+            if len(data) - i < plen:
+                raise Exception("Invalid data in info/refs")
+            line = data[i:i + plen]
+            i += plen
+            if not read_headers:
+                if plen == 0:
+                    read_headers = True
+                continue
+            if not read_advertisement:
+                read_advertisement = True
+                continue
+            if plen == 0:
+                # The terminating null
+                continue
+            line = line.strip()
+            revision, ref = line.split()
+            ret[ref] = revision
+        return ret
+
+    def getGitUrl(self, project):
+        url = 'ssh://%s@%s:%s/%s' % (self.user, self.server, self.port,
+                                     project.name)
+        return url
+
+    def getGitwebUrl(self, project, sha=None):
+        url = '%s/gitweb?p=%s.git' % (self.baseurl, project)
+        if sha:
+            url += ';a=commitdiff;h=' + sha
+        return url
+
+    def onLoad(self):
+        self.log.debug("Starting Gerrit Connection/Watchers")
+        self._start_watcher_thread()
+        self._start_event_connector()
+
+    def onStop(self):
+        self.log.debug("Stopping Gerrit Conncetion/Watchers")
+        self._stop_watcher_thread()
+        self._stop_event_connector()
+
+    def _stop_watcher_thread(self):
+        if self.watcher_thread:
+            self.watcher_thread.stop()
+            self.watcher_thread.join()
+
+    def _start_watcher_thread(self):
+        self.event_queue = Queue.Queue()
+        self.watcher_thread = GerritWatcher(
+            self,
+            self.user,
+            self.server,
+            self.port,
+            keyfile=self.keyfile)
+        self.watcher_thread.start()
+
+    def _stop_event_connector(self):
+        if self.gerrit_event_connector:
+            self.gerrit_event_connector.stop()
+            self.gerrit_event_connector.join()
+
+    def _start_event_connector(self):
+        self.gerrit_event_connector = GerritEventConnector(
+            self, delay=self.event_delay)
+        self.gerrit_event_connector.start()
+
+
+def getSchema():
+    gerrit_connection = v.Any(str, v.Schema({}, extra=True))
+    return gerrit_connection
diff --git a/zuul/connection/smtp.py b/zuul/connection/smtp.py
index d3eccff..fde2089 100644
--- a/zuul/connection/smtp.py
+++ b/zuul/connection/smtp.py
@@ -1,63 +1,63 @@
-# Copyright 2014 Rackspace Australia
-#
-# Licensed under the Apache License, Version 2.0 (the "License"); you may
-# not use this file except in compliance with the License. You may obtain
-# a copy of the License at
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-# License for the specific language governing permissions and limitations
-# under the License.
-
-import logging
-import voluptuous as v
-import smtplib
-
-from email.mime.text import MIMEText
-
-from zuul.connection import BaseConnection
-
-
-class SMTPConnection(BaseConnection):
-    driver_name = 'smtp'
-    log = logging.getLogger("connection.smtp")
-
-    def __init__(self, connection_name, connection_config):
-
-        super(SMTPConnection, self).__init__(connection_name,
-                                             connection_config)
-
-        self.smtp_server = self.connection_config.get(
-            'server', 'localhost')
-        self.smtp_port = self.connection_config.get('port', 25)
-        self.smtp_default_from = self.connection_config.get(
-            'default_from', 'zuul')
-        self.smtp_default_to = self.connection_config.get(
-            'default_to', 'zuul')
-
-    def sendMail(self, subject, message, from_email=None, to_email=None):
-        # Create a text/plain email message
-        from_email = from_email \
-            if from_email is not None else self.smtp_default_from
-        to_email = to_email if to_email is not None else self.smtp_default_to
-
-        msg = MIMEText(message)
-        msg['Subject'] = subject
-        msg['From'] = from_email
-        msg['To'] = to_email
-
-        try:
-            s = smtplib.SMTP(self.smtp_server, self.smtp_port)
-            s.sendmail(from_email, to_email.split(','), msg.as_string())
-            s.quit()
-        except:
-            return "Could not send email via SMTP"
-        return
-
-
-def getSchema():
-    smtp_connection = v.Any(str, v.Schema({}, extra=True))
-    return smtp_connection
+# Copyright 2014 Rackspace Australia
+#
+# Licensed under the Apache License, Version 2.0 (the "License"); you may
+# not use this file except in compliance with the License. You may obtain
+# a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+# License for the specific language governing permissions and limitations
+# under the License.
+
+import logging
+import voluptuous as v
+import smtplib
+
+from email.mime.text import MIMEText
+
+from zuul.connection import BaseConnection
+
+
+class SMTPConnection(BaseConnection):
+    driver_name = 'smtp'
+    log = logging.getLogger("connection.smtp")
+
+    def __init__(self, connection_name, connection_config):
+
+        super(SMTPConnection, self).__init__(connection_name,
+                                             connection_config)
+
+        self.smtp_server = self.connection_config.get(
+            'server', 'localhost')
+        self.smtp_port = self.connection_config.get('port', 25)
+        self.smtp_default_from = self.connection_config.get(
+            'default_from', 'zuul')
+        self.smtp_default_to = self.connection_config.get(
+            'default_to', 'zuul')
+
+    def sendMail(self, subject, message, from_email=None, to_email=None):
+        # Create a text/plain email message
+        from_email = from_email \
+            if from_email is not None else self.smtp_default_from
+        to_email = to_email if to_email is not None else self.smtp_default_to
+
+        msg = MIMEText(message)
+        msg['Subject'] = subject
+        msg['From'] = from_email
+        msg['To'] = to_email
+
+        try:
+            s = smtplib.SMTP(self.smtp_server, self.smtp_port)
+            s.sendmail(from_email, to_email.split(','), msg.as_string())
+            s.quit()
+        except:
+            return "Could not send email via SMTP"
+        return
+
+
+def getSchema():
+    smtp_connection = v.Any(str, v.Schema({}, extra=True))
+    return smtp_connection
diff --git a/zuul/exceptions.py b/zuul/exceptions.py
index 2bd2c6b..d5d5d74 100644
--- a/zuul/exceptions.py
+++ b/zuul/exceptions.py
@@ -1,26 +1,26 @@
-# Copyright 2015 Rackspace Australia
-#
-# Licensed under the Apache License, Version 2.0 (the "License"); you may
-# not use this file except in compliance with the License. You may obtain
-# a copy of the License at
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-# License for the specific language governing permissions and limitations
-# under the License.
-
-
-class ChangeNotFound(Exception):
-    def __init__(self, number, ps):
-        self.number = number
-        self.ps = ps
-        self.change = "%s,%s" % (str(number), str(ps))
-        message = "Change %s not found" % self.change
-        super(ChangeNotFound, self).__init__(message)
-
-
-class MergeFailure(Exception):
-    pass
+# Copyright 2015 Rackspace Australia
+#
+# Licensed under the Apache License, Version 2.0 (the "License"); you may
+# not use this file except in compliance with the License. You may obtain
+# a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+# License for the specific language governing permissions and limitations
+# under the License.
+
+
+class ChangeNotFound(Exception):
+    def __init__(self, number, ps):
+        self.number = number
+        self.ps = ps
+        self.change = "%s,%s" % (str(number), str(ps))
+        message = "Change %s not found" % self.change
+        super(ChangeNotFound, self).__init__(message)
+
+
+class MergeFailure(Exception):
+    pass
diff --git a/zuul/launcher/gearman.py b/zuul/launcher/gearman.py
index 69fb71b..15fedad 100644
--- a/zuul/launcher/gearman.py
+++ b/zuul/launcher/gearman.py
@@ -1,534 +1,534 @@
-# Copyright 2012 Hewlett-Packard Development Company, L.P.
-#
-# Licensed under the Apache License, Version 2.0 (the "License"); you may
-# not use this file except in compliance with the License. You may obtain
-# a copy of the License at
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-# License for the specific language governing permissions and limitations
-# under the License.
-
-import gear
-import inspect
-import json
-import logging
-import os
-import time
-import threading
-from uuid import uuid4
-
-import zuul.model
-from zuul.model import Build
-
-
-class GearmanCleanup(threading.Thread):
-    """ A thread that checks to see if outstanding builds have
-    completed without reporting back. """
-    log = logging.getLogger("zuul.GearmanCleanup")
-
-    def __init__(self, gearman):
-        threading.Thread.__init__(self)
-        self.daemon = True
-        self.gearman = gearman
-        self.wake_event = threading.Event()
-        self._stopped = False
-
-    def stop(self):
-        self._stopped = True
-        self.wake_event.set()
-
-    def run(self):
-        while True:
-            self.wake_event.wait(300)
-            if self._stopped:
-                return
-            try:
-                self.gearman.lookForLostBuilds()
-            except:
-                self.log.exception("Exception checking builds:")
-
-
-def getJobData(job):
-    if not len(job.data):
-        return {}
-    d = job.data[-1]
-    if not d:
-        return {}
-    return json.loads(d)
-
-
-class ZuulGearmanClient(gear.Client):
-    def __init__(self, zuul_gearman):
-        super(ZuulGearmanClient, self).__init__()
-        self.__zuul_gearman = zuul_gearman
-
-    def handleWorkComplete(self, packet):
-        job = super(ZuulGearmanClient, self).handleWorkComplete(packet)
-        self.__zuul_gearman.onBuildCompleted(job)
-        return job
-
-    def handleWorkFail(self, packet):
-        job = super(ZuulGearmanClient, self).handleWorkFail(packet)
-        self.__zuul_gearman.onBuildCompleted(job)
-        return job
-
-    def handleWorkException(self, packet):
-        job = super(ZuulGearmanClient, self).handleWorkException(packet)
-        self.__zuul_gearman.onBuildCompleted(job)
-        return job
-
-    def handleWorkStatus(self, packet):
-        job = super(ZuulGearmanClient, self).handleWorkStatus(packet)
-        self.__zuul_gearman.onWorkStatus(job)
-        return job
-
-    def handleWorkData(self, packet):
-        job = super(ZuulGearmanClient, self).handleWorkData(packet)
-        self.__zuul_gearman.onWorkStatus(job)
-        return job
-
-    def handleDisconnect(self, job):
-        job = super(ZuulGearmanClient, self).handleDisconnect(job)
-        self.__zuul_gearman.onDisconnect(job)
-
-    def handleStatusRes(self, packet):
-        try:
-            job = super(ZuulGearmanClient, self).handleStatusRes(packet)
-        except gear.UnknownJobError:
-            handle = packet.getArgument(0)
-            for build in self.__zuul_gearman.builds:
-                if build.__gearman_job.handle == handle:
-                    self.__zuul_gearman.onUnknownJob(job)
-
-    def waitForGearmanToSettle(self):
-        # If we're running the internal gearman server, it's possible
-        # that after a restart or reload, we may be immediately ready
-        # to run jobs but all the gearman workers may not have
-        # registered yet.  Give them a sporting chance to show up
-        # before we start declaring jobs lost because we don't have
-        # gearman functions registered for them.
-
-        # Spend up to 30 seconds after we connect to the gearman
-        # server waiting for the set of defined jobs to become
-        # consistent over a sliding 5 second window.
-
-        self.log.info("Waiting for connection to internal Gearman server")
-        self.waitForServer()
-        self.log.info("Waiting for gearman function set to settle")
-        start = time.time()
-        last_change = start
-        all_functions = set()
-        while time.time() - start < 30:
-            now = time.time()
-            last_functions = set()
-            for connection in self.active_connections:
-                try:
-                    req = gear.StatusAdminRequest()
-                    connection.sendAdminRequest(req, timeout=300)
-                except Exception:
-                    self.log.exception("Exception while checking functions")
-                    continue
-                for line in req.response.split('\n'):
-                    parts = [x.strip() for x in line.split()]
-                    if not parts or parts[0] == '.':
-                        continue
-                    last_functions.add(parts[0])
-            if last_functions != all_functions:
-                last_change = now
-                all_functions.update(last_functions)
-            else:
-                if now - last_change > 5:
-                    self.log.info("Gearman function set has settled")
-                    break
-            time.sleep(1)
-        self.log.info("Done waiting for Gearman server")
-
-
-class Gearman(object):
-    log = logging.getLogger("zuul.Gearman")
-    negative_function_cache_ttl = 5
-
-    def __init__(self, config, sched, swift):
-        self.config = config
-        self.sched = sched
-        self.swift = swift
-        self.builds = {}
-        self.meta_jobs = {}  # A list of meta-jobs like stop or describe
-
-        server = config.get('gearman', 'server')
-        if config.has_option('gearman', 'port'):
-            port = config.get('gearman', 'port')
-        else:
-            port = 4730
-
-        self.gearman = ZuulGearmanClient(self)
-        self.gearman.addServer(server, port)
-
-        if (config.has_option('gearman_server', 'start') and
-            config.getboolean('gearman_server', 'start')):
-            self.gearman.waitForGearmanToSettle()
-
-        self.cleanup_thread = GearmanCleanup(self)
-        self.cleanup_thread.start()
-        self.function_cache = set()
-        self.function_cache_time = 0
-
-    def stop(self):
-        self.log.debug("Stopping")
-        self.cleanup_thread.stop()
-        self.cleanup_thread.join()
-        self.gearman.shutdown()
-        self.log.debug("Stopped")
-
-    def isJobRegistered(self, name):
-        if self.function_cache_time:
-            for connection in self.gearman.active_connections:
-                if connection.connect_time > self.function_cache_time:
-                    self.function_cache = set()
-                    self.function_cache_time = 0
-                    break
-        if name in self.function_cache:
-            self.log.debug("Function %s is registered" % name)
-            return True
-        if ((time.time() - self.function_cache_time) <
-            self.negative_function_cache_ttl):
-            self.log.debug("Function %s is not registered "
-                           "(negative ttl in effect)" % name)
-            return False
-        self.function_cache_time = time.time()
-        for connection in self.gearman.active_connections:
-            try:
-                req = gear.StatusAdminRequest()
-                connection.sendAdminRequest(req, timeout=300)
-            except Exception:
-                self.log.exception("Exception while checking functions")
-                continue
-            for line in req.response.split('\n'):
-                parts = [x.strip() for x in line.split()]
-                if not parts or parts[0] == '.':
-                    continue
-                self.function_cache.add(parts[0])
-        if name in self.function_cache:
-            self.log.debug("Function %s is registered" % name)
-            return True
-        self.log.debug("Function %s is not registered" % name)
-        return False
-
-    def updateBuildParams(self, job, item, params):
-        """Allow the job to modify and add build parameters"""
-
-        # NOTE(jhesketh): The params need to stay in a key=value data pair
-        # as workers cannot necessarily handle lists.
-
-        if job.swift and self.swift.connection:
-
-            for name, s in job.swift.items():
-                swift_instructions = {}
-                s_config = {}
-                s_config.update((k, v.format(item=item, job=job,
-                                             change=item.change))
-                                if isinstance(v, basestring)
-                                else (k, v)
-                                for k, v in s.items())
-
-                (swift_instructions['URL'],
-                 swift_instructions['HMAC_BODY'],
-                 swift_instructions['SIGNATURE']) = \
-                    self.swift.generate_form_post_middleware_params(
-                        params['LOG_PATH'], **s_config)
-
-                if 'logserver_prefix' in s_config:
-                    swift_instructions['LOGSERVER_PREFIX'] = \
-                        s_config['logserver_prefix']
-                elif self.config.has_option('swift',
-                                            'default_logserver_prefix'):
-                    swift_instructions['LOGSERVER_PREFIX'] = \
-                        self.config.get('swift', 'default_logserver_prefix')
-
-                # Create a set of zuul instructions for each instruction-set
-                # given  in the form of NAME_PARAMETER=VALUE
-                for key, value in swift_instructions.items():
-                    params['_'.join(['SWIFT', name, key])] = value
-
-        if callable(job.parameter_function):
-            pargs = inspect.getargspec(job.parameter_function)
-            if len(pargs.args) == 2:
-                job.parameter_function(item, params)
-            else:
-                job.parameter_function(item, job, params)
-            self.log.debug("Custom parameter function used for job %s, "
-                           "change: %s, params: %s" % (job, item.change,
-                                                       params))
-
-    def launch(self, job, item, pipeline, dependent_items=[]):
-        uuid = str(uuid4().hex)
-        self.log.info(
-            "Launch job %s (uuid: %s) for change %s with dependent "
-            "changes %s" % (
-                job, uuid, item.change,
-                [x.change for x in dependent_items]))
-        dependent_items = dependent_items[:]
-        dependent_items.reverse()
-        params = dict(ZUUL_UUID=uuid,
-                      ZUUL_PROJECT=item.change.project.name)
-        params['ZUUL_PIPELINE'] = pipeline.name
-        params['ZUUL_URL'] = item.current_build_set.zuul_url
-        params['ZUUL_VOTING'] = job.voting and '1' or '0'
-        if hasattr(item.change, 'refspec'):
-            changes_str = '^'.join(
-                ['%s:%s:%s' % (i.change.project.name, i.change.branch,
-                               i.change.refspec)
-                 for i in dependent_items + [item]])
-            params['ZUUL_BRANCH'] = item.change.branch
-            params['ZUUL_CHANGES'] = changes_str
-            params['ZUUL_REF'] = ('refs/zuul/%s/%s' %
-                                  (item.change.branch,
-                                   item.current_build_set.ref))
-            params['ZUUL_COMMIT'] = item.current_build_set.commit
-
-            zuul_changes = ' '.join(['%s,%s' % (i.change.number,
-                                                i.change.patchset)
-                                     for i in dependent_items + [item]])
-            params['ZUUL_CHANGE_IDS'] = zuul_changes
-            params['ZUUL_CHANGE'] = str(item.change.number)
-            params['ZUUL_PATCHSET'] = str(item.change.patchset)
-        if hasattr(item.change, 'ref'):
-            params['ZUUL_REFNAME'] = item.change.ref
-            params['ZUUL_OLDREV'] = item.change.oldrev
-            params['ZUUL_NEWREV'] = item.change.newrev
-
-            params['ZUUL_REF'] = item.change.ref
-            params['ZUUL_COMMIT'] = item.change.newrev
-
-        # The destination_path is a unqiue path for this build request
-        # and generally where the logs are expected to be placed
-        destination_path = os.path.join(item.change.getBasePath(),
-                                        pipeline.name, job.name, uuid[:7])
-        params['BASE_LOG_PATH'] = item.change.getBasePath()
-        params['LOG_PATH'] = destination_path
-
-        # Allow the job to update the params
-        self.updateBuildParams(job, item, params)
-
-        # This is what we should be heading toward for parameters:
-
-        # required:
-        # ZUUL_UUID
-        # ZUUL_REF (/refs/zuul/..., /refs/tags/foo, master)
-        # ZUUL_COMMIT
-
-        # optional:
-        # ZUUL_PROJECT
-        # ZUUL_PIPELINE
-
-        # optional (changes only):
-        # ZUUL_BRANCH
-        # ZUUL_CHANGE
-        # ZUUL_CHANGE_IDS
-        # ZUUL_PATCHSET
-
-        # optional (ref updated only):
-        # ZUUL_OLDREV
-        # ZUUL_NEWREV
-
-        if 'ZUUL_NODE' in params:
-            name = "build:%s:%s" % (job.name, params['ZUUL_NODE'])
-        else:
-            name = "build:%s" % job.name
-        build = Build(job, uuid)
-        build.parameters = params
-
-        if job.name == 'noop':
-            self.sched.onBuildCompleted(build, 'SUCCESS')
-            return build
-
-        gearman_job = gear.Job(name, json.dumps(params),
-                               unique=uuid)
-        build.__gearman_job = gearman_job
-        self.builds[uuid] = build
-
-        if not self.isJobRegistered(gearman_job.name):
-            self.log.error("Job %s is not registered with Gearman" %
-                           gearman_job)
-            self.onBuildCompleted(gearman_job, 'NOT_REGISTERED')
-            return build
-
-        if pipeline.precedence == zuul.model.PRECEDENCE_NORMAL:
-            precedence = gear.PRECEDENCE_NORMAL
-        elif pipeline.precedence == zuul.model.PRECEDENCE_HIGH:
-            precedence = gear.PRECEDENCE_HIGH
-        elif pipeline.precedence == zuul.model.PRECEDENCE_LOW:
-            precedence = gear.PRECEDENCE_LOW
-
-        try:
-            self.gearman.submitJob(gearman_job, precedence=precedence,
-                                   timeout=300)
-        except Exception:
-            self.log.exception("Unable to submit job to Gearman")
-            self.onBuildCompleted(gearman_job, 'EXCEPTION')
-            return build
-
-        if not gearman_job.handle:
-            self.log.error("No job handle was received for %s after"
-                           " 300 seconds; marking as lost." %
-                           gearman_job)
-            self.onBuildCompleted(gearman_job, 'NO_HANDLE')
-
-        self.log.debug("Received handle %s for %s" % (gearman_job.handle,
-                                                      build))
-
-        return build
-
-    def cancel(self, build):
-        self.log.info("Cancel build %s for job %s" % (build, build.job))
-
-        build.canceled = True
-        try:
-            job = build.__gearman_job  # noqa
-        except AttributeError:
-            self.log.debug("Build %s has no associated gearman job" % build)
-            return
-
-        if build.number is not None:
-            self.log.debug("Build %s has already started" % build)
-            self.cancelRunningBuild(build)
-            self.log.debug("Canceled running build %s" % build)
-            return
-        else:
-            self.log.debug("Build %s has not started yet" % build)
-
-        self.log.debug("Looking for build %s in queue" % build)
-        if self.cancelJobInQueue(build):
-            self.log.debug("Removed build %s from queue" % build)
-            return
-
-        time.sleep(1)
-
-        self.log.debug("Still unable to find build %s to cancel" % build)
-        if build.number:
-            self.log.debug("Build %s has just started" % build)
-            self.log.debug("Canceled running build %s" % build)
-            self.cancelRunningBuild(build)
-            return
-        self.log.debug("Unable to cancel build %s" % build)
-
-    def onBuildCompleted(self, job, result=None):
-        if job.unique in self.meta_jobs:
-            del self.meta_jobs[job.unique]
-            return
-
-        build = self.builds.get(job.unique)
-        if build:
-            data = getJobData(job)
-            build.node_labels = data.get('node_labels', [])
-            build.node_name = data.get('node_name')
-            if not build.canceled:
-                if result is None:
-                    result = data.get('result')
-                if result is None:
-                    build.retry = True
-                self.log.info("Build %s complete, result %s" %
-                              (job, result))
-                self.sched.onBuildCompleted(build, result)
-            # The test suite expects the build to be removed from the
-            # internal dict after it's added to the report queue.
-            del self.builds[job.unique]
-        else:
-            if not job.name.startswith("stop:"):
-                self.log.error("Unable to find build %s" % job.unique)
-
-    def onWorkStatus(self, job):
-        data = getJobData(job)
-        self.log.debug("Build %s update %s" % (job, data))
-        build = self.builds.get(job.unique)
-        if build:
-            # Allow URL to be updated
-            build.url = data.get('url') or build.url
-            # Update information about worker
-            build.worker.updateFromData(data)
-
-            if build.number is None:
-                self.log.info("Build %s started" % job)
-                build.number = data.get('number')
-                build.__gearman_manager = data.get('manager')
-                self.sched.onBuildStarted(build)
-
-            if job.denominator:
-                build.estimated_time = float(job.denominator) / 1000
-        else:
-            self.log.error("Unable to find build %s" % job.unique)
-
-    def onDisconnect(self, job):
-        self.log.info("Gearman job %s lost due to disconnect" % job)
-        self.onBuildCompleted(job)
-
-    def onUnknownJob(self, job):
-        self.log.info("Gearman job %s lost due to unknown handle" % job)
-        self.onBuildCompleted(job, 'LOST')
-
-    def cancelJobInQueue(self, build):
-        job = build.__gearman_job
-
-        req = gear.CancelJobAdminRequest(job.handle)
-        job.connection.sendAdminRequest(req, timeout=300)
-        self.log.debug("Response to cancel build %s request: %s" %
-                       (build, req.response.strip()))
-        if req.response.startswith("OK"):
-            try:
-                del self.builds[job.unique]
-            except:
-                pass
-            return True
-        return False
-
-    def cancelRunningBuild(self, build):
-        stop_uuid = str(uuid4().hex)
-        data = dict(name=build.job.name,
-                    number=build.number)
-        stop_job = gear.Job("stop:%s" % build.__gearman_manager,
-                            json.dumps(data), unique=stop_uuid)
-        self.meta_jobs[stop_uuid] = stop_job
-        self.log.debug("Submitting stop job: %s", stop_job)
-        self.gearman.submitJob(stop_job, precedence=gear.PRECEDENCE_HIGH,
-                               timeout=300)
-        return True
-
-    def setBuildDescription(self, build, desc):
-        try:
-            name = "set_description:%s" % build.__gearman_manager
-        except AttributeError:
-            # We haven't yet received the first data packet that tells
-            # us where the job is running.
-            return False
-
-        if not self.isJobRegistered(name):
-            return False
-
-        desc_uuid = str(uuid4().hex)
-        data = dict(name=build.job.name,
-                    number=build.number,
-                    html_description=desc)
-        desc_job = gear.Job(name, json.dumps(data), unique=desc_uuid)
-        self.meta_jobs[desc_uuid] = desc_job
-        self.log.debug("Submitting describe job: %s", desc_job)
-        self.gearman.submitJob(desc_job, precedence=gear.PRECEDENCE_LOW,
-                               timeout=300)
-        return True
-
-    def lookForLostBuilds(self):
-        self.log.debug("Looking for lost builds")
-        for build in self.builds.values():
-            if build.result:
-                # The build has finished, it will be removed
-                continue
-            job = build.__gearman_job
-            if not job.handle:
-                # The build hasn't been enqueued yet
-                continue
-            p = gear.Packet(gear.constants.REQ, gear.constants.GET_STATUS,
-                            job.handle)
-            job.connection.sendPacket(p)
+# Copyright 2012 Hewlett-Packard Development Company, L.P.
+#
+# Licensed under the Apache License, Version 2.0 (the "License"); you may
+# not use this file except in compliance with the License. You may obtain
+# a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+# License for the specific language governing permissions and limitations
+# under the License.
+
+import gear
+import inspect
+import json
+import logging
+import os
+import time
+import threading
+from uuid import uuid4
+
+import zuul.model
+from zuul.model import Build
+
+
+class GearmanCleanup(threading.Thread):
+    """ A thread that checks to see if outstanding builds have
+    completed without reporting back. """
+    log = logging.getLogger("zuul.GearmanCleanup")
+
+    def __init__(self, gearman):
+        threading.Thread.__init__(self)
+        self.daemon = True
+        self.gearman = gearman
+        self.wake_event = threading.Event()
+        self._stopped = False
+
+    def stop(self):
+        self._stopped = True
+        self.wake_event.set()
+
+    def run(self):
+        while True:
+            self.wake_event.wait(300)
+            if self._stopped:
+                return
+            try:
+                self.gearman.lookForLostBuilds()
+            except:
+                self.log.exception("Exception checking builds:")
+
+
+def getJobData(job):
+    if not len(job.data):
+        return {}
+    d = job.data[-1]
+    if not d:
+        return {}
+    return json.loads(d)
+
+
+class ZuulGearmanClient(gear.Client):
+    def __init__(self, zuul_gearman):
+        super(ZuulGearmanClient, self).__init__()
+        self.__zuul_gearman = zuul_gearman
+
+    def handleWorkComplete(self, packet):
+        job = super(ZuulGearmanClient, self).handleWorkComplete(packet)
+        self.__zuul_gearman.onBuildCompleted(job)
+        return job
+
+    def handleWorkFail(self, packet):
+        job = super(ZuulGearmanClient, self).handleWorkFail(packet)
+        self.__zuul_gearman.onBuildCompleted(job)
+        return job
+
+    def handleWorkException(self, packet):
+        job = super(ZuulGearmanClient, self).handleWorkException(packet)
+        self.__zuul_gearman.onBuildCompleted(job)
+        return job
+
+    def handleWorkStatus(self, packet):
+        job = super(ZuulGearmanClient, self).handleWorkStatus(packet)
+        self.__zuul_gearman.onWorkStatus(job)
+        return job
+
+    def handleWorkData(self, packet):
+        job = super(ZuulGearmanClient, self).handleWorkData(packet)
+        self.__zuul_gearman.onWorkStatus(job)
+        return job
+
+    def handleDisconnect(self, job):
+        job = super(ZuulGearmanClient, self).handleDisconnect(job)
+        self.__zuul_gearman.onDisconnect(job)
+
+    def handleStatusRes(self, packet):
+        try:
+            job = super(ZuulGearmanClient, self).handleStatusRes(packet)
+        except gear.UnknownJobError:
+            handle = packet.getArgument(0)
+            for build in self.__zuul_gearman.builds:
+                if build.__gearman_job.handle == handle:
+                    self.__zuul_gearman.onUnknownJob(job)
+
+    def waitForGearmanToSettle(self):
+        # If we're running the internal gearman server, it's possible
+        # that after a restart or reload, we may be immediately ready
+        # to run jobs but all the gearman workers may not have
+        # registered yet.  Give them a sporting chance to show up
+        # before we start declaring jobs lost because we don't have
+        # gearman functions registered for them.
+
+        # Spend up to 30 seconds after we connect to the gearman
+        # server waiting for the set of defined jobs to become
+        # consistent over a sliding 5 second window.
+
+        self.log.info("Waiting for connection to internal Gearman server")
+        self.waitForServer()
+        self.log.info("Waiting for gearman function set to settle")
+        start = time.time()
+        last_change = start
+        all_functions = set()
+        while time.time() - start < 30:
+            now = time.time()
+            last_functions = set()
+            for connection in self.active_connections:
+                try:
+                    req = gear.StatusAdminRequest()
+                    connection.sendAdminRequest(req, timeout=300)
+                except Exception:
+                    self.log.exception("Exception while checking functions")
+                    continue
+                for line in req.response.split('\n'):
+                    parts = [x.strip() for x in line.split()]
+                    if not parts or parts[0] == '.':
+                        continue
+                    last_functions.add(parts[0])
+            if last_functions != all_functions:
+                last_change = now
+                all_functions.update(last_functions)
+            else:
+                if now - last_change > 5:
+                    self.log.info("Gearman function set has settled")
+                    break
+            time.sleep(1)
+        self.log.info("Done waiting for Gearman server")
+
+
+class Gearman(object):
+    log = logging.getLogger("zuul.Gearman")
+    negative_function_cache_ttl = 5
+
+    def __init__(self, config, sched, swift):
+        self.config = config
+        self.sched = sched
+        self.swift = swift
+        self.builds = {}
+        self.meta_jobs = {}  # A list of meta-jobs like stop or describe
+
+        server = config.get('gearman', 'server')
+        if config.has_option('gearman', 'port'):
+            port = config.get('gearman', 'port')
+        else:
+            port = 4730
+
+        self.gearman = ZuulGearmanClient(self)
+        self.gearman.addServer(server, port)
+
+        if (config.has_option('gearman_server', 'start') and
+            config.getboolean('gearman_server', 'start')):
+            self.gearman.waitForGearmanToSettle()
+
+        self.cleanup_thread = GearmanCleanup(self)
+        self.cleanup_thread.start()
+        self.function_cache = set()
+        self.function_cache_time = 0
+
+    def stop(self):
+        self.log.debug("Stopping")
+        self.cleanup_thread.stop()
+        self.cleanup_thread.join()
+        self.gearman.shutdown()
+        self.log.debug("Stopped")
+
+    def isJobRegistered(self, name):
+        if self.function_cache_time:
+            for connection in self.gearman.active_connections:
+                if connection.connect_time > self.function_cache_time:
+                    self.function_cache = set()
+                    self.function_cache_time = 0
+                    break
+        if name in self.function_cache:
+            self.log.debug("Function %s is registered" % name)
+            return True
+        if ((time.time() - self.function_cache_time) <
+            self.negative_function_cache_ttl):
+            self.log.debug("Function %s is not registered "
+                           "(negative ttl in effect)" % name)
+            return False
+        self.function_cache_time = time.time()
+        for connection in self.gearman.active_connections:
+            try:
+                req = gear.StatusAdminRequest()
+                connection.sendAdminRequest(req, timeout=300)
+            except Exception:
+                self.log.exception("Exception while checking functions")
+                continue
+            for line in req.response.split('\n'):
+                parts = [x.strip() for x in line.split()]
+                if not parts or parts[0] == '.':
+                    continue
+                self.function_cache.add(parts[0])
+        if name in self.function_cache:
+            self.log.debug("Function %s is registered" % name)
+            return True
+        self.log.debug("Function %s is not registered" % name)
+        return False
+
+    def updateBuildParams(self, job, item, params):
+        """Allow the job to modify and add build parameters"""
+
+        # NOTE(jhesketh): The params need to stay in a key=value data pair
+        # as workers cannot necessarily handle lists.
+
+        if job.swift and self.swift.connection:
+
+            for name, s in job.swift.items():
+                swift_instructions = {}
+                s_config = {}
+                s_config.update((k, v.format(item=item, job=job,
+                                             change=item.change))
+                                if isinstance(v, basestring)
+                                else (k, v)
+                                for k, v in s.items())
+
+                (swift_instructions['URL'],
+                 swift_instructions['HMAC_BODY'],
+                 swift_instructions['SIGNATURE']) = \
+                    self.swift.generate_form_post_middleware_params(
+                        params['LOG_PATH'], **s_config)
+
+                if 'logserver_prefix' in s_config:
+                    swift_instructions['LOGSERVER_PREFIX'] = \
+                        s_config['logserver_prefix']
+                elif self.config.has_option('swift',
+                                            'default_logserver_prefix'):
+                    swift_instructions['LOGSERVER_PREFIX'] = \
+                        self.config.get('swift', 'default_logserver_prefix')
+
+                # Create a set of zuul instructions for each instruction-set
+                # given  in the form of NAME_PARAMETER=VALUE
+                for key, value in swift_instructions.items():
+                    params['_'.join(['SWIFT', name, key])] = value
+
+        if callable(job.parameter_function):
+            pargs = inspect.getargspec(job.parameter_function)
+            if len(pargs.args) == 2:
+                job.parameter_function(item, params)
+            else:
+                job.parameter_function(item, job, params)
+            self.log.debug("Custom parameter function used for job %s, "
+                           "change: %s, params: %s" % (job, item.change,
+                                                       params))
+
+    def launch(self, job, item, pipeline, dependent_items=[]):
+        uuid = str(uuid4().hex)
+        self.log.info(
+            "Launch job %s (uuid: %s) for change %s with dependent "
+            "changes %s" % (
+                job, uuid, item.change,
+                [x.change for x in dependent_items]))
+        dependent_items = dependent_items[:]
+        dependent_items.reverse()
+        params = dict(ZUUL_UUID=uuid,
+                      ZUUL_PROJECT=item.change.project.name)
+        params['ZUUL_PIPELINE'] = pipeline.name
+        params['ZUUL_URL'] = item.current_build_set.zuul_url
+        params['ZUUL_VOTING'] = job.voting and '1' or '0'
+        if hasattr(item.change, 'refspec'):
+            changes_str = '^'.join(
+                ['%s:%s:%s' % (i.change.project.name, i.change.branch,
+                               i.change.refspec)
+                 for i in dependent_items + [item]])
+            params['ZUUL_BRANCH'] = item.change.branch
+            params['ZUUL_CHANGES'] = changes_str
+            params['ZUUL_REF'] = ('refs/zuul/%s/%s' %
+                                  (item.change.branch,
+                                   item.current_build_set.ref))
+            params['ZUUL_COMMIT'] = item.current_build_set.commit
+
+            zuul_changes = ' '.join(['%s,%s' % (i.change.number,
+                                                i.change.patchset)
+                                     for i in dependent_items + [item]])
+            params['ZUUL_CHANGE_IDS'] = zuul_changes
+            params['ZUUL_CHANGE'] = str(item.change.number)
+            params['ZUUL_PATCHSET'] = str(item.change.patchset)
+        if hasattr(item.change, 'ref'):
+            params['ZUUL_REFNAME'] = item.change.ref
+            params['ZUUL_OLDREV'] = item.change.oldrev
+            params['ZUUL_NEWREV'] = item.change.newrev
+
+            params['ZUUL_REF'] = item.change.ref
+            params['ZUUL_COMMIT'] = item.change.newrev
+
+        # The destination_path is a unqiue path for this build request
+        # and generally where the logs are expected to be placed
+        destination_path = os.path.join(item.change.getBasePath(),
+                                        pipeline.name, job.name, uuid[:7])
+        params['BASE_LOG_PATH'] = item.change.getBasePath()
+        params['LOG_PATH'] = destination_path
+
+        # Allow the job to update the params
+        self.updateBuildParams(job, item, params)
+
+        # This is what we should be heading toward for parameters:
+
+        # required:
+        # ZUUL_UUID
+        # ZUUL_REF (/refs/zuul/..., /refs/tags/foo, master)
+        # ZUUL_COMMIT
+
+        # optional:
+        # ZUUL_PROJECT
+        # ZUUL_PIPELINE
+
+        # optional (changes only):
+        # ZUUL_BRANCH
+        # ZUUL_CHANGE
+        # ZUUL_CHANGE_IDS
+        # ZUUL_PATCHSET
+
+        # optional (ref updated only):
+        # ZUUL_OLDREV
+        # ZUUL_NEWREV
+
+        if 'ZUUL_NODE' in params:
+            name = "build:%s:%s" % (job.name, params['ZUUL_NODE'])
+        else:
+            name = "build:%s" % job.name
+        build = Build(job, uuid)
+        build.parameters = params
+
+        if job.name == 'noop':
+            self.sched.onBuildCompleted(build, 'SUCCESS')
+            return build
+
+        gearman_job = gear.Job(name, json.dumps(params),
+                               unique=uuid)
+        build.__gearman_job = gearman_job
+        self.builds[uuid] = build
+
+        if not self.isJobRegistered(gearman_job.name):
+            self.log.error("Job %s is not registered with Gearman" %
+                           gearman_job)
+            self.onBuildCompleted(gearman_job, 'NOT_REGISTERED')
+            return build
+
+        if pipeline.precedence == zuul.model.PRECEDENCE_NORMAL:
+            precedence = gear.PRECEDENCE_NORMAL
+        elif pipeline.precedence == zuul.model.PRECEDENCE_HIGH:
+            precedence = gear.PRECEDENCE_HIGH
+        elif pipeline.precedence == zuul.model.PRECEDENCE_LOW:
+            precedence = gear.PRECEDENCE_LOW
+
+        try:
+            self.gearman.submitJob(gearman_job, precedence=precedence,
+                                   timeout=300)
+        except Exception:
+            self.log.exception("Unable to submit job to Gearman")
+            self.onBuildCompleted(gearman_job, 'EXCEPTION')
+            return build
+
+        if not gearman_job.handle:
+            self.log.error("No job handle was received for %s after"
+                           " 300 seconds; marking as lost." %
+                           gearman_job)
+            self.onBuildCompleted(gearman_job, 'NO_HANDLE')
+
+        self.log.debug("Received handle %s for %s" % (gearman_job.handle,
+                                                      build))
+
+        return build
+
+    def cancel(self, build):
+        self.log.info("Cancel build %s for job %s" % (build, build.job))
+
+        build.canceled = True
+        try:
+            job = build.__gearman_job  # noqa
+        except AttributeError:
+            self.log.debug("Build %s has no associated gearman job" % build)
+            return
+
+        if build.number is not None:
+            self.log.debug("Build %s has already started" % build)
+            self.cancelRunningBuild(build)
+            self.log.debug("Canceled running build %s" % build)
+            return
+        else:
+            self.log.debug("Build %s has not started yet" % build)
+
+        self.log.debug("Looking for build %s in queue" % build)
+        if self.cancelJobInQueue(build):
+            self.log.debug("Removed build %s from queue" % build)
+            return
+
+        time.sleep(1)
+
+        self.log.debug("Still unable to find build %s to cancel" % build)
+        if build.number:
+            self.log.debug("Build %s has just started" % build)
+            self.log.debug("Canceled running build %s" % build)
+            self.cancelRunningBuild(build)
+            return
+        self.log.debug("Unable to cancel build %s" % build)
+
+    def onBuildCompleted(self, job, result=None):
+        if job.unique in self.meta_jobs:
+            del self.meta_jobs[job.unique]
+            return
+
+        build = self.builds.get(job.unique)
+        if build:
+            data = getJobData(job)
+            build.node_labels = data.get('node_labels', [])
+            build.node_name = data.get('node_name')
+            if not build.canceled:
+                if result is None:
+                    result = data.get('result')
+                if result is None:
+                    build.retry = True
+                self.log.info("Build %s complete, result %s" %
+                              (job, result))
+                self.sched.onBuildCompleted(build, result)
+            # The test suite expects the build to be removed from the
+            # internal dict after it's added to the report queue.
+            del self.builds[job.unique]
+        else:
+            if not job.name.startswith("stop:"):
+                self.log.error("Unable to find build %s" % job.unique)
+
+    def onWorkStatus(self, job):
+        data = getJobData(job)
+        self.log.debug("Build %s update %s" % (job, data))
+        build = self.builds.get(job.unique)
+        if build:
+            # Allow URL to be updated
+            build.url = data.get('url') or build.url
+            # Update information about worker
+            build.worker.updateFromData(data)
+
+            if build.number is None:
+                self.log.info("Build %s started" % job)
+                build.number = data.get('number')
+                build.__gearman_manager = data.get('manager')
+                self.sched.onBuildStarted(build)
+
+            if job.denominator:
+                build.estimated_time = float(job.denominator) / 1000
+        else:
+            self.log.error("Unable to find build %s" % job.unique)
+
+    def onDisconnect(self, job):
+        self.log.info("Gearman job %s lost due to disconnect" % job)
+        self.onBuildCompleted(job)
+
+    def onUnknownJob(self, job):
+        self.log.info("Gearman job %s lost due to unknown handle" % job)
+        self.onBuildCompleted(job, 'LOST')
+
+    def cancelJobInQueue(self, build):
+        job = build.__gearman_job
+
+        req = gear.CancelJobAdminRequest(job.handle)
+        job.connection.sendAdminRequest(req, timeout=300)
+        self.log.debug("Response to cancel build %s request: %s" %
+                       (build, req.response.strip()))
+        if req.response.startswith("OK"):
+            try:
+                del self.builds[job.unique]
+            except:
+                pass
+            return True
+        return False
+
+    def cancelRunningBuild(self, build):
+        stop_uuid = str(uuid4().hex)
+        data = dict(name=build.job.name,
+                    number=build.number)
+        stop_job = gear.Job("stop:%s" % build.__gearman_manager,
+                            json.dumps(data), unique=stop_uuid)
+        self.meta_jobs[stop_uuid] = stop_job
+        self.log.debug("Submitting stop job: %s", stop_job)
+        self.gearman.submitJob(stop_job, precedence=gear.PRECEDENCE_HIGH,
+                               timeout=300)
+        return True
+
+    def setBuildDescription(self, build, desc):
+        try:
+            name = "set_description:%s" % build.__gearman_manager
+        except AttributeError:
+            # We haven't yet received the first data packet that tells
+            # us where the job is running.
+            return False
+
+        if not self.isJobRegistered(name):
+            return False
+
+        desc_uuid = str(uuid4().hex)
+        data = dict(name=build.job.name,
+                    number=build.number,
+                    html_description=desc)
+        desc_job = gear.Job(name, json.dumps(data), unique=desc_uuid)
+        self.meta_jobs[desc_uuid] = desc_job
+        self.log.debug("Submitting describe job: %s", desc_job)
+        self.gearman.submitJob(desc_job, precedence=gear.PRECEDENCE_LOW,
+                               timeout=300)
+        return True
+
+    def lookForLostBuilds(self):
+        self.log.debug("Looking for lost builds")
+        for build in self.builds.values():
+            if build.result:
+                # The build has finished, it will be removed
+                continue
+            job = build.__gearman_job
+            if not job.handle:
+                # The build hasn't been enqueued yet
+                continue
+            p = gear.Packet(gear.constants.REQ, gear.constants.GET_STATUS,
+                            job.handle)
+            job.connection.sendPacket(p)
diff --git a/zuul/layoutvalidator.py b/zuul/layoutvalidator.py
index e1e8ac6..517a464 100644
--- a/zuul/layoutvalidator.py
+++ b/zuul/layoutvalidator.py
@@ -1,344 +1,344 @@
-# Copyright 2013 OpenStack Foundation
-# Copyright 2013 Antoine "hashar" Musso
-# Copyright 2013 Wikimedia Foundation Inc.
-# Copyright 2014 Hewlett-Packard Development Company, L.P.
-#
-# Licensed under the Apache License, Version 2.0 (the "License"); you may
-# not use this file except in compliance with the License. You may obtain
-# a copy of the License at
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-# License for the specific language governing permissions and limitations
-# under the License.
-
-import voluptuous as v
-import string
-
-
-# Several forms accept either a single item or a list, this makes
-# specifying that in the schema easy (and explicit).
-def toList(x):
-    return v.Any([x], x)
-
-
-class LayoutSchema(object):
-    include = {'python-file': str}
-    includes = [include]
-
-    manager = v.Any('IndependentPipelineManager',
-                    'DependentPipelineManager')
-
-    precedence = v.Any('normal', 'low', 'high')
-
-    approval = v.Schema({'username': str,
-                         'email-filter': str,
-                         'email': str,
-                         'older-than': str,
-                         'newer-than': str,
-                         }, extra=True)
-
-    require = {'approval': toList(approval),
-               'open': bool,
-               'current-patchset': bool,
-               'status': toList(str)}
-
-    reject = {'approval': toList(approval)}
-
-    window = v.All(int, v.Range(min=0))
-    window_floor = v.All(int, v.Range(min=1))
-    window_type = v.Any('linear', 'exponential')
-    window_factor = v.All(int, v.Range(min=1))
-
-    pipeline = {v.Required('name'): str,
-                v.Required('manager'): manager,
-                'source': str,
-                'precedence': precedence,
-                'description': str,
-                'require': require,
-                'reject': reject,
-                'success-message': str,
-                'failure-message': str,
-                'merge-failure-message': str,
-                'footer-message': str,
-                'dequeue-on-new-patchset': bool,
-                'ignore-dependencies': bool,
-                'disable-after-consecutive-failures':
-                    v.All(int, v.Range(min=1)),
-                'window': window,
-                'window-floor': window_floor,
-                'window-increase-type': window_type,
-                'window-increase-factor': window_factor,
-                'window-decrease-type': window_type,
-                'window-decrease-factor': window_factor,
-                }
-
-    project_template = {v.Required('name'): str}
-    project_templates = [project_template]
-
-    swift = {v.Required('name'): str,
-             'container': str,
-             'expiry': int,
-             'max_file_size': int,
-             'max-file-size': int,
-             'max_file_count': int,
-             'max-file-count': int,
-             'logserver_prefix': str,
-             'logserver-prefix': str,
-             }
-
-    skip_if = {'project': str,
-               'branch': str,
-               'all-files-match-any': toList(str),
-               }
-
-    job = {v.Required('name'): str,
-           'queue-name': str,
-           'failure-message': str,
-           'success-message': str,
-           'failure-pattern': str,
-           'success-pattern': str,
-           'hold-following-changes': bool,
-           'voting': bool,
-           'mutex': str,
-           'tags': toList(str),
-           'parameter-function': str,
-           'branch': toList(str),
-           'files': toList(str),
-           'swift': toList(swift),
-           'skip-if': toList(skip_if),
-           }
-    jobs = [job]
-
-    job_name = v.Schema(v.Match("^\S+$"))
-
-    def validateJob(self, value, path=[]):
-        if isinstance(value, list):
-            for (i, val) in enumerate(value):
-                self.validateJob(val, path + [i])
-        elif isinstance(value, dict):
-            for k, val in value.items():
-                self.validateJob(val, path + [k])
-        else:
-            self.job_name.schema(value)
-
-    def validateTemplateCalls(self, calls):
-        """ Verify a project pass the parameters required
-            by a project-template
-        """
-        for call in calls:
-            schema = self.templates_schemas[call.get('name')]
-            schema(call)
-
-    def collectFormatParam(self, tree):
-        """In a nested tree of string, dict and list, find out any named
-           parameters that might be used by str.format().  This is used to find
-           out whether projects are passing all the required parameters when
-           using a project template.
-
-            Returns a set() of all the named parameters found.
-        """
-        parameters = set()
-        if isinstance(tree, str):
-            # parse() returns a tuple of
-            # (literal_text, field_name, format_spec, conversion)
-            # We are just looking for field_name
-            parameters = set([t[1] for t in string.Formatter().parse(tree)
-                              if t[1] is not None])
-        elif isinstance(tree, list):
-            for item in tree:
-                parameters.update(self.collectFormatParam(item))
-        elif isinstance(tree, dict):
-            for item in tree:
-                parameters.update(self.collectFormatParam(tree[item]))
-
-        return parameters
-
-    def getDriverSchema(self, dtype, connections):
-        # TODO(jhesketh): Make the driver discovery dynamic
-        connection_drivers = {
-            'trigger': {
-                'gerrit': 'zuul.trigger.gerrit',
-            },
-            'reporter': {
-                'gerrit': 'zuul.reporter.gerrit',
-                'smtp': 'zuul.reporter.smtp',
-            },
-        }
-        standard_drivers = {
-            'trigger': {
-                'timer': 'zuul.trigger.timer',
-                'zuul': 'zuul.trigger.zuultrigger',
-            }
-        }
-
-        schema = {}
-        # Add the configured connections as available layout options
-        for connection_name, connection in connections.items():
-            for dname, dmod in connection_drivers.get(dtype, {}).items():
-                if connection.driver_name == dname:
-                    schema[connection_name] = toList(__import__(
-                        connection_drivers[dtype][dname],
-                        fromlist=['']).getSchema())
-
-        # Standard drivers are always available and don't require a unique
-        # (connection) name
-        for dname, dmod in standard_drivers.get(dtype, {}).items():
-            schema[dname] = toList(__import__(
-                standard_drivers[dtype][dname], fromlist=['']).getSchema())
-
-        return schema
-
-    def getSchema(self, data, connections=None):
-        if not isinstance(data, dict):
-            raise Exception("Malformed layout configuration: top-level type "
-                            "should be a dictionary")
-        pipelines = data.get('pipelines')
-        if not pipelines:
-            pipelines = []
-        pipelines = [p['name'] for p in pipelines if 'name' in p]
-
-        # Whenever a project uses a template, it better have to exist
-        project_templates = data.get('project-templates', [])
-        template_names = [t['name'] for t in project_templates
-                          if 'name' in t]
-
-        # A project using a template must pass all parameters to it.
-        # We first collect each templates parameters and craft a new
-        # schema for each of the template. That will later be used
-        # by validateTemplateCalls().
-        self.templates_schemas = {}
-        for t_name in template_names:
-            # Find out the parameters used inside each templates:
-            template = [t for t in project_templates
-                        if t['name'] == t_name]
-            template_parameters = self.collectFormatParam(template)
-
-            # Craft the templates schemas
-            schema = {v.Required('name'): v.Any(*template_names)}
-            for required_param in template_parameters:
-                # special case 'name' which will be automatically provided
-                if required_param == 'name':
-                    continue
-                # add this template parameters as requirements:
-                schema.update({v.Required(required_param): str})
-
-            # Register the schema for validateTemplateCalls()
-            self.templates_schemas[t_name] = v.Schema(schema)
-
-        project = {'name': str,
-                   'merge-mode': v.Any('merge', 'merge-resolve,',
-                                       'cherry-pick'),
-                   'template': self.validateTemplateCalls,
-                   }
-
-        # And project should refers to existing pipelines
-        for p in pipelines:
-            project[p] = self.validateJob
-        projects = [project]
-
-        # Sub schema to validate a project template has existing
-        # pipelines and jobs.
-        project_template = {'name': str}
-        for p in pipelines:
-            project_template[p] = self.validateJob
-        project_templates = [project_template]
-
-        # TODO(jhesketh): source schema is still defined above as sources
-        # currently aren't key/value so there is nothing to validate. Need to
-        # revisit this and figure out how to allow drivers with and without
-        # params. eg support all:
-        #   source: gerrit
-        # and
-        #   source:
-        #     gerrit:
-        #       - val
-        #       - val2
-        # and
-        #   source:
-        #     gerrit: something
-        # etc...
-        self.pipeline['trigger'] = v.Required(
-            self.getDriverSchema('trigger', connections))
-        for action in ['start', 'success', 'failure', 'merge-failure',
-                       'disabled']:
-            self.pipeline[action] = self.getDriverSchema('reporter',
-                                                         connections)
-
-        # Gather our sub schemas
-        schema = v.Schema({'includes': self.includes,
-                           v.Required('pipelines'): [self.pipeline],
-                           'jobs': self.jobs,
-                           'project-templates': project_templates,
-                           v.Required('projects'): projects,
-                           })
-        return schema
-
-
-class LayoutValidator(object):
-    def checkDuplicateNames(self, data, path):
-        items = []
-        for i, item in enumerate(data):
-            if item['name'] in items:
-                raise v.Invalid("Duplicate name: %s" % item['name'],
-                                path + [i])
-            items.append(item['name'])
-
-    def extraDriverValidation(self, dtype, driver_data, connections=None):
-        # Some drivers may have extra validation to run on the layout
-        # TODO(jhesketh): Make the driver discovery dynamic
-        connection_drivers = {
-            'trigger': {
-                'gerrit': 'zuul.trigger.gerrit',
-            },
-            'reporter': {
-                'gerrit': 'zuul.reporter.gerrit',
-                'smtp': 'zuul.reporter.smtp',
-            },
-        }
-        standard_drivers = {
-            'trigger': {
-                'timer': 'zuul.trigger.timer',
-                'zuul': 'zuul.trigger.zuultrigger',
-            }
-        }
-
-        for dname, d_conf in driver_data.items():
-            for connection_name, connection in connections.items():
-                if connection_name == dname:
-                    if (connection.driver_name in
-                        connection_drivers.get(dtype, {}).keys()):
-                        module = __import__(
-                            connection_drivers[dtype][connection.driver_name],
-                            fromlist=['']
-                        )
-                        if 'validate_conf' in dir(module):
-                            module.validate_conf(d_conf)
-                    break
-            if dname in standard_drivers.get(dtype, {}).keys():
-                module = __import__(standard_drivers[dtype][dname],
-                                    fromlist=[''])
-                if 'validate_conf' in dir(module):
-                    module.validate_conf(d_conf)
-
-    def validate(self, data, connections=None):
-        schema = LayoutSchema().getSchema(data, connections)
-        schema(data)
-        self.checkDuplicateNames(data['pipelines'], ['pipelines'])
-        if 'jobs' in data:
-            self.checkDuplicateNames(data['jobs'], ['jobs'])
-        self.checkDuplicateNames(data['projects'], ['projects'])
-        if 'project-templates' in data:
-            self.checkDuplicateNames(
-                data['project-templates'], ['project-templates'])
-
-        for pipeline in data['pipelines']:
-            self.extraDriverValidation('trigger', pipeline['trigger'],
-                                       connections)
-            for action in ['start', 'success', 'failure', 'merge-failure']:
-                if action in pipeline:
-                    self.extraDriverValidation('reporter', pipeline[action],
-                                               connections)
+# Copyright 2013 OpenStack Foundation
+# Copyright 2013 Antoine "hashar" Musso
+# Copyright 2013 Wikimedia Foundation Inc.
+# Copyright 2014 Hewlett-Packard Development Company, L.P.
+#
+# Licensed under the Apache License, Version 2.0 (the "License"); you may
+# not use this file except in compliance with the License. You may obtain
+# a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+# License for the specific language governing permissions and limitations
+# under the License.
+
+import voluptuous as v
+import string
+
+
+# Several forms accept either a single item or a list, this makes
+# specifying that in the schema easy (and explicit).
+def toList(x):
+    return v.Any([x], x)
+
+
+class LayoutSchema(object):
+    include = {'python-file': str}
+    includes = [include]
+
+    manager = v.Any('IndependentPipelineManager',
+                    'DependentPipelineManager')
+
+    precedence = v.Any('normal', 'low', 'high')
+
+    approval = v.Schema({'username': str,
+                         'email-filter': str,
+                         'email': str,
+                         'older-than': str,
+                         'newer-than': str,
+                         }, extra=True)
+
+    require = {'approval': toList(approval),
+               'open': bool,
+               'current-patchset': bool,
+               'status': toList(str)}
+
+    reject = {'approval': toList(approval)}
+
+    window = v.All(int, v.Range(min=0))
+    window_floor = v.All(int, v.Range(min=1))
+    window_type = v.Any('linear', 'exponential')
+    window_factor = v.All(int, v.Range(min=1))
+
+    pipeline = {v.Required('name'): str,
+                v.Required('manager'): manager,
+                'source': str,
+                'precedence': precedence,
+                'description': str,
+                'require': require,
+                'reject': reject,
+                'success-message': str,
+                'failure-message': str,
+                'merge-failure-message': str,
+                'footer-message': str,
+                'dequeue-on-new-patchset': bool,
+                'ignore-dependencies': bool,
+                'disable-after-consecutive-failures':
+                    v.All(int, v.Range(min=1)),
+                'window': window,
+                'window-floor': window_floor,
+                'window-increase-type': window_type,
+                'window-increase-factor': window_factor,
+                'window-decrease-type': window_type,
+                'window-decrease-factor': window_factor,
+                }
+
+    project_template = {v.Required('name'): str}
+    project_templates = [project_template]
+
+    swift = {v.Required('name'): str,
+             'container': str,
+             'expiry': int,
+             'max_file_size': int,
+             'max-file-size': int,
+             'max_file_count': int,
+             'max-file-count': int,
+             'logserver_prefix': str,
+             'logserver-prefix': str,
+             }
+
+    skip_if = {'project': str,
+               'branch': str,
+               'all-files-match-any': toList(str),
+               }
+
+    job = {v.Required('name'): str,
+           'queue-name': str,
+           'failure-message': str,
+           'success-message': str,
+           'failure-pattern': str,
+           'success-pattern': str,
+           'hold-following-changes': bool,
+           'voting': bool,
+           'mutex': str,
+           'tags': toList(str),
+           'parameter-function': str,
+           'branch': toList(str),
+           'files': toList(str),
+           'swift': toList(swift),
+           'skip-if': toList(skip_if),
+           }
+    jobs = [job]
+
+    job_name = v.Schema(v.Match("^\S+$"))
+
+    def validateJob(self, value, path=[]):
+        if isinstance(value, list):
+            for (i, val) in enumerate(value):
+                self.validateJob(val, path + [i])
+        elif isinstance(value, dict):
+            for k, val in value.items():
+                self.validateJob(val, path + [k])
+        else:
+            self.job_name.schema(value)
+
+    def validateTemplateCalls(self, calls):
+        """ Verify a project pass the parameters required
+            by a project-template
+        """
+        for call in calls:
+            schema = self.templates_schemas[call.get('name')]
+            schema(call)
+
+    def collectFormatParam(self, tree):
+        """In a nested tree of string, dict and list, find out any named
+           parameters that might be used by str.format().  This is used to find
+           out whether projects are passing all the required parameters when
+           using a project template.
+
+            Returns a set() of all the named parameters found.
+        """
+        parameters = set()
+        if isinstance(tree, str):
+            # parse() returns a tuple of
+            # (literal_text, field_name, format_spec, conversion)
+            # We are just looking for field_name
+            parameters = set([t[1] for t in string.Formatter().parse(tree)
+                              if t[1] is not None])
+        elif isinstance(tree, list):
+            for item in tree:
+                parameters.update(self.collectFormatParam(item))
+        elif isinstance(tree, dict):
+            for item in tree:
+                parameters.update(self.collectFormatParam(tree[item]))
+
+        return parameters
+
+    def getDriverSchema(self, dtype, connections):
+        # TODO(jhesketh): Make the driver discovery dynamic
+        connection_drivers = {
+            'trigger': {
+                'gerrit': 'zuul.trigger.gerrit',
+            },
+            'reporter': {
+                'gerrit': 'zuul.reporter.gerrit',
+                'smtp': 'zuul.reporter.smtp',
+            },
+        }
+        standard_drivers = {
+            'trigger': {
+                'timer': 'zuul.trigger.timer',
+                'zuul': 'zuul.trigger.zuultrigger',
+            }
+        }
+
+        schema = {}
+        # Add the configured connections as available layout options
+        for connection_name, connection in connections.items():
+            for dname, dmod in connection_drivers.get(dtype, {}).items():
+                if connection.driver_name == dname:
+                    schema[connection_name] = toList(__import__(
+                        connection_drivers[dtype][dname],
+                        fromlist=['']).getSchema())
+
+        # Standard drivers are always available and don't require a unique
+        # (connection) name
+        for dname, dmod in standard_drivers.get(dtype, {}).items():
+            schema[dname] = toList(__import__(
+                standard_drivers[dtype][dname], fromlist=['']).getSchema())
+
+        return schema
+
+    def getSchema(self, data, connections=None):
+        if not isinstance(data, dict):
+            raise Exception("Malformed layout configuration: top-level type "
+                            "should be a dictionary")
+        pipelines = data.get('pipelines')
+        if not pipelines:
+            pipelines = []
+        pipelines = [p['name'] for p in pipelines if 'name' in p]
+
+        # Whenever a project uses a template, it better have to exist
+        project_templates = data.get('project-templates', [])
+        template_names = [t['name'] for t in project_templates
+                          if 'name' in t]
+
+        # A project using a template must pass all parameters to it.
+        # We first collect each templates parameters and craft a new
+        # schema for each of the template. That will later be used
+        # by validateTemplateCalls().
+        self.templates_schemas = {}
+        for t_name in template_names:
+            # Find out the parameters used inside each templates:
+            template = [t for t in project_templates
+                        if t['name'] == t_name]
+            template_parameters = self.collectFormatParam(template)
+
+            # Craft the templates schemas
+            schema = {v.Required('name'): v.Any(*template_names)}
+            for required_param in template_parameters:
+                # special case 'name' which will be automatically provided
+                if required_param == 'name':
+                    continue
+                # add this template parameters as requirements:
+                schema.update({v.Required(required_param): str})
+
+            # Register the schema for validateTemplateCalls()
+            self.templates_schemas[t_name] = v.Schema(schema)
+
+        project = {'name': str,
+                   'merge-mode': v.Any('merge', 'merge-resolve,',
+                                       'cherry-pick'),
+                   'template': self.validateTemplateCalls,
+                   }
+
+        # And project should refers to existing pipelines
+        for p in pipelines:
+            project[p] = self.validateJob
+        projects = [project]
+
+        # Sub schema to validate a project template has existing
+        # pipelines and jobs.
+        project_template = {'name': str}
+        for p in pipelines:
+            project_template[p] = self.validateJob
+        project_templates = [project_template]
+
+        # TODO(jhesketh): source schema is still defined above as sources
+        # currently aren't key/value so there is nothing to validate. Need to
+        # revisit this and figure out how to allow drivers with and without
+        # params. eg support all:
+        #   source: gerrit
+        # and
+        #   source:
+        #     gerrit:
+        #       - val
+        #       - val2
+        # and
+        #   source:
+        #     gerrit: something
+        # etc...
+        self.pipeline['trigger'] = v.Required(
+            self.getDriverSchema('trigger', connections))
+        for action in ['start', 'success', 'failure', 'merge-failure',
+                       'disabled']:
+            self.pipeline[action] = self.getDriverSchema('reporter',
+                                                         connections)
+
+        # Gather our sub schemas
+        schema = v.Schema({'includes': self.includes,
+                           v.Required('pipelines'): [self.pipeline],
+                           'jobs': self.jobs,
+                           'project-templates': project_templates,
+                           v.Required('projects'): projects,
+                           })
+        return schema
+
+
+class LayoutValidator(object):
+    def checkDuplicateNames(self, data, path):
+        items = []
+        for i, item in enumerate(data):
+            if item['name'] in items:
+                raise v.Invalid("Duplicate name: %s" % item['name'],
+                                path + [i])
+            items.append(item['name'])
+
+    def extraDriverValidation(self, dtype, driver_data, connections=None):
+        # Some drivers may have extra validation to run on the layout
+        # TODO(jhesketh): Make the driver discovery dynamic
+        connection_drivers = {
+            'trigger': {
+                'gerrit': 'zuul.trigger.gerrit',
+            },
+            'reporter': {
+                'gerrit': 'zuul.reporter.gerrit',
+                'smtp': 'zuul.reporter.smtp',
+            },
+        }
+        standard_drivers = {
+            'trigger': {
+                'timer': 'zuul.trigger.timer',
+                'zuul': 'zuul.trigger.zuultrigger',
+            }
+        }
+
+        for dname, d_conf in driver_data.items():
+            for connection_name, connection in connections.items():
+                if connection_name == dname:
+                    if (connection.driver_name in
+                        connection_drivers.get(dtype, {}).keys()):
+                        module = __import__(
+                            connection_drivers[dtype][connection.driver_name],
+                            fromlist=['']
+                        )
+                        if 'validate_conf' in dir(module):
+                            module.validate_conf(d_conf)
+                    break
+            if dname in standard_drivers.get(dtype, {}).keys():
+                module = __import__(standard_drivers[dtype][dname],
+                                    fromlist=[''])
+                if 'validate_conf' in dir(module):
+                    module.validate_conf(d_conf)
+
+    def validate(self, data, connections=None):
+        schema = LayoutSchema().getSchema(data, connections)
+        schema(data)
+        self.checkDuplicateNames(data['pipelines'], ['pipelines'])
+        if 'jobs' in data:
+            self.checkDuplicateNames(data['jobs'], ['jobs'])
+        self.checkDuplicateNames(data['projects'], ['projects'])
+        if 'project-templates' in data:
+            self.checkDuplicateNames(
+                data['project-templates'], ['project-templates'])
+
+        for pipeline in data['pipelines']:
+            self.extraDriverValidation('trigger', pipeline['trigger'],
+                                       connections)
+            for action in ['start', 'success', 'failure', 'merge-failure']:
+                if action in pipeline:
+                    self.extraDriverValidation('reporter', pipeline[action],
+                                               connections)
diff --git a/zuul/lib/clonemapper.py b/zuul/lib/clonemapper.py
index ae558cd..27a6e1b 100644
--- a/zuul/lib/clonemapper.py
+++ b/zuul/lib/clonemapper.py
@@ -1,78 +1,78 @@
-# Copyright 2014 Antoine "hashar" Musso
-# Copyright 2014 Wikimedia Foundation Inc.
-#
-# Licensed under the Apache License, Version 2.0 (the "License"); you may
-# not use this file except in compliance with the License. You may obtain
-# a copy of the License at
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-# License for the specific language governing permissions and limitations
-# under the License.
-
-from collections import defaultdict
-import extras
-import logging
-import os
-import re
-
-OrderedDict = extras.try_imports(['collections.OrderedDict',
-                                  'ordereddict.OrderedDict'])
-
-
-class CloneMapper(object):
-    log = logging.getLogger("zuul.CloneMapper")
-
-    def __init__(self, clonemap, projects):
-        self.clonemap = clonemap
-        self.projects = projects
-
-    def expand(self, workspace):
-        self.log.info("Workspace path set to: %s", workspace)
-
-        is_valid = True
-        ret = OrderedDict()
-        for project in self.projects:
-            dests = []
-            for mapping in self.clonemap:
-                if re.match(r'^%s$' % mapping['name'],
-                            project):
-                    # Might be matched more than one time
-                    dests.append(
-                        re.sub(mapping['name'], mapping['dest'], project))
-
-            if len(dests) > 1:
-                self.log.error("Duplicate destinations for %s: %s.",
-                               project, dests)
-                is_valid = False
-            elif len(dests) == 0:
-                self.log.debug("Using %s as destination (unmatched)",
-                               project)
-                ret[project] = [project]
-            else:
-                ret[project] = dests
-
-        if not is_valid:
-            raise Exception("Expansion error. Check error messages above")
-
-        self.log.info("Mapping projects to workspace...")
-        for project, dest in ret.iteritems():
-            dest = os.path.normpath(os.path.join(workspace, dest[0]))
-            ret[project] = dest
-            self.log.info("  %s -> %s", project, dest)
-
-        self.log.debug("Checking overlap in destination directories...")
-        check = defaultdict(list)
-        for project, dest in ret.iteritems():
-            check[dest].append(project)
-
-        dupes = dict((d, p) for (d, p) in check.iteritems() if len(p) > 1)
-        if dupes:
-            raise Exception("Some projects share the same destination: %s",
-                            dupes)
-
-        self.log.info("Expansion completed.")
-        return ret
+# Copyright 2014 Antoine "hashar" Musso
+# Copyright 2014 Wikimedia Foundation Inc.
+#
+# Licensed under the Apache License, Version 2.0 (the "License"); you may
+# not use this file except in compliance with the License. You may obtain
+# a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+# License for the specific language governing permissions and limitations
+# under the License.
+
+from collections import defaultdict
+import extras
+import logging
+import os
+import re
+
+OrderedDict = extras.try_imports(['collections.OrderedDict',
+                                  'ordereddict.OrderedDict'])
+
+
+class CloneMapper(object):
+    log = logging.getLogger("zuul.CloneMapper")
+
+    def __init__(self, clonemap, projects):
+        self.clonemap = clonemap
+        self.projects = projects
+
+    def expand(self, workspace):
+        self.log.info("Workspace path set to: %s", workspace)
+
+        is_valid = True
+        ret = OrderedDict()
+        for project in self.projects:
+            dests = []
+            for mapping in self.clonemap:
+                if re.match(r'^%s$' % mapping['name'],
+                            project):
+                    # Might be matched more than one time
+                    dests.append(
+                        re.sub(mapping['name'], mapping['dest'], project))
+
+            if len(dests) > 1:
+                self.log.error("Duplicate destinations for %s: %s.",
+                               project, dests)
+                is_valid = False
+            elif len(dests) == 0:
+                self.log.debug("Using %s as destination (unmatched)",
+                               project)
+                ret[project] = [project]
+            else:
+                ret[project] = dests
+
+        if not is_valid:
+            raise Exception("Expansion error. Check error messages above")
+
+        self.log.info("Mapping projects to workspace...")
+        for project, dest in ret.iteritems():
+            dest = os.path.normpath(os.path.join(workspace, dest[0]))
+            ret[project] = dest
+            self.log.info("  %s -> %s", project, dest)
+
+        self.log.debug("Checking overlap in destination directories...")
+        check = defaultdict(list)
+        for project, dest in ret.iteritems():
+            check[dest].append(project)
+
+        dupes = dict((d, p) for (d, p) in check.iteritems() if len(p) > 1)
+        if dupes:
+            raise Exception("Some projects share the same destination: %s",
+                            dupes)
+
+        self.log.info("Expansion completed.")
+        return ret
diff --git a/zuul/lib/cloner.py b/zuul/lib/cloner.py
index 77f52fa..e8fcb97 100644
--- a/zuul/lib/cloner.py
+++ b/zuul/lib/cloner.py
@@ -1,206 +1,206 @@
-# Copyright 2014 Antoine "hashar" Musso
-# Copyright 2014 Wikimedia Foundation Inc.
-#
-# Licensed under the Apache License, Version 2.0 (the "License"); you may
-# not use this file except in compliance with the License. You may obtain
-# a copy of the License at
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-# License for the specific language governing permissions and limitations
-# under the License.
-
-import git
-import logging
-import os
-import re
-import yaml
-
-from git import GitCommandError
-from zuul.lib.clonemapper import CloneMapper
-from zuul.merger.merger import Repo
-
-
-class Cloner(object):
-    log = logging.getLogger("zuul.Cloner")
-
-    def __init__(self, git_base_url, projects, workspace, zuul_branch,
-                 zuul_ref, zuul_url, branch=None, clone_map_file=None,
-                 project_branches=None, cache_dir=None,
-                 cache_no_hardlinks=None):
-
-        self.clone_map = []
-        self.dests = None
-
-        self.branch = branch
-        self.git_url = git_base_url
-        self.cache_dir = cache_dir
-        self.cache_no_hardlinks = cache_no_hardlinks
-        self.projects = projects
-        self.workspace = workspace
-        self.zuul_branch = zuul_branch or ''
-        self.zuul_ref = zuul_ref or ''
-        self.zuul_url = zuul_url
-        self.project_branches = project_branches or {}
-
-        if clone_map_file:
-            self.readCloneMap(clone_map_file)
-
-    def readCloneMap(self, clone_map_file):
-        clone_map_file = os.path.expanduser(clone_map_file)
-        if not os.path.exists(clone_map_file):
-            raise Exception("Unable to read clone map file at %s." %
-                            clone_map_file)
-        clone_map_file = open(clone_map_file)
-        self.clone_map = yaml.load(clone_map_file).get('clonemap')
-        self.log.info("Loaded map containing %s rules", len(self.clone_map))
-        return self.clone_map
-
-    def execute(self):
-        mapper = CloneMapper(self.clone_map, self.projects)
-        dests = mapper.expand(workspace=self.workspace)
-
-        self.log.info("Preparing %s repositories", len(dests))
-        for project, dest in dests.iteritems():
-            self.prepareRepo(project, dest)
-        self.log.info("Prepared all repositories")
-
-    def cloneUpstream(self, project, dest):
-        # Check for a cached git repo first
-        git_cache = '%s/%s' % (self.cache_dir, project)
-        git_cache_bare = '%s.git' % (git_cache)
-        git_upstream = '%s/%s' % (self.git_url, project)
-
-        repo_is_cloned = os.path.exists(os.path.join(dest, '.git'))
-
-        repo_cache = None
-        if (self.cache_dir and not repo_is_cloned):
-            if os.path.exists(git_cache_bare):
-                repo_cache = git_cache_bare
-            elif os.path.exists(git_cache):
-                repo_cache = git_cache
-
-            if repo_cache:
-                if self.cache_no_hardlinks:
-                    # file:// tells git not to hard-link across repos
-                    repo_cache = 'file://%s' % repo_cache
-
-                self.log.info("Creating repo %s from cache %s",
-                              project, repo_cache)
-                new_repo = git.Repo.clone_from(repo_cache, dest)
-                self.log.info("Updating origin remote in repo %s to %s",
-                              project, git_upstream)
-                new_repo.remotes.origin.config_writer.set('url', git_upstream)
-
-        if not repo_cache:
-            self.log.info("Creating repo %s from upstream %s",
-                          project, git_upstream)
-
-        repo = Repo(
-            remote=git_upstream,
-            local=dest,
-            email=None,
-            username=None)
-
-        if not repo.isInitialized():
-            raise Exception("Error cloning %s to %s" % (git_upstream, dest))
-
-        return repo
-
-    def fetchFromZuul(self, repo, project, ref):
-        zuul_remote = '%s/%s' % (self.zuul_url, project)
-
-        try:
-            repo.fetchFrom(zuul_remote, ref)
-            self.log.debug("Fetched ref %s from %s", ref, project)
-            return True
-        except ValueError:
-            self.log.debug("Project %s in Zuul does not have ref %s",
-                           project, ref)
-            return False
-        except GitCommandError as error:
-            # Bail out if fetch fails due to infrastructure reasons
-            if error.stderr.startswith('fatal: unable to access'):
-                raise
-            self.log.debug("Project %s in Zuul does not have ref %s",
-                           project, ref)
-            return False
-
-    def prepareRepo(self, project, dest):
-        """Clone a repository for project at dest and apply a reference
-        suitable for testing. The reference lookup is attempted in this order:
-
-         1) Zuul reference for the indicated branch
-         2) Zuul reference for the master branch
-         3) The tip of the indicated branch
-         4) The tip of the master branch
-
-        The "indicated branch" is one of the following:
-
-         A) The project-specific override branch (from project_branches arg)
-         B) The user specified branch (from the branch arg)
-         C) ZUUL_BRANCH (from the zuul_branch arg)
-        """
-
-        repo = self.cloneUpstream(project, dest)
-
-        # Ensure that we don't have stale remotes around
-        repo.prune()
-        # We must reset after pruning because reseting sets HEAD to point
-        # at refs/remotes/origin/master, but `git branch` which prune runs
-        # explodes if HEAD does not point at something in refs/heads.
-        # Later with repo.checkout() we set HEAD to something that
-        # `git branch` is happy with.
-        repo.reset()
-
-        indicated_branch = self.branch or self.zuul_branch
-        if project in self.project_branches:
-            indicated_branch = self.project_branches[project]
-
-        if indicated_branch:
-            override_zuul_ref = re.sub(self.zuul_branch, indicated_branch,
-                                       self.zuul_ref)
-        else:
-            override_zuul_ref = None
-
-        if indicated_branch and repo.hasBranch(indicated_branch):
-            self.log.info("upstream repo has branch %s", indicated_branch)
-            fallback_branch = indicated_branch
-        else:
-            self.log.info("upstream repo is missing branch %s",
-                          self.branch)
-            # FIXME should be origin HEAD branch which might not be 'master'
-            fallback_branch = 'master'
-
-        if self.zuul_branch:
-            fallback_zuul_ref = re.sub(self.zuul_branch, fallback_branch,
-                                       self.zuul_ref)
-        else:
-            fallback_zuul_ref = None
-
-        # If we have a non empty zuul_ref to use, use it. Otherwise we fall
-        # back to checking out the branch.
-        if ((override_zuul_ref and
-            self.fetchFromZuul(repo, project, override_zuul_ref)) or
-            (fallback_zuul_ref and
-             fallback_zuul_ref != override_zuul_ref and
-            self.fetchFromZuul(repo, project, fallback_zuul_ref))):
-            # Work around a bug in GitPython which can not parse FETCH_HEAD
-            gitcmd = git.Git(dest)
-            fetch_head = gitcmd.rev_parse('FETCH_HEAD')
-            repo.checkout(fetch_head)
-            self.log.info("Prepared %s repo with commit %s",
-                          project, fetch_head)
-        else:
-            # Checkout branch
-            self.log.info("Falling back to branch %s", fallback_branch)
-            try:
-                commit = repo.checkout('remotes/origin/%s' % fallback_branch)
-            except (ValueError, GitCommandError):
-                self.log.exception("Fallback branch not found: %s",
-                                   fallback_branch)
-            self.log.info("Prepared %s repo with branch %s at commit %s",
-                          project, fallback_branch, commit)
+# Copyright 2014 Antoine "hashar" Musso
+# Copyright 2014 Wikimedia Foundation Inc.
+#
+# Licensed under the Apache License, Version 2.0 (the "License"); you may
+# not use this file except in compliance with the License. You may obtain
+# a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+# License for the specific language governing permissions and limitations
+# under the License.
+
+import git
+import logging
+import os
+import re
+import yaml
+
+from git import GitCommandError
+from zuul.lib.clonemapper import CloneMapper
+from zuul.merger.merger import Repo
+
+
+class Cloner(object):
+    log = logging.getLogger("zuul.Cloner")
+
+    def __init__(self, git_base_url, projects, workspace, zuul_branch,
+                 zuul_ref, zuul_url, branch=None, clone_map_file=None,
+                 project_branches=None, cache_dir=None,
+                 cache_no_hardlinks=None):
+
+        self.clone_map = []
+        self.dests = None
+
+        self.branch = branch
+        self.git_url = git_base_url
+        self.cache_dir = cache_dir
+        self.cache_no_hardlinks = cache_no_hardlinks
+        self.projects = projects
+        self.workspace = workspace
+        self.zuul_branch = zuul_branch or ''
+        self.zuul_ref = zuul_ref or ''
+        self.zuul_url = zuul_url
+        self.project_branches = project_branches or {}
+
+        if clone_map_file:
+            self.readCloneMap(clone_map_file)
+
+    def readCloneMap(self, clone_map_file):
+        clone_map_file = os.path.expanduser(clone_map_file)
+        if not os.path.exists(clone_map_file):
+            raise Exception("Unable to read clone map file at %s." %
+                            clone_map_file)
+        clone_map_file = open(clone_map_file)
+        self.clone_map = yaml.load(clone_map_file).get('clonemap')
+        self.log.info("Loaded map containing %s rules", len(self.clone_map))
+        return self.clone_map
+
+    def execute(self):
+        mapper = CloneMapper(self.clone_map, self.projects)
+        dests = mapper.expand(workspace=self.workspace)
+
+        self.log.info("Preparing %s repositories", len(dests))
+        for project, dest in dests.iteritems():
+            self.prepareRepo(project, dest)
+        self.log.info("Prepared all repositories")
+
+    def cloneUpstream(self, project, dest):
+        # Check for a cached git repo first
+        git_cache = '%s/%s' % (self.cache_dir, project)
+        git_cache_bare = '%s.git' % (git_cache)
+        git_upstream = '%s/%s' % (self.git_url, project)
+
+        repo_is_cloned = os.path.exists(os.path.join(dest, '.git'))
+
+        repo_cache = None
+        if (self.cache_dir and not repo_is_cloned):
+            if os.path.exists(git_cache_bare):
+                repo_cache = git_cache_bare
+            elif os.path.exists(git_cache):
+                repo_cache = git_cache
+
+            if repo_cache:
+                if self.cache_no_hardlinks:
+                    # file:// tells git not to hard-link across repos
+                    repo_cache = 'file://%s' % repo_cache
+
+                self.log.info("Creating repo %s from cache %s",
+                              project, repo_cache)
+                new_repo = git.Repo.clone_from(repo_cache, dest)
+                self.log.info("Updating origin remote in repo %s to %s",
+                              project, git_upstream)
+                new_repo.remotes.origin.config_writer.set('url', git_upstream)
+
+        if not repo_cache:
+            self.log.info("Creating repo %s from upstream %s",
+                          project, git_upstream)
+
+        repo = Repo(
+            remote=git_upstream,
+            local=dest,
+            email=None,
+            username=None)
+
+        if not repo.isInitialized():
+            raise Exception("Error cloning %s to %s" % (git_upstream, dest))
+
+        return repo
+
+    def fetchFromZuul(self, repo, project, ref):
+        zuul_remote = '%s/%s' % (self.zuul_url, project)
+
+        try:
+            repo.fetchFrom(zuul_remote, ref)
+            self.log.debug("Fetched ref %s from %s", ref, project)
+            return True
+        except ValueError:
+            self.log.debug("Project %s in Zuul does not have ref %s",
+                           project, ref)
+            return False
+        except GitCommandError as error:
+            # Bail out if fetch fails due to infrastructure reasons
+            if error.stderr.startswith('fatal: unable to access'):
+                raise
+            self.log.debug("Project %s in Zuul does not have ref %s",
+                           project, ref)
+            return False
+
+    def prepareRepo(self, project, dest):
+        """Clone a repository for project at dest and apply a reference
+        suitable for testing. The reference lookup is attempted in this order:
+
+         1) Zuul reference for the indicated branch
+         2) Zuul reference for the master branch
+         3) The tip of the indicated branch
+         4) The tip of the master branch
+
+        The "indicated branch" is one of the following:
+
+         A) The project-specific override branch (from project_branches arg)
+         B) The user specified branch (from the branch arg)
+         C) ZUUL_BRANCH (from the zuul_branch arg)
+        """
+
+        repo = self.cloneUpstream(project, dest)
+
+        # Ensure that we don't have stale remotes around
+        repo.prune()
+        # We must reset after pruning because reseting sets HEAD to point
+        # at refs/remotes/origin/master, but `git branch` which prune runs
+        # explodes if HEAD does not point at something in refs/heads.
+        # Later with repo.checkout() we set HEAD to something that
+        # `git branch` is happy with.
+        repo.reset()
+
+        indicated_branch = self.branch or self.zuul_branch
+        if project in self.project_branches:
+            indicated_branch = self.project_branches[project]
+
+        if indicated_branch:
+            override_zuul_ref = re.sub(self.zuul_branch, indicated_branch,
+                                       self.zuul_ref)
+        else:
+            override_zuul_ref = None
+
+        if indicated_branch and repo.hasBranch(indicated_branch):
+            self.log.info("upstream repo has branch %s", indicated_branch)
+            fallback_branch = indicated_branch
+        else:
+            self.log.info("upstream repo is missing branch %s",
+                          self.branch)
+            # FIXME should be origin HEAD branch which might not be 'master'
+            fallback_branch = 'master'
+
+        if self.zuul_branch:
+            fallback_zuul_ref = re.sub(self.zuul_branch, fallback_branch,
+                                       self.zuul_ref)
+        else:
+            fallback_zuul_ref = None
+
+        # If we have a non empty zuul_ref to use, use it. Otherwise we fall
+        # back to checking out the branch.
+        if ((override_zuul_ref and
+            self.fetchFromZuul(repo, project, override_zuul_ref)) or
+            (fallback_zuul_ref and
+             fallback_zuul_ref != override_zuul_ref and
+            self.fetchFromZuul(repo, project, fallback_zuul_ref))):
+            # Work around a bug in GitPython which can not parse FETCH_HEAD
+            gitcmd = git.Git(dest)
+            fetch_head = gitcmd.rev_parse('FETCH_HEAD')
+            repo.checkout(fetch_head)
+            self.log.info("Prepared %s repo with commit %s",
+                          project, fetch_head)
+        else:
+            # Checkout branch
+            self.log.info("Falling back to branch %s", fallback_branch)
+            try:
+                commit = repo.checkout('remotes/origin/%s' % fallback_branch)
+            except (ValueError, GitCommandError):
+                self.log.exception("Fallback branch not found: %s",
+                                   fallback_branch)
+            self.log.info("Prepared %s repo with branch %s at commit %s",
+                          project, fallback_branch, commit)
diff --git a/zuul/lib/connections.py b/zuul/lib/connections.py
index 92ddb0f..cd2d7f8 100644
--- a/zuul/lib/connections.py
+++ b/zuul/lib/connections.py
@@ -1,66 +1,66 @@
-# Copyright 2015 Rackspace Australia
-#
-# Licensed under the Apache License, Version 2.0 (the "License"); you may
-# not use this file except in compliance with the License. You may obtain
-# a copy of the License at
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-# License for the specific language governing permissions and limitations
-# under the License.
-
-import re
-
-import zuul.connection.gerrit
-import zuul.connection.smtp
-
-
-def configure_connections(config):
-    # Register connections from the config
-
-    # TODO(jhesketh): import connection modules dynamically
-    connections = {}
-
-    for section_name in config.sections():
-        con_match = re.match(r'^connection ([\'\"]?)(.*)(\1)$',
-                             section_name, re.I)
-        if not con_match:
-            continue
-        con_name = con_match.group(2)
-        con_config = dict(config.items(section_name))
-
-        if 'driver' not in con_config:
-            raise Exception("No driver specified for connection %s."
-                            % con_name)
-
-        con_driver = con_config['driver']
-
-        # TODO(jhesketh): load the required class automatically
-        if con_driver == 'gerrit':
-            connections[con_name] = \
-                zuul.connection.gerrit.GerritConnection(con_name,
-                                                        con_config)
-        elif con_driver == 'smtp':
-            connections[con_name] = \
-                zuul.connection.smtp.SMTPConnection(con_name, con_config)
-        else:
-            raise Exception("Unknown driver, %s, for connection %s"
-                            % (con_config['driver'], con_name))
-
-    # If the [gerrit] or [smtp] sections still exist, load them in as a
-    # connection named 'gerrit' or 'smtp' respectfully
-
-    if 'gerrit' in config.sections():
-        connections['gerrit'] = \
-            zuul.connection.gerrit.GerritConnection(
-                'gerrit', dict(config.items('gerrit')))
-
-    if 'smtp' in config.sections():
-        connections['smtp'] = \
-            zuul.connection.smtp.SMTPConnection(
-                'smtp', dict(config.items('smtp')))
-
-    return connections
+# Copyright 2015 Rackspace Australia
+#
+# Licensed under the Apache License, Version 2.0 (the "License"); you may
+# not use this file except in compliance with the License. You may obtain
+# a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+# License for the specific language governing permissions and limitations
+# under the License.
+
+import re
+
+import zuul.connection.gerrit
+import zuul.connection.smtp
+
+
+def configure_connections(config):
+    # Register connections from the config
+
+    # TODO(jhesketh): import connection modules dynamically
+    connections = {}
+
+    for section_name in config.sections():
+        con_match = re.match(r'^connection ([\'\"]?)(.*)(\1)$',
+                             section_name, re.I)
+        if not con_match:
+            continue
+        con_name = con_match.group(2)
+        con_config = dict(config.items(section_name))
+
+        if 'driver' not in con_config:
+            raise Exception("No driver specified for connection %s."
+                            % con_name)
+
+        con_driver = con_config['driver']
+
+        # TODO(jhesketh): load the required class automatically
+        if con_driver == 'gerrit':
+            connections[con_name] = \
+                zuul.connection.gerrit.GerritConnection(con_name,
+                                                        con_config)
+        elif con_driver == 'smtp':
+            connections[con_name] = \
+                zuul.connection.smtp.SMTPConnection(con_name, con_config)
+        else:
+            raise Exception("Unknown driver, %s, for connection %s"
+                            % (con_config['driver'], con_name))
+
+    # If the [gerrit] or [smtp] sections still exist, load them in as a
+    # connection named 'gerrit' or 'smtp' respectfully
+
+    if 'gerrit' in config.sections():
+        connections['gerrit'] = \
+            zuul.connection.gerrit.GerritConnection(
+                'gerrit', dict(config.items('gerrit')))
+
+    if 'smtp' in config.sections():
+        connections['smtp'] = \
+            zuul.connection.smtp.SMTPConnection(
+                'smtp', dict(config.items('smtp')))
+
+    return connections
diff --git a/zuul/lib/swift.py b/zuul/lib/swift.py
index 3c411d3..0c34500 100644
--- a/zuul/lib/swift.py
+++ b/zuul/lib/swift.py
@@ -1,168 +1,168 @@
-# Copyright 2014 Rackspace Australia
-#
-# Licensed under the Apache License, Version 2.0 (the "License"); you may
-# not use this file except in compliance with the License. You may obtain
-# a copy of the License at
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-# License for the specific language governing permissions and limitations
-# under the License.
-
-import hmac
-from hashlib import sha1
-import logging
-from time import time
-import os
-import random
-import six
-import string
-import urlparse
-
-
-class Swift(object):
-    log = logging.getLogger("zuul.lib.swift")
-
-    def __init__(self, config):
-        self.config = config
-        self.connection = False
-        if self.config.has_option('swift', 'X-Account-Meta-Temp-Url-Key'):
-            self.secure_key = self.config.get('swift',
-                                              'X-Account-Meta-Temp-Url-Key')
-        else:
-            self.secure_key = ''.join(
-                random.choice(string.ascii_uppercase + string.digits)
-                for x in range(20)
-            )
-
-        self.storage_url = ''
-        if self.config.has_option('swift', 'X-Storage-Url'):
-            self.storage_url = self.config.get('swift', 'X-Storage-Url')
-
-        try:
-            if self.config.has_section('swift'):
-                if (not self.config.has_option('swift', 'Send-Temp-Url-Key')
-                    or self.config.getboolean('swift',
-                                              'Send-Temp-Url-Key')):
-                    self.connect()
-
-                    # Tell swift of our key
-                    headers = {}
-                    headers['X-Account-Meta-Temp-Url-Key'] = self.secure_key
-                    self.connection.post_account(headers)
-
-                if not self.config.has_option('swift', 'X-Storage-Url'):
-                    self.connect()
-                    self.storage_url = self.connection.get_auth()[0]
-        except Exception as e:
-            self.log.warning("Unable to set up swift. Signed storage URL is "
-                             "likely to be wrong. %s" % e)
-
-    def connect(self):
-        if not self.connection:
-            authurl = self.config.get('swift', 'authurl')
-
-            user = (self.config.get('swift', 'user')
-                    if self.config.has_option('swift', 'user') else None)
-            key = (self.config.get('swift', 'key')
-                   if self.config.has_option('swift', 'key') else None)
-            retries = (self.config.get('swift', 'retries')
-                       if self.config.has_option('swift', 'retries') else 5)
-            preauthurl = (self.config.get('swift', 'preauthurl')
-                          if self.config.has_option('swift', 'preauthurl')
-                          else None)
-            preauthtoken = (self.config.get('swift', 'preauthtoken')
-                            if self.config.has_option('swift', 'preauthtoken')
-                            else None)
-            snet = (self.config.get('swift', 'snet')
-                    if self.config.has_option('swift', 'snet') else False)
-            starting_backoff = (self.config.get('swift', 'starting_backoff')
-                                if self.config.has_option('swift',
-                                                          'starting_backoff')
-                                else 1)
-            max_backoff = (self.config.get('swift', 'max_backoff')
-                           if self.config.has_option('swift', 'max_backoff')
-                           else 64)
-            tenant_name = (self.config.get('swift', 'tenant_name')
-                           if self.config.has_option('swift', 'tenant_name')
-                           else None)
-            auth_version = (self.config.get('swift', 'auth_version')
-                            if self.config.has_option('swift', 'auth_version')
-                            else 2.0)
-            cacert = (self.config.get('swift', 'cacert')
-                      if self.config.has_option('swift', 'cacert') else None)
-            insecure = (self.config.get('swift', 'insecure')
-                        if self.config.has_option('swift', 'insecure')
-                        else False)
-            ssl_compression = (self.config.get('swift', 'ssl_compression')
-                               if self.config.has_option('swift',
-                                                         'ssl_compression')
-                               else True)
-
-            available_os_options = ['tenant_id', 'auth_token', 'service_type',
-                                    'endpoint_type', 'tenant_name',
-                                    'object_storage_url', 'region_name']
-
-            os_options = {}
-            for os_option in available_os_options:
-                if self.config.has_option('swift', os_option):
-                    os_options[os_option] = self.config.get('swift', os_option)
-
-            import swiftclient
-            self.connection = swiftclient.client.Connection(
-                authurl=authurl, user=user, key=key, retries=retries,
-                preauthurl=preauthurl, preauthtoken=preauthtoken, snet=snet,
-                starting_backoff=starting_backoff, max_backoff=max_backoff,
-                tenant_name=tenant_name, os_options=os_options,
-                auth_version=auth_version, cacert=cacert, insecure=insecure,
-                ssl_compression=ssl_compression)
-
-    def generate_form_post_middleware_params(self, destination_prefix='',
-                                             **kwargs):
-        """Generate the FormPost middleware params for the given settings"""
-
-        # Define the available settings and their defaults
-        settings = {
-            'container': '',
-            'expiry': 7200,
-            'max_file_size': 104857600,
-            'max_file_count': 10,
-            'file_path_prefix': ''
-        }
-
-        for key, default in six.iteritems(settings):
-            # TODO(jeblair): Remove the following two lines after a
-            # deprecation period for the underscore variants of the
-            # settings in YAML.
-            if key in kwargs:
-                settings[key] = kwargs[key]
-            # Since we prefer '-' rather than '_' in YAML, look up
-            # keys there using hyphens.  Continue to use underscores
-            # everywhere else.
-            altkey = key.replace('_', '-')
-            if altkey in kwargs:
-                settings[key] = kwargs[altkey]
-            elif self.config.has_option('swift', 'default_' + key):
-                settings[key] = self.config.get('swift', 'default_' + key)
-            # TODO: these are always strings; some should be converted
-            # to ints.
-
-        expires = int(time() + int(settings['expiry']))
-        redirect = ''
-
-        url = os.path.join(self.storage_url, settings['container'],
-                           settings['file_path_prefix'],
-                           destination_prefix)
-        u = urlparse.urlparse(url)
-
-        hmac_body = '%s\n%s\n%s\n%s\n%s' % (u.path, redirect,
-                                            settings['max_file_size'],
-                                            settings['max_file_count'],
-                                            expires)
-
-        signature = hmac.new(self.secure_key, hmac_body, sha1).hexdigest()
-
-        return url, hmac_body, signature
+# Copyright 2014 Rackspace Australia
+#
+# Licensed under the Apache License, Version 2.0 (the "License"); you may
+# not use this file except in compliance with the License. You may obtain
+# a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+# License for the specific language governing permissions and limitations
+# under the License.
+
+import hmac
+from hashlib import sha1
+import logging
+from time import time
+import os
+import random
+import six
+import string
+import urlparse
+
+
+class Swift(object):
+    log = logging.getLogger("zuul.lib.swift")
+
+    def __init__(self, config):
+        self.config = config
+        self.connection = False
+        if self.config.has_option('swift', 'X-Account-Meta-Temp-Url-Key'):
+            self.secure_key = self.config.get('swift',
+                                              'X-Account-Meta-Temp-Url-Key')
+        else:
+            self.secure_key = ''.join(
+                random.choice(string.ascii_uppercase + string.digits)
+                for x in range(20)
+            )
+
+        self.storage_url = ''
+        if self.config.has_option('swift', 'X-Storage-Url'):
+            self.storage_url = self.config.get('swift', 'X-Storage-Url')
+
+        try:
+            if self.config.has_section('swift'):
+                if (not self.config.has_option('swift', 'Send-Temp-Url-Key')
+                    or self.config.getboolean('swift',
+                                              'Send-Temp-Url-Key')):
+                    self.connect()
+
+                    # Tell swift of our key
+                    headers = {}
+                    headers['X-Account-Meta-Temp-Url-Key'] = self.secure_key
+                    self.connection.post_account(headers)
+
+                if not self.config.has_option('swift', 'X-Storage-Url'):
+                    self.connect()
+                    self.storage_url = self.connection.get_auth()[0]
+        except Exception as e:
+            self.log.warning("Unable to set up swift. Signed storage URL is "
+                             "likely to be wrong. %s" % e)
+
+    def connect(self):
+        if not self.connection:
+            authurl = self.config.get('swift', 'authurl')
+
+            user = (self.config.get('swift', 'user')
+                    if self.config.has_option('swift', 'user') else None)
+            key = (self.config.get('swift', 'key')
+                   if self.config.has_option('swift', 'key') else None)
+            retries = (self.config.get('swift', 'retries')
+                       if self.config.has_option('swift', 'retries') else 5)
+            preauthurl = (self.config.get('swift', 'preauthurl')
+                          if self.config.has_option('swift', 'preauthurl')
+                          else None)
+            preauthtoken = (self.config.get('swift', 'preauthtoken')
+                            if self.config.has_option('swift', 'preauthtoken')
+                            else None)
+            snet = (self.config.get('swift', 'snet')
+                    if self.config.has_option('swift', 'snet') else False)
+            starting_backoff = (self.config.get('swift', 'starting_backoff')
+                                if self.config.has_option('swift',
+                                                          'starting_backoff')
+                                else 1)
+            max_backoff = (self.config.get('swift', 'max_backoff')
+                           if self.config.has_option('swift', 'max_backoff')
+                           else 64)
+            tenant_name = (self.config.get('swift', 'tenant_name')
+                           if self.config.has_option('swift', 'tenant_name')
+                           else None)
+            auth_version = (self.config.get('swift', 'auth_version')
+                            if self.config.has_option('swift', 'auth_version')
+                            else 2.0)
+            cacert = (self.config.get('swift', 'cacert')
+                      if self.config.has_option('swift', 'cacert') else None)
+            insecure = (self.config.get('swift', 'insecure')
+                        if self.config.has_option('swift', 'insecure')
+                        else False)
+            ssl_compression = (self.config.get('swift', 'ssl_compression')
+                               if self.config.has_option('swift',
+                                                         'ssl_compression')
+                               else True)
+
+            available_os_options = ['tenant_id', 'auth_token', 'service_type',
+                                    'endpoint_type', 'tenant_name',
+                                    'object_storage_url', 'region_name']
+
+            os_options = {}
+            for os_option in available_os_options:
+                if self.config.has_option('swift', os_option):
+                    os_options[os_option] = self.config.get('swift', os_option)
+
+            import swiftclient
+            self.connection = swiftclient.client.Connection(
+                authurl=authurl, user=user, key=key, retries=retries,
+                preauthurl=preauthurl, preauthtoken=preauthtoken, snet=snet,
+                starting_backoff=starting_backoff, max_backoff=max_backoff,
+                tenant_name=tenant_name, os_options=os_options,
+                auth_version=auth_version, cacert=cacert, insecure=insecure,
+                ssl_compression=ssl_compression)
+
+    def generate_form_post_middleware_params(self, destination_prefix='',
+                                             **kwargs):
+        """Generate the FormPost middleware params for the given settings"""
+
+        # Define the available settings and their defaults
+        settings = {
+            'container': '',
+            'expiry': 7200,
+            'max_file_size': 104857600,
+            'max_file_count': 10,
+            'file_path_prefix': ''
+        }
+
+        for key, default in six.iteritems(settings):
+            # TODO(jeblair): Remove the following two lines after a
+            # deprecation period for the underscore variants of the
+            # settings in YAML.
+            if key in kwargs:
+                settings[key] = kwargs[key]
+            # Since we prefer '-' rather than '_' in YAML, look up
+            # keys there using hyphens.  Continue to use underscores
+            # everywhere else.
+            altkey = key.replace('_', '-')
+            if altkey in kwargs:
+                settings[key] = kwargs[altkey]
+            elif self.config.has_option('swift', 'default_' + key):
+                settings[key] = self.config.get('swift', 'default_' + key)
+            # TODO: these are always strings; some should be converted
+            # to ints.
+
+        expires = int(time() + int(settings['expiry']))
+        redirect = ''
+
+        url = os.path.join(self.storage_url, settings['container'],
+                           settings['file_path_prefix'],
+                           destination_prefix)
+        u = urlparse.urlparse(url)
+
+        hmac_body = '%s\n%s\n%s\n%s\n%s' % (u.path, redirect,
+                                            settings['max_file_size'],
+                                            settings['max_file_count'],
+                                            expires)
+
+        signature = hmac.new(self.secure_key, hmac_body, sha1).hexdigest()
+
+        return url, hmac_body, signature
diff --git a/zuul/merger/client.py b/zuul/merger/client.py
index 950c385..5daa8df 100644
--- a/zuul/merger/client.py
+++ b/zuul/merger/client.py
@@ -1,123 +1,123 @@
-# Copyright 2014 OpenStack Foundation
-#
-# Licensed under the Apache License, Version 2.0 (the "License"); you may
-# not use this file except in compliance with the License. You may obtain
-# a copy of the License at
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-# License for the specific language governing permissions and limitations
-# under the License.
-
-import json
-import logging
-from uuid import uuid4
-
-import gear
-
-import zuul.model
-
-
-def getJobData(job):
-    if not len(job.data):
-        return {}
-    d = job.data[-1]
-    if not d:
-        return {}
-    return json.loads(d)
-
-
-class MergeGearmanClient(gear.Client):
-    def __init__(self, merge_client):
-        super(MergeGearmanClient, self).__init__()
-        self.__merge_client = merge_client
-
-    def handleWorkComplete(self, packet):
-        job = super(MergeGearmanClient, self).handleWorkComplete(packet)
-        self.__merge_client.onBuildCompleted(job)
-        return job
-
-    def handleWorkFail(self, packet):
-        job = super(MergeGearmanClient, self).handleWorkFail(packet)
-        self.__merge_client.onBuildCompleted(job)
-        return job
-
-    def handleWorkException(self, packet):
-        job = super(MergeGearmanClient, self).handleWorkException(packet)
-        self.__merge_client.onBuildCompleted(job)
-        return job
-
-    def handleDisconnect(self, job):
-        job = super(MergeGearmanClient, self).handleDisconnect(job)
-        self.__merge_client.onBuildCompleted(job)
-
-
-class MergeClient(object):
-    log = logging.getLogger("zuul.MergeClient")
-
-    def __init__(self, config, sched):
-        self.config = config
-        self.sched = sched
-        server = self.config.get('gearman', 'server')
-        if self.config.has_option('gearman', 'port'):
-            port = self.config.get('gearman', 'port')
-        else:
-            port = 4730
-        self.log.debug("Connecting to gearman at %s:%s" % (server, port))
-        self.gearman = MergeGearmanClient(self)
-        self.gearman.addServer(server, port)
-        self.log.debug("Waiting for gearman")
-        self.gearman.waitForServer()
-        self.build_sets = {}
-
-    def stop(self):
-        self.gearman.shutdown()
-
-    def areMergesOutstanding(self):
-        if self.build_sets:
-            return True
-        return False
-
-    def submitJob(self, name, data, build_set,
-                  precedence=zuul.model.PRECEDENCE_NORMAL):
-        uuid = str(uuid4().hex)
-        job = gear.Job(name,
-                       json.dumps(data),
-                       unique=uuid)
-        self.log.debug("Submitting job %s with data %s" % (job, data))
-        self.build_sets[uuid] = build_set
-        self.gearman.submitJob(job, precedence=precedence,
-                               timeout=300)
-
-    def mergeChanges(self, items, build_set,
-                     precedence=zuul.model.PRECEDENCE_NORMAL):
-        data = dict(items=items)
-        self.submitJob('merger:merge', data, build_set, precedence)
-
-    def updateRepo(self, project, url, build_set,
-                   precedence=zuul.model.PRECEDENCE_NORMAL):
-        data = dict(project=project,
-                    url=url)
-        self.submitJob('merger:update', data, build_set, precedence)
-
-    def onBuildCompleted(self, job):
-        build_set = self.build_sets.get(job.unique)
-        if build_set:
-            data = getJobData(job)
-            zuul_url = data.get('zuul_url')
-            merged = data.get('merged', False)
-            updated = data.get('updated', False)
-            commit = data.get('commit')
-            self.log.info("Merge %s complete, merged: %s, updated: %s, "
-                          "commit: %s" %
-                          (job, merged, updated, build_set.commit))
-            self.sched.onMergeCompleted(build_set, zuul_url,
-                                        merged, updated, commit)
-            # The test suite expects the build_set to be removed from
-            # the internal dict after the wake flag is set.
-            del self.build_sets[job.unique]
-        else:
-            self.log.error("Unable to find build set for uuid %s" % job.unique)
+# Copyright 2014 OpenStack Foundation
+#
+# Licensed under the Apache License, Version 2.0 (the "License"); you may
+# not use this file except in compliance with the License. You may obtain
+# a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+# License for the specific language governing permissions and limitations
+# under the License.
+
+import json
+import logging
+from uuid import uuid4
+
+import gear
+
+import zuul.model
+
+
+def getJobData(job):
+    if not len(job.data):
+        return {}
+    d = job.data[-1]
+    if not d:
+        return {}
+    return json.loads(d)
+
+
+class MergeGearmanClient(gear.Client):
+    def __init__(self, merge_client):
+        super(MergeGearmanClient, self).__init__()
+        self.__merge_client = merge_client
+
+    def handleWorkComplete(self, packet):
+        job = super(MergeGearmanClient, self).handleWorkComplete(packet)
+        self.__merge_client.onBuildCompleted(job)
+        return job
+
+    def handleWorkFail(self, packet):
+        job = super(MergeGearmanClient, self).handleWorkFail(packet)
+        self.__merge_client.onBuildCompleted(job)
+        return job
+
+    def handleWorkException(self, packet):
+        job = super(MergeGearmanClient, self).handleWorkException(packet)
+        self.__merge_client.onBuildCompleted(job)
+        return job
+
+    def handleDisconnect(self, job):
+        job = super(MergeGearmanClient, self).handleDisconnect(job)
+        self.__merge_client.onBuildCompleted(job)
+
+
+class MergeClient(object):
+    log = logging.getLogger("zuul.MergeClient")
+
+    def __init__(self, config, sched):
+        self.config = config
+        self.sched = sched
+        server = self.config.get('gearman', 'server')
+        if self.config.has_option('gearman', 'port'):
+            port = self.config.get('gearman', 'port')
+        else:
+            port = 4730
+        self.log.debug("Connecting to gearman at %s:%s" % (server, port))
+        self.gearman = MergeGearmanClient(self)
+        self.gearman.addServer(server, port)
+        self.log.debug("Waiting for gearman")
+        self.gearman.waitForServer()
+        self.build_sets = {}
+
+    def stop(self):
+        self.gearman.shutdown()
+
+    def areMergesOutstanding(self):
+        if self.build_sets:
+            return True
+        return False
+
+    def submitJob(self, name, data, build_set,
+                  precedence=zuul.model.PRECEDENCE_NORMAL):
+        uuid = str(uuid4().hex)
+        job = gear.Job(name,
+                       json.dumps(data),
+                       unique=uuid)
+        self.log.debug("Submitting job %s with data %s" % (job, data))
+        self.build_sets[uuid] = build_set
+        self.gearman.submitJob(job, precedence=precedence,
+                               timeout=300)
+
+    def mergeChanges(self, items, build_set,
+                     precedence=zuul.model.PRECEDENCE_NORMAL):
+        data = dict(items=items)
+        self.submitJob('merger:merge', data, build_set, precedence)
+
+    def updateRepo(self, project, url, build_set,
+                   precedence=zuul.model.PRECEDENCE_NORMAL):
+        data = dict(project=project,
+                    url=url)
+        self.submitJob('merger:update', data, build_set, precedence)
+
+    def onBuildCompleted(self, job):
+        build_set = self.build_sets.get(job.unique)
+        if build_set:
+            data = getJobData(job)
+            zuul_url = data.get('zuul_url')
+            merged = data.get('merged', False)
+            updated = data.get('updated', False)
+            commit = data.get('commit')
+            self.log.info("Merge %s complete, merged: %s, updated: %s, "
+                          "commit: %s" %
+                          (job, merged, updated, build_set.commit))
+            self.sched.onMergeCompleted(build_set, zuul_url,
+                                        merged, updated, commit)
+            # The test suite expects the build_set to be removed from
+            # the internal dict after the wake flag is set.
+            del self.build_sets[job.unique]
+        else:
+            self.log.error("Unable to find build set for uuid %s" % job.unique)
diff --git a/zuul/merger/merger.py b/zuul/merger/merger.py
index fed8394..fa378cf 100644
--- a/zuul/merger/merger.py
+++ b/zuul/merger/merger.py
@@ -1,355 +1,355 @@
-# Copyright 2012 Hewlett-Packard Development Company, L.P.
-# Copyright 2013-2014 OpenStack Foundation
-#
-# Licensed under the Apache License, Version 2.0 (the "License"); you may
-# not use this file except in compliance with the License. You may obtain
-# a copy of the License at
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-# License for the specific language governing permissions and limitations
-# under the License.
-
-import git
-import os
-import logging
-
-import zuul.model
-
-
-def reset_repo_to_head(repo):
-    # This lets us reset the repo even if there is a file in the root
-    # directory named 'HEAD'.  Currently, GitPython does not allow us
-    # to instruct it to always include the '--' to disambiguate.  This
-    # should no longer be necessary if this PR merges:
-    #   https://github.com/gitpython-developers/GitPython/pull/319
-    try:
-        repo.git.reset('--hard', 'HEAD', '--')
-    except git.GitCommandError as e:
-        # git nowadays may use 1 as status to indicate there are still unstaged
-        # modifications after the reset
-        if e.status != 1:
-            raise
-
-
-class ZuulReference(git.Reference):
-    _common_path_default = "refs/zuul"
-    _points_to_commits_only = True
-
-
-class Repo(object):
-    log = logging.getLogger("zuul.Repo")
-
-    def __init__(self, remote, local, email, username):
-        self.remote_url = remote
-        self.local_path = local
-        self.email = email
-        self.username = username
-        self._initialized = False
-        try:
-            self._ensure_cloned()
-        except:
-            self.log.exception("Unable to initialize repo for %s" % remote)
-
-    def _ensure_cloned(self):
-        repo_is_cloned = os.path.exists(os.path.join(self.local_path, '.git'))
-        if self._initialized and repo_is_cloned:
-            return
-        # If the repo does not exist, clone the repo.
-        if not repo_is_cloned:
-            self.log.debug("Cloning from %s to %s" % (self.remote_url,
-                                                      self.local_path))
-            git.Repo.clone_from(self.remote_url, self.local_path)
-        repo = git.Repo(self.local_path)
-        if self.email:
-            repo.config_writer().set_value('user', 'email',
-                                           self.email)
-        if self.username:
-            repo.config_writer().set_value('user', 'name',
-                                           self.username)
-        config_writer = repo.config_writer()
-        try:
-            # GitConfigParser.write() acquires a lock but does not release it.
-            # The lock is released in the object's __del__ method, which is
-            # invoked when the object is about to be dereferenced. This is not
-            # a reliable means of ensuring the lock is released, because it can
-            # break if there is a circular reference keeping the object alive,
-            # or if another GitConfigParser object for the same repository is
-            # initiated while a reference to the existing one is still held.
-            config_writer.write()
-        finally:
-            config_writer._lock._release_lock()
-        self._initialized = True
-
-    def isInitialized(self):
-        return self._initialized
-
-    def createRepoObject(self):
-        try:
-            self._ensure_cloned()
-            repo = git.Repo(self.local_path)
-        except:
-            self.log.exception("Unable to initialize repo for %s" %
-                               self.local_path)
-        return repo
-
-    def reset(self):
-        self.log.debug("Resetting repository %s" % self.local_path)
-        self.update()
-        repo = self.createRepoObject()
-        origin = repo.remotes.origin
-        for ref in origin.refs:
-            if ref.remote_head == 'HEAD':
-                continue
-            repo.create_head(ref.remote_head, ref, force=True)
-
-        # Reset to remote HEAD (usually origin/master)
-        repo.head.reference = origin.refs['HEAD']
-        reset_repo_to_head(repo)
-        repo.git.clean('-x', '-f', '-d')
-
-    def prune(self):
-        repo = self.createRepoObject()
-        origin = repo.remotes.origin
-        stale_refs = origin.stale_refs
-        if stale_refs:
-            self.log.debug("Pruning stale refs: %s", stale_refs)
-            git.refs.RemoteReference.delete(repo, *stale_refs)
-
-    def getBranchHead(self, branch):
-        repo = self.createRepoObject()
-        branch_head = repo.heads[branch]
-        return branch_head.commit
-
-    def hasBranch(self, branch):
-        repo = self.createRepoObject()
-        origin = repo.remotes.origin
-        return branch in origin.refs
-
-    def getCommitFromRef(self, refname):
-        repo = self.createRepoObject()
-        if refname not in repo.refs:
-            return None
-        ref = repo.refs[refname]
-        return ref.commit
-
-    def checkout(self, ref):
-        repo = self.createRepoObject()
-        self.log.debug("Checking out %s" % ref)
-        repo.head.reference = ref
-        reset_repo_to_head(repo)
-        return repo.head.commit
-
-    def cherryPick(self, ref):
-        repo = self.createRepoObject()
-        self.log.debug("Cherry-picking %s" % ref)
-        self.fetch(ref)
-        repo.git.cherry_pick("FETCH_HEAD")
-        return repo.head.commit
-
-    def merge(self, ref, strategy=None):
-        repo = self.createRepoObject()
-        args = []
-        if strategy:
-            args += ['-s', strategy]
-        args.append('FETCH_HEAD')
-        self.fetch(ref)
-        self.log.debug("Merging %s with args %s" % (ref, args))
-        repo.git.merge(*args)
-        return repo.head.commit
-
-    def fetch(self, ref):
-        repo = self.createRepoObject()
-        # The git.remote.fetch method may read in git progress info and
-        # interpret it improperly causing an AssertionError. Because the
-        # data was fetched properly subsequent fetches don't seem to fail.
-        # So try again if an AssertionError is caught.
-        origin = repo.remotes.origin
-        try:
-            origin.fetch(ref)
-        except AssertionError:
-            origin.fetch(ref)
-
-    def fetchFrom(self, repository, refspec):
-        repo = self.createRepoObject()
-        repo.git.fetch(repository, refspec)
-
-    def createZuulRef(self, ref, commit='HEAD'):
-        repo = self.createRepoObject()
-        self.log.debug("CreateZuulRef %s at %s on %s" % (ref, commit, repo))
-        ref = ZuulReference.create(repo, ref, commit)
-        return ref.commit
-
-    def push(self, local, remote):
-        repo = self.createRepoObject()
-        self.log.debug("Pushing %s:%s to %s" % (local, remote,
-                                                self.remote_url))
-        repo.remotes.origin.push('%s:%s' % (local, remote))
-
-    def update(self):
-        repo = self.createRepoObject()
-        self.log.debug("Updating repository %s" % self.local_path)
-        origin = repo.remotes.origin
-        origin.update()
-
-
-class Merger(object):
-    log = logging.getLogger("zuul.Merger")
-
-    def __init__(self, working_root, connections, email, username):
-        self.repos = {}
-        self.working_root = working_root
-        if not os.path.exists(working_root):
-            os.makedirs(working_root)
-        self._makeSSHWrappers(working_root, connections)
-        self.email = email
-        self.username = username
-
-    def _makeSSHWrappers(self, working_root, connections):
-        for connection_name, connection in connections.items():
-            sshkey = connection.connection_config.get('sshkey')
-            if sshkey:
-                self._makeSSHWrapper(sshkey, working_root, connection_name)
-
-    def _makeSSHWrapper(self, key, merge_root, connection_name='default'):
-        wrapper_name = '.ssh_wrapper_%s' % connection_name
-        name = os.path.join(merge_root, wrapper_name)
-        fd = open(name, 'w')
-        fd.write('#!/bin/bash\n')
-        fd.write('ssh -i %s $@\n' % key)
-        fd.close()
-        os.chmod(name, 0755)
-
-    def addProject(self, project, url):
-        repo = None
-        try:
-            path = os.path.join(self.working_root, project)
-            repo = Repo(url, path, self.email, self.username)
-
-            self.repos[project] = repo
-        except Exception:
-            self.log.exception("Unable to add project %s" % project)
-        return repo
-
-    def getRepo(self, project, url):
-        if project in self.repos:
-            return self.repos[project]
-        if not url:
-            raise Exception("Unable to set up repo for project %s"
-                            " without a url" % (project,))
-        return self.addProject(project, url)
-
-    def updateRepo(self, project, url):
-        repo = self.getRepo(project, url)
-        try:
-            self.log.info("Updating local repository %s", project)
-            repo.update()
-        except Exception:
-            self.log.exception("Unable to update %s", project)
-
-    def _mergeChange(self, item, ref):
-        repo = self.getRepo(item['project'], item['url'])
-        try:
-            repo.checkout(ref)
-        except Exception:
-            self.log.exception("Unable to checkout %s" % ref)
-            return None
-
-        try:
-            mode = item['merge_mode']
-            if mode == zuul.model.MERGER_MERGE:
-                commit = repo.merge(item['refspec'])
-            elif mode == zuul.model.MERGER_MERGE_RESOLVE:
-                commit = repo.merge(item['refspec'], 'resolve')
-            elif mode == zuul.model.MERGER_CHERRY_PICK:
-                commit = repo.cherryPick(item['refspec'])
-            else:
-                raise Exception("Unsupported merge mode: %s" % mode)
-        except git.GitCommandError:
-            # Log git exceptions at debug level because they are
-            # usually benign merge conflicts
-            self.log.debug("Unable to merge %s" % item, exc_info=True)
-            return None
-        except Exception:
-            self.log.exception("Exception while merging a change:")
-            return None
-
-        return commit
-
-    def _setGitSsh(self, connection_name):
-        wrapper_name = '.ssh_wrapper_%s' % connection_name
-        name = os.path.join(self.working_root, wrapper_name)
-        if os.path.isfile(name):
-            os.environ['GIT_SSH'] = name
-        elif 'GIT_SSH' in os.environ:
-            del os.environ['GIT_SSH']
-
-    def _mergeItem(self, item, recent):
-        self.log.debug("Processing refspec %s for project %s / %s ref %s" %
-                       (item['refspec'], item['project'], item['branch'],
-                        item['ref']))
-        self._setGitSsh(item['connection_name'])
-        repo = self.getRepo(item['project'], item['url'])
-        key = (item['project'], item['branch'])
-        # See if we have a commit for this change already in this repo
-        zuul_ref = item['branch'] + '/' + item['ref']
-        commit = repo.getCommitFromRef(zuul_ref)
-        if commit:
-            self.log.debug("Found commit %s for ref %s" % (commit, zuul_ref))
-            # Store this as the most recent commit for this
-            # project-branch
-            recent[key] = commit
-            return commit
-        self.log.debug("Unable to find commit for ref %s" % (zuul_ref,))
-        # We need to merge the change
-        # Get the most recent commit for this project-branch
-        base = recent.get(key)
-        if not base:
-            # There is none, so use the branch tip
-            # we need to reset here in order to call getBranchHead
-            self.log.debug("No base commit found for %s" % (key,))
-            try:
-                repo.reset()
-            except Exception:
-                self.log.exception("Unable to reset repo %s" % repo)
-                return None
-            base = repo.getBranchHead(item['branch'])
-        else:
-            self.log.debug("Found base commit %s for %s" % (base, key,))
-        # Merge the change
-        commit = self._mergeChange(item, base)
-        if not commit:
-            return None
-        # Store this commit as the most recent for this project-branch
-        recent[key] = commit
-        # Set the Zuul ref for this item to point to the most recent
-        # commits of each project-branch
-        for key, mrc in recent.items():
-            project, branch = key
-            try:
-                repo = self.getRepo(project, None)
-                zuul_ref = branch + '/' + item['ref']
-                repo.createZuulRef(zuul_ref, mrc)
-            except Exception:
-                self.log.exception("Unable to set zuul ref %s for "
-                                   "item %s" % (zuul_ref, item))
-                return None
-        return commit
-
-    def mergeChanges(self, items):
-        recent = {}
-        commit = None
-        for item in items:
-            if item.get("number") and item.get("patchset"):
-                self.log.debug("Merging for change %s,%s." %
-                               (item["number"], item["patchset"]))
-            elif item.get("newrev") and item.get("oldrev"):
-                self.log.debug("Merging for rev %s with oldrev %s." %
-                               (item["newrev"], item["oldrev"]))
-            commit = self._mergeItem(item, recent)
-            if not commit:
-                return None
-        return commit.hexsha
+# Copyright 2012 Hewlett-Packard Development Company, L.P.
+# Copyright 2013-2014 OpenStack Foundation
+#
+# Licensed under the Apache License, Version 2.0 (the "License"); you may
+# not use this file except in compliance with the License. You may obtain
+# a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+# License for the specific language governing permissions and limitations
+# under the License.
+
+import git
+import os
+import logging
+
+import zuul.model
+
+
+def reset_repo_to_head(repo):
+    # This lets us reset the repo even if there is a file in the root
+    # directory named 'HEAD'.  Currently, GitPython does not allow us
+    # to instruct it to always include the '--' to disambiguate.  This
+    # should no longer be necessary if this PR merges:
+    #   https://github.com/gitpython-developers/GitPython/pull/319
+    try:
+        repo.git.reset('--hard', 'HEAD', '--')
+    except git.GitCommandError as e:
+        # git nowadays may use 1 as status to indicate there are still unstaged
+        # modifications after the reset
+        if e.status != 1:
+            raise
+
+
+class ZuulReference(git.Reference):
+    _common_path_default = "refs/zuul"
+    _points_to_commits_only = True
+
+
+class Repo(object):
+    log = logging.getLogger("zuul.Repo")
+
+    def __init__(self, remote, local, email, username):
+        self.remote_url = remote
+        self.local_path = local
+        self.email = email
+        self.username = username
+        self._initialized = False
+        try:
+            self._ensure_cloned()
+        except:
+            self.log.exception("Unable to initialize repo for %s" % remote)
+
+    def _ensure_cloned(self):
+        repo_is_cloned = os.path.exists(os.path.join(self.local_path, '.git'))
+        if self._initialized and repo_is_cloned:
+            return
+        # If the repo does not exist, clone the repo.
+        if not repo_is_cloned:
+            self.log.debug("Cloning from %s to %s" % (self.remote_url,
+                                                      self.local_path))
+            git.Repo.clone_from(self.remote_url, self.local_path)
+        repo = git.Repo(self.local_path)
+        if self.email:
+            repo.config_writer().set_value('user', 'email',
+                                           self.email)
+        if self.username:
+            repo.config_writer().set_value('user', 'name',
+                                           self.username)
+        config_writer = repo.config_writer()
+        try:
+            # GitConfigParser.write() acquires a lock but does not release it.
+            # The lock is released in the object's __del__ method, which is
+            # invoked when the object is about to be dereferenced. This is not
+            # a reliable means of ensuring the lock is released, because it can
+            # break if there is a circular reference keeping the object alive,
+            # or if another GitConfigParser object for the same repository is
+            # initiated while a reference to the existing one is still held.
+            config_writer.write()
+        finally:
+            config_writer._lock._release_lock()
+        self._initialized = True
+
+    def isInitialized(self):
+        return self._initialized
+
+    def createRepoObject(self):
+        try:
+            self._ensure_cloned()
+            repo = git.Repo(self.local_path)
+        except:
+            self.log.exception("Unable to initialize repo for %s" %
+                               self.local_path)
+        return repo
+
+    def reset(self):
+        self.log.debug("Resetting repository %s" % self.local_path)
+        self.update()
+        repo = self.createRepoObject()
+        origin = repo.remotes.origin
+        for ref in origin.refs:
+            if ref.remote_head == 'HEAD':
+                continue
+            repo.create_head(ref.remote_head, ref, force=True)
+
+        # Reset to remote HEAD (usually origin/master)
+        repo.head.reference = origin.refs['HEAD']
+        reset_repo_to_head(repo)
+        repo.git.clean('-x', '-f', '-d')
+
+    def prune(self):
+        repo = self.createRepoObject()
+        origin = repo.remotes.origin
+        stale_refs = origin.stale_refs
+        if stale_refs:
+            self.log.debug("Pruning stale refs: %s", stale_refs)
+            git.refs.RemoteReference.delete(repo, *stale_refs)
+
+    def getBranchHead(self, branch):
+        repo = self.createRepoObject()
+        branch_head = repo.heads[branch]
+        return branch_head.commit
+
+    def hasBranch(self, branch):
+        repo = self.createRepoObject()
+        origin = repo.remotes.origin
+        return branch in origin.refs
+
+    def getCommitFromRef(self, refname):
+        repo = self.createRepoObject()
+        if refname not in repo.refs:
+            return None
+        ref = repo.refs[refname]
+        return ref.commit
+
+    def checkout(self, ref):
+        repo = self.createRepoObject()
+        self.log.debug("Checking out %s" % ref)
+        repo.head.reference = ref
+        reset_repo_to_head(repo)
+        return repo.head.commit
+
+    def cherryPick(self, ref):
+        repo = self.createRepoObject()
+        self.log.debug("Cherry-picking %s" % ref)
+        self.fetch(ref)
+        repo.git.cherry_pick("FETCH_HEAD")
+        return repo.head.commit
+
+    def merge(self, ref, strategy=None):
+        repo = self.createRepoObject()
+        args = []
+        if strategy:
+            args += ['-s', strategy]
+        args.append('FETCH_HEAD')
+        self.fetch(ref)
+        self.log.debug("Merging %s with args %s" % (ref, args))
+        repo.git.merge(*args)
+        return repo.head.commit
+
+    def fetch(self, ref):
+        repo = self.createRepoObject()
+        # The git.remote.fetch method may read in git progress info and
+        # interpret it improperly causing an AssertionError. Because the
+        # data was fetched properly subsequent fetches don't seem to fail.
+        # So try again if an AssertionError is caught.
+        origin = repo.remotes.origin
+        try:
+            origin.fetch(ref)
+        except AssertionError:
+            origin.fetch(ref)
+
+    def fetchFrom(self, repository, refspec):
+        repo = self.createRepoObject()
+        repo.git.fetch(repository, refspec)
+
+    def createZuulRef(self, ref, commit='HEAD'):
+        repo = self.createRepoObject()
+        self.log.debug("CreateZuulRef %s at %s on %s" % (ref, commit, repo))
+        ref = ZuulReference.create(repo, ref, commit)
+        return ref.commit
+
+    def push(self, local, remote):
+        repo = self.createRepoObject()
+        self.log.debug("Pushing %s:%s to %s" % (local, remote,
+                                                self.remote_url))
+        repo.remotes.origin.push('%s:%s' % (local, remote))
+
+    def update(self):
+        repo = self.createRepoObject()
+        self.log.debug("Updating repository %s" % self.local_path)
+        origin = repo.remotes.origin
+        origin.update()
+
+
+class Merger(object):
+    log = logging.getLogger("zuul.Merger")
+
+    def __init__(self, working_root, connections, email, username):
+        self.repos = {}
+        self.working_root = working_root
+        if not os.path.exists(working_root):
+            os.makedirs(working_root)
+        self._makeSSHWrappers(working_root, connections)
+        self.email = email
+        self.username = username
+
+    def _makeSSHWrappers(self, working_root, connections):
+        for connection_name, connection in connections.items():
+            sshkey = connection.connection_config.get('sshkey')
+            if sshkey:
+                self._makeSSHWrapper(sshkey, working_root, connection_name)
+
+    def _makeSSHWrapper(self, key, merge_root, connection_name='default'):
+        wrapper_name = '.ssh_wrapper_%s' % connection_name
+        name = os.path.join(merge_root, wrapper_name)
+        fd = open(name, 'w')
+        fd.write('#!/bin/bash\n')
+        fd.write('ssh -i %s $@\n' % key)
+        fd.close()
+        os.chmod(name, 0755)
+
+    def addProject(self, project, url):
+        repo = None
+        try:
+            path = os.path.join(self.working_root, project)
+            repo = Repo(url, path, self.email, self.username)
+
+            self.repos[project] = repo
+        except Exception:
+            self.log.exception("Unable to add project %s" % project)
+        return repo
+
+    def getRepo(self, project, url):
+        if project in self.repos:
+            return self.repos[project]
+        if not url:
+            raise Exception("Unable to set up repo for project %s"
+                            " without a url" % (project,))
+        return self.addProject(project, url)
+
+    def updateRepo(self, project, url):
+        repo = self.getRepo(project, url)
+        try:
+            self.log.info("Updating local repository %s", project)
+            repo.update()
+        except Exception:
+            self.log.exception("Unable to update %s", project)
+
+    def _mergeChange(self, item, ref):
+        repo = self.getRepo(item['project'], item['url'])
+        try:
+            repo.checkout(ref)
+        except Exception:
+            self.log.exception("Unable to checkout %s" % ref)
+            return None
+
+        try:
+            mode = item['merge_mode']
+            if mode == zuul.model.MERGER_MERGE:
+                commit = repo.merge(item['refspec'])
+            elif mode == zuul.model.MERGER_MERGE_RESOLVE:
+                commit = repo.merge(item['refspec'], 'resolve')
+            elif mode == zuul.model.MERGER_CHERRY_PICK:
+                commit = repo.cherryPick(item['refspec'])
+            else:
+                raise Exception("Unsupported merge mode: %s" % mode)
+        except git.GitCommandError:
+            # Log git exceptions at debug level because they are
+            # usually benign merge conflicts
+            self.log.debug("Unable to merge %s" % item, exc_info=True)
+            return None
+        except Exception:
+            self.log.exception("Exception while merging a change:")
+            return None
+
+        return commit
+
+    def _setGitSsh(self, connection_name):
+        wrapper_name = '.ssh_wrapper_%s' % connection_name
+        name = os.path.join(self.working_root, wrapper_name)
+        if os.path.isfile(name):
+            os.environ['GIT_SSH'] = name
+        elif 'GIT_SSH' in os.environ:
+            del os.environ['GIT_SSH']
+
+    def _mergeItem(self, item, recent):
+        self.log.debug("Processing refspec %s for project %s / %s ref %s" %
+                       (item['refspec'], item['project'], item['branch'],
+                        item['ref']))
+        self._setGitSsh(item['connection_name'])
+        repo = self.getRepo(item['project'], item['url'])
+        key = (item['project'], item['branch'])
+        # See if we have a commit for this change already in this repo
+        zuul_ref = item['branch'] + '/' + item['ref']
+        commit = repo.getCommitFromRef(zuul_ref)
+        if commit:
+            self.log.debug("Found commit %s for ref %s" % (commit, zuul_ref))
+            # Store this as the most recent commit for this
+            # project-branch
+            recent[key] = commit
+            return commit
+        self.log.debug("Unable to find commit for ref %s" % (zuul_ref,))
+        # We need to merge the change
+        # Get the most recent commit for this project-branch
+        base = recent.get(key)
+        if not base:
+            # There is none, so use the branch tip
+            # we need to reset here in order to call getBranchHead
+            self.log.debug("No base commit found for %s" % (key,))
+            try:
+                repo.reset()
+            except Exception:
+                self.log.exception("Unable to reset repo %s" % repo)
+                return None
+            base = repo.getBranchHead(item['branch'])
+        else:
+            self.log.debug("Found base commit %s for %s" % (base, key,))
+        # Merge the change
+        commit = self._mergeChange(item, base)
+        if not commit:
+            return None
+        # Store this commit as the most recent for this project-branch
+        recent[key] = commit
+        # Set the Zuul ref for this item to point to the most recent
+        # commits of each project-branch
+        for key, mrc in recent.items():
+            project, branch = key
+            try:
+                repo = self.getRepo(project, None)
+                zuul_ref = branch + '/' + item['ref']
+                repo.createZuulRef(zuul_ref, mrc)
+            except Exception:
+                self.log.exception("Unable to set zuul ref %s for "
+                                   "item %s" % (zuul_ref, item))
+                return None
+        return commit
+
+    def mergeChanges(self, items):
+        recent = {}
+        commit = None
+        for item in items:
+            if item.get("number") and item.get("patchset"):
+                self.log.debug("Merging for change %s,%s." %
+                               (item["number"], item["patchset"]))
+            elif item.get("newrev") and item.get("oldrev"):
+                self.log.debug("Merging for rev %s with oldrev %s." %
+                               (item["newrev"], item["oldrev"]))
+            commit = self._mergeItem(item, recent)
+            if not commit:
+                return None
+        return commit.hexsha
diff --git a/zuul/merger/server.py b/zuul/merger/server.py
index 30cd732..0983571 100644
--- a/zuul/merger/server.py
+++ b/zuul/merger/server.py
@@ -1,115 +1,115 @@
-# Copyright 2014 OpenStack Foundation
-#
-# Licensed under the Apache License, Version 2.0 (the "License"); you may
-# not use this file except in compliance with the License. You may obtain
-# a copy of the License at
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-# License for the specific language governing permissions and limitations
-# under the License.
-
-import json
-import logging
-import threading
-import traceback
-
-import gear
-
-import merger
-
-
-class MergeServer(object):
-    log = logging.getLogger("zuul.MergeServer")
-
-    def __init__(self, config, connections={}):
-        self.config = config
-        self.zuul_url = config.get('merger', 'zuul_url')
-
-        if self.config.has_option('merger', 'git_dir'):
-            merge_root = self.config.get('merger', 'git_dir')
-        else:
-            merge_root = '/var/lib/zuul/git'
-
-        if self.config.has_option('merger', 'git_user_email'):
-            merge_email = self.config.get('merger', 'git_user_email')
-        else:
-            merge_email = None
-
-        if self.config.has_option('merger', 'git_user_name'):
-            merge_name = self.config.get('merger', 'git_user_name')
-        else:
-            merge_name = None
-
-        self.merger = merger.Merger(merge_root, connections, merge_email,
-                                    merge_name)
-
-    def start(self):
-        self._running = True
-        server = self.config.get('gearman', 'server')
-        if self.config.has_option('gearman', 'port'):
-            port = self.config.get('gearman', 'port')
-        else:
-            port = 4730
-        self.worker = gear.Worker('Zuul Merger')
-        self.worker.addServer(server, port)
-        self.log.debug("Waiting for server")
-        self.worker.waitForServer()
-        self.log.debug("Registering")
-        self.register()
-        self.log.debug("Starting worker")
-        self.thread = threading.Thread(target=self.run)
-        self.thread.daemon = True
-        self.thread.start()
-
-    def register(self):
-        self.worker.registerFunction("merger:merge")
-        self.worker.registerFunction("merger:update")
-
-    def stop(self):
-        self.log.debug("Stopping")
-        self._running = False
-        self.worker.shutdown()
-        self.log.debug("Stopped")
-
-    def join(self):
-        self.thread.join()
-
-    def run(self):
-        self.log.debug("Starting merge listener")
-        while self._running:
-            try:
-                job = self.worker.getJob()
-                try:
-                    if job.name == 'merger:merge':
-                        self.log.debug("Got merge job: %s" % job.unique)
-                        self.merge(job)
-                    elif job.name == 'merger:update':
-                        self.log.debug("Got update job: %s" % job.unique)
-                        self.update(job)
-                    else:
-                        self.log.error("Unable to handle job %s" % job.name)
-                        job.sendWorkFail()
-                except Exception:
-                    self.log.exception("Exception while running job")
-                    job.sendWorkException(traceback.format_exc())
-            except Exception:
-                self.log.exception("Exception while getting job")
-
-    def merge(self, job):
-        args = json.loads(job.arguments)
-        commit = self.merger.mergeChanges(args['items'])
-        result = dict(merged=(commit is not None),
-                      commit=commit,
-                      zuul_url=self.zuul_url)
-        job.sendWorkComplete(json.dumps(result))
-
-    def update(self, job):
-        args = json.loads(job.arguments)
-        self.merger.updateRepo(args['project'], args['url'])
-        result = dict(updated=True,
-                      zuul_url=self.zuul_url)
-        job.sendWorkComplete(json.dumps(result))
+# Copyright 2014 OpenStack Foundation
+#
+# Licensed under the Apache License, Version 2.0 (the "License"); you may
+# not use this file except in compliance with the License. You may obtain
+# a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+# License for the specific language governing permissions and limitations
+# under the License.
+
+import json
+import logging
+import threading
+import traceback
+
+import gear
+
+import merger
+
+
+class MergeServer(object):
+    log = logging.getLogger("zuul.MergeServer")
+
+    def __init__(self, config, connections={}):
+        self.config = config
+        self.zuul_url = config.get('merger', 'zuul_url')
+
+        if self.config.has_option('merger', 'git_dir'):
+            merge_root = self.config.get('merger', 'git_dir')
+        else:
+            merge_root = '/var/lib/zuul/git'
+
+        if self.config.has_option('merger', 'git_user_email'):
+            merge_email = self.config.get('merger', 'git_user_email')
+        else:
+            merge_email = None
+
+        if self.config.has_option('merger', 'git_user_name'):
+            merge_name = self.config.get('merger', 'git_user_name')
+        else:
+            merge_name = None
+
+        self.merger = merger.Merger(merge_root, connections, merge_email,
+                                    merge_name)
+
+    def start(self):
+        self._running = True
+        server = self.config.get('gearman', 'server')
+        if self.config.has_option('gearman', 'port'):
+            port = self.config.get('gearman', 'port')
+        else:
+            port = 4730
+        self.worker = gear.Worker('Zuul Merger')
+        self.worker.addServer(server, port)
+        self.log.debug("Waiting for server")
+        self.worker.waitForServer()
+        self.log.debug("Registering")
+        self.register()
+        self.log.debug("Starting worker")
+        self.thread = threading.Thread(target=self.run)
+        self.thread.daemon = True
+        self.thread.start()
+
+    def register(self):
+        self.worker.registerFunction("merger:merge")
+        self.worker.registerFunction("merger:update")
+
+    def stop(self):
+        self.log.debug("Stopping")
+        self._running = False
+        self.worker.shutdown()
+        self.log.debug("Stopped")
+
+    def join(self):
+        self.thread.join()
+
+    def run(self):
+        self.log.debug("Starting merge listener")
+        while self._running:
+            try:
+                job = self.worker.getJob()
+                try:
+                    if job.name == 'merger:merge':
+                        self.log.debug("Got merge job: %s" % job.unique)
+                        self.merge(job)
+                    elif job.name == 'merger:update':
+                        self.log.debug("Got update job: %s" % job.unique)
+                        self.update(job)
+                    else:
+                        self.log.error("Unable to handle job %s" % job.name)
+                        job.sendWorkFail()
+                except Exception:
+                    self.log.exception("Exception while running job")
+                    job.sendWorkException(traceback.format_exc())
+            except Exception:
+                self.log.exception("Exception while getting job")
+
+    def merge(self, job):
+        args = json.loads(job.arguments)
+        commit = self.merger.mergeChanges(args['items'])
+        result = dict(merged=(commit is not None),
+                      commit=commit,
+                      zuul_url=self.zuul_url)
+        job.sendWorkComplete(json.dumps(result))
+
+    def update(self, job):
+        args = json.loads(job.arguments)
+        self.merger.updateRepo(args['project'], args['url'])
+        result = dict(updated=True,
+                      zuul_url=self.zuul_url)
+        job.sendWorkComplete(json.dumps(result))
diff --git a/zuul/model.py b/zuul/model.py
index 5bea5d0..436ba61 100644
--- a/zuul/model.py
+++ b/zuul/model.py
@@ -1,1382 +1,1382 @@
-# Copyright 2012 Hewlett-Packard Development Company, L.P.
-#
-# Licensed under the Apache License, Version 2.0 (the "License"); you may
-# not use this file except in compliance with the License. You may obtain
-# a copy of the License at
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-# License for the specific language governing permissions and limitations
-# under the License.
-
-import copy
-import re
-import time
-from uuid import uuid4
-import extras
-
-OrderedDict = extras.try_imports(['collections.OrderedDict',
-                                  'ordereddict.OrderedDict'])
-
-
-EMPTY_GIT_REF = '0' * 40  # git sha of all zeros, used during creates/deletes
-
-MERGER_MERGE = 1          # "git merge"
-MERGER_MERGE_RESOLVE = 2  # "git merge -s resolve"
-MERGER_CHERRY_PICK = 3    # "git cherry-pick"
-
-MERGER_MAP = {
-    'merge': MERGER_MERGE,
-    'merge-resolve': MERGER_MERGE_RESOLVE,
-    'cherry-pick': MERGER_CHERRY_PICK,
-}
-
-PRECEDENCE_NORMAL = 0
-PRECEDENCE_LOW = 1
-PRECEDENCE_HIGH = 2
-
-PRECEDENCE_MAP = {
-    None: PRECEDENCE_NORMAL,
-    'low': PRECEDENCE_LOW,
-    'normal': PRECEDENCE_NORMAL,
-    'high': PRECEDENCE_HIGH,
-}
-
-
-def time_to_seconds(s):
-    if s.endswith('s'):
-        return int(s[:-1])
-    if s.endswith('m'):
-        return int(s[:-1]) * 60
-    if s.endswith('h'):
-        return int(s[:-1]) * 60 * 60
-    if s.endswith('d'):
-        return int(s[:-1]) * 24 * 60 * 60
-    if s.endswith('w'):
-        return int(s[:-1]) * 7 * 24 * 60 * 60
-    raise Exception("Unable to parse time value: %s" % s)
-
-
-def normalizeCategory(name):
-    name = name.lower()
-    return re.sub(' ', '-', name)
-
-
-class Pipeline(object):
-    """A top-level pipeline such as check, gate, post, etc."""
-    def __init__(self, name):
-        self.name = name
-        self.description = None
-        self.failure_message = None
-        self.merge_failure_message = None
-        self.success_message = None
-        self.footer_message = None
-        self.dequeue_on_new_patchset = True
-        self.ignore_dependencies = False
-        self.job_trees = {}  # project -> JobTree
-        self.manager = None
-        self.queues = []
-        self.precedence = PRECEDENCE_NORMAL
-        self.source = None
-        self.start_actions = []
-        self.success_actions = []
-        self.failure_actions = []
-        self.merge_failure_actions = []
-        self.disabled_actions = []
-        self.disable_at = None
-        self._consecutive_failures = 0
-        self._disabled = False
-        self.window = None
-        self.window_floor = None
-        self.window_increase_type = None
-        self.window_increase_factor = None
-        self.window_decrease_type = None
-        self.window_decrease_factor = None
-
-    def __repr__(self):
-        return '<Pipeline %s>' % self.name
-
-    def setManager(self, manager):
-        self.manager = manager
-
-    def addProject(self, project):
-        job_tree = JobTree(None)  # Null job == job tree root
-        self.job_trees[project] = job_tree
-        return job_tree
-
-    def getProjects(self):
-        return sorted(self.job_trees.keys(), lambda a, b: cmp(a.name, b.name))
-
-    def addQueue(self, queue):
-        self.queues.append(queue)
-
-    def getQueue(self, project):
-        for queue in self.queues:
-            if project in queue.projects:
-                return queue
-        return None
-
-    def removeQueue(self, queue):
-        self.queues.remove(queue)
-
-    def getJobTree(self, project):
-        tree = self.job_trees.get(project)
-        return tree
-
-    def getJobs(self, item):
-        if not item.live:
-            return []
-        tree = self.getJobTree(item.change.project)
-        if not tree:
-            return []
-        return item.change.filterJobs(tree.getJobs())
-
-    def _findJobsToRun(self, job_trees, item, mutex):
-        torun = []
-        if item.item_ahead:
-            # Only run jobs if any 'hold' jobs on the change ahead
-            # have completed successfully.
-            if self.isHoldingFollowingChanges(item.item_ahead):
-                return []
-        for tree in job_trees:
-            job = tree.job
-            result = None
-            if job:
-                if not job.changeMatches(item.change):
-                    continue
-                build = item.current_build_set.getBuild(job.name)
-                if build:
-                    result = build.result
-                else:
-                    # There is no build for the root of this job tree,
-                    # so we should run it.
-                    if mutex.acquire(item, job):
-                        # If this job needs a mutex, either acquire it or make
-                        # sure that we have it before running the job.
-                        torun.append(job)
-            # If there is no job, this is a null job tree, and we should
-            # run all of its jobs.
-            if result == 'SUCCESS' or not job:
-                torun.extend(self._findJobsToRun(tree.job_trees, item, mutex))
-        return torun
-
-    def findJobsToRun(self, item, mutex):
-        if not item.live:
-            return []
-        tree = self.getJobTree(item.change.project)
-        if not tree:
-            return []
-        return self._findJobsToRun(tree.job_trees, item, mutex)
-
-    def haveAllJobsStarted(self, item):
-        for job in self.getJobs(item):
-            build = item.current_build_set.getBuild(job.name)
-            if not build or not build.start_time:
-                return False
-        return True
-
-    def areAllJobsComplete(self, item):
-        for job in self.getJobs(item):
-            build = item.current_build_set.getBuild(job.name)
-            if not build or not build.result:
-                return False
-        return True
-
-    def didAllJobsSucceed(self, item):
-        for job in self.getJobs(item):
-            if not job.voting:
-                continue
-            build = item.current_build_set.getBuild(job.name)
-            if not build:
-                return False
-            if build.result != 'SUCCESS':
-                return False
-        return True
-
-    def didMergerSucceed(self, item):
-        if item.current_build_set.unable_to_merge:
-            return False
-        return True
-
-    def didAnyJobFail(self, item):
-        for job in self.getJobs(item):
-            if not job.voting:
-                continue
-            build = item.current_build_set.getBuild(job.name)
-            if build and build.result and (build.result != 'SUCCESS'):
-                return True
-        return False
-
-    def isHoldingFollowingChanges(self, item):
-        if not item.live:
-            return False
-        for job in self.getJobs(item):
-            if not job.hold_following_changes:
-                continue
-            build = item.current_build_set.getBuild(job.name)
-            if not build:
-                return True
-            if build.result != 'SUCCESS':
-                return True
-
-        if not item.item_ahead:
-            return False
-        return self.isHoldingFollowingChanges(item.item_ahead)
-
-    def setResult(self, item, build):
-        if build.retry:
-            item.removeBuild(build)
-        elif build.result != 'SUCCESS':
-            # Get a JobTree from a Job so we can find only its dependent jobs
-            root = self.getJobTree(item.change.project)
-            tree = root.getJobTreeForJob(build.job)
-            for job in tree.getJobs():
-                fakebuild = Build(job, None)
-                fakebuild.result = 'SKIPPED'
-                item.addBuild(fakebuild)
-
-    def setUnableToMerge(self, item):
-        item.current_build_set.unable_to_merge = True
-        root = self.getJobTree(item.change.project)
-        for job in root.getJobs():
-            fakebuild = Build(job, None)
-            fakebuild.result = 'SKIPPED'
-            item.addBuild(fakebuild)
-
-    def setDequeuedNeedingChange(self, item):
-        item.dequeued_needing_change = True
-        root = self.getJobTree(item.change.project)
-        for job in root.getJobs():
-            fakebuild = Build(job, None)
-            fakebuild.result = 'SKIPPED'
-            item.addBuild(fakebuild)
-
-    def getChangesInQueue(self):
-        changes = []
-        for shared_queue in self.queues:
-            changes.extend([x.change for x in shared_queue.queue])
-        return changes
-
-    def getAllItems(self):
-        items = []
-        for shared_queue in self.queues:
-            items.extend(shared_queue.queue)
-        return items
-
-    def formatStatusJSON(self, url_pattern=None):
-        j_pipeline = dict(name=self.name,
-                          description=self.description)
-        j_queues = []
-        j_pipeline['change_queues'] = j_queues
-        for queue in self.queues:
-            j_queue = dict(name=queue.name)
-            j_queues.append(j_queue)
-            j_queue['heads'] = []
-            j_queue['window'] = queue.window
-
-            j_changes = []
-            for e in queue.queue:
-                if not e.item_ahead:
-                    if j_changes:
-                        j_queue['heads'].append(j_changes)
-                    j_changes = []
-                j_changes.append(e.formatJSON(url_pattern))
-                if (len(j_changes) > 1 and
-                        (j_changes[-2]['remaining_time'] is not None) and
-                        (j_changes[-1]['remaining_time'] is not None)):
-                    j_changes[-1]['remaining_time'] = max(
-                        j_changes[-2]['remaining_time'],
-                        j_changes[-1]['remaining_time'])
-            if j_changes:
-                j_queue['heads'].append(j_changes)
-        return j_pipeline
-
-
-class ChangeQueue(object):
-    """DependentPipelines have multiple parallel queues shared by
-    different projects; this is one of them.  For instance, there may
-    a queue shared by interrelated projects foo and bar, and a second
-    queue for independent project baz.  Pipelines have one or more
-    ChangeQueues."""
-    def __init__(self, pipeline, window=0, window_floor=1,
-                 window_increase_type='linear', window_increase_factor=1,
-                 window_decrease_type='exponential', window_decrease_factor=2):
-        self.pipeline = pipeline
-        self.name = ''
-        self.assigned_name = None
-        self.generated_name = None
-        self.projects = []
-        self._jobs = set()
-        self.queue = []
-        self.window = window
-        self.window_floor = window_floor
-        self.window_increase_type = window_increase_type
-        self.window_increase_factor = window_increase_factor
-        self.window_decrease_type = window_decrease_type
-        self.window_decrease_factor = window_decrease_factor
-
-    def __repr__(self):
-        return '<ChangeQueue %s: %s>' % (self.pipeline.name, self.name)
-
-    def getJobs(self):
-        return self._jobs
-
-    def addProject(self, project):
-        if project not in self.projects:
-            self.projects.append(project)
-            self._jobs |= set(self.pipeline.getJobTree(project).getJobs())
-
-            names = [x.name for x in self.projects]
-            names.sort()
-            self.generated_name = ', '.join(names)
-
-            for job in self._jobs:
-                if job.queue_name:
-                    if (self.assigned_name and
-                            job.queue_name != self.assigned_name):
-                        raise Exception("More than one name assigned to "
-                                        "change queue: %s != %s" %
-                                        (self.assigned_name, job.queue_name))
-                    self.assigned_name = job.queue_name
-            self.name = self.assigned_name or self.generated_name
-
-    def enqueueChange(self, change):
-        item = QueueItem(self, change)
-        self.enqueueItem(item)
-        item.enqueue_time = time.time()
-        return item
-
-    def enqueueItem(self, item):
-        item.pipeline = self.pipeline
-        item.queue = self
-        if self.queue:
-            item.item_ahead = self.queue[-1]
-            item.item_ahead.items_behind.append(item)
-        self.queue.append(item)
-
-    def dequeueItem(self, item):
-        if item in self.queue:
-            self.queue.remove(item)
-        if item.item_ahead:
-            item.item_ahead.items_behind.remove(item)
-        for item_behind in item.items_behind:
-            if item.item_ahead:
-                item.item_ahead.items_behind.append(item_behind)
-            item_behind.item_ahead = item.item_ahead
-        item.item_ahead = None
-        item.items_behind = []
-        item.dequeue_time = time.time()
-
-    def moveItem(self, item, item_ahead):
-        if item.item_ahead == item_ahead:
-            return False
-        # Remove from current location
-        if item.item_ahead:
-            item.item_ahead.items_behind.remove(item)
-        for item_behind in item.items_behind:
-            if item.item_ahead:
-                item.item_ahead.items_behind.append(item_behind)
-            item_behind.item_ahead = item.item_ahead
-        # Add to new location
-        item.item_ahead = item_ahead
-        item.items_behind = []
-        if item.item_ahead:
-            item.item_ahead.items_behind.append(item)
-        return True
-
-    def mergeChangeQueue(self, other):
-        for project in other.projects:
-            self.addProject(project)
-        self.window = min(self.window, other.window)
-        # TODO merge semantics
-
-    def isActionable(self, item):
-        if self.window:
-            return item in self.queue[:self.window]
-        else:
-            return True
-
-    def increaseWindowSize(self):
-        if self.window:
-            if self.window_increase_type == 'linear':
-                self.window += self.window_increase_factor
-            elif self.window_increase_type == 'exponential':
-                self.window *= self.window_increase_factor
-
-    def decreaseWindowSize(self):
-        if self.window:
-            if self.window_decrease_type == 'linear':
-                self.window = max(
-                    self.window_floor,
-                    self.window - self.window_decrease_factor)
-            elif self.window_decrease_type == 'exponential':
-                self.window = max(
-                    self.window_floor,
-                    self.window / self.window_decrease_factor)
-
-
-class Project(object):
-    def __init__(self, name, foreign=False):
-        self.name = name
-        self.merge_mode = MERGER_MERGE_RESOLVE
-        # foreign projects are those referenced in dependencies
-        # of layout projects, this should matter
-        # when deciding whether to enqueue their changes
-        self.foreign = foreign
-
-    def __str__(self):
-        return self.name
-
-    def __repr__(self):
-        return '<Project %s>' % (self.name)
-
-
-class Job(object):
-    def __init__(self, name):
-        # If you add attributes here, be sure to add them to the copy method.
-        self.name = name
-        self.queue_name = None
-        self.failure_message = None
-        self.success_message = None
-        self.failure_pattern = None
-        self.success_pattern = None
-        self.parameter_function = None
-        self.tags = set()
-        self.mutex = None
-        # A metajob should only supply values for attributes that have
-        # been explicitly provided, so avoid setting boolean defaults.
-        if self.is_metajob:
-            self.hold_following_changes = None
-            self.voting = None
-        else:
-            self.hold_following_changes = False
-            self.voting = True
-        self.branches = []
-        self._branches = []
-        self.files = []
-        self._files = []
-        self.skip_if_matcher = None
-        self.swift = {}
-
-    def __str__(self):
-        return self.name
-
-    def __repr__(self):
-        return '<Job %s>' % (self.name)
-
-    @property
-    def is_metajob(self):
-        return self.name.startswith('^')
-
-    def copy(self, other):
-        if other.failure_message:
-            self.failure_message = other.failure_message
-        if other.success_message:
-            self.success_message = other.success_message
-        if other.failure_pattern:
-            self.failure_pattern = other.failure_pattern
-        if other.success_pattern:
-            self.success_pattern = other.success_pattern
-        if other.parameter_function:
-            self.parameter_function = other.parameter_function
-        if other.branches:
-            self.branches = other.branches[:]
-            self._branches = other._branches[:]
-        if other.files:
-            self.files = other.files[:]
-            self._files = other._files[:]
-        if other.skip_if_matcher:
-            self.skip_if_matcher = other.skip_if_matcher.copy()
-        if other.swift:
-            self.swift.update(other.swift)
-        if other.mutex:
-            self.mutex = other.mutex
-        # Tags are merged via a union rather than a destructive copy
-        # because they are intended to accumulate as metajobs are
-        # applied.
-        if other.tags:
-            self.tags = self.tags.union(other.tags)
-        # Only non-None values should be copied for boolean attributes.
-        if other.hold_following_changes is not None:
-            self.hold_following_changes = other.hold_following_changes
-        if other.voting is not None:
-            self.voting = other.voting
-
-    def changeMatches(self, change):
-        matches_branch = False
-        for branch in self.branches:
-            if hasattr(change, 'branch') and branch.match(change.branch):
-                matches_branch = True
-            if hasattr(change, 'ref') and branch.match(change.ref):
-                matches_branch = True
-        if self.branches and not matches_branch:
-            return False
-
-        matches_file = False
-        for f in self.files:
-            if hasattr(change, 'files'):
-                for cf in change.files:
-                    if f.match(cf):
-                        matches_file = True
-        if self.files and not matches_file:
-            return False
-
-        if self.skip_if_matcher and self.skip_if_matcher.matches(change):
-            return False
-
-        return True
-
-
-class JobTree(object):
-    """ A JobTree represents an instance of one Job, and holds JobTrees
-    whose jobs should be run if that Job succeeds.  A root node of a
-    JobTree will have no associated Job. """
-
-    def __init__(self, job):
-        self.job = job
-        self.job_trees = []
-
-    def addJob(self, job):
-        if job not in [x.job for x in self.job_trees]:
-            t = JobTree(job)
-            self.job_trees.append(t)
-            return t
-        for tree in self.job_trees:
-            if tree.job == job:
-                return tree
-
-    def getJobs(self):
-        jobs = []
-        for x in self.job_trees:
-            jobs.append(x.job)
-            jobs.extend(x.getJobs())
-        return jobs
-
-    def getJobTreeForJob(self, job):
-        if self.job == job:
-            return self
-        for tree in self.job_trees:
-            ret = tree.getJobTreeForJob(job)
-            if ret:
-                return ret
-        return None
-
-
-class Build(object):
-    def __init__(self, job, uuid):
-        self.job = job
-        self.uuid = uuid
-        self.url = None
-        self.number = None
-        self.result = None
-        self.build_set = None
-        self.launch_time = time.time()
-        self.start_time = None
-        self.end_time = None
-        self.estimated_time = None
-        self.pipeline = None
-        self.canceled = False
-        self.retry = False
-        self.parameters = {}
-        self.worker = Worker()
-        self.node_labels = []
-        self.node_name = None
-
-    def __repr__(self):
-        return ('<Build %s of %s on %s>' %
-                (self.uuid, self.job.name, self.worker))
-
-
-class Worker(object):
-    """A model of the worker running a job"""
-    def __init__(self):
-        self.name = "Unknown"
-        self.hostname = None
-        self.ips = []
-        self.fqdn = None
-        self.program = None
-        self.version = None
-        self.extra = {}
-
-    def updateFromData(self, data):
-        """Update worker information if contained in the WORK_DATA response."""
-        self.name = data.get('worker_name', self.name)
-        self.hostname = data.get('worker_hostname', self.hostname)
-        self.ips = data.get('worker_ips', self.ips)
-        self.fqdn = data.get('worker_fqdn', self.fqdn)
-        self.program = data.get('worker_program', self.program)
-        self.version = data.get('worker_version', self.version)
-        self.extra = data.get('worker_extra', self.extra)
-
-    def __repr__(self):
-        return '<Worker %s>' % self.name
-
-
-class BuildSet(object):
-    # Merge states:
-    NEW = 1
-    PENDING = 2
-    COMPLETE = 3
-
-    states_map = {
-        1: 'NEW',
-        2: 'PENDING',
-        3: 'COMPLETE',
-    }
-
-    def __init__(self, item):
-        self.item = item
-        self.other_changes = []
-        self.builds = {}
-        self.result = None
-        self.next_build_set = None
-        self.previous_build_set = None
-        self.ref = None
-        self.commit = None
-        self.zuul_url = None
-        self.unable_to_merge = False
-        self.failing_reasons = []
-        self.merge_state = self.NEW
-
-    def __repr__(self):
-        return '<BuildSet item: %s #builds: %s merge state: %s>' % (
-            self.item,
-            len(self.builds),
-            self.getStateName(self.merge_state))
-
-    def setConfiguration(self):
-        # The change isn't enqueued until after it's created
-        # so we don't know what the other changes ahead will be
-        # until jobs start.
-        if not self.other_changes:
-            next_item = self.item.item_ahead
-            while next_item:
-                self.other_changes.append(next_item.change)
-                next_item = next_item.item_ahead
-        if not self.ref:
-            self.ref = 'Z' + uuid4().hex
-
-    def getStateName(self, state_num):
-        return self.states_map.get(
-            state_num, 'UNKNOWN (%s)' % state_num)
-
-    def addBuild(self, build):
-        self.builds[build.job.name] = build
-        build.build_set = self
-
-    def removeBuild(self, build):
-        del self.builds[build.job.name]
-
-    def getBuild(self, job_name):
-        return self.builds.get(job_name)
-
-    def getBuilds(self):
-        keys = self.builds.keys()
-        keys.sort()
-        return [self.builds.get(x) for x in keys]
-
-
-class QueueItem(object):
-    """A changish inside of a Pipeline queue"""
-
-    def __init__(self, queue, change):
-        self.pipeline = queue.pipeline
-        self.queue = queue
-        self.change = change  # a changeish
-        self.build_sets = []
-        self.dequeued_needing_change = False
-        self.current_build_set = BuildSet(self)
-        self.build_sets.append(self.current_build_set)
-        self.item_ahead = None
-        self.items_behind = []
-        self.enqueue_time = None
-        self.dequeue_time = None
-        self.reported = False
-        self.active = False  # Whether an item is within an active window
-        self.live = True  # Whether an item is intended to be processed at all
-
-    def __repr__(self):
-        if self.pipeline:
-            pipeline = self.pipeline.name
-        else:
-            pipeline = None
-        return '<QueueItem 0x%x for %s in %s>' % (
-            id(self), self.change, pipeline)
-
-    def resetAllBuilds(self):
-        old = self.current_build_set
-        self.current_build_set.result = 'CANCELED'
-        self.current_build_set = BuildSet(self)
-        old.next_build_set = self.current_build_set
-        self.current_build_set.previous_build_set = old
-        self.build_sets.append(self.current_build_set)
-
-    def addBuild(self, build):
-        self.current_build_set.addBuild(build)
-        build.pipeline = self.pipeline
-
-    def removeBuild(self, build):
-        self.current_build_set.removeBuild(build)
-
-    def setReportedResult(self, result):
-        self.current_build_set.result = result
-
-    def formatJobResult(self, job, url_pattern=None):
-        build = self.current_build_set.getBuild(job.name)
-        result = build.result
-        pattern = url_pattern
-        if result == 'SUCCESS':
-            if job.success_message:
-                result = job.success_message
-            if job.success_pattern:
-                pattern = job.success_pattern
-        elif result == 'FAILURE':
-            if job.failure_message:
-                result = job.failure_message
-            if job.failure_pattern:
-                pattern = job.failure_pattern
-        url = None
-        if pattern:
-            try:
-                url = pattern.format(change=self.change,
-                                     pipeline=self.pipeline,
-                                     job=job,
-                                     build=build)
-            except Exception:
-                pass  # FIXME: log this or something?
-        if not url:
-            url = build.url or job.name
-        return (result, url)
-
-    def formatJSON(self, url_pattern=None):
-        changeish = self.change
-        ret = {}
-        ret['active'] = self.active
-        ret['live'] = self.live
-        if hasattr(changeish, 'url') and changeish.url is not None:
-            ret['url'] = changeish.url
-        else:
-            ret['url'] = None
-        ret['id'] = changeish._id()
-        if self.item_ahead:
-            ret['item_ahead'] = self.item_ahead.change._id()
-        else:
-            ret['item_ahead'] = None
-        ret['items_behind'] = [i.change._id() for i in self.items_behind]
-        ret['failing_reasons'] = self.current_build_set.failing_reasons
-        ret['zuul_ref'] = self.current_build_set.ref
-        if changeish.project:
-            ret['project'] = changeish.project.name
-        else:
-            # For cross-project dependencies with the depends-on
-            # project not known to zuul, the project is None
-            # Set it to a static value
-            ret['project'] = "Unknown Project"
-        ret['enqueue_time'] = int(self.enqueue_time * 1000)
-        ret['jobs'] = []
-        if hasattr(changeish, 'owner'):
-            ret['owner'] = changeish.owner
-        else:
-            ret['owner'] = None
-        max_remaining = 0
-        for job in self.pipeline.getJobs(self):
-            now = time.time()
-            build = self.current_build_set.getBuild(job.name)
-            elapsed = None
-            remaining = None
-            result = None
-            build_url = None
-            report_url = None
-            worker = None
-            if build:
-                result = build.result
-                build_url = build.url
-                (unused, report_url) = self.formatJobResult(job, url_pattern)
-                if build.start_time:
-                    if build.end_time:
-                        elapsed = int((build.end_time -
-                                       build.start_time) * 1000)
-                        remaining = 0
-                    else:
-                        elapsed = int((now - build.start_time) * 1000)
-                        if build.estimated_time:
-                            remaining = max(
-                                int(build.estimated_time * 1000) - elapsed,
-                                0)
-                worker = {
-                    'name': build.worker.name,
-                    'hostname': build.worker.hostname,
-                    'ips': build.worker.ips,
-                    'fqdn': build.worker.fqdn,
-                    'program': build.worker.program,
-                    'version': build.worker.version,
-                    'extra': build.worker.extra
-                }
-            if remaining and remaining > max_remaining:
-                max_remaining = remaining
-
-            ret['jobs'].append({
-                'name': job.name,
-                'elapsed_time': elapsed,
-                'remaining_time': remaining,
-                'url': build_url,
-                'report_url': report_url,
-                'result': result,
-                'voting': job.voting,
-                'uuid': build.uuid if build else None,
-                'launch_time': build.launch_time if build else None,
-                'start_time': build.start_time if build else None,
-                'end_time': build.end_time if build else None,
-                'estimated_time': build.estimated_time if build else None,
-                'pipeline': build.pipeline.name if build else None,
-                'canceled': build.canceled if build else None,
-                'retry': build.retry if build else None,
-                'number': build.number if build else None,
-                'node_labels': build.node_labels if build else [],
-                'node_name': build.node_name if build else None,
-                'worker': worker,
-            })
-
-        if self.pipeline.haveAllJobsStarted(self):
-            ret['remaining_time'] = max_remaining
-        else:
-            ret['remaining_time'] = None
-        return ret
-
-    def formatStatus(self, indent=0, html=False):
-        changeish = self.change
-        indent_str = ' ' * indent
-        ret = ''
-        if html and hasattr(changeish, 'url') and changeish.url is not None:
-            ret += '%sProject %s change <a href="%s">%s</a>\n' % (
-                indent_str,
-                changeish.project.name,
-                changeish.url,
-                changeish._id())
-        else:
-            ret += '%sProject %s change %s based on %s\n' % (
-                indent_str,
-                changeish.project.name,
-                changeish._id(),
-                self.item_ahead)
-        for job in self.pipeline.getJobs(self):
-            build = self.current_build_set.getBuild(job.name)
-            if build:
-                result = build.result
-            else:
-                result = None
-            job_name = job.name
-            if not job.voting:
-                voting = ' (non-voting)'
-            else:
-                voting = ''
-            if html:
-                if build:
-                    url = build.url
-                else:
-                    url = None
-                if url is not None:
-                    job_name = '<a href="%s">%s</a>' % (url, job_name)
-            ret += '%s  %s: %s%s' % (indent_str, job_name, result, voting)
-            ret += '\n'
-        return ret
-
-
-class Changeish(object):
-    """Something like a change; either a change or a ref"""
-
-    def __init__(self, project):
-        self.project = project
-
-    def getBasePath(self):
-        base_path = ''
-        if hasattr(self, 'refspec'):
-            base_path = "%s/%s/%s" % (
-                self.number[-2:], self.number, self.patchset)
-        elif hasattr(self, 'ref'):
-            base_path = "%s/%s" % (self.newrev[:2], self.newrev)
-
-        return base_path
-
-    def equals(self, other):
-        raise NotImplementedError()
-
-    def isUpdateOf(self, other):
-        raise NotImplementedError()
-
-    def filterJobs(self, jobs):
-        return filter(lambda job: job.changeMatches(self), jobs)
-
-    def getRelatedChanges(self):
-        return set()
-
-
-class Change(Changeish):
-    def __init__(self, project):
-        super(Change, self).__init__(project)
-        self.branch = None
-        self.number = None
-        self.url = None
-        self.patchset = None
-        self.refspec = None
-
-        self.files = []
-        self.needs_changes = []
-        self.needed_by_changes = []
-        self.is_current_patchset = True
-        self.can_merge = False
-        self.is_merged = False
-        self.failed_to_merge = False
-        self.approvals = []
-        self.open = None
-        self.status = None
-        self.owner = None
-
-    def _id(self):
-        return '%s,%s' % (self.number, self.patchset)
-
-    def __repr__(self):
-        return '<Change 0x%x %s>' % (id(self), self._id())
-
-    def equals(self, other):
-        if self.number == other.number and self.patchset == other.patchset:
-            return True
-        return False
-
-    def isUpdateOf(self, other):
-        if ((hasattr(other, 'number') and self.number == other.number) and
-            (hasattr(other, 'patchset') and
-             self.patchset is not None and
-             other.patchset is not None and
-             int(self.patchset) > int(other.patchset))):
-            return True
-        return False
-
-    def getRelatedChanges(self):
-        related = set()
-        for c in self.needs_changes:
-            related.add(c)
-        for c in self.needed_by_changes:
-            related.add(c)
-            related.update(c.getRelatedChanges())
-        return related
-
-
-class Ref(Changeish):
-    def __init__(self, project):
-        super(Ref, self).__init__(project)
-        self.ref = None
-        self.oldrev = None
-        self.newrev = None
-
-    def _id(self):
-        return self.newrev
-
-    def __repr__(self):
-        rep = None
-        if self.newrev == '0000000000000000000000000000000000000000':
-            rep = '<Ref 0x%x deletes %s from %s' % (
-                  id(self), self.ref, self.oldrev)
-        elif self.oldrev == '0000000000000000000000000000000000000000':
-            rep = '<Ref 0x%x creates %s on %s>' % (
-                  id(self), self.ref, self.newrev)
-        else:
-            # Catch all
-            rep = '<Ref 0x%x %s updated %s..%s>' % (
-                  id(self), self.ref, self.oldrev, self.newrev)
-
-        return rep
-
-    def equals(self, other):
-        if (self.project == other.project
-            and self.ref == other.ref
-            and self.newrev == other.newrev):
-            return True
-        return False
-
-    def isUpdateOf(self, other):
-        return False
-
-
-class NullChange(Changeish):
-    def __repr__(self):
-        return '<NullChange for %s>' % (self.project)
-
-    def _id(self):
-        return None
-
-    def equals(self, other):
-        if (self.project == other.project
-            and other._id() is None):
-            return True
-        return False
-
-    def isUpdateOf(self, other):
-        return False
-
-
-class TriggerEvent(object):
-    def __init__(self):
-        self.data = None
-        # common
-        self.type = None
-        self.project_name = None
-        self.trigger_name = None
-        # Representation of the user account that performed the event.
-        self.account = None
-        # patchset-created, comment-added, etc.
-        self.change_number = None
-        self.change_url = None
-        self.patch_number = None
-        self.refspec = None
-        self.approvals = []
-        self.branch = None
-        self.comment = None
-        # ref-updated
-        self.ref = None
-        self.oldrev = None
-        self.newrev = None
-        # timer
-        self.timespec = None
-        # zuultrigger
-        self.pipeline_name = None
-        # For events that arrive with a destination pipeline (eg, from
-        # an admin command, etc):
-        self.forced_pipeline = None
-
-    def __repr__(self):
-        ret = '<TriggerEvent %s %s' % (self.type, self.project_name)
-
-        if self.branch:
-            ret += " %s" % self.branch
-        if self.change_number:
-            ret += " %s,%s" % (self.change_number, self.patch_number)
-        if self.approvals:
-            ret += ' ' + ', '.join(
-                ['%s:%s' % (a['type'], a['value']) for a in self.approvals])
-        ret += '>'
-
-        return ret
-
-
-class BaseFilter(object):
-    def __init__(self, required_approvals=[], reject_approvals=[]):
-        self._required_approvals = copy.deepcopy(required_approvals)
-        self.required_approvals = self._tidy_approvals(required_approvals)
-        self._reject_approvals = copy.deepcopy(reject_approvals)
-        self.reject_approvals = self._tidy_approvals(reject_approvals)
-
-    def _tidy_approvals(self, approvals):
-        for a in approvals:
-            for k, v in a.items():
-                if k == 'username':
-                    pass
-                elif k in ['email', 'email-filter']:
-                    a['email'] = re.compile(v)
-                elif k == 'newer-than':
-                    a[k] = time_to_seconds(v)
-                elif k == 'older-than':
-                    a[k] = time_to_seconds(v)
-            if 'email-filter' in a:
-                del a['email-filter']
-        return approvals
-
-    def _match_approval_required_approval(self, rapproval, approval):
-        # Check if the required approval and approval match
-        if 'description' not in approval:
-            return False
-        now = time.time()
-        by = approval.get('by', {})
-        for k, v in rapproval.items():
-            if k == 'username':
-                if (by.get('username', '') != v):
-                        return False
-            elif k == 'email':
-                if (not v.search(by.get('email', ''))):
-                        return False
-            elif k == 'newer-than':
-                t = now - v
-                if (approval['grantedOn'] < t):
-                        return False
-            elif k == 'older-than':
-                t = now - v
-                if (approval['grantedOn'] >= t):
-                    return False
-            else:
-                if not isinstance(v, list):
-                    v = [v]
-                if (normalizeCategory(approval['description']) != k or
-                        int(approval['value']) not in v):
-                    return False
-        return True
-
-    def matchesApprovals(self, change):
-        if (self.required_approvals and not change.approvals
-                or self.reject_approvals and not change.approvals):
-            # A change with no approvals can not match
-            return False
-
-        # TODO(jhesketh): If we wanted to optimise this slightly we could
-        # analyse both the REQUIRE and REJECT filters by looping over the
-        # approvals on the change and keeping track of what we have checked
-        # rather than needing to loop on the change approvals twice
-        return (self.matchesRequiredApprovals(change) and
-                self.matchesNoRejectApprovals(change))
-
-    def matchesRequiredApprovals(self, change):
-        # Check if any approvals match the requirements
-        for rapproval in self.required_approvals:
-            matches_rapproval = False
-            for approval in change.approvals:
-                if self._match_approval_required_approval(rapproval, approval):
-                    # We have a matching approval so this requirement is
-                    # fulfilled
-                    matches_rapproval = True
-                    break
-            if not matches_rapproval:
-                return False
-        return True
-
-    def matchesNoRejectApprovals(self, change):
-        # Check to make sure no approvals match a reject criteria
-        for rapproval in self.reject_approvals:
-            for approval in change.approvals:
-                if self._match_approval_required_approval(rapproval, approval):
-                    # A reject approval has been matched, so we reject
-                    # immediately
-                    return False
-        # To get here no rejects can have been matched so we should be good to
-        # queue
-        return True
-
-
-class EventFilter(BaseFilter):
-    def __init__(self, trigger, types=[], branches=[], refs=[],
-                 event_approvals={}, comments=[], emails=[], usernames=[],
-                 timespecs=[], required_approvals=[], reject_approvals=[],
-                 pipelines=[], ignore_deletes=True):
-        super(EventFilter, self).__init__(
-            required_approvals=required_approvals,
-            reject_approvals=reject_approvals)
-        self.trigger = trigger
-        self._types = types
-        self._branches = branches
-        self._refs = refs
-        self._comments = comments
-        self._emails = emails
-        self._usernames = usernames
-        self._pipelines = pipelines
-        self.types = [re.compile(x) for x in types]
-        self.branches = [re.compile(x) for x in branches]
-        self.refs = [re.compile(x) for x in refs]
-        self.comments = [re.compile(x) for x in comments]
-        self.emails = [re.compile(x) for x in emails]
-        self.usernames = [re.compile(x) for x in usernames]
-        self.pipelines = [re.compile(x) for x in pipelines]
-        self.event_approvals = event_approvals
-        self.timespecs = timespecs
-        self.ignore_deletes = ignore_deletes
-
-    def __repr__(self):
-        ret = '<EventFilter'
-
-        if self._types:
-            ret += ' types: %s' % ', '.join(self._types)
-        if self._pipelines:
-            ret += ' pipelines: %s' % ', '.join(self._pipelines)
-        if self._branches:
-            ret += ' branches: %s' % ', '.join(self._branches)
-        if self._refs:
-            ret += ' refs: %s' % ', '.join(self._refs)
-        if self.ignore_deletes:
-            ret += ' ignore_deletes: %s' % self.ignore_deletes
-        if self.event_approvals:
-            ret += ' event_approvals: %s' % ', '.join(
-                ['%s:%s' % a for a in self.event_approvals.items()])
-        if self.required_approvals:
-            ret += ' required_approvals: %s' % ', '.join(
-                ['%s' % a for a in self._required_approvals])
-        if self.reject_approvals:
-            ret += ' reject_approvals: %s' % ', '.join(
-                ['%s' % a for a in self._reject_approvals])
-        if self._comments:
-            ret += ' comments: %s' % ', '.join(self._comments)
-        if self._emails:
-            ret += ' emails: %s' % ', '.join(self._emails)
-        if self._usernames:
-            ret += ' username_filters: %s' % ', '.join(self._usernames)
-        if self.timespecs:
-            ret += ' timespecs: %s' % ', '.join(self.timespecs)
-        ret += '>'
-
-        return ret
-
-    def matches(self, event, change):
-        # event types are ORed
-        matches_type = False
-        for etype in self.types:
-            if etype.match(event.type):
-                matches_type = True
-        if self.types and not matches_type:
-            return False
-
-        # pipelines are ORed
-        matches_pipeline = False
-        for epipe in self.pipelines:
-            if epipe.match(event.pipeline_name):
-                matches_pipeline = True
-        if self.pipelines and not matches_pipeline:
-            return False
-
-        # branches are ORed
-        matches_branch = False
-        for branch in self.branches:
-            if branch.match(event.branch):
-                matches_branch = True
-        if self.branches and not matches_branch:
-            return False
-
-        # refs are ORed
-        matches_ref = False
-        if event.ref is not None:
-            for ref in self.refs:
-                if ref.match(event.ref):
-                    matches_ref = True
-        if self.refs and not matches_ref:
-            return False
-        if self.ignore_deletes and event.newrev == EMPTY_GIT_REF:
-            # If the updated ref has an empty git sha (all 0s),
-            # then the ref is being deleted
-            return False
-
-        # comments are ORed
-        matches_comment_re = False
-        for comment_re in self.comments:
-            if (event.comment is not None and
-                comment_re.search(event.comment)):
-                matches_comment_re = True
-        if self.comments and not matches_comment_re:
-            return False
-
-        # We better have an account provided by Gerrit to do
-        # email filtering.
-        if event.account is not None:
-            account_email = event.account.get('email')
-            # emails are ORed
-            matches_email_re = False
-            for email_re in self.emails:
-                if (account_email is not None and
-                        email_re.search(account_email)):
-                    matches_email_re = True
-            if self.emails and not matches_email_re:
-                return False
-
-            # usernames are ORed
-            account_username = event.account.get('username')
-            matches_username_re = False
-            for username_re in self.usernames:
-                if (account_username is not None and
-                    username_re.search(account_username)):
-                    matches_username_re = True
-            if self.usernames and not matches_username_re:
-                return False
-
-        # approvals are ANDed
-        for category, value in self.event_approvals.items():
-            matches_approval = False
-            for eapproval in event.approvals:
-                if (normalizeCategory(eapproval['description']) == category and
-                    int(eapproval['value']) == int(value)):
-                    matches_approval = True
-            if not matches_approval:
-                return False
-
-        # required approvals are ANDed (reject approvals are ORed)
-        if not self.matchesApprovals(change):
-            return False
-
-        # timespecs are ORed
-        matches_timespec = False
-        for timespec in self.timespecs:
-            if (event.timespec == timespec):
-                matches_timespec = True
-        if self.timespecs and not matches_timespec:
-            return False
-
-        return True
-
-
-class ChangeishFilter(BaseFilter):
-    def __init__(self, open=None, current_patchset=None,
-                 statuses=[], required_approvals=[],
-                 reject_approvals=[]):
-        super(ChangeishFilter, self).__init__(
-            required_approvals=required_approvals,
-            reject_approvals=reject_approvals)
-        self.open = open
-        self.current_patchset = current_patchset
-        self.statuses = statuses
-
-    def __repr__(self):
-        ret = '<ChangeishFilter'
-
-        if self.open is not None:
-            ret += ' open: %s' % self.open
-        if self.current_patchset is not None:
-            ret += ' current-patchset: %s' % self.current_patchset
-        if self.statuses:
-            ret += ' statuses: %s' % ', '.join(self.statuses)
-        if self.required_approvals:
-            ret += (' required_approvals: %s' %
-                    str(self.required_approvals))
-        if self.reject_approvals:
-            ret += (' reject_approvals: %s' %
-                    str(self.reject_approvals))
-        ret += '>'
-
-        return ret
-
-    def matches(self, change):
-        if self.open is not None:
-            if self.open != change.open:
-                return False
-
-        if self.current_patchset is not None:
-            if self.current_patchset != change.is_current_patchset:
-                return False
-
-        if self.statuses:
-            if change.status not in self.statuses:
-                return False
-
-        # required approvals are ANDed (reject approvals are ORed)
-        if not self.matchesApprovals(change):
-            return False
-
-        return True
-
-
-class Layout(object):
-    def __init__(self):
-        self.projects = {}
-        self.pipelines = OrderedDict()
-        self.jobs = {}
-        self.metajobs = []
-
-    def getJob(self, name):
-        if name in self.jobs:
-            return self.jobs[name]
-        job = Job(name)
-        if job.is_metajob:
-            regex = re.compile(name)
-            self.metajobs.append((regex, job))
-        else:
-            # Apply attributes from matching meta-jobs
-            for regex, metajob in self.metajobs:
-                if regex.match(name):
-                    job.copy(metajob)
-            self.jobs[name] = job
-        return job
+# Copyright 2012 Hewlett-Packard Development Company, L.P.
+#
+# Licensed under the Apache License, Version 2.0 (the "License"); you may
+# not use this file except in compliance with the License. You may obtain
+# a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+# License for the specific language governing permissions and limitations
+# under the License.
+
+import copy
+import re
+import time
+from uuid import uuid4
+import extras
+
+OrderedDict = extras.try_imports(['collections.OrderedDict',
+                                  'ordereddict.OrderedDict'])
+
+
+EMPTY_GIT_REF = '0' * 40  # git sha of all zeros, used during creates/deletes
+
+MERGER_MERGE = 1          # "git merge"
+MERGER_MERGE_RESOLVE = 2  # "git merge -s resolve"
+MERGER_CHERRY_PICK = 3    # "git cherry-pick"
+
+MERGER_MAP = {
+    'merge': MERGER_MERGE,
+    'merge-resolve': MERGER_MERGE_RESOLVE,
+    'cherry-pick': MERGER_CHERRY_PICK,
+}
+
+PRECEDENCE_NORMAL = 0
+PRECEDENCE_LOW = 1
+PRECEDENCE_HIGH = 2
+
+PRECEDENCE_MAP = {
+    None: PRECEDENCE_NORMAL,
+    'low': PRECEDENCE_LOW,
+    'normal': PRECEDENCE_NORMAL,
+    'high': PRECEDENCE_HIGH,
+}
+
+
+def time_to_seconds(s):
+    if s.endswith('s'):
+        return int(s[:-1])
+    if s.endswith('m'):
+        return int(s[:-1]) * 60
+    if s.endswith('h'):
+        return int(s[:-1]) * 60 * 60
+    if s.endswith('d'):
+        return int(s[:-1]) * 24 * 60 * 60
+    if s.endswith('w'):
+        return int(s[:-1]) * 7 * 24 * 60 * 60
+    raise Exception("Unable to parse time value: %s" % s)
+
+
+def normalizeCategory(name):
+    name = name.lower()
+    return re.sub(' ', '-', name)
+
+
+class Pipeline(object):
+    """A top-level pipeline such as check, gate, post, etc."""
+    def __init__(self, name):
+        self.name = name
+        self.description = None
+        self.failure_message = None
+        self.merge_failure_message = None
+        self.success_message = None
+        self.footer_message = None
+        self.dequeue_on_new_patchset = True
+        self.ignore_dependencies = False
+        self.job_trees = {}  # project -> JobTree
+        self.manager = None
+        self.queues = []
+        self.precedence = PRECEDENCE_NORMAL
+        self.source = None
+        self.start_actions = []
+        self.success_actions = []
+        self.failure_actions = []
+        self.merge_failure_actions = []
+        self.disabled_actions = []
+        self.disable_at = None
+        self._consecutive_failures = 0
+        self._disabled = False
+        self.window = None
+        self.window_floor = None
+        self.window_increase_type = None
+        self.window_increase_factor = None
+        self.window_decrease_type = None
+        self.window_decrease_factor = None
+
+    def __repr__(self):
+        return '<Pipeline %s>' % self.name
+
+    def setManager(self, manager):
+        self.manager = manager
+
+    def addProject(self, project):
+        job_tree = JobTree(None)  # Null job == job tree root
+        self.job_trees[project] = job_tree
+        return job_tree
+
+    def getProjects(self):
+        return sorted(self.job_trees.keys(), lambda a, b: cmp(a.name, b.name))
+
+    def addQueue(self, queue):
+        self.queues.append(queue)
+
+    def getQueue(self, project):
+        for queue in self.queues:
+            if project in queue.projects:
+                return queue
+        return None
+
+    def removeQueue(self, queue):
+        self.queues.remove(queue)
+
+    def getJobTree(self, project):
+        tree = self.job_trees.get(project)
+        return tree
+
+    def getJobs(self, item):
+        if not item.live:
+            return []
+        tree = self.getJobTree(item.change.project)
+        if not tree:
+            return []
+        return item.change.filterJobs(tree.getJobs())
+
+    def _findJobsToRun(self, job_trees, item, mutex):
+        torun = []
+        if item.item_ahead:
+            # Only run jobs if any 'hold' jobs on the change ahead
+            # have completed successfully.
+            if self.isHoldingFollowingChanges(item.item_ahead):
+                return []
+        for tree in job_trees:
+            job = tree.job
+            result = None
+            if job:
+                if not job.changeMatches(item.change):
+                    continue
+                build = item.current_build_set.getBuild(job.name)
+                if build:
+                    result = build.result
+                else:
+                    # There is no build for the root of this job tree,
+                    # so we should run it.
+                    if mutex.acquire(item, job):
+                        # If this job needs a mutex, either acquire it or make
+                        # sure that we have it before running the job.
+                        torun.append(job)
+            # If there is no job, this is a null job tree, and we should
+            # run all of its jobs.
+            if result == 'SUCCESS' or not job:
+                torun.extend(self._findJobsToRun(tree.job_trees, item, mutex))
+        return torun
+
+    def findJobsToRun(self, item, mutex):
+        if not item.live:
+            return []
+        tree = self.getJobTree(item.change.project)
+        if not tree:
+            return []
+        return self._findJobsToRun(tree.job_trees, item, mutex)
+
+    def haveAllJobsStarted(self, item):
+        for job in self.getJobs(item):
+            build = item.current_build_set.getBuild(job.name)
+            if not build or not build.start_time:
+                return False
+        return True
+
+    def areAllJobsComplete(self, item):
+        for job in self.getJobs(item):
+            build = item.current_build_set.getBuild(job.name)
+            if not build or not build.result:
+                return False
+        return True
+
+    def didAllJobsSucceed(self, item):
+        for job in self.getJobs(item):
+            if not job.voting:
+                continue
+            build = item.current_build_set.getBuild(job.name)
+            if not build:
+                return False
+            if build.result != 'SUCCESS':
+                return False
+        return True
+
+    def didMergerSucceed(self, item):
+        if item.current_build_set.unable_to_merge:
+            return False
+        return True
+
+    def didAnyJobFail(self, item):
+        for job in self.getJobs(item):
+            if not job.voting:
+                continue
+            build = item.current_build_set.getBuild(job.name)
+            if build and build.result and (build.result != 'SUCCESS'):
+                return True
+        return False
+
+    def isHoldingFollowingChanges(self, item):
+        if not item.live:
+            return False
+        for job in self.getJobs(item):
+            if not job.hold_following_changes:
+                continue
+            build = item.current_build_set.getBuild(job.name)
+            if not build:
+                return True
+            if build.result != 'SUCCESS':
+                return True
+
+        if not item.item_ahead:
+            return False
+        return self.isHoldingFollowingChanges(item.item_ahead)
+
+    def setResult(self, item, build):
+        if build.retry:
+            item.removeBuild(build)
+        elif build.result != 'SUCCESS':
+            # Get a JobTree from a Job so we can find only its dependent jobs
+            root = self.getJobTree(item.change.project)
+            tree = root.getJobTreeForJob(build.job)
+            for job in tree.getJobs():
+                fakebuild = Build(job, None)
+                fakebuild.result = 'SKIPPED'
+                item.addBuild(fakebuild)
+
+    def setUnableToMerge(self, item):
+        item.current_build_set.unable_to_merge = True
+        root = self.getJobTree(item.change.project)
+        for job in root.getJobs():
+            fakebuild = Build(job, None)
+            fakebuild.result = 'SKIPPED'
+            item.addBuild(fakebuild)
+
+    def setDequeuedNeedingChange(self, item):
+        item.dequeued_needing_change = True
+        root = self.getJobTree(item.change.project)
+        for job in root.getJobs():
+            fakebuild = Build(job, None)
+            fakebuild.result = 'SKIPPED'
+            item.addBuild(fakebuild)
+
+    def getChangesInQueue(self):
+        changes = []
+        for shared_queue in self.queues:
+            changes.extend([x.change for x in shared_queue.queue])
+        return changes
+
+    def getAllItems(self):
+        items = []
+        for shared_queue in self.queues:
+            items.extend(shared_queue.queue)
+        return items
+
+    def formatStatusJSON(self, url_pattern=None):
+        j_pipeline = dict(name=self.name,
+                          description=self.description)
+        j_queues = []
+        j_pipeline['change_queues'] = j_queues
+        for queue in self.queues:
+            j_queue = dict(name=queue.name)
+            j_queues.append(j_queue)
+            j_queue['heads'] = []
+            j_queue['window'] = queue.window
+
+            j_changes = []
+            for e in queue.queue:
+                if not e.item_ahead:
+                    if j_changes:
+                        j_queue['heads'].append(j_changes)
+                    j_changes = []
+                j_changes.append(e.formatJSON(url_pattern))
+                if (len(j_changes) > 1 and
+                        (j_changes[-2]['remaining_time'] is not None) and
+                        (j_changes[-1]['remaining_time'] is not None)):
+                    j_changes[-1]['remaining_time'] = max(
+                        j_changes[-2]['remaining_time'],
+                        j_changes[-1]['remaining_time'])
+            if j_changes:
+                j_queue['heads'].append(j_changes)
+        return j_pipeline
+
+
+class ChangeQueue(object):
+    """DependentPipelines have multiple parallel queues shared by
+    different projects; this is one of them.  For instance, there may
+    a queue shared by interrelated projects foo and bar, and a second
+    queue for independent project baz.  Pipelines have one or more
+    ChangeQueues."""
+    def __init__(self, pipeline, window=0, window_floor=1,
+                 window_increase_type='linear', window_increase_factor=1,
+                 window_decrease_type='exponential', window_decrease_factor=2):
+        self.pipeline = pipeline
+        self.name = ''
+        self.assigned_name = None
+        self.generated_name = None
+        self.projects = []
+        self._jobs = set()
+        self.queue = []
+        self.window = window
+        self.window_floor = window_floor
+        self.window_increase_type = window_increase_type
+        self.window_increase_factor = window_increase_factor
+        self.window_decrease_type = window_decrease_type
+        self.window_decrease_factor = window_decrease_factor
+
+    def __repr__(self):
+        return '<ChangeQueue %s: %s>' % (self.pipeline.name, self.name)
+
+    def getJobs(self):
+        return self._jobs
+
+    def addProject(self, project):
+        if project not in self.projects:
+            self.projects.append(project)
+            self._jobs |= set(self.pipeline.getJobTree(project).getJobs())
+
+            names = [x.name for x in self.projects]
+            names.sort()
+            self.generated_name = ', '.join(names)
+
+            for job in self._jobs:
+                if job.queue_name:
+                    if (self.assigned_name and
+                            job.queue_name != self.assigned_name):
+                        raise Exception("More than one name assigned to "
+                                        "change queue: %s != %s" %
+                                        (self.assigned_name, job.queue_name))
+                    self.assigned_name = job.queue_name
+            self.name = self.assigned_name or self.generated_name
+
+    def enqueueChange(self, change):
+        item = QueueItem(self, change)
+        self.enqueueItem(item)
+        item.enqueue_time = time.time()
+        return item
+
+    def enqueueItem(self, item):
+        item.pipeline = self.pipeline
+        item.queue = self
+        if self.queue:
+            item.item_ahead = self.queue[-1]
+            item.item_ahead.items_behind.append(item)
+        self.queue.append(item)
+
+    def dequeueItem(self, item):
+        if item in self.queue:
+            self.queue.remove(item)
+        if item.item_ahead:
+            item.item_ahead.items_behind.remove(item)
+        for item_behind in item.items_behind:
+            if item.item_ahead:
+                item.item_ahead.items_behind.append(item_behind)
+            item_behind.item_ahead = item.item_ahead
+        item.item_ahead = None
+        item.items_behind = []
+        item.dequeue_time = time.time()
+
+    def moveItem(self, item, item_ahead):
+        if item.item_ahead == item_ahead:
+            return False
+        # Remove from current location
+        if item.item_ahead:
+            item.item_ahead.items_behind.remove(item)
+        for item_behind in item.items_behind:
+            if item.item_ahead:
+                item.item_ahead.items_behind.append(item_behind)
+            item_behind.item_ahead = item.item_ahead
+        # Add to new location
+        item.item_ahead = item_ahead
+        item.items_behind = []
+        if item.item_ahead:
+            item.item_ahead.items_behind.append(item)
+        return True
+
+    def mergeChangeQueue(self, other):
+        for project in other.projects:
+            self.addProject(project)
+        self.window = min(self.window, other.window)
+        # TODO merge semantics
+
+    def isActionable(self, item):
+        if self.window:
+            return item in self.queue[:self.window]
+        else:
+            return True
+
+    def increaseWindowSize(self):
+        if self.window:
+            if self.window_increase_type == 'linear':
+                self.window += self.window_increase_factor
+            elif self.window_increase_type == 'exponential':
+                self.window *= self.window_increase_factor
+
+    def decreaseWindowSize(self):
+        if self.window:
+            if self.window_decrease_type == 'linear':
+                self.window = max(
+                    self.window_floor,
+                    self.window - self.window_decrease_factor)
+            elif self.window_decrease_type == 'exponential':
+                self.window = max(
+                    self.window_floor,
+                    self.window / self.window_decrease_factor)
+
+
+class Project(object):
+    def __init__(self, name, foreign=False):
+        self.name = name
+        self.merge_mode = MERGER_MERGE_RESOLVE
+        # foreign projects are those referenced in dependencies
+        # of layout projects, this should matter
+        # when deciding whether to enqueue their changes
+        self.foreign = foreign
+
+    def __str__(self):
+        return self.name
+
+    def __repr__(self):
+        return '<Project %s>' % (self.name)
+
+
+class Job(object):
+    def __init__(self, name):
+        # If you add attributes here, be sure to add them to the copy method.
+        self.name = name
+        self.queue_name = None
+        self.failure_message = None
+        self.success_message = None
+        self.failure_pattern = None
+        self.success_pattern = None
+        self.parameter_function = None
+        self.tags = set()
+        self.mutex = None
+        # A metajob should only supply values for attributes that have
+        # been explicitly provided, so avoid setting boolean defaults.
+        if self.is_metajob:
+            self.hold_following_changes = None
+            self.voting = None
+        else:
+            self.hold_following_changes = False
+            self.voting = True
+        self.branches = []
+        self._branches = []
+        self.files = []
+        self._files = []
+        self.skip_if_matcher = None
+        self.swift = {}
+
+    def __str__(self):
+        return self.name
+
+    def __repr__(self):
+        return '<Job %s>' % (self.name)
+
+    @property
+    def is_metajob(self):
+        return self.name.startswith('^')
+
+    def copy(self, other):
+        if other.failure_message:
+            self.failure_message = other.failure_message
+        if other.success_message:
+            self.success_message = other.success_message
+        if other.failure_pattern:
+            self.failure_pattern = other.failure_pattern
+        if other.success_pattern:
+            self.success_pattern = other.success_pattern
+        if other.parameter_function:
+            self.parameter_function = other.parameter_function
+        if other.branches:
+            self.branches = other.branches[:]
+            self._branches = other._branches[:]
+        if other.files:
+            self.files = other.files[:]
+            self._files = other._files[:]
+        if other.skip_if_matcher:
+            self.skip_if_matcher = other.skip_if_matcher.copy()
+        if other.swift:
+            self.swift.update(other.swift)
+        if other.mutex:
+            self.mutex = other.mutex
+        # Tags are merged via a union rather than a destructive copy
+        # because they are intended to accumulate as metajobs are
+        # applied.
+        if other.tags:
+            self.tags = self.tags.union(other.tags)
+        # Only non-None values should be copied for boolean attributes.
+        if other.hold_following_changes is not None:
+            self.hold_following_changes = other.hold_following_changes
+        if other.voting is not None:
+            self.voting = other.voting
+
+    def changeMatches(self, change):
+        matches_branch = False
+        for branch in self.branches:
+            if hasattr(change, 'branch') and branch.match(change.branch):
+                matches_branch = True
+            if hasattr(change, 'ref') and branch.match(change.ref):
+                matches_branch = True
+        if self.branches and not matches_branch:
+            return False
+
+        matches_file = False
+        for f in self.files:
+            if hasattr(change, 'files'):
+                for cf in change.files:
+                    if f.match(cf):
+                        matches_file = True
+        if self.files and not matches_file:
+            return False
+
+        if self.skip_if_matcher and self.skip_if_matcher.matches(change):
+            return False
+
+        return True
+
+
+class JobTree(object):
+    """ A JobTree represents an instance of one Job, and holds JobTrees
+    whose jobs should be run if that Job succeeds.  A root node of a
+    JobTree will have no associated Job. """
+
+    def __init__(self, job):
+        self.job = job
+        self.job_trees = []
+
+    def addJob(self, job):
+        if job not in [x.job for x in self.job_trees]:
+            t = JobTree(job)
+            self.job_trees.append(t)
+            return t
+        for tree in self.job_trees:
+            if tree.job == job:
+                return tree
+
+    def getJobs(self):
+        jobs = []
+        for x in self.job_trees:
+            jobs.append(x.job)
+            jobs.extend(x.getJobs())
+        return jobs
+
+    def getJobTreeForJob(self, job):
+        if self.job == job:
+            return self
+        for tree in self.job_trees:
+            ret = tree.getJobTreeForJob(job)
+            if ret:
+                return ret
+        return None
+
+
+class Build(object):
+    def __init__(self, job, uuid):
+        self.job = job
+        self.uuid = uuid
+        self.url = None
+        self.number = None
+        self.result = None
+        self.build_set = None
+        self.launch_time = time.time()
+        self.start_time = None
+        self.end_time = None
+        self.estimated_time = None
+        self.pipeline = None
+        self.canceled = False
+        self.retry = False
+        self.parameters = {}
+        self.worker = Worker()
+        self.node_labels = []
+        self.node_name = None
+
+    def __repr__(self):
+        return ('<Build %s of %s on %s>' %
+                (self.uuid, self.job.name, self.worker))
+
+
+class Worker(object):
+    """A model of the worker running a job"""
+    def __init__(self):
+        self.name = "Unknown"
+        self.hostname = None
+        self.ips = []
+        self.fqdn = None
+        self.program = None
+        self.version = None
+        self.extra = {}
+
+    def updateFromData(self, data):
+        """Update worker information if contained in the WORK_DATA response."""
+        self.name = data.get('worker_name', self.name)
+        self.hostname = data.get('worker_hostname', self.hostname)
+        self.ips = data.get('worker_ips', self.ips)
+        self.fqdn = data.get('worker_fqdn', self.fqdn)
+        self.program = data.get('worker_program', self.program)
+        self.version = data.get('worker_version', self.version)
+        self.extra = data.get('worker_extra', self.extra)
+
+    def __repr__(self):
+        return '<Worker %s>' % self.name
+
+
+class BuildSet(object):
+    # Merge states:
+    NEW = 1
+    PENDING = 2
+    COMPLETE = 3
+
+    states_map = {
+        1: 'NEW',
+        2: 'PENDING',
+        3: 'COMPLETE',
+    }
+
+    def __init__(self, item):
+        self.item = item
+        self.other_changes = []
+        self.builds = {}
+        self.result = None
+        self.next_build_set = None
+        self.previous_build_set = None
+        self.ref = None
+        self.commit = None
+        self.zuul_url = None
+        self.unable_to_merge = False
+        self.failing_reasons = []
+        self.merge_state = self.NEW
+
+    def __repr__(self):
+        return '<BuildSet item: %s #builds: %s merge state: %s>' % (
+            self.item,
+            len(self.builds),
+            self.getStateName(self.merge_state))
+
+    def setConfiguration(self):
+        # The change isn't enqueued until after it's created
+        # so we don't know what the other changes ahead will be
+        # until jobs start.
+        if not self.other_changes:
+            next_item = self.item.item_ahead
+            while next_item:
+                self.other_changes.append(next_item.change)
+                next_item = next_item.item_ahead
+        if not self.ref:
+            self.ref = 'Z' + uuid4().hex
+
+    def getStateName(self, state_num):
+        return self.states_map.get(
+            state_num, 'UNKNOWN (%s)' % state_num)
+
+    def addBuild(self, build):
+        self.builds[build.job.name] = build
+        build.build_set = self
+
+    def removeBuild(self, build):
+        del self.builds[build.job.name]
+
+    def getBuild(self, job_name):
+        return self.builds.get(job_name)
+
+    def getBuilds(self):
+        keys = self.builds.keys()
+        keys.sort()
+        return [self.builds.get(x) for x in keys]
+
+
+class QueueItem(object):
+    """A changish inside of a Pipeline queue"""
+
+    def __init__(self, queue, change):
+        self.pipeline = queue.pipeline
+        self.queue = queue
+        self.change = change  # a changeish
+        self.build_sets = []
+        self.dequeued_needing_change = False
+        self.current_build_set = BuildSet(self)
+        self.build_sets.append(self.current_build_set)
+        self.item_ahead = None
+        self.items_behind = []
+        self.enqueue_time = None
+        self.dequeue_time = None
+        self.reported = False
+        self.active = False  # Whether an item is within an active window
+        self.live = True  # Whether an item is intended to be processed at all
+
+    def __repr__(self):
+        if self.pipeline:
+            pipeline = self.pipeline.name
+        else:
+            pipeline = None
+        return '<QueueItem 0x%x for %s in %s>' % (
+            id(self), self.change, pipeline)
+
+    def resetAllBuilds(self):
+        old = self.current_build_set
+        self.current_build_set.result = 'CANCELED'
+        self.current_build_set = BuildSet(self)
+        old.next_build_set = self.current_build_set
+        self.current_build_set.previous_build_set = old
+        self.build_sets.append(self.current_build_set)
+
+    def addBuild(self, build):
+        self.current_build_set.addBuild(build)
+        build.pipeline = self.pipeline
+
+    def removeBuild(self, build):
+        self.current_build_set.removeBuild(build)
+
+    def setReportedResult(self, result):
+        self.current_build_set.result = result
+
+    def formatJobResult(self, job, url_pattern=None):
+        build = self.current_build_set.getBuild(job.name)
+        result = build.result
+        pattern = url_pattern
+        if result == 'SUCCESS':
+            if job.success_message:
+                result = job.success_message
+            if job.success_pattern:
+                pattern = job.success_pattern
+        elif result == 'FAILURE':
+            if job.failure_message:
+                result = job.failure_message
+            if job.failure_pattern:
+                pattern = job.failure_pattern
+        url = None
+        if pattern:
+            try:
+                url = pattern.format(change=self.change,
+                                     pipeline=self.pipeline,
+                                     job=job,
+                                     build=build)
+            except Exception:
+                pass  # FIXME: log this or something?
+        if not url:
+            url = build.url or job.name
+        return (result, url)
+
+    def formatJSON(self, url_pattern=None):
+        changeish = self.change
+        ret = {}
+        ret['active'] = self.active
+        ret['live'] = self.live
+        if hasattr(changeish, 'url') and changeish.url is not None:
+            ret['url'] = changeish.url
+        else:
+            ret['url'] = None
+        ret['id'] = changeish._id()
+        if self.item_ahead:
+            ret['item_ahead'] = self.item_ahead.change._id()
+        else:
+            ret['item_ahead'] = None
+        ret['items_behind'] = [i.change._id() for i in self.items_behind]
+        ret['failing_reasons'] = self.current_build_set.failing_reasons
+        ret['zuul_ref'] = self.current_build_set.ref
+        if changeish.project:
+            ret['project'] = changeish.project.name
+        else:
+            # For cross-project dependencies with the depends-on
+            # project not known to zuul, the project is None
+            # Set it to a static value
+            ret['project'] = "Unknown Project"
+        ret['enqueue_time'] = int(self.enqueue_time * 1000)
+        ret['jobs'] = []
+        if hasattr(changeish, 'owner'):
+            ret['owner'] = changeish.owner
+        else:
+            ret['owner'] = None
+        max_remaining = 0
+        for job in self.pipeline.getJobs(self):
+            now = time.time()
+            build = self.current_build_set.getBuild(job.name)
+            elapsed = None
+            remaining = None
+            result = None
+            build_url = None
+            report_url = None
+            worker = None
+            if build:
+                result = build.result
+                build_url = build.url
+                (unused, report_url) = self.formatJobResult(job, url_pattern)
+                if build.start_time:
+                    if build.end_time:
+                        elapsed = int((build.end_time -
+                                       build.start_time) * 1000)
+                        remaining = 0
+                    else:
+                        elapsed = int((now - build.start_time) * 1000)
+                        if build.estimated_time:
+                            remaining = max(
+                                int(build.estimated_time * 1000) - elapsed,
+                                0)
+                worker = {
+                    'name': build.worker.name,
+                    'hostname': build.worker.hostname,
+                    'ips': build.worker.ips,
+                    'fqdn': build.worker.fqdn,
+                    'program': build.worker.program,
+                    'version': build.worker.version,
+                    'extra': build.worker.extra
+                }
+            if remaining and remaining > max_remaining:
+                max_remaining = remaining
+
+            ret['jobs'].append({
+                'name': job.name,
+                'elapsed_time': elapsed,
+                'remaining_time': remaining,
+                'url': build_url,
+                'report_url': report_url,
+                'result': result,
+                'voting': job.voting,
+                'uuid': build.uuid if build else None,
+                'launch_time': build.launch_time if build else None,
+                'start_time': build.start_time if build else None,
+                'end_time': build.end_time if build else None,
+                'estimated_time': build.estimated_time if build else None,
+                'pipeline': build.pipeline.name if build else None,
+                'canceled': build.canceled if build else None,
+                'retry': build.retry if build else None,
+                'number': build.number if build else None,
+                'node_labels': build.node_labels if build else [],
+                'node_name': build.node_name if build else None,
+                'worker': worker,
+            })
+
+        if self.pipeline.haveAllJobsStarted(self):
+            ret['remaining_time'] = max_remaining
+        else:
+            ret['remaining_time'] = None
+        return ret
+
+    def formatStatus(self, indent=0, html=False):
+        changeish = self.change
+        indent_str = ' ' * indent
+        ret = ''
+        if html and hasattr(changeish, 'url') and changeish.url is not None:
+            ret += '%sProject %s change <a href="%s">%s</a>\n' % (
+                indent_str,
+                changeish.project.name,
+                changeish.url,
+                changeish._id())
+        else:
+            ret += '%sProject %s change %s based on %s\n' % (
+                indent_str,
+                changeish.project.name,
+                changeish._id(),
+                self.item_ahead)
+        for job in self.pipeline.getJobs(self):
+            build = self.current_build_set.getBuild(job.name)
+            if build:
+                result = build.result
+            else:
+                result = None
+            job_name = job.name
+            if not job.voting:
+                voting = ' (non-voting)'
+            else:
+                voting = ''
+            if html:
+                if build:
+                    url = build.url
+                else:
+                    url = None
+                if url is not None:
+                    job_name = '<a href="%s">%s</a>' % (url, job_name)
+            ret += '%s  %s: %s%s' % (indent_str, job_name, result, voting)
+            ret += '\n'
+        return ret
+
+
+class Changeish(object):
+    """Something like a change; either a change or a ref"""
+
+    def __init__(self, project):
+        self.project = project
+
+    def getBasePath(self):
+        base_path = ''
+        if hasattr(self, 'refspec'):
+            base_path = "%s/%s/%s" % (
+                self.number[-2:], self.number, self.patchset)
+        elif hasattr(self, 'ref'):
+            base_path = "%s/%s" % (self.newrev[:2], self.newrev)
+
+        return base_path
+
+    def equals(self, other):
+        raise NotImplementedError()
+
+    def isUpdateOf(self, other):
+        raise NotImplementedError()
+
+    def filterJobs(self, jobs):
+        return filter(lambda job: job.changeMatches(self), jobs)
+
+    def getRelatedChanges(self):
+        return set()
+
+
+class Change(Changeish):
+    def __init__(self, project):
+        super(Change, self).__init__(project)
+        self.branch = None
+        self.number = None
+        self.url = None
+        self.patchset = None
+        self.refspec = None
+
+        self.files = []
+        self.needs_changes = []
+        self.needed_by_changes = []
+        self.is_current_patchset = True
+        self.can_merge = False
+        self.is_merged = False
+        self.failed_to_merge = False
+        self.approvals = []
+        self.open = None
+        self.status = None
+        self.owner = None
+
+    def _id(self):
+        return '%s,%s' % (self.number, self.patchset)
+
+    def __repr__(self):
+        return '<Change 0x%x %s>' % (id(self), self._id())
+
+    def equals(self, other):
+        if self.number == other.number and self.patchset == other.patchset:
+            return True
+        return False
+
+    def isUpdateOf(self, other):
+        if ((hasattr(other, 'number') and self.number == other.number) and
+            (hasattr(other, 'patchset') and
+             self.patchset is not None and
+             other.patchset is not None and
+             int(self.patchset) > int(other.patchset))):
+            return True
+        return False
+
+    def getRelatedChanges(self):
+        related = set()
+        for c in self.needs_changes:
+            related.add(c)
+        for c in self.needed_by_changes:
+            related.add(c)
+            related.update(c.getRelatedChanges())
+        return related
+
+
+class Ref(Changeish):
+    def __init__(self, project):
+        super(Ref, self).__init__(project)
+        self.ref = None
+        self.oldrev = None
+        self.newrev = None
+
+    def _id(self):
+        return self.newrev
+
+    def __repr__(self):
+        rep = None
+        if self.newrev == '0000000000000000000000000000000000000000':
+            rep = '<Ref 0x%x deletes %s from %s' % (
+                  id(self), self.ref, self.oldrev)
+        elif self.oldrev == '0000000000000000000000000000000000000000':
+            rep = '<Ref 0x%x creates %s on %s>' % (
+                  id(self), self.ref, self.newrev)
+        else:
+            # Catch all
+            rep = '<Ref 0x%x %s updated %s..%s>' % (
+                  id(self), self.ref, self.oldrev, self.newrev)
+
+        return rep
+
+    def equals(self, other):
+        if (self.project == other.project
+            and self.ref == other.ref
+            and self.newrev == other.newrev):
+            return True
+        return False
+
+    def isUpdateOf(self, other):
+        return False
+
+
+class NullChange(Changeish):
+    def __repr__(self):
+        return '<NullChange for %s>' % (self.project)
+
+    def _id(self):
+        return None
+
+    def equals(self, other):
+        if (self.project == other.project
+            and other._id() is None):
+            return True
+        return False
+
+    def isUpdateOf(self, other):
+        return False
+
+
+class TriggerEvent(object):
+    def __init__(self):
+        self.data = None
+        # common
+        self.type = None
+        self.project_name = None
+        self.trigger_name = None
+        # Representation of the user account that performed the event.
+        self.account = None
+        # patchset-created, comment-added, etc.
+        self.change_number = None
+        self.change_url = None
+        self.patch_number = None
+        self.refspec = None
+        self.approvals = []
+        self.branch = None
+        self.comment = None
+        # ref-updated
+        self.ref = None
+        self.oldrev = None
+        self.newrev = None
+        # timer
+        self.timespec = None
+        # zuultrigger
+        self.pipeline_name = None
+        # For events that arrive with a destination pipeline (eg, from
+        # an admin command, etc):
+        self.forced_pipeline = None
+
+    def __repr__(self):
+        ret = '<TriggerEvent %s %s' % (self.type, self.project_name)
+
+        if self.branch:
+            ret += " %s" % self.branch
+        if self.change_number:
+            ret += " %s,%s" % (self.change_number, self.patch_number)
+        if self.approvals:
+            ret += ' ' + ', '.join(
+                ['%s:%s' % (a['type'], a['value']) for a in self.approvals])
+        ret += '>'
+
+        return ret
+
+
+class BaseFilter(object):
+    def __init__(self, required_approvals=[], reject_approvals=[]):
+        self._required_approvals = copy.deepcopy(required_approvals)
+        self.required_approvals = self._tidy_approvals(required_approvals)
+        self._reject_approvals = copy.deepcopy(reject_approvals)
+        self.reject_approvals = self._tidy_approvals(reject_approvals)
+
+    def _tidy_approvals(self, approvals):
+        for a in approvals:
+            for k, v in a.items():
+                if k == 'username':
+                    pass
+                elif k in ['email', 'email-filter']:
+                    a['email'] = re.compile(v)
+                elif k == 'newer-than':
+                    a[k] = time_to_seconds(v)
+                elif k == 'older-than':
+                    a[k] = time_to_seconds(v)
+            if 'email-filter' in a:
+                del a['email-filter']
+        return approvals
+
+    def _match_approval_required_approval(self, rapproval, approval):
+        # Check if the required approval and approval match
+        if 'description' not in approval:
+            return False
+        now = time.time()
+        by = approval.get('by', {})
+        for k, v in rapproval.items():
+            if k == 'username':
+                if (by.get('username', '') != v):
+                        return False
+            elif k == 'email':
+                if (not v.search(by.get('email', ''))):
+                        return False
+            elif k == 'newer-than':
+                t = now - v
+                if (approval['grantedOn'] < t):
+                        return False
+            elif k == 'older-than':
+                t = now - v
+                if (approval['grantedOn'] >= t):
+                    return False
+            else:
+                if not isinstance(v, list):
+                    v = [v]
+                if (normalizeCategory(approval['description']) != k or
+                        int(approval['value']) not in v):
+                    return False
+        return True
+
+    def matchesApprovals(self, change):
+        if (self.required_approvals and not change.approvals
+                or self.reject_approvals and not change.approvals):
+            # A change with no approvals can not match
+            return False
+
+        # TODO(jhesketh): If we wanted to optimise this slightly we could
+        # analyse both the REQUIRE and REJECT filters by looping over the
+        # approvals on the change and keeping track of what we have checked
+        # rather than needing to loop on the change approvals twice
+        return (self.matchesRequiredApprovals(change) and
+                self.matchesNoRejectApprovals(change))
+
+    def matchesRequiredApprovals(self, change):
+        # Check if any approvals match the requirements
+        for rapproval in self.required_approvals:
+            matches_rapproval = False
+            for approval in change.approvals:
+                if self._match_approval_required_approval(rapproval, approval):
+                    # We have a matching approval so this requirement is
+                    # fulfilled
+                    matches_rapproval = True
+                    break
+            if not matches_rapproval:
+                return False
+        return True
+
+    def matchesNoRejectApprovals(self, change):
+        # Check to make sure no approvals match a reject criteria
+        for rapproval in self.reject_approvals:
+            for approval in change.approvals:
+                if self._match_approval_required_approval(rapproval, approval):
+                    # A reject approval has been matched, so we reject
+                    # immediately
+                    return False
+        # To get here no rejects can have been matched so we should be good to
+        # queue
+        return True
+
+
+class EventFilter(BaseFilter):
+    def __init__(self, trigger, types=[], branches=[], refs=[],
+                 event_approvals={}, comments=[], emails=[], usernames=[],
+                 timespecs=[], required_approvals=[], reject_approvals=[],
+                 pipelines=[], ignore_deletes=True):
+        super(EventFilter, self).__init__(
+            required_approvals=required_approvals,
+            reject_approvals=reject_approvals)
+        self.trigger = trigger
+        self._types = types
+        self._branches = branches
+        self._refs = refs
+        self._comments = comments
+        self._emails = emails
+        self._usernames = usernames
+        self._pipelines = pipelines
+        self.types = [re.compile(x) for x in types]
+        self.branches = [re.compile(x) for x in branches]
+        self.refs = [re.compile(x) for x in refs]
+        self.comments = [re.compile(x) for x in comments]
+        self.emails = [re.compile(x) for x in emails]
+        self.usernames = [re.compile(x) for x in usernames]
+        self.pipelines = [re.compile(x) for x in pipelines]
+        self.event_approvals = event_approvals
+        self.timespecs = timespecs
+        self.ignore_deletes = ignore_deletes
+
+    def __repr__(self):
+        ret = '<EventFilter'
+
+        if self._types:
+            ret += ' types: %s' % ', '.join(self._types)
+        if self._pipelines:
+            ret += ' pipelines: %s' % ', '.join(self._pipelines)
+        if self._branches:
+            ret += ' branches: %s' % ', '.join(self._branches)
+        if self._refs:
+            ret += ' refs: %s' % ', '.join(self._refs)
+        if self.ignore_deletes:
+            ret += ' ignore_deletes: %s' % self.ignore_deletes
+        if self.event_approvals:
+            ret += ' event_approvals: %s' % ', '.join(
+                ['%s:%s' % a for a in self.event_approvals.items()])
+        if self.required_approvals:
+            ret += ' required_approvals: %s' % ', '.join(
+                ['%s' % a for a in self._required_approvals])
+        if self.reject_approvals:
+            ret += ' reject_approvals: %s' % ', '.join(
+                ['%s' % a for a in self._reject_approvals])
+        if self._comments:
+            ret += ' comments: %s' % ', '.join(self._comments)
+        if self._emails:
+            ret += ' emails: %s' % ', '.join(self._emails)
+        if self._usernames:
+            ret += ' username_filters: %s' % ', '.join(self._usernames)
+        if self.timespecs:
+            ret += ' timespecs: %s' % ', '.join(self.timespecs)
+        ret += '>'
+
+        return ret
+
+    def matches(self, event, change):
+        # event types are ORed
+        matches_type = False
+        for etype in self.types:
+            if etype.match(event.type):
+                matches_type = True
+        if self.types and not matches_type:
+            return False
+
+        # pipelines are ORed
+        matches_pipeline = False
+        for epipe in self.pipelines:
+            if epipe.match(event.pipeline_name):
+                matches_pipeline = True
+        if self.pipelines and not matches_pipeline:
+            return False
+
+        # branches are ORed
+        matches_branch = False
+        for branch in self.branches:
+            if branch.match(event.branch):
+                matches_branch = True
+        if self.branches and not matches_branch:
+            return False
+
+        # refs are ORed
+        matches_ref = False
+        if event.ref is not None:
+            for ref in self.refs:
+                if ref.match(event.ref):
+                    matches_ref = True
+        if self.refs and not matches_ref:
+            return False
+        if self.ignore_deletes and event.newrev == EMPTY_GIT_REF:
+            # If the updated ref has an empty git sha (all 0s),
+            # then the ref is being deleted
+            return False
+
+        # comments are ORed
+        matches_comment_re = False
+        for comment_re in self.comments:
+            if (event.comment is not None and
+                comment_re.search(event.comment)):
+                matches_comment_re = True
+        if self.comments and not matches_comment_re:
+            return False
+
+        # We better have an account provided by Gerrit to do
+        # email filtering.
+        if event.account is not None:
+            account_email = event.account.get('email')
+            # emails are ORed
+            matches_email_re = False
+            for email_re in self.emails:
+                if (account_email is not None and
+                        email_re.search(account_email)):
+                    matches_email_re = True
+            if self.emails and not matches_email_re:
+                return False
+
+            # usernames are ORed
+            account_username = event.account.get('username')
+            matches_username_re = False
+            for username_re in self.usernames:
+                if (account_username is not None and
+                    username_re.search(account_username)):
+                    matches_username_re = True
+            if self.usernames and not matches_username_re:
+                return False
+
+        # approvals are ANDed
+        for category, value in self.event_approvals.items():
+            matches_approval = False
+            for eapproval in event.approvals:
+                if (normalizeCategory(eapproval['description']) == category and
+                    int(eapproval['value']) == int(value)):
+                    matches_approval = True
+            if not matches_approval:
+                return False
+
+        # required approvals are ANDed (reject approvals are ORed)
+        if not self.matchesApprovals(change):
+            return False
+
+        # timespecs are ORed
+        matches_timespec = False
+        for timespec in self.timespecs:
+            if (event.timespec == timespec):
+                matches_timespec = True
+        if self.timespecs and not matches_timespec:
+            return False
+
+        return True
+
+
+class ChangeishFilter(BaseFilter):
+    def __init__(self, open=None, current_patchset=None,
+                 statuses=[], required_approvals=[],
+                 reject_approvals=[]):
+        super(ChangeishFilter, self).__init__(
+            required_approvals=required_approvals,
+            reject_approvals=reject_approvals)
+        self.open = open
+        self.current_patchset = current_patchset
+        self.statuses = statuses
+
+    def __repr__(self):
+        ret = '<ChangeishFilter'
+
+        if self.open is not None:
+            ret += ' open: %s' % self.open
+        if self.current_patchset is not None:
+            ret += ' current-patchset: %s' % self.current_patchset
+        if self.statuses:
+            ret += ' statuses: %s' % ', '.join(self.statuses)
+        if self.required_approvals:
+            ret += (' required_approvals: %s' %
+                    str(self.required_approvals))
+        if self.reject_approvals:
+            ret += (' reject_approvals: %s' %
+                    str(self.reject_approvals))
+        ret += '>'
+
+        return ret
+
+    def matches(self, change):
+        if self.open is not None:
+            if self.open != change.open:
+                return False
+
+        if self.current_patchset is not None:
+            if self.current_patchset != change.is_current_patchset:
+                return False
+
+        if self.statuses:
+            if change.status not in self.statuses:
+                return False
+
+        # required approvals are ANDed (reject approvals are ORed)
+        if not self.matchesApprovals(change):
+            return False
+
+        return True
+
+
+class Layout(object):
+    def __init__(self):
+        self.projects = {}
+        self.pipelines = OrderedDict()
+        self.jobs = {}
+        self.metajobs = []
+
+    def getJob(self, name):
+        if name in self.jobs:
+            return self.jobs[name]
+        job = Job(name)
+        if job.is_metajob:
+            regex = re.compile(name)
+            self.metajobs.append((regex, job))
+        else:
+            # Apply attributes from matching meta-jobs
+            for regex, metajob in self.metajobs:
+                if regex.match(name):
+                    job.copy(metajob)
+            self.jobs[name] = job
+        return job
diff --git a/zuul/reporter/__init__.py b/zuul/reporter/__init__.py
index 0569fbe..8b632ff 100644
--- a/zuul/reporter/__init__.py
+++ b/zuul/reporter/__init__.py
@@ -1,148 +1,148 @@
-# Copyright 2014 Rackspace Australia
-#
-# Licensed under the Apache License, Version 2.0 (the "License"); you may
-# not use this file except in compliance with the License. You may obtain
-# a copy of the License at
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-# License for the specific language governing permissions and limitations
-# under the License.
-
-import abc
-
-import six
-
-
-@six.add_metaclass(abc.ABCMeta)
-class BaseReporter(object):
-    """Base class for reporters.
-
-    Defines the exact public methods that must be supplied.
-    """
-
-    def __init__(self, reporter_config={}, sched=None, connection=None):
-        self.reporter_config = reporter_config
-        self.sched = sched
-        self.connection = connection
-        self._action = None
-
-    def setAction(self, action):
-        self._action = action
-
-    def stop(self):
-        """Stop the reporter."""
-
-    @abc.abstractmethod
-    def report(self, source, pipeline, item):
-        """Send the compiled report message."""
-
-    def getSubmitAllowNeeds(self):
-        """Get a list of code review labels that are allowed to be
-        "needed" in the submit records for a change, with respect
-        to this queue.  In other words, the list of review labels
-        this reporter itself is likely to set before submitting.
-        """
-        return []
-
-    def postConfig(self):
-        """Run tasks after configuration is reloaded"""
-
-    def _getFormatter(self):
-        format_methods = {
-            'start': self._formatItemReportStart,
-            'success': self._formatItemReportSuccess,
-            'failure': self._formatItemReportFailure,
-            'merge-failure': self._formatItemReportMergeFailure,
-            'disabled': self._formatItemReportDisabled
-        }
-        return format_methods[self._action]
-
-    def _formatItemReport(self, pipeline, item):
-        """Format a report from the given items. Usually to provide results to
-        a reporter taking free-form text."""
-        ret = self._getFormatter()(pipeline, item)
-
-        if pipeline.footer_message:
-            ret += '\n' + pipeline.footer_message
-
-        return ret
-
-    def _formatItemReportStart(self, pipeline, item):
-        msg = "Starting %s jobs." % pipeline.name
-        if self.sched.config.has_option('zuul', 'status_url'):
-            msg += "\n" + self.sched.config.get('zuul', 'status_url')
-        return msg
-
-    def _formatItemReportSuccess(self, pipeline, item):
-        return (pipeline.success_message + '\n\n' +
-                self._formatItemReportJobs(pipeline, item))
-
-    def _formatItemReportFailure(self, pipeline, item):
-        if item.dequeued_needing_change:
-            msg = 'This change depends on a change that failed to merge.\n'
-        elif not pipeline.didMergerSucceed(item):
-            msg = pipeline.merge_failure_message
-        else:
-            msg = (pipeline.failure_message + '\n\n' +
-                   self._formatItemReportJobs(pipeline, item))
-        return msg
-
-    def _formatItemReportMergeFailure(self, pipeline, item):
-        return pipeline.merge_failure_message
-
-    def _formatItemReportDisabled(self, pipeline, item):
-        if item.current_build_set.result == 'SUCCESS':
-            return self._formatItemReportSuccess(pipeline, item)
-        elif item.current_build_set.result == 'FAILURE':
-            return self._formatItemReportFailure(pipeline, item)
-        else:
-            return self._formatItemReport(pipeline, item)
-
-    def _formatItemReportJobs(self, pipeline, item):
-        # Return the list of jobs portion of the report
-        ret = ''
-
-        if self.sched.config.has_option('zuul', 'url_pattern'):
-            url_pattern = self.sched.config.get('zuul', 'url_pattern')
-        else:
-            url_pattern = None
-
-        for job in pipeline.getJobs(item):
-            build = item.current_build_set.getBuild(job.name)
-            (result, url) = item.formatJobResult(job, url_pattern)
-            if not job.voting:
-                voting = ' (non-voting)'
-            else:
-                voting = ''
-
-            if self.sched.config and self.sched.config.has_option(
-                'zuul', 'report_times'):
-                report_times = self.sched.config.getboolean(
-                    'zuul', 'report_times')
-            else:
-                report_times = True
-
-            if report_times and build.end_time and build.start_time:
-                dt = int(build.end_time - build.start_time)
-                m, s = divmod(dt, 60)
-                h, m = divmod(m, 60)
-                if h:
-                    elapsed = ' in %dh %02dm %02ds' % (h, m, s)
-                elif m:
-                    elapsed = ' in %dm %02ds' % (m, s)
-                else:
-                    elapsed = ' in %ds' % (s)
-            else:
-                elapsed = ''
-            name = ''
-            if self.sched.config.has_option('zuul', 'job_name_in_report'):
-                if self.sched.config.getboolean('zuul',
-                                                'job_name_in_report'):
-                    name = job.name + ' '
-            ret += '- %s%s : %s%s%s\n' % (name, url, result, elapsed,
-                                          voting)
-        return ret
+# Copyright 2014 Rackspace Australia
+#
+# Licensed under the Apache License, Version 2.0 (the "License"); you may
+# not use this file except in compliance with the License. You may obtain
+# a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+# License for the specific language governing permissions and limitations
+# under the License.
+
+import abc
+
+import six
+
+
+@six.add_metaclass(abc.ABCMeta)
+class BaseReporter(object):
+    """Base class for reporters.
+
+    Defines the exact public methods that must be supplied.
+    """
+
+    def __init__(self, reporter_config={}, sched=None, connection=None):
+        self.reporter_config = reporter_config
+        self.sched = sched
+        self.connection = connection
+        self._action = None
+
+    def setAction(self, action):
+        self._action = action
+
+    def stop(self):
+        """Stop the reporter."""
+
+    @abc.abstractmethod
+    def report(self, source, pipeline, item):
+        """Send the compiled report message."""
+
+    def getSubmitAllowNeeds(self):
+        """Get a list of code review labels that are allowed to be
+        "needed" in the submit records for a change, with respect
+        to this queue.  In other words, the list of review labels
+        this reporter itself is likely to set before submitting.
+        """
+        return []
+
+    def postConfig(self):
+        """Run tasks after configuration is reloaded"""
+
+    def _getFormatter(self):
+        format_methods = {
+            'start': self._formatItemReportStart,
+            'success': self._formatItemReportSuccess,
+            'failure': self._formatItemReportFailure,
+            'merge-failure': self._formatItemReportMergeFailure,
+            'disabled': self._formatItemReportDisabled
+        }
+        return format_methods[self._action]
+
+    def _formatItemReport(self, pipeline, item):
+        """Format a report from the given items. Usually to provide results to
+        a reporter taking free-form text."""
+        ret = self._getFormatter()(pipeline, item)
+
+        if pipeline.footer_message:
+            ret += '\n' + pipeline.footer_message
+
+        return ret
+
+    def _formatItemReportStart(self, pipeline, item):
+        msg = "Starting %s jobs." % pipeline.name
+        if self.sched.config.has_option('zuul', 'status_url'):
+            msg += "\n" + self.sched.config.get('zuul', 'status_url')
+        return msg
+
+    def _formatItemReportSuccess(self, pipeline, item):
+        return (pipeline.success_message + '\n\n' +
+                self._formatItemReportJobs(pipeline, item))
+
+    def _formatItemReportFailure(self, pipeline, item):
+        if item.dequeued_needing_change:
+            msg = 'This change depends on a change that failed to merge.\n'
+        elif not pipeline.didMergerSucceed(item):
+            msg = pipeline.merge_failure_message
+        else:
+            msg = (pipeline.failure_message + '\n\n' +
+                   self._formatItemReportJobs(pipeline, item))
+        return msg
+
+    def _formatItemReportMergeFailure(self, pipeline, item):
+        return pipeline.merge_failure_message
+
+    def _formatItemReportDisabled(self, pipeline, item):
+        if item.current_build_set.result == 'SUCCESS':
+            return self._formatItemReportSuccess(pipeline, item)
+        elif item.current_build_set.result == 'FAILURE':
+            return self._formatItemReportFailure(pipeline, item)
+        else:
+            return self._formatItemReport(pipeline, item)
+
+    def _formatItemReportJobs(self, pipeline, item):
+        # Return the list of jobs portion of the report
+        ret = ''
+
+        if self.sched.config.has_option('zuul', 'url_pattern'):
+            url_pattern = self.sched.config.get('zuul', 'url_pattern')
+        else:
+            url_pattern = None
+
+        for job in pipeline.getJobs(item):
+            build = item.current_build_set.getBuild(job.name)
+            (result, url) = item.formatJobResult(job, url_pattern)
+            if not job.voting:
+                voting = ' (non-voting)'
+            else:
+                voting = ''
+
+            if self.sched.config and self.sched.config.has_option(
+                'zuul', 'report_times'):
+                report_times = self.sched.config.getboolean(
+                    'zuul', 'report_times')
+            else:
+                report_times = True
+
+            if report_times and build.end_time and build.start_time:
+                dt = int(build.end_time - build.start_time)
+                m, s = divmod(dt, 60)
+                h, m = divmod(m, 60)
+                if h:
+                    elapsed = ' in %dh %02dm %02ds' % (h, m, s)
+                elif m:
+                    elapsed = ' in %dm %02ds' % (m, s)
+                else:
+                    elapsed = ' in %ds' % (s)
+            else:
+                elapsed = ''
+            name = ''
+            if self.sched.config.has_option('zuul', 'job_name_in_report'):
+                if self.sched.config.getboolean('zuul',
+                                                'job_name_in_report'):
+                    name = job.name + ' '
+            ret += '- %s%s : %s%s%s\n' % (name, url, result, elapsed,
+                                          voting)
+        return ret
diff --git a/zuul/reporter/gerrit.py b/zuul/reporter/gerrit.py
index 1427449..9998193 100644
--- a/zuul/reporter/gerrit.py
+++ b/zuul/reporter/gerrit.py
@@ -1,52 +1,52 @@
-# Copyright 2013 Rackspace Australia
-#
-# Licensed under the Apache License, Version 2.0 (the "License"); you may
-# not use this file except in compliance with the License. You may obtain
-# a copy of the License at
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-# License for the specific language governing permissions and limitations
-# under the License.
-
-import logging
-import voluptuous as v
-
-
-from zuul.reporter import BaseReporter
-
-
-class GerritReporter(BaseReporter):
-    """Sends off reports to Gerrit."""
-
-    name = 'gerrit'
-    log = logging.getLogger("zuul.reporter.gerrit.Reporter")
-
-    def report(self, source, pipeline, item):
-        """Send a message to gerrit."""
-        message = self._formatItemReport(pipeline, item)
-
-        self.log.debug("Report change %s, params %s, message: %s" %
-                       (item.change, self.reporter_config, message))
-        changeid = '%s,%s' % (item.change.number, item.change.patchset)
-        item.change._ref_sha = source.getRefSha(
-            item.change.project.name, 'refs/heads/' + item.change.branch)
-
-        return self.connection.review(item.change.project.name, changeid,
-                                      message, self.reporter_config)
-
-    def getSubmitAllowNeeds(self):
-        """Get a list of code review labels that are allowed to be
-        "needed" in the submit records for a change, with respect
-        to this queue.  In other words, the list of review labels
-        this reporter itself is likely to set before submitting.
-        """
-        return self.reporter_config
-
-
-def getSchema():
-    gerrit_reporter = v.Any(str, v.Schema({}, extra=True))
-    return gerrit_reporter
+# Copyright 2013 Rackspace Australia
+#
+# Licensed under the Apache License, Version 2.0 (the "License"); you may
+# not use this file except in compliance with the License. You may obtain
+# a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+# License for the specific language governing permissions and limitations
+# under the License.
+
+import logging
+import voluptuous as v
+
+
+from zuul.reporter import BaseReporter
+
+
+class GerritReporter(BaseReporter):
+    """Sends off reports to Gerrit."""
+
+    name = 'gerrit'
+    log = logging.getLogger("zuul.reporter.gerrit.Reporter")
+
+    def report(self, source, pipeline, item):
+        """Send a message to gerrit."""
+        message = self._formatItemReport(pipeline, item)
+
+        self.log.debug("Report change %s, params %s, message: %s" %
+                       (item.change, self.reporter_config, message))
+        changeid = '%s,%s' % (item.change.number, item.change.patchset)
+        item.change._ref_sha = source.getRefSha(
+            item.change.project.name, 'refs/heads/' + item.change.branch)
+
+        return self.connection.review(item.change.project.name, changeid,
+                                      message, self.reporter_config)
+
+    def getSubmitAllowNeeds(self):
+        """Get a list of code review labels that are allowed to be
+        "needed" in the submit records for a change, with respect
+        to this queue.  In other words, the list of review labels
+        this reporter itself is likely to set before submitting.
+        """
+        return self.reporter_config
+
+
+def getSchema():
+    gerrit_reporter = v.Any(str, v.Schema({}, extra=True))
+    return gerrit_reporter
diff --git a/zuul/reporter/smtp.py b/zuul/reporter/smtp.py
index 586b941..d94ee21 100644
--- a/zuul/reporter/smtp.py
+++ b/zuul/reporter/smtp.py
@@ -1,55 +1,55 @@
-# Copyright 2013 Rackspace Australia
-#
-# Licensed under the Apache License, Version 2.0 (the "License"); you may
-# not use this file except in compliance with the License. You may obtain
-# a copy of the License at
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-# License for the specific language governing permissions and limitations
-# under the License.
-
-import logging
-import voluptuous as v
-
-from zuul.reporter import BaseReporter
-
-
-class SMTPReporter(BaseReporter):
-    """Sends off reports to emails via SMTP."""
-
-    name = 'smtp'
-    log = logging.getLogger("zuul.reporter.smtp.Reporter")
-
-    def report(self, source, pipeline, item):
-        """Send the compiled report message via smtp."""
-        message = self._formatItemReport(pipeline, item)
-
-        self.log.debug("Report change %s, params %s, message: %s" %
-                       (item.change, self.reporter_config, message))
-
-        from_email = self.reporter_config['from'] \
-            if 'from' in self.reporter_config else None
-        to_email = self.reporter_config['to'] \
-            if 'to' in self.reporter_config else None
-
-        if 'subject' in self.reporter_config:
-            subject = self.reporter_config['subject'].format(
-                change=item.change)
-        else:
-            subject = "Report for change %s" % item.change
-
-        self.connection.sendMail(subject, message, from_email, to_email)
-
-
-def getSchema():
-    smtp_reporter = v.Schema({
-        'connection': str,
-        'to': str,
-        'from': str,
-        'subject': str,
-    })
-    return smtp_reporter
+# Copyright 2013 Rackspace Australia
+#
+# Licensed under the Apache License, Version 2.0 (the "License"); you may
+# not use this file except in compliance with the License. You may obtain
+# a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+# License for the specific language governing permissions and limitations
+# under the License.
+
+import logging
+import voluptuous as v
+
+from zuul.reporter import BaseReporter
+
+
+class SMTPReporter(BaseReporter):
+    """Sends off reports to emails via SMTP."""
+
+    name = 'smtp'
+    log = logging.getLogger("zuul.reporter.smtp.Reporter")
+
+    def report(self, source, pipeline, item):
+        """Send the compiled report message via smtp."""
+        message = self._formatItemReport(pipeline, item)
+
+        self.log.debug("Report change %s, params %s, message: %s" %
+                       (item.change, self.reporter_config, message))
+
+        from_email = self.reporter_config['from'] \
+            if 'from' in self.reporter_config else None
+        to_email = self.reporter_config['to'] \
+            if 'to' in self.reporter_config else None
+
+        if 'subject' in self.reporter_config:
+            subject = self.reporter_config['subject'].format(
+                change=item.change)
+        else:
+            subject = "Report for change %s" % item.change
+
+        self.connection.sendMail(subject, message, from_email, to_email)
+
+
+def getSchema():
+    smtp_reporter = v.Schema({
+        'connection': str,
+        'to': str,
+        'from': str,
+        'subject': str,
+    })
+    return smtp_reporter
diff --git a/zuul/rpcclient.py b/zuul/rpcclient.py
index 609f636..278f283 100644
--- a/zuul/rpcclient.py
+++ b/zuul/rpcclient.py
@@ -1,84 +1,84 @@
-# Copyright 2013 OpenStack Foundation
-#
-# Licensed under the Apache License, Version 2.0 (the "License"); you may
-# not use this file except in compliance with the License. You may obtain
-# a copy of the License at
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-# License for the specific language governing permissions and limitations
-# under the License.
-
-import json
-import logging
-import time
-
-import gear
-
-
-class RPCFailure(Exception):
-    pass
-
-
-class RPCClient(object):
-    log = logging.getLogger("zuul.RPCClient")
-
-    def __init__(self, server, port):
-        self.log.debug("Connecting to gearman at %s:%s" % (server, port))
-        self.gearman = gear.Client()
-        self.gearman.addServer(server, port)
-        self.log.debug("Waiting for gearman")
-        self.gearman.waitForServer()
-
-    def submitJob(self, name, data):
-        self.log.debug("Submitting job %s with data %s" % (name, data))
-        job = gear.Job(name,
-                       json.dumps(data),
-                       unique=str(time.time()))
-        self.gearman.submitJob(job, timeout=300)
-
-        self.log.debug("Waiting for job completion")
-        while not job.complete:
-            time.sleep(0.1)
-        if job.exception:
-            raise RPCFailure(job.exception)
-        self.log.debug("Job complete, success: %s" % (not job.failure))
-        return job
-
-    def enqueue(self, pipeline, project, trigger, change):
-        data = {'pipeline': pipeline,
-                'project': project,
-                'trigger': trigger,
-                'change': change,
-                }
-        return not self.submitJob('zuul:enqueue', data).failure
-
-    def enqueue_ref(self, pipeline, project, trigger, ref, oldrev, newrev):
-        data = {'pipeline': pipeline,
-                'project': project,
-                'trigger': trigger,
-                'ref': ref,
-                'oldrev': oldrev,
-                'newrev': newrev,
-                }
-        return not self.submitJob('zuul:enqueue_ref', data).failure
-
-    def promote(self, pipeline, change_ids):
-        data = {'pipeline': pipeline,
-                'change_ids': change_ids,
-                }
-        return not self.submitJob('zuul:promote', data).failure
-
-    def get_running_jobs(self):
-        data = {}
-        job = self.submitJob('zuul:get_running_jobs', data)
-        if job.failure:
-            return False
-        else:
-            return json.loads(job.data[0])
-
-    def shutdown(self):
-        self.gearman.shutdown()
+# Copyright 2013 OpenStack Foundation
+#
+# Licensed under the Apache License, Version 2.0 (the "License"); you may
+# not use this file except in compliance with the License. You may obtain
+# a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+# License for the specific language governing permissions and limitations
+# under the License.
+
+import json
+import logging
+import time
+
+import gear
+
+
+class RPCFailure(Exception):
+    pass
+
+
+class RPCClient(object):
+    log = logging.getLogger("zuul.RPCClient")
+
+    def __init__(self, server, port):
+        self.log.debug("Connecting to gearman at %s:%s" % (server, port))
+        self.gearman = gear.Client()
+        self.gearman.addServer(server, port)
+        self.log.debug("Waiting for gearman")
+        self.gearman.waitForServer()
+
+    def submitJob(self, name, data):
+        self.log.debug("Submitting job %s with data %s" % (name, data))
+        job = gear.Job(name,
+                       json.dumps(data),
+                       unique=str(time.time()))
+        self.gearman.submitJob(job, timeout=300)
+
+        self.log.debug("Waiting for job completion")
+        while not job.complete:
+            time.sleep(0.1)
+        if job.exception:
+            raise RPCFailure(job.exception)
+        self.log.debug("Job complete, success: %s" % (not job.failure))
+        return job
+
+    def enqueue(self, pipeline, project, trigger, change):
+        data = {'pipeline': pipeline,
+                'project': project,
+                'trigger': trigger,
+                'change': change,
+                }
+        return not self.submitJob('zuul:enqueue', data).failure
+
+    def enqueue_ref(self, pipeline, project, trigger, ref, oldrev, newrev):
+        data = {'pipeline': pipeline,
+                'project': project,
+                'trigger': trigger,
+                'ref': ref,
+                'oldrev': oldrev,
+                'newrev': newrev,
+                }
+        return not self.submitJob('zuul:enqueue_ref', data).failure
+
+    def promote(self, pipeline, change_ids):
+        data = {'pipeline': pipeline,
+                'change_ids': change_ids,
+                }
+        return not self.submitJob('zuul:promote', data).failure
+
+    def get_running_jobs(self):
+        data = {}
+        job = self.submitJob('zuul:get_running_jobs', data)
+        if job.failure:
+            return False
+        else:
+            return json.loads(job.data[0])
+
+    def shutdown(self):
+        self.gearman.shutdown()
diff --git a/zuul/rpclistener.py b/zuul/rpclistener.py
index d54da9f..ba852be 100644
--- a/zuul/rpclistener.py
+++ b/zuul/rpclistener.py
@@ -1,159 +1,159 @@
-# Copyright 2012 Hewlett-Packard Development Company, L.P.
-# Copyright 2013 OpenStack Foundation
-#
-# Licensed under the Apache License, Version 2.0 (the "License"); you may
-# not use this file except in compliance with the License. You may obtain
-# a copy of the License at
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-# License for the specific language governing permissions and limitations
-# under the License.
-
-import json
-import logging
-import threading
-import traceback
-
-import gear
-import six
-
-import model
-
-
-class RPCListener(object):
-    log = logging.getLogger("zuul.RPCListener")
-
-    def __init__(self, config, sched):
-        self.config = config
-        self.sched = sched
-
-    def start(self):
-        self._running = True
-        server = self.config.get('gearman', 'server')
-        if self.config.has_option('gearman', 'port'):
-            port = self.config.get('gearman', 'port')
-        else:
-            port = 4730
-        self.worker = gear.Worker('Zuul RPC Listener')
-        self.worker.addServer(server, port)
-        self.thread = threading.Thread(target=self.run)
-        self.thread.daemon = True
-        self.thread.start()
-        self.worker.waitForServer()
-        self.register()
-
-    def register(self):
-        self.worker.registerFunction("zuul:enqueue")
-        self.worker.registerFunction("zuul:enqueue_ref")
-        self.worker.registerFunction("zuul:promote")
-        self.worker.registerFunction("zuul:get_running_jobs")
-
-    def stop(self):
-        self.log.debug("Stopping")
-        self._running = False
-        self.worker.shutdown()
-        self.log.debug("Stopped")
-
-    def join(self):
-        self.thread.join()
-
-    def run(self):
-        self.log.debug("Starting RPC listener")
-        while self._running:
-            try:
-                job = self.worker.getJob()
-                z, jobname = job.name.split(':')
-                self.log.debug("Received job %s" % job.name)
-                attrname = 'handle_' + jobname
-                if hasattr(self, attrname):
-                    f = getattr(self, attrname)
-                    if callable(f):
-                        try:
-                            f(job)
-                        except Exception:
-                            self.log.exception("Exception while running job")
-                            job.sendWorkException(traceback.format_exc())
-                    else:
-                        job.sendWorkFail()
-                else:
-                    job.sendWorkFail()
-            except Exception:
-                self.log.exception("Exception while getting job")
-
-    def _common_enqueue(self, job):
-        args = json.loads(job.arguments)
-        event = model.TriggerEvent()
-        errors = ''
-
-        trigger = self.sched.triggers.get(args['trigger'])
-        if trigger:
-            event.trigger_name = args['trigger']
-        else:
-            errors += 'Invalid trigger: %s\n' % (args['trigger'],)
-
-        project = self.sched.layout.projects.get(args['project'])
-        if project:
-            event.project_name = args['project']
-        else:
-            errors += 'Invalid project: %s\n' % (args['project'],)
-
-        pipeline = self.sched.layout.pipelines.get(args['pipeline'])
-        if pipeline:
-            event.forced_pipeline = args['pipeline']
-        else:
-            errors += 'Invalid pipeline: %s\n' % (args['pipeline'],)
-
-        return (args, event, errors, pipeline, project)
-
-    def handle_enqueue(self, job):
-        (args, event, errors, pipeline, project) = self._common_enqueue(job)
-
-        if not errors:
-            event.change_number, event.patch_number = args['change'].split(',')
-            try:
-                pipeline.source.getChange(event, project)
-            except Exception:
-                errors += 'Invalid change: %s\n' % (args['change'],)
-
-        if errors:
-            job.sendWorkException(errors.encode('utf8'))
-        else:
-            self.sched.enqueue(event)
-            job.sendWorkComplete()
-
-    def handle_enqueue_ref(self, job):
-        (args, event, errors, pipeline, project) = self._common_enqueue(job)
-
-        if not errors:
-            event.ref = args['ref']
-            event.oldrev = args['oldrev']
-            event.newrev = args['newrev']
-
-        if errors:
-            job.sendWorkException(errors.encode('utf8'))
-        else:
-            self.sched.enqueue(event)
-            job.sendWorkComplete()
-
-    def handle_promote(self, job):
-        args = json.loads(job.arguments)
-        pipeline_name = args['pipeline']
-        change_ids = args['change_ids']
-        self.sched.promote(pipeline_name, change_ids)
-        job.sendWorkComplete()
-
-    def handle_get_running_jobs(self, job):
-        # args = json.loads(job.arguments)
-        # TODO: use args to filter by pipeline etc
-        running_items = []
-        for pipeline_name, pipeline in six.iteritems(
-                self.sched.layout.pipelines):
-            for queue in pipeline.queues:
-                for item in queue.queue:
-                    running_items.append(item.formatJSON())
-
-        job.sendWorkComplete(json.dumps(running_items))
+# Copyright 2012 Hewlett-Packard Development Company, L.P.
+# Copyright 2013 OpenStack Foundation
+#
+# Licensed under the Apache License, Version 2.0 (the "License"); you may
+# not use this file except in compliance with the License. You may obtain
+# a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+# License for the specific language governing permissions and limitations
+# under the License.
+
+import json
+import logging
+import threading
+import traceback
+
+import gear
+import six
+
+import model
+
+
+class RPCListener(object):
+    log = logging.getLogger("zuul.RPCListener")
+
+    def __init__(self, config, sched):
+        self.config = config
+        self.sched = sched
+
+    def start(self):
+        self._running = True
+        server = self.config.get('gearman', 'server')
+        if self.config.has_option('gearman', 'port'):
+            port = self.config.get('gearman', 'port')
+        else:
+            port = 4730
+        self.worker = gear.Worker('Zuul RPC Listener')
+        self.worker.addServer(server, port)
+        self.thread = threading.Thread(target=self.run)
+        self.thread.daemon = True
+        self.thread.start()
+        self.worker.waitForServer()
+        self.register()
+
+    def register(self):
+        self.worker.registerFunction("zuul:enqueue")
+        self.worker.registerFunction("zuul:enqueue_ref")
+        self.worker.registerFunction("zuul:promote")
+        self.worker.registerFunction("zuul:get_running_jobs")
+
+    def stop(self):
+        self.log.debug("Stopping")
+        self._running = False
+        self.worker.shutdown()
+        self.log.debug("Stopped")
+
+    def join(self):
+        self.thread.join()
+
+    def run(self):
+        self.log.debug("Starting RPC listener")
+        while self._running:
+            try:
+                job = self.worker.getJob()
+                z, jobname = job.name.split(':')
+                self.log.debug("Received job %s" % job.name)
+                attrname = 'handle_' + jobname
+                if hasattr(self, attrname):
+                    f = getattr(self, attrname)
+                    if callable(f):
+                        try:
+                            f(job)
+                        except Exception:
+                            self.log.exception("Exception while running job")
+                            job.sendWorkException(traceback.format_exc())
+                    else:
+                        job.sendWorkFail()
+                else:
+                    job.sendWorkFail()
+            except Exception:
+                self.log.exception("Exception while getting job")
+
+    def _common_enqueue(self, job):
+        args = json.loads(job.arguments)
+        event = model.TriggerEvent()
+        errors = ''
+
+        trigger = self.sched.triggers.get(args['trigger'])
+        if trigger:
+            event.trigger_name = args['trigger']
+        else:
+            errors += 'Invalid trigger: %s\n' % (args['trigger'],)
+
+        project = self.sched.layout.projects.get(args['project'])
+        if project:
+            event.project_name = args['project']
+        else:
+            errors += 'Invalid project: %s\n' % (args['project'],)
+
+        pipeline = self.sched.layout.pipelines.get(args['pipeline'])
+        if pipeline:
+            event.forced_pipeline = args['pipeline']
+        else:
+            errors += 'Invalid pipeline: %s\n' % (args['pipeline'],)
+
+        return (args, event, errors, pipeline, project)
+
+    def handle_enqueue(self, job):
+        (args, event, errors, pipeline, project) = self._common_enqueue(job)
+
+        if not errors:
+            event.change_number, event.patch_number = args['change'].split(',')
+            try:
+                pipeline.source.getChange(event, project)
+            except Exception:
+                errors += 'Invalid change: %s\n' % (args['change'],)
+
+        if errors:
+            job.sendWorkException(errors.encode('utf8'))
+        else:
+            self.sched.enqueue(event)
+            job.sendWorkComplete()
+
+    def handle_enqueue_ref(self, job):
+        (args, event, errors, pipeline, project) = self._common_enqueue(job)
+
+        if not errors:
+            event.ref = args['ref']
+            event.oldrev = args['oldrev']
+            event.newrev = args['newrev']
+
+        if errors:
+            job.sendWorkException(errors.encode('utf8'))
+        else:
+            self.sched.enqueue(event)
+            job.sendWorkComplete()
+
+    def handle_promote(self, job):
+        args = json.loads(job.arguments)
+        pipeline_name = args['pipeline']
+        change_ids = args['change_ids']
+        self.sched.promote(pipeline_name, change_ids)
+        job.sendWorkComplete()
+
+    def handle_get_running_jobs(self, job):
+        # args = json.loads(job.arguments)
+        # TODO: use args to filter by pipeline etc
+        running_items = []
+        for pipeline_name, pipeline in six.iteritems(
+                self.sched.layout.pipelines):
+            for queue in pipeline.queues:
+                for item in queue.queue:
+                    running_items.append(item.formatJSON())
+
+        job.sendWorkComplete(json.dumps(running_items))
diff --git a/zuul/scheduler.py b/zuul/scheduler.py
index b631344..1907c4f 100644
--- a/zuul/scheduler.py
+++ b/zuul/scheduler.py
@@ -1,2161 +1,2161 @@
-# Copyright 2012-2015 Hewlett-Packard Development Company, L.P.
-# Copyright 2013 OpenStack Foundation
-# Copyright 2013 Antoine "hashar" Musso
-# Copyright 2013 Wikimedia Foundation Inc.
-#
-# Licensed under the Apache License, Version 2.0 (the "License"); you may
-# not use this file except in compliance with the License. You may obtain
-# a copy of the License at
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-# License for the specific language governing permissions and limitations
-# under the License.
-
-import extras
-import json
-import logging
-import os
-import pickle
-from six.moves import queue as Queue
-import re
-import sys
-import threading
-import time
-import yaml
-
-import layoutvalidator
-import model
-from model import Pipeline, Project, ChangeQueue
-from model import ChangeishFilter, NullChange
-from zuul import change_matcher, exceptions
-from zuul import version as zuul_version
-
-statsd = extras.try_import('statsd.statsd')
-
-
-def deep_format(obj, paramdict):
-    """Apply the paramdict via str.format() to all string objects found within
-       the supplied obj. Lists and dicts are traversed recursively.
-
-       Borrowed from Jenkins Job Builder project"""
-    if isinstance(obj, str):
-        ret = obj.format(**paramdict)
-    elif isinstance(obj, list):
-        ret = []
-        for item in obj:
-            ret.append(deep_format(item, paramdict))
-    elif isinstance(obj, dict):
-        ret = {}
-        for item in obj:
-            exp_item = item.format(**paramdict)
-
-            ret[exp_item] = deep_format(obj[item], paramdict)
-    else:
-        ret = obj
-    return ret
-
-
-class MutexHandler(object):
-    log = logging.getLogger("zuul.MutexHandler")
-
-    def __init__(self):
-        self.mutexes = {}
-
-    def acquire(self, item, job):
-        if not job.mutex:
-            return True
-        mutex_name = job.mutex
-        m = self.mutexes.get(mutex_name)
-        if not m:
-            # The mutex is not held, acquire it
-            self._acquire(mutex_name, item, job.name)
-            return True
-        held_item, held_job_name = m
-        if held_item is item and held_job_name == job.name:
-            # This item already holds the mutex
-            return True
-        held_build = held_item.current_build_set.getBuild(held_job_name)
-        if held_build and held_build.result:
-            # The build that held the mutex is complete, release it
-            # and let the new item have it.
-            self.log.error("Held mutex %s being released because "
-                           "the build that holds it is complete" %
-                           (mutex_name,))
-            self._release(mutex_name, item, job.name)
-            self._acquire(mutex_name, item, job.name)
-            return True
-        return False
-
-    def release(self, item, job):
-        if not job.mutex:
-            return
-        mutex_name = job.mutex
-        m = self.mutexes.get(mutex_name)
-        if not m:
-            # The mutex is not held, nothing to do
-            self.log.error("Mutex can not be released for %s "
-                           "because the mutex is not held" %
-                           (item,))
-            return
-        held_item, held_job_name = m
-        if held_item is item and held_job_name == job.name:
-            # This item holds the mutex
-            self._release(mutex_name, item, job.name)
-            return
-        self.log.error("Mutex can not be released for %s "
-                       "which does not hold it" %
-                       (item,))
-
-    def _acquire(self, mutex_name, item, job_name):
-        self.log.debug("Job %s of item %s acquiring mutex %s" %
-                       (job_name, item, mutex_name))
-        self.mutexes[mutex_name] = (item, job_name)
-
-    def _release(self, mutex_name, item, job_name):
-        self.log.debug("Job %s of item %s releasing mutex %s" %
-                       (job_name, item, mutex_name))
-        del self.mutexes[mutex_name]
-
-
-class ManagementEvent(object):
-    """An event that should be processed within the main queue run loop"""
-    def __init__(self):
-        self._wait_event = threading.Event()
-        self._exception = None
-        self._traceback = None
-
-    def exception(self, e, tb):
-        self._exception = e
-        self._traceback = tb
-        self._wait_event.set()
-
-    def done(self):
-        self._wait_event.set()
-
-    def wait(self, timeout=None):
-        self._wait_event.wait(timeout)
-        if self._exception:
-            raise self._exception, None, self._traceback
-        return self._wait_event.is_set()
-
-
-class ReconfigureEvent(ManagementEvent):
-    """Reconfigure the scheduler.  The layout will be (re-)loaded from
-    the path specified in the configuration.
-
-    :arg ConfigParser config: the new configuration
-    """
-    def __init__(self, config):
-        super(ReconfigureEvent, self).__init__()
-        self.config = config
-
-
-class PromoteEvent(ManagementEvent):
-    """Promote one or more changes to the head of the queue.
-
-    :arg str pipeline_name: the name of the pipeline
-    :arg list change_ids: a list of strings of change ids in the form
-        1234,1
-    """
-
-    def __init__(self, pipeline_name, change_ids):
-        super(PromoteEvent, self).__init__()
-        self.pipeline_name = pipeline_name
-        self.change_ids = change_ids
-
-
-class EnqueueEvent(ManagementEvent):
-    """Enqueue a change into a pipeline
-
-    :arg TriggerEvent trigger_event: a TriggerEvent describing the
-        trigger, pipeline, and change to enqueue
-    """
-
-    def __init__(self, trigger_event):
-        super(EnqueueEvent, self).__init__()
-        self.trigger_event = trigger_event
-
-
-class ResultEvent(object):
-    """An event that needs to modify the pipeline state due to a
-    result from an external system."""
-
-    pass
-
-
-class BuildStartedEvent(ResultEvent):
-    """A build has started.
-
-    :arg Build build: The build which has started.
-    """
-
-    def __init__(self, build):
-        self.build = build
-
-
-class BuildCompletedEvent(ResultEvent):
-    """A build has completed
-
-    :arg Build build: The build which has completed.
-    """
-
-    def __init__(self, build):
-        self.build = build
-
-
-class MergeCompletedEvent(ResultEvent):
-    """A remote merge operation has completed
-
-    :arg BuildSet build_set: The build_set which is ready.
-    :arg str zuul_url: The URL of the Zuul Merger.
-    :arg bool merged: Whether the merge succeeded (changes with refs).
-    :arg bool updated: Whether the repo was updated (changes without refs).
-    :arg str commit: The SHA of the merged commit (changes with refs).
-    """
-
-    def __init__(self, build_set, zuul_url, merged, updated, commit):
-        self.build_set = build_set
-        self.zuul_url = zuul_url
-        self.merged = merged
-        self.updated = updated
-        self.commit = commit
-
-
-def toList(item):
-    if not item:
-        return []
-    if isinstance(item, list):
-        return item
-    return [item]
-
-
-class Scheduler(threading.Thread):
-    log = logging.getLogger("zuul.Scheduler")
-
-    def __init__(self, config):
-        threading.Thread.__init__(self)
-        self.daemon = True
-        self.wake_event = threading.Event()
-        self.layout_lock = threading.Lock()
-        self.run_handler_lock = threading.Lock()
-        self._pause = False
-        self._exit = False
-        self._stopped = False
-        self.launcher = None
-        self.merger = None
-        self.mutex = MutexHandler()
-        self.connections = dict()
-        # Despite triggers being part of the pipeline, there is one trigger set
-        # per scheduler. The pipeline handles the trigger filters but since
-        # the events are handled by the scheduler itself it needs to handle
-        # the loading of the triggers.
-        # self.triggers['connection_name'] = triggerObject
-        self.triggers = dict()
-        self.config = config
-
-        self.trigger_event_queue = Queue.Queue()
-        self.result_event_queue = Queue.Queue()
-        self.management_event_queue = Queue.Queue()
-        self.layout = model.Layout()
-
-        self.zuul_version = zuul_version.version_info.release_string()
-        self.last_reconfigured = None
-
-        # A set of reporter configuration keys to action mapping
-        self._reporter_actions = {
-            'start': 'start_actions',
-            'success': 'success_actions',
-            'failure': 'failure_actions',
-            'merge-failure': 'merge_failure_actions',
-            'disabled': 'disabled_actions',
-        }
-
-    def stop(self):
-        self._stopped = True
-        self._unloadDrivers()
-        self.stopConnections()
-        self.wake_event.set()
-
-    def testConfig(self, config_path, connections):
-        # Take the list of set up connections directly here rather than with
-        # registerConnections as we don't want to do the onLoad event yet.
-        return self._parseConfig(config_path, connections)
-
-    def _parseSkipIf(self, config_job):
-        cm = change_matcher
-        skip_matchers = []
-
-        for config_skip in config_job.get('skip-if', []):
-            nested_matchers = []
-
-            project_regex = config_skip.get('project')
-            if project_regex:
-                nested_matchers.append(cm.ProjectMatcher(project_regex))
-
-            branch_regex = config_skip.get('branch')
-            if branch_regex:
-                nested_matchers.append(cm.BranchMatcher(branch_regex))
-
-            file_regexes = toList(config_skip.get('all-files-match-any'))
-            if file_regexes:
-                file_matchers = [cm.FileMatcher(x) for x in file_regexes]
-                all_files_matcher = cm.MatchAllFiles(file_matchers)
-                nested_matchers.append(all_files_matcher)
-
-            # All patterns need to match a given skip-if predicate
-            skip_matchers.append(cm.MatchAll(nested_matchers))
-
-        if skip_matchers:
-            # Any skip-if predicate can be matched to trigger a skip
-            return cm.MatchAny(skip_matchers)
-
-    def registerConnections(self, connections, load=True):
-        # load: whether or not to trigger the onLoad for the connection. This
-        # is useful for not doing a full load during layout validation.
-        self.connections = connections
-        for connection_name, connection in self.connections.items():
-            connection.registerScheduler(self)
-            if load:
-                connection.onLoad()
-
-    def stopConnections(self):
-        for connection_name, connection in self.connections.items():
-            connection.onStop()
-
-    def _unloadDrivers(self):
-        for trigger in self.triggers.values():
-            trigger.stop()
-        self.triggers = {}
-        for pipeline in self.layout.pipelines.values():
-            pipeline.source.stop()
-            for action in self._reporter_actions.values():
-                for reporter in pipeline.__getattribute__(action):
-                    reporter.stop()
-
-    def _getDriver(self, dtype, connection_name, driver_config={}):
-        # Instantiate a driver such as a trigger, source or reporter
-        # TODO(jhesketh): Make this list dynamic or use entrypoints etc.
-        # Stevedore was not a good fit here due to the nature of triggers.
-        # Specifically we don't want to load a trigger per a pipeline as one
-        # trigger can listen to a stream (from gerrit, for example) and the
-        # scheduler decides which eventfilter to use. As such we want to load
-        # trigger+connection pairs uniquely.
-        drivers = {
-            'source': {
-                'gerrit': 'zuul.source.gerrit:GerritSource',
-            },
-            'trigger': {
-                'gerrit': 'zuul.trigger.gerrit:GerritTrigger',
-                'timer': 'zuul.trigger.timer:TimerTrigger',
-                'zuul': 'zuul.trigger.zuultrigger:ZuulTrigger',
-            },
-            'reporter': {
-                'gerrit': 'zuul.reporter.gerrit:GerritReporter',
-                'smtp': 'zuul.reporter.smtp:SMTPReporter',
-            },
-        }
-
-        # TODO(jhesketh): Check the connection_name exists
-        if connection_name in self.connections.keys():
-            driver_name = self.connections[connection_name].driver_name
-            connection = self.connections[connection_name]
-        else:
-            # In some cases a driver may not be related to a connection. For
-            # example, the 'timer' or 'zuul' triggers.
-            driver_name = connection_name
-            connection = None
-        driver = drivers[dtype][driver_name].split(':')
-        driver_instance = getattr(
-            __import__(driver[0], fromlist=['']), driver[1])(
-                driver_config, self, connection
-        )
-
-        if connection:
-            connection.registerUse(dtype, driver_instance)
-
-        return driver_instance
-
-    def _getSourceDriver(self, connection_name):
-        return self._getDriver('source', connection_name)
-
-    def _getReporterDriver(self, connection_name, driver_config={}):
-        return self._getDriver('reporter', connection_name, driver_config)
-
-    def _getTriggerDriver(self, connection_name, driver_config={}):
-        return self._getDriver('trigger', connection_name, driver_config)
-
-    def _parseConfig(self, config_path, connections):
-        layout = model.Layout()
-        project_templates = {}
-
-        if config_path:
-            config_path = os.path.expanduser(config_path)
-            if not os.path.exists(config_path):
-                raise Exception("Unable to read layout config file at %s" %
-                                config_path)
-        with open(config_path) as config_file:
-            data = yaml.load(config_file)
-
-        validator = layoutvalidator.LayoutValidator()
-        validator.validate(data, connections)
-
-        config_env = {}
-        for include in data.get('includes', []):
-            if 'python-file' in include:
-                fn = include['python-file']
-                if not os.path.isabs(fn):
-                    base = os.path.dirname(os.path.realpath(config_path))
-                    fn = os.path.join(base, fn)
-                fn = os.path.expanduser(fn)
-                execfile(fn, config_env)
-
-        for conf_pipeline in data.get('pipelines', []):
-            pipeline = Pipeline(conf_pipeline['name'])
-            pipeline.description = conf_pipeline.get('description')
-            # TODO(jeblair): remove backwards compatibility:
-            pipeline.source = self._getSourceDriver(
-                conf_pipeline.get('source', 'gerrit'))
-            precedence = model.PRECEDENCE_MAP[conf_pipeline.get('precedence')]
-            pipeline.precedence = precedence
-            pipeline.failure_message = conf_pipeline.get('failure-message',
-                                                         "Build failed.")
-            pipeline.merge_failure_message = conf_pipeline.get(
-                'merge-failure-message', "Merge Failed.\n\nThis change or one "
-                "of its cross-repo dependencies was unable to be "
-                "automatically merged with the current state of its "
-                "repository. Please rebase the change and upload a new "
-                "patchset.")
-            pipeline.success_message = conf_pipeline.get('success-message',
-                                                         "Build succeeded.")
-            pipeline.footer_message = conf_pipeline.get('footer-message', "")
-            pipeline.dequeue_on_new_patchset = conf_pipeline.get(
-                'dequeue-on-new-patchset', True)
-            pipeline.ignore_dependencies = conf_pipeline.get(
-                'ignore-dependencies', False)
-
-            for conf_key, action in self._reporter_actions.items():
-                reporter_set = []
-                if conf_pipeline.get(conf_key):
-                    for reporter_name, params \
-                        in conf_pipeline.get(conf_key).items():
-                        reporter = self._getReporterDriver(reporter_name,
-                                                           params)
-                        reporter.setAction(conf_key)
-                        reporter_set.append(reporter)
-                setattr(pipeline, action, reporter_set)
-
-            # If merge-failure actions aren't explicit, use the failure actions
-            if not pipeline.merge_failure_actions:
-                pipeline.merge_failure_actions = pipeline.failure_actions
-
-            pipeline.disable_at = conf_pipeline.get(
-                'disable-after-consecutive-failures', None)
-
-            pipeline.window = conf_pipeline.get('window', 20)
-            pipeline.window_floor = conf_pipeline.get('window-floor', 3)
-            pipeline.window_increase_type = conf_pipeline.get(
-                'window-increase-type', 'linear')
-            pipeline.window_increase_factor = conf_pipeline.get(
-                'window-increase-factor', 1)
-            pipeline.window_decrease_type = conf_pipeline.get(
-                'window-decrease-type', 'exponential')
-            pipeline.window_decrease_factor = conf_pipeline.get(
-                'window-decrease-factor', 2)
-
-            manager = globals()[conf_pipeline['manager']](self, pipeline)
-            pipeline.setManager(manager)
-            layout.pipelines[conf_pipeline['name']] = pipeline
-
-            if 'require' in conf_pipeline or 'reject' in conf_pipeline:
-                require = conf_pipeline.get('require', {})
-                reject = conf_pipeline.get('reject', {})
-                f = ChangeishFilter(
-                    open=require.get('open'),
-                    current_patchset=require.get('current-patchset'),
-                    statuses=toList(require.get('status')),
-                    required_approvals=toList(require.get('approval')),
-                    reject_approvals=toList(reject.get('approval'))
-                )
-                manager.changeish_filters.append(f)
-
-            for trigger_name, trigger_config\
-                in conf_pipeline.get('trigger').items():
-                if trigger_name not in self.triggers.keys():
-                    self.triggers[trigger_name] = \
-                        self._getTriggerDriver(trigger_name, trigger_config)
-
-            for trigger_name, trigger in self.triggers.items():
-                if trigger_name in conf_pipeline['trigger']:
-                    manager.event_filters += trigger.getEventFilters(
-                        conf_pipeline['trigger'][trigger_name])
-
-        for project_template in data.get('project-templates', []):
-            # Make sure the template only contains valid pipelines
-            tpl = dict(
-                (pipe_name, project_template.get(pipe_name))
-                for pipe_name in layout.pipelines.keys()
-                if pipe_name in project_template
-            )
-            project_templates[project_template.get('name')] = tpl
-
-        for config_job in data.get('jobs', []):
-            job = layout.getJob(config_job['name'])
-            # Be careful to only set attributes explicitly present on
-            # this job, to avoid squashing attributes set by a meta-job.
-            m = config_job.get('queue-name', None)
-            if m:
-                job.queue_name = m
-            m = config_job.get('failure-message', None)
-            if m:
-                job.failure_message = m
-            m = config_job.get('success-message', None)
-            if m:
-                job.success_message = m
-            m = config_job.get('failure-pattern', None)
-            if m:
-                job.failure_pattern = m
-            m = config_job.get('success-pattern', None)
-            if m:
-                job.success_pattern = m
-            m = config_job.get('hold-following-changes', False)
-            if m:
-                job.hold_following_changes = True
-            m = config_job.get('voting', None)
-            if m is not None:
-                job.voting = m
-            m = config_job.get('mutex', None)
-            if m is not None:
-                job.mutex = m
-            tags = toList(config_job.get('tags'))
-            if tags:
-                # Tags are merged via a union rather than a
-                # destructive copy because they are intended to
-                # accumulate onto any previously applied tags from
-                # metajobs.
-                job.tags = job.tags.union(set(tags))
-            fname = config_job.get('parameter-function', None)
-            if fname:
-                func = config_env.get(fname, None)
-                if not func:
-                    raise Exception("Unable to find function %s" % fname)
-                job.parameter_function = func
-            branches = toList(config_job.get('branch'))
-            if branches:
-                job._branches = branches
-                job.branches = [re.compile(x) for x in branches]
-            files = toList(config_job.get('files'))
-            if files:
-                job._files = files
-                job.files = [re.compile(x) for x in files]
-            skip_if_matcher = self._parseSkipIf(config_job)
-            if skip_if_matcher:
-                job.skip_if_matcher = skip_if_matcher
-            swift = toList(config_job.get('swift'))
-            if swift:
-                for s in swift:
-                    job.swift[s['name']] = s
-
-        def add_jobs(job_tree, config_jobs):
-            for job in config_jobs:
-                if isinstance(job, list):
-                    for x in job:
-                        add_jobs(job_tree, x)
-                if isinstance(job, dict):
-                    for parent, children in job.items():
-                        parent_tree = job_tree.addJob(layout.getJob(parent))
-                        add_jobs(parent_tree, children)
-                if isinstance(job, str):
-                    job_tree.addJob(layout.getJob(job))
-
-        for config_project in data.get('projects', []):
-            project = Project(config_project['name'])
-            shortname = config_project['name'].split('/')[-1]
-
-            # This is reversed due to the prepend operation below, so
-            # the ultimate order is templates (in order) followed by
-            # statically defined jobs.
-            for requested_template in reversed(
-                config_project.get('template', [])):
-                # Fetch the template from 'project-templates'
-                tpl = project_templates.get(
-                    requested_template.get('name'))
-                # Expand it with the project context
-                requested_template['name'] = shortname
-                expanded = deep_format(tpl, requested_template)
-                # Finally merge the expansion with whatever has been
-                # already defined for this project.  Prepend our new
-                # jobs to existing ones (which may have been
-                # statically defined or defined by other templates).
-                for pipeline in layout.pipelines.values():
-                    if pipeline.name in expanded:
-                        config_project.update(
-                            {pipeline.name: expanded[pipeline.name] +
-                             config_project.get(pipeline.name, [])})
-
-            layout.projects[config_project['name']] = project
-            mode = config_project.get('merge-mode', 'merge-resolve')
-            project.merge_mode = model.MERGER_MAP[mode]
-            for pipeline in layout.pipelines.values():
-                if pipeline.name in config_project:
-                    job_tree = pipeline.addProject(project)
-                    config_jobs = config_project[pipeline.name]
-                    add_jobs(job_tree, config_jobs)
-
-        # All jobs should be defined at this point, get rid of
-        # metajobs so that getJob isn't doing anything weird.
-        layout.metajobs = []
-
-        for pipeline in layout.pipelines.values():
-            pipeline.manager._postConfig(layout)
-
-        return layout
-
-    def setLauncher(self, launcher):
-        self.launcher = launcher
-
-    def setMerger(self, merger):
-        self.merger = merger
-
-    def getProject(self, name, create_foreign=False):
-        self.layout_lock.acquire()
-        p = None
-        try:
-            p = self.layout.projects.get(name)
-            if p is None and create_foreign:
-                self.log.info("Registering foreign project: %s" % name)
-                p = Project(name, foreign=True)
-                self.layout.projects[name] = p
-        finally:
-            self.layout_lock.release()
-        return p
-
-    def addEvent(self, event):
-        self.log.debug("Adding trigger event: %s" % event)
-        try:
-            if statsd:
-                statsd.incr('gerrit.event.%s' % event.type)
-        except:
-            self.log.exception("Exception reporting event stats")
-        self.trigger_event_queue.put(event)
-        self.wake_event.set()
-        self.log.debug("Done adding trigger event: %s" % event)
-
-    def onBuildStarted(self, build):
-        self.log.debug("Adding start event for build: %s" % build)
-        build.start_time = time.time()
-        event = BuildStartedEvent(build)
-        self.result_event_queue.put(event)
-        self.wake_event.set()
-        self.log.debug("Done adding start event for build: %s" % build)
-
-    def onBuildCompleted(self, build, result):
-        self.log.debug("Adding complete event for build: %s result: %s" % (
-            build, result))
-        build.end_time = time.time()
-        # Note, as soon as the result is set, other threads may act
-        # upon this, even though the event hasn't been fully
-        # processed.  Ensure that any other data from the event (eg,
-        # timing) is recorded before setting the result.
-        build.result = result
-        try:
-            if statsd and build.pipeline:
-                jobname = build.job.name.replace('.', '_')
-                key = 'zuul.pipeline.%s.all_jobs' % build.pipeline.name
-                statsd.incr(key)
-                for label in build.node_labels:
-                    # Jenkins includes the node name in its list of labels, so
-                    # we filter it out here, since that is not statistically
-                    # interesting.
-                    if label == build.node_name:
-                        continue
-                    dt = int((build.start_time - build.launch_time) * 1000)
-                    key = 'zuul.pipeline.%s.label.%s.wait_time' % (
-                        build.pipeline.name, label)
-                    statsd.timing(key, dt)
-                key = 'zuul.pipeline.%s.job.%s.%s' % (build.pipeline.name,
-                                                      jobname, build.result)
-                if build.result in ['SUCCESS', 'FAILURE'] and build.start_time:
-                    dt = int((build.end_time - build.start_time) * 1000)
-                    statsd.timing(key, dt)
-                statsd.incr(key)
-
-                key = 'zuul.pipeline.%s.job.%s.wait_time' % (
-                    build.pipeline.name, jobname)
-                dt = int((build.start_time - build.launch_time) * 1000)
-                statsd.timing(key, dt)
-        except:
-            self.log.exception("Exception reporting runtime stats")
-        event = BuildCompletedEvent(build)
-        self.result_event_queue.put(event)
-        self.wake_event.set()
-        self.log.debug("Done adding complete event for build: %s" % build)
-
-    def onMergeCompleted(self, build_set, zuul_url, merged, updated, commit):
-        self.log.debug("Adding merge complete event for build set: %s" %
-                       build_set)
-        event = MergeCompletedEvent(build_set, zuul_url,
-                                    merged, updated, commit)
-        self.result_event_queue.put(event)
-        self.wake_event.set()
-
-    def reconfigure(self, config):
-        self.log.debug("Prepare to reconfigure")
-        event = ReconfigureEvent(config)
-        self.management_event_queue.put(event)
-        self.wake_event.set()
-        self.log.debug("Waiting for reconfiguration")
-        event.wait()
-        self.log.debug("Reconfiguration complete")
-        self.last_reconfigured = int(time.time())
-
-    def promote(self, pipeline_name, change_ids):
-        event = PromoteEvent(pipeline_name, change_ids)
-        self.management_event_queue.put(event)
-        self.wake_event.set()
-        self.log.debug("Waiting for promotion")
-        event.wait()
-        self.log.debug("Promotion complete")
-
-    def enqueue(self, trigger_event):
-        event = EnqueueEvent(trigger_event)
-        self.management_event_queue.put(event)
-        self.wake_event.set()
-        self.log.debug("Waiting for enqueue")
-        event.wait()
-        self.log.debug("Enqueue complete")
-
-    def exit(self):
-        self.log.debug("Prepare to exit")
-        self._pause = True
-        self._exit = True
-        self.wake_event.set()
-        self.log.debug("Waiting for exit")
-
-    def _get_queue_pickle_file(self):
-        if self.config.has_option('zuul', 'state_dir'):
-            state_dir = os.path.expanduser(self.config.get('zuul',
-                                                           'state_dir'))
-        else:
-            state_dir = '/var/lib/zuul'
-        return os.path.join(state_dir, 'queue.pickle')
-
-    def _save_queue(self):
-        pickle_file = self._get_queue_pickle_file()
-        events = []
-        while not self.trigger_event_queue.empty():
-            events.append(self.trigger_event_queue.get())
-        self.log.debug("Queue length is %s" % len(events))
-        if events:
-            self.log.debug("Saving queue")
-            pickle.dump(events, open(pickle_file, 'wb'))
-
-    def _load_queue(self):
-        pickle_file = self._get_queue_pickle_file()
-        if os.path.exists(pickle_file):
-            self.log.debug("Loading queue")
-            events = pickle.load(open(pickle_file, 'rb'))
-            self.log.debug("Queue length is %s" % len(events))
-            for event in events:
-                self.trigger_event_queue.put(event)
-        else:
-            self.log.debug("No queue file found")
-
-    def _delete_queue(self):
-        pickle_file = self._get_queue_pickle_file()
-        if os.path.exists(pickle_file):
-            self.log.debug("Deleting saved queue")
-            os.unlink(pickle_file)
-
-    def resume(self):
-        try:
-            self._load_queue()
-        except:
-            self.log.exception("Unable to load queue")
-        try:
-            self._delete_queue()
-        except:
-            self.log.exception("Unable to delete saved queue")
-        self.log.debug("Resuming queue processing")
-        self.wake_event.set()
-
-    def _doPauseEvent(self):
-        if self._exit:
-            self.log.debug("Exiting")
-            self._save_queue()
-            os._exit(0)
-
-    def _doReconfigureEvent(self, event):
-        # This is called in the scheduler loop after another thread submits
-        # a request
-        self.layout_lock.acquire()
-        self.config = event.config
-        try:
-            self.log.debug("Performing reconfiguration")
-            self._unloadDrivers()
-            layout = self._parseConfig(
-                self.config.get('zuul', 'layout_config'), self.connections)
-            for name, new_pipeline in layout.pipelines.items():
-                old_pipeline = self.layout.pipelines.get(name)
-                if not old_pipeline:
-                    if self.layout.pipelines:
-                        # Don't emit this warning on startup
-                        self.log.warning("No old pipeline matching %s found "
-                                         "when reconfiguring" % name)
-                    continue
-                self.log.debug("Re-enqueueing changes for pipeline %s" % name)
-                items_to_remove = []
-                builds_to_cancel = []
-                last_head = None
-                for shared_queue in old_pipeline.queues:
-                    for item in shared_queue.queue:
-                        if not item.item_ahead:
-                            last_head = item
-                        item.item_ahead = None
-                        item.items_behind = []
-                        item.pipeline = None
-                        item.queue = None
-                        project_name = item.change.project.name
-                        item.change.project = layout.projects.get(project_name)
-                        if not item.change.project:
-                            self.log.debug("Project %s not defined, "
-                                           "re-instantiating as foreign" %
-                                           project_name)
-                            project = Project(project_name, foreign=True)
-                            layout.projects[project_name] = project
-                            item.change.project = project
-                        item_jobs = new_pipeline.getJobs(item)
-                        for build in item.current_build_set.getBuilds():
-                            job = layout.jobs.get(build.job.name)
-                            if job and job in item_jobs:
-                                build.job = job
-                            else:
-                                item.removeBuild(build)
-                                builds_to_cancel.append(build)
-                        if not new_pipeline.manager.reEnqueueItem(item,
-                                                                  last_head):
-                            items_to_remove.append(item)
-                for item in items_to_remove:
-                    for build in item.current_build_set.getBuilds():
-                        builds_to_cancel.append(build)
-                for build in builds_to_cancel:
-                    self.log.warning(
-                        "Canceling build %s during reconfiguration" % (build,))
-                    try:
-                        self.launcher.cancel(build)
-                    except Exception:
-                        self.log.exception(
-                            "Exception while canceling build %s "
-                            "for change %s" % (build, item.change))
-            self.layout = layout
-            self.maintainConnectionCache()
-            for trigger in self.triggers.values():
-                trigger.postConfig()
-            for pipeline in self.layout.pipelines.values():
-                pipeline.source.postConfig()
-                for action in self._reporter_actions.values():
-                    for reporter in pipeline.__getattribute__(action):
-                        reporter.postConfig()
-            if statsd:
-                try:
-                    for pipeline in self.layout.pipelines.values():
-                        items = len(pipeline.getAllItems())
-                        # stats.gauges.zuul.pipeline.NAME.current_changes
-                        key = 'zuul.pipeline.%s' % pipeline.name
-                        statsd.gauge(key + '.current_changes', items)
-                except Exception:
-                    self.log.exception("Exception reporting initial "
-                                       "pipeline stats:")
-        finally:
-            self.layout_lock.release()
-
-    def _doPromoteEvent(self, event):
-        pipeline = self.layout.pipelines[event.pipeline_name]
-        change_ids = [c.split(',') for c in event.change_ids]
-        items_to_enqueue = []
-        change_queue = None
-        for shared_queue in pipeline.queues:
-            if change_queue:
-                break
-            for item in shared_queue.queue:
-                if (item.change.number == change_ids[0][0] and
-                        item.change.patchset == change_ids[0][1]):
-                    change_queue = shared_queue
-                    break
-        if not change_queue:
-            raise Exception("Unable to find shared change queue for %s" %
-                            event.change_ids[0])
-        for number, patchset in change_ids:
-            found = False
-            for item in change_queue.queue:
-                if (item.change.number == number and
-                        item.change.patchset == patchset):
-                    found = True
-                    items_to_enqueue.append(item)
-                    break
-            if not found:
-                raise Exception("Unable to find %s,%s in queue %s" %
-                                (number, patchset, change_queue))
-        for item in change_queue.queue[:]:
-            if item not in items_to_enqueue:
-                items_to_enqueue.append(item)
-            pipeline.manager.cancelJobs(item)
-            pipeline.manager.dequeueItem(item)
-        for item in items_to_enqueue:
-            pipeline.manager.addChange(
-                item.change,
-                enqueue_time=item.enqueue_time,
-                quiet=True,
-                ignore_requirements=True)
-
-    def _doEnqueueEvent(self, event):
-        project = self.layout.projects.get(event.project_name)
-        pipeline = self.layout.pipelines[event.forced_pipeline]
-        change = pipeline.source.getChange(event, project)
-        self.log.debug("Event %s for change %s was directly assigned "
-                       "to pipeline %s" % (event, change, self))
-        self.log.info("Adding %s, %s to %s" %
-                      (project, change, pipeline))
-        pipeline.manager.addChange(change, ignore_requirements=True)
-
-    def _areAllBuildsComplete(self):
-        self.log.debug("Checking if all builds are complete")
-        waiting = False
-        if self.merger.areMergesOutstanding():
-            waiting = True
-        for pipeline in self.layout.pipelines.values():
-            for item in pipeline.getAllItems():
-                for build in item.current_build_set.getBuilds():
-                    if build.result is None:
-                        self.log.debug("%s waiting on %s" %
-                                       (pipeline.manager, build))
-                        waiting = True
-        if not waiting:
-            self.log.debug("All builds are complete")
-            return True
-        self.log.debug("All builds are not complete")
-        return False
-
-    def run(self):
-        if statsd:
-            self.log.debug("Statsd enabled")
-        else:
-            self.log.debug("Statsd disabled because python statsd "
-                           "package not found")
-        while True:
-            self.log.debug("Run handler sleeping")
-            self.wake_event.wait()
-            self.wake_event.clear()
-            if self._stopped:
-                self.log.debug("Run handler stopping")
-                return
-            self.log.debug("Run handler awake")
-            self.run_handler_lock.acquire()
-            try:
-                while not self.management_event_queue.empty():
-                    self.process_management_queue()
-
-                # Give result events priority -- they let us stop builds,
-                # whereas trigger evensts cause us to launch builds.
-                while not self.result_event_queue.empty():
-                    self.process_result_queue()
-
-                if not self._pause:
-                    while not self.trigger_event_queue.empty():
-                        self.process_event_queue()
-
-                if self._pause and self._areAllBuildsComplete():
-                    self._doPauseEvent()
-
-                for pipeline in self.layout.pipelines.values():
-                    while pipeline.manager.processQueue():
-                        pass
-
-            except Exception:
-                self.log.exception("Exception in run handler:")
-                # There may still be more events to process
-                self.wake_event.set()
-            finally:
-                self.run_handler_lock.release()
-
-    def maintainConnectionCache(self):
-        relevant = set()
-        for pipeline in self.layout.pipelines.values():
-            self.log.debug("Gather relevant cache items for: %s" % pipeline)
-            for item in pipeline.getAllItems():
-                relevant.add(item.change)
-                relevant.update(item.change.getRelatedChanges())
-        for connection in self.connections.values():
-            connection.maintainCache(relevant)
-            self.log.debug(
-                "End maintain connection cache for: %s" % connection)
-        self.log.debug("Connection cache size: %s" % len(relevant))
-
-    def process_event_queue(self):
-        self.log.debug("Fetching trigger event")
-        event = self.trigger_event_queue.get()
-        self.log.debug("Processing trigger event %s" % event)
-        try:
-            project = self.layout.projects.get(event.project_name)
-
-            for pipeline in self.layout.pipelines.values():
-                # Get the change even if the project is unknown to us for the
-                # use of updating the cache if there is another change
-                # depending on this foreign one.
-                try:
-                    change = pipeline.source.getChange(event, project)
-                except exceptions.ChangeNotFound as e:
-                    self.log.debug("Unable to get change %s from source %s. "
-                                   "(most likely looking for a change from "
-                                   "another connection trigger)",
-                                   e.change, pipeline.source)
-                    continue
-                if not project or project.foreign:
-                    self.log.debug("Project %s not found" % event.project_name)
-                    continue
-                if event.type == 'patchset-created':
-                    pipeline.manager.removeOldVersionsOfChange(change)
-                elif event.type == 'change-abandoned':
-                    pipeline.manager.removeAbandonedChange(change)
-                if pipeline.manager.eventMatches(event, change):
-                    self.log.info("Adding %s, %s to %s" %
-                                  (project, change, pipeline))
-                    pipeline.manager.addChange(change)
-        finally:
-            self.trigger_event_queue.task_done()
-
-    def process_management_queue(self):
-        self.log.debug("Fetching management event")
-        event = self.management_event_queue.get()
-        self.log.debug("Processing management event %s" % event)
-        try:
-            if isinstance(event, ReconfigureEvent):
-                self._doReconfigureEvent(event)
-            elif isinstance(event, PromoteEvent):
-                self._doPromoteEvent(event)
-            elif isinstance(event, EnqueueEvent):
-                self._doEnqueueEvent(event.trigger_event)
-            else:
-                self.log.error("Unable to handle event %s" % event)
-            event.done()
-        except Exception as e:
-            event.exception(e, sys.exc_info()[2])
-        self.management_event_queue.task_done()
-
-    def process_result_queue(self):
-        self.log.debug("Fetching result event")
-        event = self.result_event_queue.get()
-        self.log.debug("Processing result event %s" % event)
-        try:
-            if isinstance(event, BuildStartedEvent):
-                self._doBuildStartedEvent(event)
-            elif isinstance(event, BuildCompletedEvent):
-                self._doBuildCompletedEvent(event)
-            elif isinstance(event, MergeCompletedEvent):
-                self._doMergeCompletedEvent(event)
-            else:
-                self.log.error("Unable to handle event %s" % event)
-        finally:
-            self.result_event_queue.task_done()
-
-    def _doBuildStartedEvent(self, event):
-        build = event.build
-        if build.build_set is not build.build_set.item.current_build_set:
-            self.log.warning("Build %s is not in the current build set" %
-                             (build,))
-            return
-        pipeline = build.build_set.item.pipeline
-        if not pipeline:
-            self.log.warning("Build %s is not associated with a pipeline" %
-                             (build,))
-            return
-        pipeline.manager.onBuildStarted(event.build)
-
-    def _doBuildCompletedEvent(self, event):
-        build = event.build
-        if build.build_set is not build.build_set.item.current_build_set:
-            self.log.warning("Build %s is not in the current build set" %
-                             (build,))
-            return
-        pipeline = build.build_set.item.pipeline
-        if not pipeline:
-            self.log.warning("Build %s is not associated with a pipeline" %
-                             (build,))
-            return
-        pipeline.manager.onBuildCompleted(event.build)
-
-    def _doMergeCompletedEvent(self, event):
-        build_set = event.build_set
-        if build_set is not build_set.item.current_build_set:
-            self.log.warning("Build set %s is not current" % (build_set,))
-            return
-        pipeline = build_set.item.pipeline
-        if not pipeline:
-            self.log.warning("Build set %s is not associated with a pipeline" %
-                             (build_set,))
-            return
-        pipeline.manager.onMergeCompleted(event)
-
-    def formatStatusJSON(self):
-        if self.config.has_option('zuul', 'url_pattern'):
-            url_pattern = self.config.get('zuul', 'url_pattern')
-        else:
-            url_pattern = None
-
-        data = {}
-
-        data['zuul_version'] = self.zuul_version
-
-        if self._pause:
-            ret = '<p><b>Queue only mode:</b> preparing to '
-            if self._exit:
-                ret += 'exit'
-            ret += ', queue length: %s' % self.trigger_event_queue.qsize()
-            ret += '</p>'
-            data['message'] = ret
-
-        data['trigger_event_queue'] = {}
-        data['trigger_event_queue']['length'] = \
-            self.trigger_event_queue.qsize()
-        data['result_event_queue'] = {}
-        data['result_event_queue']['length'] = \
-            self.result_event_queue.qsize()
-
-        if self.last_reconfigured:
-            data['last_reconfigured'] = self.last_reconfigured * 1000
-
-        pipelines = []
-        data['pipelines'] = pipelines
-        for pipeline in self.layout.pipelines.values():
-            pipelines.append(pipeline.formatStatusJSON(url_pattern))
-        return json.dumps(data)
-
-
-class BasePipelineManager(object):
-    log = logging.getLogger("zuul.BasePipelineManager")
-
-    def __init__(self, sched, pipeline):
-        self.sched = sched
-        self.pipeline = pipeline
-        self.event_filters = []
-        self.changeish_filters = []
-
-    def __str__(self):
-        return "<%s %s>" % (self.__class__.__name__, self.pipeline.name)
-
-    def _postConfig(self, layout):
-        self.log.info("Configured Pipeline Manager %s" % self.pipeline.name)
-        self.log.info("  Source: %s" % self.pipeline.source)
-        self.log.info("  Requirements:")
-        for f in self.changeish_filters:
-            self.log.info("    %s" % f)
-        self.log.info("  Events:")
-        for e in self.event_filters:
-            self.log.info("    %s" % e)
-        self.log.info("  Projects:")
-
-        def log_jobs(tree, indent=0):
-            istr = '    ' + ' ' * indent
-            if tree.job:
-                efilters = ''
-                for b in tree.job._branches:
-                    efilters += str(b)
-                for f in tree.job._files:
-                    efilters += str(f)
-                if tree.job.skip_if_matcher:
-                    efilters += str(tree.job.skip_if_matcher)
-                if efilters:
-                    efilters = ' ' + efilters
-                tags = []
-                if tree.job.hold_following_changes:
-                    tags.append('[hold]')
-                if not tree.job.voting:
-                    tags.append('[nonvoting]')
-                if tree.job.mutex:
-                    tags.append('[mutex: %s]' % tree.job.mutex)
-                tags = ' '.join(tags)
-                self.log.info("%s%s%s %s" % (istr, repr(tree.job),
-                                             efilters, tags))
-            for x in tree.job_trees:
-                log_jobs(x, indent + 2)
-
-        for p in layout.projects.values():
-            tree = self.pipeline.getJobTree(p)
-            if tree:
-                self.log.info("    %s" % p)
-                log_jobs(tree)
-        self.log.info("  On start:")
-        self.log.info("    %s" % self.pipeline.start_actions)
-        self.log.info("  On success:")
-        self.log.info("    %s" % self.pipeline.success_actions)
-        self.log.info("  On failure:")
-        self.log.info("    %s" % self.pipeline.failure_actions)
-        self.log.info("  On merge-failure:")
-        self.log.info("    %s" % self.pipeline.merge_failure_actions)
-        self.log.info("  When disabled:")
-        self.log.info("    %s" % self.pipeline.disabled_actions)
-
-    def getSubmitAllowNeeds(self):
-        # Get a list of code review labels that are allowed to be
-        # "needed" in the submit records for a change, with respect
-        # to this queue.  In other words, the list of review labels
-        # this queue itself is likely to set before submitting.
-        allow_needs = set()
-        for action_reporter in self.pipeline.success_actions:
-            allow_needs.update(action_reporter.getSubmitAllowNeeds())
-        return allow_needs
-
-    def eventMatches(self, event, change):
-        if event.forced_pipeline:
-            if event.forced_pipeline == self.pipeline.name:
-                self.log.debug("Event %s for change %s was directly assigned "
-                               "to pipeline %s" % (event, change, self))
-                return True
-            else:
-                return False
-        for ef in self.event_filters:
-            if ef.matches(event, change):
-                self.log.debug("Event %s for change %s matched %s "
-                               "in pipeline %s" % (event, change, ef, self))
-                return True
-        return False
-
-    def isChangeAlreadyInPipeline(self, change):
-        # Checks live items in the pipeline
-        for item in self.pipeline.getAllItems():
-            if item.live and change.equals(item.change):
-                return True
-        return False
-
-    def isChangeAlreadyInQueue(self, change, change_queue):
-        # Checks any item in the specified change queue
-        for item in change_queue.queue:
-            if change.equals(item.change):
-                return True
-        return False
-
-    def reportStart(self, item):
-        if not self.pipeline._disabled:
-            try:
-                self.log.info("Reporting start, action %s item %s" %
-                              (self.pipeline.start_actions, item))
-                ret = self.sendReport(self.pipeline.start_actions,
-                                      self.pipeline.source, item)
-                if ret:
-                    self.log.error("Reporting item start %s received: %s" %
-                                   (item, ret))
-            except:
-                self.log.exception("Exception while reporting start:")
-
-    def sendReport(self, action_reporters, source, item,
-                   message=None):
-        """Sends the built message off to configured reporters.
-
-        Takes the action_reporters, item, message and extra options and
-        sends them to the pluggable reporters.
-        """
-        report_errors = []
-        if len(action_reporters) > 0:
-            for reporter in action_reporters:
-                ret = reporter.report(source, self.pipeline, item)
-                if ret:
-                    report_errors.append(ret)
-            if len(report_errors) == 0:
-                return
-        return report_errors
-
-    def isChangeReadyToBeEnqueued(self, change):
-        return True
-
-    def enqueueChangesAhead(self, change, quiet, ignore_requirements,
-                            change_queue):
-        return True
-
-    def enqueueChangesBehind(self, change, quiet, ignore_requirements,
-                             change_queue):
-        return True
-
-    def checkForChangesNeededBy(self, change, change_queue):
-        return True
-
-    def getFailingDependentItems(self, item):
-        return None
-
-    def getDependentItems(self, item):
-        orig_item = item
-        items = []
-        while item.item_ahead:
-            items.append(item.item_ahead)
-            item = item.item_ahead
-        self.log.info("Change %s depends on changes %s" %
-                      (orig_item.change,
-                       [x.change for x in items]))
-        return items
-
-    def getItemForChange(self, change):
-        for item in self.pipeline.getAllItems():
-            if item.change.equals(change):
-                return item
-        return None
-
-    def findOldVersionOfChangeAlreadyInQueue(self, change):
-        for item in self.pipeline.getAllItems():
-            if not item.live:
-                continue
-            if change.isUpdateOf(item.change):
-                return item
-        return None
-
-    def removeOldVersionsOfChange(self, change):
-        if not self.pipeline.dequeue_on_new_patchset:
-            return
-        old_item = self.findOldVersionOfChangeAlreadyInQueue(change)
-        if old_item:
-            self.log.debug("Change %s is a new version of %s, removing %s" %
-                           (change, old_item.change, old_item))
-            self.removeItem(old_item)
-
-    def removeAbandonedChange(self, change):
-        self.log.debug("Change %s abandoned, removing." % change)
-        for item in self.pipeline.getAllItems():
-            if not item.live:
-                continue
-            if item.change.equals(change):
-                self.removeItem(item)
-
-    def reEnqueueItem(self, item, last_head):
-        with self.getChangeQueue(item.change, last_head.queue) as change_queue:
-            if change_queue:
-                self.log.debug("Re-enqueing change %s in queue %s" %
-                               (item.change, change_queue))
-                change_queue.enqueueItem(item)
-
-                # Re-set build results in case any new jobs have been
-                # added to the tree.
-                for build in item.current_build_set.getBuilds():
-                    if build.result:
-                        self.pipeline.setResult(item, build)
-                # Similarly, reset the item state.
-                if item.current_build_set.unable_to_merge:
-                    self.pipeline.setUnableToMerge(item)
-                if item.dequeued_needing_change:
-                    self.pipeline.setDequeuedNeedingChange(item)
-
-                self.reportStats(item)
-                return True
-            else:
-                self.log.error("Unable to find change queue for project %s" %
-                               item.change.project)
-                return False
-
-    def addChange(self, change, quiet=False, enqueue_time=None,
-                  ignore_requirements=False, live=True,
-                  change_queue=None):
-        self.log.debug("Considering adding change %s" % change)
-
-        # If we are adding a live change, check if it's a live item
-        # anywhere in the pipeline.  Otherwise, we will perform the
-        # duplicate check below on the specific change_queue.
-        if live and self.isChangeAlreadyInPipeline(change):
-            self.log.debug("Change %s is already in pipeline, "
-                           "ignoring" % change)
-            return True
-
-        if not self.isChangeReadyToBeEnqueued(change):
-            self.log.debug("Change %s is not ready to be enqueued, ignoring" %
-                           change)
-            return False
-
-        if not ignore_requirements:
-            for f in self.changeish_filters:
-                if not f.matches(change):
-                    self.log.debug("Change %s does not match pipeline "
-                                   "requirement %s" % (change, f))
-                    return False
-
-        with self.getChangeQueue(change, change_queue) as change_queue:
-            if not change_queue:
-                self.log.debug("Unable to find change queue for "
-                               "change %s in project %s" %
-                               (change, change.project))
-                return False
-
-            if not self.enqueueChangesAhead(change, quiet, ignore_requirements,
-                                            change_queue):
-                self.log.debug("Failed to enqueue changes "
-                               "ahead of %s" % change)
-                return False
-
-            if self.isChangeAlreadyInQueue(change, change_queue):
-                self.log.debug("Change %s is already in queue, "
-                               "ignoring" % change)
-                return True
-
-            self.log.debug("Adding change %s to queue %s" %
-                           (change, change_queue))
-            item = change_queue.enqueueChange(change)
-            if enqueue_time:
-                item.enqueue_time = enqueue_time
-            item.live = live
-            self.reportStats(item)
-            if not quiet:
-                if len(self.pipeline.start_actions) > 0:
-                    self.reportStart(item)
-            self.enqueueChangesBehind(change, quiet, ignore_requirements,
-                                      change_queue)
-            for trigger in self.sched.triggers.values():
-                trigger.onChangeEnqueued(item.change, self.pipeline)
-            return True
-
-    def dequeueItem(self, item):
-        self.log.debug("Removing change %s from queue" % item.change)
-        item.queue.dequeueItem(item)
-
-    def removeItem(self, item):
-        # Remove an item from the queue, probably because it has been
-        # superseded by another change.
-        self.log.debug("Canceling builds behind change: %s "
-                       "because it is being removed." % item.change)
-        self.cancelJobs(item)
-        self.dequeueItem(item)
-        self.reportStats(item)
-
-    def _makeMergerItem(self, item):
-        # Create a dictionary with all info about the item needed by
-        # the merger.
-        number = None
-        patchset = None
-        oldrev = None
-        newrev = None
-        if hasattr(item.change, 'number'):
-            number = item.change.number
-            patchset = item.change.patchset
-        elif hasattr(item.change, 'newrev'):
-            oldrev = item.change.oldrev
-            newrev = item.change.newrev
-        connection_name = self.pipeline.source.connection.connection_name
-        return dict(project=item.change.project.name,
-                    url=self.pipeline.source.getGitUrl(
-                        item.change.project),
-                    connection_name=connection_name,
-                    merge_mode=item.change.project.merge_mode,
-                    refspec=item.change.refspec,
-                    branch=item.change.branch,
-                    ref=item.current_build_set.ref,
-                    number=number,
-                    patchset=patchset,
-                    oldrev=oldrev,
-                    newrev=newrev,
-                    )
-
-    def prepareRef(self, item):
-        # Returns True if the ref is ready, false otherwise
-        build_set = item.current_build_set
-        if build_set.merge_state == build_set.COMPLETE:
-            return True
-        if build_set.merge_state == build_set.PENDING:
-            return False
-        ref = build_set.ref
-        if hasattr(item.change, 'refspec') and not ref:
-            self.log.debug("Preparing ref for: %s" % item.change)
-            item.current_build_set.setConfiguration()
-            dependent_items = self.getDependentItems(item)
-            dependent_items.reverse()
-            all_items = dependent_items + [item]
-            merger_items = map(self._makeMergerItem, all_items)
-            self.sched.merger.mergeChanges(merger_items,
-                                           item.current_build_set,
-                                           self.pipeline.precedence)
-        else:
-            self.log.debug("Preparing update repo for: %s" % item.change)
-            url = self.pipeline.source.getGitUrl(item.change.project)
-            self.sched.merger.updateRepo(item.change.project.name,
-                                         url, build_set,
-                                         self.pipeline.precedence)
-        # merge:merge has been emitted properly:
-        build_set.merge_state = build_set.PENDING
-        return False
-
-    def _launchJobs(self, item, jobs):
-        self.log.debug("Launching jobs for change %s" % item.change)
-        dependent_items = self.getDependentItems(item)
-        for job in jobs:
-            self.log.debug("Found job %s for change %s" % (job, item.change))
-            try:
-                build = self.sched.launcher.launch(job, item,
-                                                   self.pipeline,
-                                                   dependent_items)
-                self.log.debug("Adding build %s of job %s to item %s" %
-                               (build, job, item))
-                item.addBuild(build)
-            except:
-                self.log.exception("Exception while launching job %s "
-                                   "for change %s:" % (job, item.change))
-
-    def launchJobs(self, item):
-        jobs = self.pipeline.findJobsToRun(item, self.sched.mutex)
-        if jobs:
-            self._launchJobs(item, jobs)
-
-    def cancelJobs(self, item, prime=True):
-        self.log.debug("Cancel jobs for change %s" % item.change)
-        canceled = False
-        old_build_set = item.current_build_set
-        if prime and item.current_build_set.ref:
-            item.resetAllBuilds()
-        for build in old_build_set.getBuilds():
-            try:
-                self.sched.launcher.cancel(build)
-            except:
-                self.log.exception("Exception while canceling build %s "
-                                   "for change %s" % (build, item.change))
-            build.result = 'CANCELED'
-            canceled = True
-        self.updateBuildDescriptions(old_build_set)
-        for item_behind in item.items_behind:
-            self.log.debug("Canceling jobs for change %s, behind change %s" %
-                           (item_behind.change, item.change))
-            if self.cancelJobs(item_behind, prime=prime):
-                canceled = True
-        return canceled
-
-    def _processOneItem(self, item, nnfi):
-        changed = False
-        item_ahead = item.item_ahead
-        if item_ahead and (not item_ahead.live):
-            item_ahead = None
-        change_queue = item.queue
-        failing_reasons = []  # Reasons this item is failing
-
-        if self.checkForChangesNeededBy(item.change, change_queue) is not True:
-            # It's not okay to enqueue this change, we should remove it.
-            self.log.info("Dequeuing change %s because "
-                          "it can no longer merge" % item.change)
-            self.cancelJobs(item)
-            self.dequeueItem(item)
-            self.pipeline.setDequeuedNeedingChange(item)
-            if item.live:
-                try:
-                    self.reportItem(item)
-                except exceptions.MergeFailure:
-                    pass
-            return (True, nnfi)
-        dep_items = self.getFailingDependentItems(item)
-        actionable = change_queue.isActionable(item)
-        item.active = actionable
-        ready = False
-        if dep_items:
-            failing_reasons.append('a needed change is failing')
-            self.cancelJobs(item, prime=False)
-        else:
-            item_ahead_merged = False
-            if (item_ahead and item_ahead.change.is_merged):
-                item_ahead_merged = True
-            if (item_ahead != nnfi and not item_ahead_merged):
-                # Our current base is different than what we expected,
-                # and it's not because our current base merged.  Something
-                # ahead must have failed.
-                self.log.info("Resetting builds for change %s because the "
-                              "item ahead, %s, is not the nearest non-failing "
-                              "item, %s" % (item.change, item_ahead, nnfi))
-                change_queue.moveItem(item, nnfi)
-                changed = True
-                self.cancelJobs(item)
-            if actionable:
-                ready = self.prepareRef(item)
-                if item.current_build_set.unable_to_merge:
-                    failing_reasons.append("it has a merge conflict")
-                    ready = False
-        if actionable and ready and self.launchJobs(item):
-            changed = True
-        if self.pipeline.didAnyJobFail(item):
-            failing_reasons.append("at least one job failed")
-        if (not item.live) and (not item.items_behind):
-            failing_reasons.append("is a non-live item with no items behind")
-            self.dequeueItem(item)
-            changed = True
-        if ((not item_ahead) and self.pipeline.areAllJobsComplete(item)
-            and item.live):
-            try:
-                self.reportItem(item)
-            except exceptions.MergeFailure:
-                failing_reasons.append("it did not merge")
-                for item_behind in item.items_behind:
-                    self.log.info("Resetting builds for change %s because the "
-                                  "item ahead, %s, failed to merge" %
-                                  (item_behind.change, item))
-                    self.cancelJobs(item_behind)
-            self.dequeueItem(item)
-            changed = True
-        elif not failing_reasons and item.live:
-            nnfi = item
-        item.current_build_set.failing_reasons = failing_reasons
-        if failing_reasons:
-            self.log.debug("%s is a failing item because %s" %
-                           (item, failing_reasons))
-        return (changed, nnfi)
-
-    def processQueue(self):
-        # Do whatever needs to be done for each change in the queue
-        self.log.debug("Starting queue processor: %s" % self.pipeline.name)
-        changed = False
-        for queue in self.pipeline.queues:
-            queue_changed = False
-            nnfi = None  # Nearest non-failing item
-            for item in queue.queue[:]:
-                item_changed, nnfi = self._processOneItem(
-                    item, nnfi)
-                if item_changed:
-                    queue_changed = True
-                self.reportStats(item)
-            if queue_changed:
-                changed = True
-                status = ''
-                for item in queue.queue:
-                    status += item.formatStatus()
-                if status:
-                    self.log.debug("Queue %s status is now:\n %s" %
-                                   (queue.name, status))
-        self.log.debug("Finished queue processor: %s (changed: %s)" %
-                       (self.pipeline.name, changed))
-        return changed
-
-    def updateBuildDescriptions(self, build_set):
-        for build in build_set.getBuilds():
-            try:
-                desc = self.formatDescription(build)
-                self.sched.launcher.setBuildDescription(build, desc)
-            except:
-                # Log the failure and let loop continue
-                self.log.error("Failed to update description for build %s" %
-                               (build))
-
-        if build_set.previous_build_set:
-            for build in build_set.previous_build_set.getBuilds():
-                try:
-                    desc = self.formatDescription(build)
-                    self.sched.launcher.setBuildDescription(build, desc)
-                except:
-                    # Log the failure and let loop continue
-                    self.log.error("Failed to update description for "
-                                   "build %s in previous build set" % (build))
-
-    def onBuildStarted(self, build):
-        self.log.debug("Build %s started" % build)
-        return True
-
-    def onBuildCompleted(self, build):
-        self.log.debug("Build %s completed" % build)
-        item = build.build_set.item
-
-        self.pipeline.setResult(item, build)
-        self.sched.mutex.release(item, build.job)
-        self.log.debug("Item %s status is now:\n %s" %
-                       (item, item.formatStatus()))
-        return True
-
-    def onMergeCompleted(self, event):
-        build_set = event.build_set
-        item = build_set.item
-        build_set.merge_state = build_set.COMPLETE
-        build_set.zuul_url = event.zuul_url
-        if event.merged:
-            build_set.commit = event.commit
-        elif event.updated:
-            if not isinstance(item.change, NullChange):
-                build_set.commit = item.change.newrev
-        if not build_set.commit and not isinstance(item.change, NullChange):
-            self.log.info("Unable to merge change %s" % item.change)
-            self.pipeline.setUnableToMerge(item)
-
-    def reportItem(self, item):
-        if not item.reported:
-            # _reportItem() returns True if it failed to report.
-            item.reported = not self._reportItem(item)
-        if self.changes_merge:
-            succeeded = self.pipeline.didAllJobsSucceed(item)
-            merged = item.reported
-            if merged:
-                merged = self.pipeline.source.isMerged(item.change,
-                                                       item.change.branch)
-            self.log.info("Reported change %s status: all-succeeded: %s, "
-                          "merged: %s" % (item.change, succeeded, merged))
-            change_queue = item.queue
-            if not (succeeded and merged):
-                self.log.debug("Reported change %s failed tests or failed "
-                               "to merge" % (item.change))
-                change_queue.decreaseWindowSize()
-                self.log.debug("%s window size decreased to %s" %
-                               (change_queue, change_queue.window))
-                raise exceptions.MergeFailure(
-                    "Change %s failed to merge" % item.change)
-            else:
-                change_queue.increaseWindowSize()
-                self.log.debug("%s window size increased to %s" %
-                               (change_queue, change_queue.window))
-
-                for trigger in self.sched.triggers.values():
-                    trigger.onChangeMerged(item.change, self.pipeline.source)
-
-    def _reportItem(self, item):
-        self.log.debug("Reporting change %s" % item.change)
-        ret = True  # Means error as returned by trigger.report
-        if not self.pipeline.getJobs(item):
-            # We don't send empty reports with +1,
-            # and the same for -1's (merge failures or transient errors)
-            # as they cannot be followed by +1's
-            self.log.debug("No jobs for change %s" % item.change)
-            actions = []
-        elif self.pipeline.didAllJobsSucceed(item):
-            self.log.debug("success %s" % (self.pipeline.success_actions))
-            actions = self.pipeline.success_actions
-            item.setReportedResult('SUCCESS')
-            self.pipeline._consecutive_failures = 0
-        elif not self.pipeline.didMergerSucceed(item):
-            actions = self.pipeline.merge_failure_actions
-            item.setReportedResult('MERGER_FAILURE')
-        else:
-            actions = self.pipeline.failure_actions
-            item.setReportedResult('FAILURE')
-            self.pipeline._consecutive_failures += 1
-        if self.pipeline._disabled:
-            actions = self.pipeline.disabled_actions
-        # Check here if we should disable so that we only use the disabled
-        # reporters /after/ the last disable_at failure is still reported as
-        # normal.
-        if (self.pipeline.disable_at and not self.pipeline._disabled and
-            self.pipeline._consecutive_failures >= self.pipeline.disable_at):
-            self.pipeline._disabled = True
-        if actions:
-            try:
-                self.log.info("Reporting item %s, actions: %s" %
-                              (item, actions))
-                ret = self.sendReport(actions, self.pipeline.source, item)
-                if ret:
-                    self.log.error("Reporting item %s received: %s" %
-                                   (item, ret))
-            except:
-                self.log.exception("Exception while reporting:")
-                item.setReportedResult('ERROR')
-        self.updateBuildDescriptions(item.current_build_set)
-        return ret
-
-    def formatDescription(self, build):
-        concurrent_changes = ''
-        concurrent_builds = ''
-        other_builds = ''
-
-        for change in build.build_set.other_changes:
-            concurrent_changes += '<li><a href="{change.url}">\
-              {change.number},{change.patchset}</a></li>'.format(
-                change=change)
-
-        change = build.build_set.item.change
-
-        for build in build.build_set.getBuilds():
-            if build.url:
-                concurrent_builds += """\
-<li>
-  <a href="{build.url}">
-  {build.job.name} #{build.number}</a>: {build.result}
-</li>
-""".format(build=build)
-            else:
-                concurrent_builds += """\
-<li>
-  {build.job.name}: {build.result}
-</li>""".format(build=build)
-
-        if build.build_set.previous_build_set:
-            other_build = build.build_set.previous_build_set.getBuild(
-                build.job.name)
-            if other_build:
-                other_builds += """\
-<li>
-  Preceded by: <a href="{build.url}">
-  {build.job.name} #{build.number}</a>
-</li>
-""".format(build=other_build)
-
-        if build.build_set.next_build_set:
-            other_build = build.build_set.next_build_set.getBuild(
-                build.job.name)
-            if other_build:
-                other_builds += """\
-<li>
-  Succeeded by: <a href="{build.url}">
-  {build.job.name} #{build.number}</a>
-</li>
-""".format(build=other_build)
-
-        result = build.build_set.result
-
-        if hasattr(change, 'number'):
-            ret = """\
-<p>
-  Triggered by change:
-    <a href="{change.url}">{change.number},{change.patchset}</a><br/>
-  Branch: <b>{change.branch}</b><br/>
-  Pipeline: <b>{self.pipeline.name}</b>
-</p>"""
-        elif hasattr(change, 'ref'):
-            ret = """\
-<p>
-  Triggered by reference:
-    {change.ref}</a><br/>
-  Old revision: <b>{change.oldrev}</b><br/>
-  New revision: <b>{change.newrev}</b><br/>
-  Pipeline: <b>{self.pipeline.name}</b>
-</p>"""
-        else:
-            ret = ""
-
-        if concurrent_changes:
-            ret += """\
-<p>
-  Other changes tested concurrently with this change:
-  <ul>{concurrent_changes}</ul>
-</p>
-"""
-        if concurrent_builds:
-            ret += """\
-<p>
-  All builds for this change set:
-  <ul>{concurrent_builds}</ul>
-</p>
-"""
-
-        if other_builds:
-            ret += """\
-<p>
-  Other build sets for this change:
-  <ul>{other_builds}</ul>
-</p>
-"""
-        if result:
-            ret += """\
-<p>
-  Reported result: <b>{result}</b>
-</p>
-"""
-
-        ret = ret.format(**locals())
-        return ret
-
-    def reportStats(self, item):
-        if not statsd:
-            return
-        try:
-            # Update the gauge on enqueue and dequeue, but timers only
-            # when dequeing.
-            if item.dequeue_time:
-                dt = int((item.dequeue_time - item.enqueue_time) * 1000)
-            else:
-                dt = None
-            items = len(self.pipeline.getAllItems())
-
-            # stats.timers.zuul.pipeline.NAME.resident_time
-            # stats_counts.zuul.pipeline.NAME.total_changes
-            # stats.gauges.zuul.pipeline.NAME.current_changes
-            key = 'zuul.pipeline.%s' % self.pipeline.name
-            statsd.gauge(key + '.current_changes', items)
-            if dt:
-                statsd.timing(key + '.resident_time', dt)
-                statsd.incr(key + '.total_changes')
-
-            # stats.timers.zuul.pipeline.NAME.ORG.PROJECT.resident_time
-            # stats_counts.zuul.pipeline.NAME.ORG.PROJECT.total_changes
-            project_name = item.change.project.name.replace('/', '.')
-            key += '.%s' % project_name
-            if dt:
-                statsd.timing(key + '.resident_time', dt)
-                statsd.incr(key + '.total_changes')
-        except:
-            self.log.exception("Exception reporting pipeline stats")
-
-
-class DynamicChangeQueueContextManager(object):
-    def __init__(self, change_queue):
-        self.change_queue = change_queue
-
-    def __enter__(self):
-        return self.change_queue
-
-    def __exit__(self, etype, value, tb):
-        if self.change_queue and not self.change_queue.queue:
-            self.change_queue.pipeline.removeQueue(self.change_queue.queue)
-
-
-class IndependentPipelineManager(BasePipelineManager):
-    log = logging.getLogger("zuul.IndependentPipelineManager")
-    changes_merge = False
-
-    def _postConfig(self, layout):
-        super(IndependentPipelineManager, self)._postConfig(layout)
-
-    def getChangeQueue(self, change, existing=None):
-        # creates a new change queue for every change
-        if existing:
-            return DynamicChangeQueueContextManager(existing)
-        if change.project not in self.pipeline.getProjects():
-            self.pipeline.addProject(change.project)
-        change_queue = ChangeQueue(self.pipeline)
-        change_queue.addProject(change.project)
-        self.pipeline.addQueue(change_queue)
-        self.log.debug("Dynamically created queue %s", change_queue)
-        return DynamicChangeQueueContextManager(change_queue)
-
-    def enqueueChangesAhead(self, change, quiet, ignore_requirements,
-                            change_queue):
-        ret = self.checkForChangesNeededBy(change, change_queue)
-        if ret in [True, False]:
-            return ret
-        self.log.debug("  Changes %s must be merged ahead of %s" %
-                       (ret, change))
-        for needed_change in ret:
-            # This differs from the dependent pipeline by enqueuing
-            # changes ahead as "not live", that is, not intended to
-            # have jobs run.  Also, pipeline requirements are always
-            # ignored (which is safe because the changes are not
-            # live).
-            r = self.addChange(needed_change, quiet=True,
-                               ignore_requirements=True,
-                               live=False, change_queue=change_queue)
-            if not r:
-                return False
-        return True
-
-    def checkForChangesNeededBy(self, change, change_queue):
-        if self.pipeline.ignore_dependencies:
-            return True
-        self.log.debug("Checking for changes needed by %s:" % change)
-        # Return true if okay to proceed enqueing this change,
-        # false if the change should not be enqueued.
-        if not hasattr(change, 'needs_changes'):
-            self.log.debug("  Changeish does not support dependencies")
-            return True
-        if not change.needs_changes:
-            self.log.debug("  No changes needed")
-            return True
-        changes_needed = []
-        for needed_change in change.needs_changes:
-            self.log.debug("  Change %s needs change %s:" % (
-                change, needed_change))
-            if needed_change.is_merged:
-                self.log.debug("  Needed change is merged")
-                continue
-            if self.isChangeAlreadyInQueue(needed_change, change_queue):
-                self.log.debug("  Needed change is already ahead in the queue")
-                continue
-            self.log.debug("  Change %s is needed" % needed_change)
-            if needed_change not in changes_needed:
-                changes_needed.append(needed_change)
-                continue
-            # This differs from the dependent pipeline check in not
-            # verifying that the dependent change is mergable.
-        if changes_needed:
-            return changes_needed
-        return True
-
-    def dequeueItem(self, item):
-        super(IndependentPipelineManager, self).dequeueItem(item)
-        # An independent pipeline manager dynamically removes empty
-        # queues
-        if not item.queue.queue:
-            self.pipeline.removeQueue(item.queue)
-
-
-class StaticChangeQueueContextManager(object):
-    def __init__(self, change_queue):
-        self.change_queue = change_queue
-
-    def __enter__(self):
-        return self.change_queue
-
-    def __exit__(self, etype, value, tb):
-        pass
-
-
-class DependentPipelineManager(BasePipelineManager):
-    log = logging.getLogger("zuul.DependentPipelineManager")
-    changes_merge = True
-
-    def __init__(self, *args, **kwargs):
-        super(DependentPipelineManager, self).__init__(*args, **kwargs)
-
-    def _postConfig(self, layout):
-        super(DependentPipelineManager, self)._postConfig(layout)
-        self.buildChangeQueues()
-
-    def buildChangeQueues(self):
-        self.log.debug("Building shared change queues")
-        change_queues = []
-
-        for project in self.pipeline.getProjects():
-            change_queue = ChangeQueue(
-                self.pipeline,
-                window=self.pipeline.window,
-                window_floor=self.pipeline.window_floor,
-                window_increase_type=self.pipeline.window_increase_type,
-                window_increase_factor=self.pipeline.window_increase_factor,
-                window_decrease_type=self.pipeline.window_decrease_type,
-                window_decrease_factor=self.pipeline.window_decrease_factor)
-            change_queue.addProject(project)
-            change_queues.append(change_queue)
-            self.log.debug("Created queue: %s" % change_queue)
-
-        # Iterate over all queues trying to combine them, and keep doing
-        # so until they can not be combined further.
-        last_change_queues = change_queues
-        while True:
-            new_change_queues = self.combineChangeQueues(last_change_queues)
-            if len(last_change_queues) == len(new_change_queues):
-                break
-            last_change_queues = new_change_queues
-
-        self.log.info("  Shared change queues:")
-        for queue in new_change_queues:
-            self.pipeline.addQueue(queue)
-            self.log.info("    %s containing %s" % (
-                queue, queue.generated_name))
-
-    def combineChangeQueues(self, change_queues):
-        self.log.debug("Combining shared queues")
-        new_change_queues = []
-        for a in change_queues:
-            merged_a = False
-            for b in new_change_queues:
-                if not a.getJobs().isdisjoint(b.getJobs()):
-                    self.log.debug("Merging queue %s into %s" % (a, b))
-                    b.mergeChangeQueue(a)
-                    merged_a = True
-                    break  # this breaks out of 'for b' and continues 'for a'
-            if not merged_a:
-                self.log.debug("Keeping queue %s" % (a))
-                new_change_queues.append(a)
-        return new_change_queues
-
-    def getChangeQueue(self, change, existing=None):
-        if existing:
-            return StaticChangeQueueContextManager(existing)
-        return StaticChangeQueueContextManager(
-            self.pipeline.getQueue(change.project))
-
-    def isChangeReadyToBeEnqueued(self, change):
-        if not self.pipeline.source.canMerge(change,
-                                             self.getSubmitAllowNeeds()):
-            self.log.debug("Change %s can not merge, ignoring" % change)
-            return False
-        return True
-
-    def enqueueChangesBehind(self, change, quiet, ignore_requirements,
-                             change_queue):
-        to_enqueue = []
-        self.log.debug("Checking for changes needing %s:" % change)
-        if not hasattr(change, 'needed_by_changes'):
-            self.log.debug("  Changeish does not support dependencies")
-            return
-        for other_change in change.needed_by_changes:
-            with self.getChangeQueue(other_change) as other_change_queue:
-                if other_change_queue != change_queue:
-                    self.log.debug("  Change %s in project %s can not be "
-                                   "enqueued in the target queue %s" %
-                                   (other_change, other_change.project,
-                                    change_queue))
-                    continue
-            if self.pipeline.source.canMerge(other_change,
-                                             self.getSubmitAllowNeeds()):
-                self.log.debug("  Change %s needs %s and is ready to merge" %
-                               (other_change, change))
-                to_enqueue.append(other_change)
-
-        if not to_enqueue:
-            self.log.debug("  No changes need %s" % change)
-
-        for other_change in to_enqueue:
-            self.addChange(other_change, quiet=quiet,
-                           ignore_requirements=ignore_requirements,
-                           change_queue=change_queue)
-
-    def enqueueChangesAhead(self, change, quiet, ignore_requirements,
-                            change_queue):
-        ret = self.checkForChangesNeededBy(change, change_queue)
-        if ret in [True, False]:
-            return ret
-        self.log.debug("  Changes %s must be merged ahead of %s" %
-                       (ret, change))
-        for needed_change in ret:
-            r = self.addChange(needed_change, quiet=quiet,
-                               ignore_requirements=ignore_requirements,
-                               change_queue=change_queue)
-            if not r:
-                return False
-        return True
-
-    def checkForChangesNeededBy(self, change, change_queue):
-        self.log.debug("Checking for changes needed by %s:" % change)
-        # Return true if okay to proceed enqueing this change,
-        # false if the change should not be enqueued.
-        if not hasattr(change, 'needs_changes'):
-            self.log.debug("  Changeish does not support dependencies")
-            return True
-        if not change.needs_changes:
-            self.log.debug("  No changes needed")
-            return True
-        changes_needed = []
-        # Ignore supplied change_queue
-        with self.getChangeQueue(change) as change_queue:
-            for needed_change in change.needs_changes:
-                self.log.debug("  Change %s needs change %s:" % (
-                    change, needed_change))
-                if needed_change.is_merged:
-                    self.log.debug("  Needed change is merged")
-                    continue
-                with self.getChangeQueue(needed_change) as needed_change_queue:
-                    if needed_change_queue != change_queue:
-                        self.log.debug("  Change %s in project %s does not "
-                                       "share a change queue with %s "
-                                       "in project %s" %
-                                       (needed_change, needed_change.project,
-                                        change, change.project))
-                        return False
-                if not needed_change.is_current_patchset:
-                    self.log.debug("  Needed change is not the "
-                                   "current patchset")
-                    return False
-                if self.isChangeAlreadyInQueue(needed_change, change_queue):
-                    self.log.debug("  Needed change is already ahead "
-                                   "in the queue")
-                    continue
-                if self.pipeline.source.canMerge(needed_change,
-                                                 self.getSubmitAllowNeeds()):
-                    self.log.debug("  Change %s is needed" % needed_change)
-                    if needed_change not in changes_needed:
-                        changes_needed.append(needed_change)
-                        continue
-                # The needed change can't be merged.
-                self.log.debug("  Change %s is needed but can not be merged" %
-                               needed_change)
-                return False
-        if changes_needed:
-            return changes_needed
-        return True
-
-    def getFailingDependentItems(self, item):
-        if not hasattr(item.change, 'needs_changes'):
-            return None
-        if not item.change.needs_changes:
-            return None
-        failing_items = set()
-        for needed_change in item.change.needs_changes:
-            needed_item = self.getItemForChange(needed_change)
-            if not needed_item:
-                continue
-            if needed_item.current_build_set.failing_reasons:
-                failing_items.add(needed_item)
-        if failing_items:
-            return failing_items
-        return None
+# Copyright 2012-2015 Hewlett-Packard Development Company, L.P.
+# Copyright 2013 OpenStack Foundation
+# Copyright 2013 Antoine "hashar" Musso
+# Copyright 2013 Wikimedia Foundation Inc.
+#
+# Licensed under the Apache License, Version 2.0 (the "License"); you may
+# not use this file except in compliance with the License. You may obtain
+# a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+# License for the specific language governing permissions and limitations
+# under the License.
+
+import extras
+import json
+import logging
+import os
+import pickle
+from six.moves import queue as Queue
+import re
+import sys
+import threading
+import time
+import yaml
+
+import layoutvalidator
+import model
+from model import Pipeline, Project, ChangeQueue
+from model import ChangeishFilter, NullChange
+from zuul import change_matcher, exceptions
+from zuul import version as zuul_version
+
+statsd = extras.try_import('statsd.statsd')
+
+
+def deep_format(obj, paramdict):
+    """Apply the paramdict via str.format() to all string objects found within
+       the supplied obj. Lists and dicts are traversed recursively.
+
+       Borrowed from Jenkins Job Builder project"""
+    if isinstance(obj, str):
+        ret = obj.format(**paramdict)
+    elif isinstance(obj, list):
+        ret = []
+        for item in obj:
+            ret.append(deep_format(item, paramdict))
+    elif isinstance(obj, dict):
+        ret = {}
+        for item in obj:
+            exp_item = item.format(**paramdict)
+
+            ret[exp_item] = deep_format(obj[item], paramdict)
+    else:
+        ret = obj
+    return ret
+
+
+class MutexHandler(object):
+    log = logging.getLogger("zuul.MutexHandler")
+
+    def __init__(self):
+        self.mutexes = {}
+
+    def acquire(self, item, job):
+        if not job.mutex:
+            return True
+        mutex_name = job.mutex
+        m = self.mutexes.get(mutex_name)
+        if not m:
+            # The mutex is not held, acquire it
+            self._acquire(mutex_name, item, job.name)
+            return True
+        held_item, held_job_name = m
+        if held_item is item and held_job_name == job.name:
+            # This item already holds the mutex
+            return True
+        held_build = held_item.current_build_set.getBuild(held_job_name)
+        if held_build and held_build.result:
+            # The build that held the mutex is complete, release it
+            # and let the new item have it.
+            self.log.error("Held mutex %s being released because "
+                           "the build that holds it is complete" %
+                           (mutex_name,))
+            self._release(mutex_name, item, job.name)
+            self._acquire(mutex_name, item, job.name)
+            return True
+        return False
+
+    def release(self, item, job):
+        if not job.mutex:
+            return
+        mutex_name = job.mutex
+        m = self.mutexes.get(mutex_name)
+        if not m:
+            # The mutex is not held, nothing to do
+            self.log.error("Mutex can not be released for %s "
+                           "because the mutex is not held" %
+                           (item,))
+            return
+        held_item, held_job_name = m
+        if held_item is item and held_job_name == job.name:
+            # This item holds the mutex
+            self._release(mutex_name, item, job.name)
+            return
+        self.log.error("Mutex can not be released for %s "
+                       "which does not hold it" %
+                       (item,))
+
+    def _acquire(self, mutex_name, item, job_name):
+        self.log.debug("Job %s of item %s acquiring mutex %s" %
+                       (job_name, item, mutex_name))
+        self.mutexes[mutex_name] = (item, job_name)
+
+    def _release(self, mutex_name, item, job_name):
+        self.log.debug("Job %s of item %s releasing mutex %s" %
+                       (job_name, item, mutex_name))
+        del self.mutexes[mutex_name]
+
+
+class ManagementEvent(object):
+    """An event that should be processed within the main queue run loop"""
+    def __init__(self):
+        self._wait_event = threading.Event()
+        self._exception = None
+        self._traceback = None
+
+    def exception(self, e, tb):
+        self._exception = e
+        self._traceback = tb
+        self._wait_event.set()
+
+    def done(self):
+        self._wait_event.set()
+
+    def wait(self, timeout=None):
+        self._wait_event.wait(timeout)
+        if self._exception:
+            raise self._exception, None, self._traceback
+        return self._wait_event.is_set()
+
+
+class ReconfigureEvent(ManagementEvent):
+    """Reconfigure the scheduler.  The layout will be (re-)loaded from
+    the path specified in the configuration.
+
+    :arg ConfigParser config: the new configuration
+    """
+    def __init__(self, config):
+        super(ReconfigureEvent, self).__init__()
+        self.config = config
+
+
+class PromoteEvent(ManagementEvent):
+    """Promote one or more changes to the head of the queue.
+
+    :arg str pipeline_name: the name of the pipeline
+    :arg list change_ids: a list of strings of change ids in the form
+        1234,1
+    """
+
+    def __init__(self, pipeline_name, change_ids):
+        super(PromoteEvent, self).__init__()
+        self.pipeline_name = pipeline_name
+        self.change_ids = change_ids
+
+
+class EnqueueEvent(ManagementEvent):
+    """Enqueue a change into a pipeline
+
+    :arg TriggerEvent trigger_event: a TriggerEvent describing the
+        trigger, pipeline, and change to enqueue
+    """
+
+    def __init__(self, trigger_event):
+        super(EnqueueEvent, self).__init__()
+        self.trigger_event = trigger_event
+
+
+class ResultEvent(object):
+    """An event that needs to modify the pipeline state due to a
+    result from an external system."""
+
+    pass
+
+
+class BuildStartedEvent(ResultEvent):
+    """A build has started.
+
+    :arg Build build: The build which has started.
+    """
+
+    def __init__(self, build):
+        self.build = build
+
+
+class BuildCompletedEvent(ResultEvent):
+    """A build has completed
+
+    :arg Build build: The build which has completed.
+    """
+
+    def __init__(self, build):
+        self.build = build
+
+
+class MergeCompletedEvent(ResultEvent):
+    """A remote merge operation has completed
+
+    :arg BuildSet build_set: The build_set which is ready.
+    :arg str zuul_url: The URL of the Zuul Merger.
+    :arg bool merged: Whether the merge succeeded (changes with refs).
+    :arg bool updated: Whether the repo was updated (changes without refs).
+    :arg str commit: The SHA of the merged commit (changes with refs).
+    """
+
+    def __init__(self, build_set, zuul_url, merged, updated, commit):
+        self.build_set = build_set
+        self.zuul_url = zuul_url
+        self.merged = merged
+        self.updated = updated
+        self.commit = commit
+
+
+def toList(item):
+    if not item:
+        return []
+    if isinstance(item, list):
+        return item
+    return [item]
+
+
+class Scheduler(threading.Thread):
+    log = logging.getLogger("zuul.Scheduler")
+
+    def __init__(self, config):
+        threading.Thread.__init__(self)
+        self.daemon = True
+        self.wake_event = threading.Event()
+        self.layout_lock = threading.Lock()
+        self.run_handler_lock = threading.Lock()
+        self._pause = False
+        self._exit = False
+        self._stopped = False
+        self.launcher = None
+        self.merger = None
+        self.mutex = MutexHandler()
+        self.connections = dict()
+        # Despite triggers being part of the pipeline, there is one trigger set
+        # per scheduler. The pipeline handles the trigger filters but since
+        # the events are handled by the scheduler itself it needs to handle
+        # the loading of the triggers.
+        # self.triggers['connection_name'] = triggerObject
+        self.triggers = dict()
+        self.config = config
+
+        self.trigger_event_queue = Queue.Queue()
+        self.result_event_queue = Queue.Queue()
+        self.management_event_queue = Queue.Queue()
+        self.layout = model.Layout()
+
+        self.zuul_version = zuul_version.version_info.release_string()
+        self.last_reconfigured = None
+
+        # A set of reporter configuration keys to action mapping
+        self._reporter_actions = {
+            'start': 'start_actions',
+            'success': 'success_actions',
+            'failure': 'failure_actions',
+            'merge-failure': 'merge_failure_actions',
+            'disabled': 'disabled_actions',
+        }
+
+    def stop(self):
+        self._stopped = True
+        self._unloadDrivers()
+        self.stopConnections()
+        self.wake_event.set()
+
+    def testConfig(self, config_path, connections):
+        # Take the list of set up connections directly here rather than with
+        # registerConnections as we don't want to do the onLoad event yet.
+        return self._parseConfig(config_path, connections)
+
+    def _parseSkipIf(self, config_job):
+        cm = change_matcher
+        skip_matchers = []
+
+        for config_skip in config_job.get('skip-if', []):
+            nested_matchers = []
+
+            project_regex = config_skip.get('project')
+            if project_regex:
+                nested_matchers.append(cm.ProjectMatcher(project_regex))
+
+            branch_regex = config_skip.get('branch')
+            if branch_regex:
+                nested_matchers.append(cm.BranchMatcher(branch_regex))
+
+            file_regexes = toList(config_skip.get('all-files-match-any'))
+            if file_regexes:
+                file_matchers = [cm.FileMatcher(x) for x in file_regexes]
+                all_files_matcher = cm.MatchAllFiles(file_matchers)
+                nested_matchers.append(all_files_matcher)
+
+            # All patterns need to match a given skip-if predicate
+            skip_matchers.append(cm.MatchAll(nested_matchers))
+
+        if skip_matchers:
+            # Any skip-if predicate can be matched to trigger a skip
+            return cm.MatchAny(skip_matchers)
+
+    def registerConnections(self, connections, load=True):
+        # load: whether or not to trigger the onLoad for the connection. This
+        # is useful for not doing a full load during layout validation.
+        self.connections = connections
+        for connection_name, connection in self.connections.items():
+            connection.registerScheduler(self)
+            if load:
+                connection.onLoad()
+
+    def stopConnections(self):
+        for connection_name, connection in self.connections.items():
+            connection.onStop()
+
+    def _unloadDrivers(self):
+        for trigger in self.triggers.values():
+            trigger.stop()
+        self.triggers = {}
+        for pipeline in self.layout.pipelines.values():
+            pipeline.source.stop()
+            for action in self._reporter_actions.values():
+                for reporter in pipeline.__getattribute__(action):
+                    reporter.stop()
+
+    def _getDriver(self, dtype, connection_name, driver_config={}):
+        # Instantiate a driver such as a trigger, source or reporter
+        # TODO(jhesketh): Make this list dynamic or use entrypoints etc.
+        # Stevedore was not a good fit here due to the nature of triggers.
+        # Specifically we don't want to load a trigger per a pipeline as one
+        # trigger can listen to a stream (from gerrit, for example) and the
+        # scheduler decides which eventfilter to use. As such we want to load
+        # trigger+connection pairs uniquely.
+        drivers = {
+            'source': {
+                'gerrit': 'zuul.source.gerrit:GerritSource',
+            },
+            'trigger': {
+                'gerrit': 'zuul.trigger.gerrit:GerritTrigger',
+                'timer': 'zuul.trigger.timer:TimerTrigger',
+                'zuul': 'zuul.trigger.zuultrigger:ZuulTrigger',
+            },
+            'reporter': {
+                'gerrit': 'zuul.reporter.gerrit:GerritReporter',
+                'smtp': 'zuul.reporter.smtp:SMTPReporter',
+            },
+        }
+
+        # TODO(jhesketh): Check the connection_name exists
+        if connection_name in self.connections.keys():
+            driver_name = self.connections[connection_name].driver_name
+            connection = self.connections[connection_name]
+        else:
+            # In some cases a driver may not be related to a connection. For
+            # example, the 'timer' or 'zuul' triggers.
+            driver_name = connection_name
+            connection = None
+        driver = drivers[dtype][driver_name].split(':')
+        driver_instance = getattr(
+            __import__(driver[0], fromlist=['']), driver[1])(
+                driver_config, self, connection
+        )
+
+        if connection:
+            connection.registerUse(dtype, driver_instance)
+
+        return driver_instance
+
+    def _getSourceDriver(self, connection_name):
+        return self._getDriver('source', connection_name)
+
+    def _getReporterDriver(self, connection_name, driver_config={}):
+        return self._getDriver('reporter', connection_name, driver_config)
+
+    def _getTriggerDriver(self, connection_name, driver_config={}):
+        return self._getDriver('trigger', connection_name, driver_config)
+
+    def _parseConfig(self, config_path, connections):
+        layout = model.Layout()
+        project_templates = {}
+
+        if config_path:
+            config_path = os.path.expanduser(config_path)
+            if not os.path.exists(config_path):
+                raise Exception("Unable to read layout config file at %s" %
+                                config_path)
+        with open(config_path) as config_file:
+            data = yaml.load(config_file)
+
+        validator = layoutvalidator.LayoutValidator()
+        validator.validate(data, connections)
+
+        config_env = {}
+        for include in data.get('includes', []):
+            if 'python-file' in include:
+                fn = include['python-file']
+                if not os.path.isabs(fn):
+                    base = os.path.dirname(os.path.realpath(config_path))
+                    fn = os.path.join(base, fn)
+                fn = os.path.expanduser(fn)
+                execfile(fn, config_env)
+
+        for conf_pipeline in data.get('pipelines', []):
+            pipeline = Pipeline(conf_pipeline['name'])
+            pipeline.description = conf_pipeline.get('description')
+            # TODO(jeblair): remove backwards compatibility:
+            pipeline.source = self._getSourceDriver(
+                conf_pipeline.get('source', 'gerrit'))
+            precedence = model.PRECEDENCE_MAP[conf_pipeline.get('precedence')]
+            pipeline.precedence = precedence
+            pipeline.failure_message = conf_pipeline.get('failure-message',
+                                                         "Build failed.")
+            pipeline.merge_failure_message = conf_pipeline.get(
+                'merge-failure-message', "Merge Failed.\n\nThis change or one "
+                "of its cross-repo dependencies was unable to be "
+                "automatically merged with the current state of its "
+                "repository. Please rebase the change and upload a new "
+                "patchset.")
+            pipeline.success_message = conf_pipeline.get('success-message',
+                                                         "Build succeeded.")
+            pipeline.footer_message = conf_pipeline.get('footer-message', "")
+            pipeline.dequeue_on_new_patchset = conf_pipeline.get(
+                'dequeue-on-new-patchset', True)
+            pipeline.ignore_dependencies = conf_pipeline.get(
+                'ignore-dependencies', False)
+
+            for conf_key, action in self._reporter_actions.items():
+                reporter_set = []
+                if conf_pipeline.get(conf_key):
+                    for reporter_name, params \
+                        in conf_pipeline.get(conf_key).items():
+                        reporter = self._getReporterDriver(reporter_name,
+                                                           params)
+                        reporter.setAction(conf_key)
+                        reporter_set.append(reporter)
+                setattr(pipeline, action, reporter_set)
+
+            # If merge-failure actions aren't explicit, use the failure actions
+            if not pipeline.merge_failure_actions:
+                pipeline.merge_failure_actions = pipeline.failure_actions
+
+            pipeline.disable_at = conf_pipeline.get(
+                'disable-after-consecutive-failures', None)
+
+            pipeline.window = conf_pipeline.get('window', 20)
+            pipeline.window_floor = conf_pipeline.get('window-floor', 3)
+            pipeline.window_increase_type = conf_pipeline.get(
+                'window-increase-type', 'linear')
+            pipeline.window_increase_factor = conf_pipeline.get(
+                'window-increase-factor', 1)
+            pipeline.window_decrease_type = conf_pipeline.get(
+                'window-decrease-type', 'exponential')
+            pipeline.window_decrease_factor = conf_pipeline.get(
+                'window-decrease-factor', 2)
+
+            manager = globals()[conf_pipeline['manager']](self, pipeline)
+            pipeline.setManager(manager)
+            layout.pipelines[conf_pipeline['name']] = pipeline
+
+            if 'require' in conf_pipeline or 'reject' in conf_pipeline:
+                require = conf_pipeline.get('require', {})
+                reject = conf_pipeline.get('reject', {})
+                f = ChangeishFilter(
+                    open=require.get('open'),
+                    current_patchset=require.get('current-patchset'),
+                    statuses=toList(require.get('status')),
+                    required_approvals=toList(require.get('approval')),
+                    reject_approvals=toList(reject.get('approval'))
+                )
+                manager.changeish_filters.append(f)
+
+            for trigger_name, trigger_config\
+                in conf_pipeline.get('trigger').items():
+                if trigger_name not in self.triggers.keys():
+                    self.triggers[trigger_name] = \
+                        self._getTriggerDriver(trigger_name, trigger_config)
+
+            for trigger_name, trigger in self.triggers.items():
+                if trigger_name in conf_pipeline['trigger']:
+                    manager.event_filters += trigger.getEventFilters(
+                        conf_pipeline['trigger'][trigger_name])
+
+        for project_template in data.get('project-templates', []):
+            # Make sure the template only contains valid pipelines
+            tpl = dict(
+                (pipe_name, project_template.get(pipe_name))
+                for pipe_name in layout.pipelines.keys()
+                if pipe_name in project_template
+            )
+            project_templates[project_template.get('name')] = tpl
+
+        for config_job in data.get('jobs', []):
+            job = layout.getJob(config_job['name'])
+            # Be careful to only set attributes explicitly present on
+            # this job, to avoid squashing attributes set by a meta-job.
+            m = config_job.get('queue-name', None)
+            if m:
+                job.queue_name = m
+            m = config_job.get('failure-message', None)
+            if m:
+                job.failure_message = m
+            m = config_job.get('success-message', None)
+            if m:
+                job.success_message = m
+            m = config_job.get('failure-pattern', None)
+            if m:
+                job.failure_pattern = m
+            m = config_job.get('success-pattern', None)
+            if m:
+                job.success_pattern = m
+            m = config_job.get('hold-following-changes', False)
+            if m:
+                job.hold_following_changes = True
+            m = config_job.get('voting', None)
+            if m is not None:
+                job.voting = m
+            m = config_job.get('mutex', None)
+            if m is not None:
+                job.mutex = m
+            tags = toList(config_job.get('tags'))
+            if tags:
+                # Tags are merged via a union rather than a
+                # destructive copy because they are intended to
+                # accumulate onto any previously applied tags from
+                # metajobs.
+                job.tags = job.tags.union(set(tags))
+            fname = config_job.get('parameter-function', None)
+            if fname:
+                func = config_env.get(fname, None)
+                if not func:
+                    raise Exception("Unable to find function %s" % fname)
+                job.parameter_function = func
+            branches = toList(config_job.get('branch'))
+            if branches:
+                job._branches = branches
+                job.branches = [re.compile(x) for x in branches]
+            files = toList(config_job.get('files'))
+            if files:
+                job._files = files
+                job.files = [re.compile(x) for x in files]
+            skip_if_matcher = self._parseSkipIf(config_job)
+            if skip_if_matcher:
+                job.skip_if_matcher = skip_if_matcher
+            swift = toList(config_job.get('swift'))
+            if swift:
+                for s in swift:
+                    job.swift[s['name']] = s
+
+        def add_jobs(job_tree, config_jobs):
+            for job in config_jobs:
+                if isinstance(job, list):
+                    for x in job:
+                        add_jobs(job_tree, x)
+                if isinstance(job, dict):
+                    for parent, children in job.items():
+                        parent_tree = job_tree.addJob(layout.getJob(parent))
+                        add_jobs(parent_tree, children)
+                if isinstance(job, str):
+                    job_tree.addJob(layout.getJob(job))
+
+        for config_project in data.get('projects', []):
+            project = Project(config_project['name'])
+            shortname = config_project['name'].split('/')[-1]
+
+            # This is reversed due to the prepend operation below, so
+            # the ultimate order is templates (in order) followed by
+            # statically defined jobs.
+            for requested_template in reversed(
+                config_project.get('template', [])):
+                # Fetch the template from 'project-templates'
+                tpl = project_templates.get(
+                    requested_template.get('name'))
+                # Expand it with the project context
+                requested_template['name'] = shortname
+                expanded = deep_format(tpl, requested_template)
+                # Finally merge the expansion with whatever has been
+                # already defined for this project.  Prepend our new
+                # jobs to existing ones (which may have been
+                # statically defined or defined by other templates).
+                for pipeline in layout.pipelines.values():
+                    if pipeline.name in expanded:
+                        config_project.update(
+                            {pipeline.name: expanded[pipeline.name] +
+                             config_project.get(pipeline.name, [])})
+
+            layout.projects[config_project['name']] = project
+            mode = config_project.get('merge-mode', 'merge-resolve')
+            project.merge_mode = model.MERGER_MAP[mode]
+            for pipeline in layout.pipelines.values():
+                if pipeline.name in config_project:
+                    job_tree = pipeline.addProject(project)
+                    config_jobs = config_project[pipeline.name]
+                    add_jobs(job_tree, config_jobs)
+
+        # All jobs should be defined at this point, get rid of
+        # metajobs so that getJob isn't doing anything weird.
+        layout.metajobs = []
+
+        for pipeline in layout.pipelines.values():
+            pipeline.manager._postConfig(layout)
+
+        return layout
+
+    def setLauncher(self, launcher):
+        self.launcher = launcher
+
+    def setMerger(self, merger):
+        self.merger = merger
+
+    def getProject(self, name, create_foreign=False):
+        self.layout_lock.acquire()
+        p = None
+        try:
+            p = self.layout.projects.get(name)
+            if p is None and create_foreign:
+                self.log.info("Registering foreign project: %s" % name)
+                p = Project(name, foreign=True)
+                self.layout.projects[name] = p
+        finally:
+            self.layout_lock.release()
+        return p
+
+    def addEvent(self, event):
+        self.log.debug("Adding trigger event: %s" % event)
+        try:
+            if statsd:
+                statsd.incr('gerrit.event.%s' % event.type)
+        except:
+            self.log.exception("Exception reporting event stats")
+        self.trigger_event_queue.put(event)
+        self.wake_event.set()
+        self.log.debug("Done adding trigger event: %s" % event)
+
+    def onBuildStarted(self, build):
+        self.log.debug("Adding start event for build: %s" % build)
+        build.start_time = time.time()
+        event = BuildStartedEvent(build)
+        self.result_event_queue.put(event)
+        self.wake_event.set()
+        self.log.debug("Done adding start event for build: %s" % build)
+
+    def onBuildCompleted(self, build, result):
+        self.log.debug("Adding complete event for build: %s result: %s" % (
+            build, result))
+        build.end_time = time.time()
+        # Note, as soon as the result is set, other threads may act
+        # upon this, even though the event hasn't been fully
+        # processed.  Ensure that any other data from the event (eg,
+        # timing) is recorded before setting the result.
+        build.result = result
+        try:
+            if statsd and build.pipeline:
+                jobname = build.job.name.replace('.', '_')
+                key = 'zuul.pipeline.%s.all_jobs' % build.pipeline.name
+                statsd.incr(key)
+                for label in build.node_labels:
+                    # Jenkins includes the node name in its list of labels, so
+                    # we filter it out here, since that is not statistically
+                    # interesting.
+                    if label == build.node_name:
+                        continue
+                    dt = int((build.start_time - build.launch_time) * 1000)
+                    key = 'zuul.pipeline.%s.label.%s.wait_time' % (
+                        build.pipeline.name, label)
+                    statsd.timing(key, dt)
+                key = 'zuul.pipeline.%s.job.%s.%s' % (build.pipeline.name,
+                                                      jobname, build.result)
+                if build.result in ['SUCCESS', 'FAILURE'] and build.start_time:
+                    dt = int((build.end_time - build.start_time) * 1000)
+                    statsd.timing(key, dt)
+                statsd.incr(key)
+
+                key = 'zuul.pipeline.%s.job.%s.wait_time' % (
+                    build.pipeline.name, jobname)
+                dt = int((build.start_time - build.launch_time) * 1000)
+                statsd.timing(key, dt)
+        except:
+            self.log.exception("Exception reporting runtime stats")
+        event = BuildCompletedEvent(build)
+        self.result_event_queue.put(event)
+        self.wake_event.set()
+        self.log.debug("Done adding complete event for build: %s" % build)
+
+    def onMergeCompleted(self, build_set, zuul_url, merged, updated, commit):
+        self.log.debug("Adding merge complete event for build set: %s" %
+                       build_set)
+        event = MergeCompletedEvent(build_set, zuul_url,
+                                    merged, updated, commit)
+        self.result_event_queue.put(event)
+        self.wake_event.set()
+
+    def reconfigure(self, config):
+        self.log.debug("Prepare to reconfigure")
+        event = ReconfigureEvent(config)
+        self.management_event_queue.put(event)
+        self.wake_event.set()
+        self.log.debug("Waiting for reconfiguration")
+        event.wait()
+        self.log.debug("Reconfiguration complete")
+        self.last_reconfigured = int(time.time())
+
+    def promote(self, pipeline_name, change_ids):
+        event = PromoteEvent(pipeline_name, change_ids)
+        self.management_event_queue.put(event)
+        self.wake_event.set()
+        self.log.debug("Waiting for promotion")
+        event.wait()
+        self.log.debug("Promotion complete")
+
+    def enqueue(self, trigger_event):
+        event = EnqueueEvent(trigger_event)
+        self.management_event_queue.put(event)
+        self.wake_event.set()
+        self.log.debug("Waiting for enqueue")
+        event.wait()
+        self.log.debug("Enqueue complete")
+
+    def exit(self):
+        self.log.debug("Prepare to exit")
+        self._pause = True
+        self._exit = True
+        self.wake_event.set()
+        self.log.debug("Waiting for exit")
+
+    def _get_queue_pickle_file(self):
+        if self.config.has_option('zuul', 'state_dir'):
+            state_dir = os.path.expanduser(self.config.get('zuul',
+                                                           'state_dir'))
+        else:
+            state_dir = '/var/lib/zuul'
+        return os.path.join(state_dir, 'queue.pickle')
+
+    def _save_queue(self):
+        pickle_file = self._get_queue_pickle_file()
+        events = []
+        while not self.trigger_event_queue.empty():
+            events.append(self.trigger_event_queue.get())
+        self.log.debug("Queue length is %s" % len(events))
+        if events:
+            self.log.debug("Saving queue")
+            pickle.dump(events, open(pickle_file, 'wb'))
+
+    def _load_queue(self):
+        pickle_file = self._get_queue_pickle_file()
+        if os.path.exists(pickle_file):
+            self.log.debug("Loading queue")
+            events = pickle.load(open(pickle_file, 'rb'))
+            self.log.debug("Queue length is %s" % len(events))
+            for event in events:
+                self.trigger_event_queue.put(event)
+        else:
+            self.log.debug("No queue file found")
+
+    def _delete_queue(self):
+        pickle_file = self._get_queue_pickle_file()
+        if os.path.exists(pickle_file):
+            self.log.debug("Deleting saved queue")
+            os.unlink(pickle_file)
+
+    def resume(self):
+        try:
+            self._load_queue()
+        except:
+            self.log.exception("Unable to load queue")
+        try:
+            self._delete_queue()
+        except:
+            self.log.exception("Unable to delete saved queue")
+        self.log.debug("Resuming queue processing")
+        self.wake_event.set()
+
+    def _doPauseEvent(self):
+        if self._exit:
+            self.log.debug("Exiting")
+            self._save_queue()
+            os._exit(0)
+
+    def _doReconfigureEvent(self, event):
+        # This is called in the scheduler loop after another thread submits
+        # a request
+        self.layout_lock.acquire()
+        self.config = event.config
+        try:
+            self.log.debug("Performing reconfiguration")
+            self._unloadDrivers()
+            layout = self._parseConfig(
+                self.config.get('zuul', 'layout_config'), self.connections)
+            for name, new_pipeline in layout.pipelines.items():
+                old_pipeline = self.layout.pipelines.get(name)
+                if not old_pipeline:
+                    if self.layout.pipelines:
+                        # Don't emit this warning on startup
+                        self.log.warning("No old pipeline matching %s found "
+                                         "when reconfiguring" % name)
+                    continue
+                self.log.debug("Re-enqueueing changes for pipeline %s" % name)
+                items_to_remove = []
+                builds_to_cancel = []
+                last_head = None
+                for shared_queue in old_pipeline.queues:
+                    for item in shared_queue.queue:
+                        if not item.item_ahead:
+                            last_head = item
+                        item.item_ahead = None
+                        item.items_behind = []
+                        item.pipeline = None
+                        item.queue = None
+                        project_name = item.change.project.name
+                        item.change.project = layout.projects.get(project_name)
+                        if not item.change.project:
+                            self.log.debug("Project %s not defined, "
+                                           "re-instantiating as foreign" %
+                                           project_name)
+                            project = Project(project_name, foreign=True)
+                            layout.projects[project_name] = project
+                            item.change.project = project
+                        item_jobs = new_pipeline.getJobs(item)
+                        for build in item.current_build_set.getBuilds():
+                            job = layout.jobs.get(build.job.name)
+                            if job and job in item_jobs:
+                                build.job = job
+                            else:
+                                item.removeBuild(build)
+                                builds_to_cancel.append(build)
+                        if not new_pipeline.manager.reEnqueueItem(item,
+                                                                  last_head):
+                            items_to_remove.append(item)
+                for item in items_to_remove:
+                    for build in item.current_build_set.getBuilds():
+                        builds_to_cancel.append(build)
+                for build in builds_to_cancel:
+                    self.log.warning(
+                        "Canceling build %s during reconfiguration" % (build,))
+                    try:
+                        self.launcher.cancel(build)
+                    except Exception:
+                        self.log.exception(
+                            "Exception while canceling build %s "
+                            "for change %s" % (build, item.change))
+            self.layout = layout
+            self.maintainConnectionCache()
+            for trigger in self.triggers.values():
+                trigger.postConfig()
+            for pipeline in self.layout.pipelines.values():
+                pipeline.source.postConfig()
+                for action in self._reporter_actions.values():
+                    for reporter in pipeline.__getattribute__(action):
+                        reporter.postConfig()
+            if statsd:
+                try:
+                    for pipeline in self.layout.pipelines.values():
+                        items = len(pipeline.getAllItems())
+                        # stats.gauges.zuul.pipeline.NAME.current_changes
+                        key = 'zuul.pipeline.%s' % pipeline.name
+                        statsd.gauge(key + '.current_changes', items)
+                except Exception:
+                    self.log.exception("Exception reporting initial "
+                                       "pipeline stats:")
+        finally:
+            self.layout_lock.release()
+
+    def _doPromoteEvent(self, event):
+        pipeline = self.layout.pipelines[event.pipeline_name]
+        change_ids = [c.split(',') for c in event.change_ids]
+        items_to_enqueue = []
+        change_queue = None
+        for shared_queue in pipeline.queues:
+            if change_queue:
+                break
+            for item in shared_queue.queue:
+                if (item.change.number == change_ids[0][0] and
+                        item.change.patchset == change_ids[0][1]):
+                    change_queue = shared_queue
+                    break
+        if not change_queue:
+            raise Exception("Unable to find shared change queue for %s" %
+                            event.change_ids[0])
+        for number, patchset in change_ids:
+            found = False
+            for item in change_queue.queue:
+                if (item.change.number == number and
+                        item.change.patchset == patchset):
+                    found = True
+                    items_to_enqueue.append(item)
+                    break
+            if not found:
+                raise Exception("Unable to find %s,%s in queue %s" %
+                                (number, patchset, change_queue))
+        for item in change_queue.queue[:]:
+            if item not in items_to_enqueue:
+                items_to_enqueue.append(item)
+            pipeline.manager.cancelJobs(item)
+            pipeline.manager.dequeueItem(item)
+        for item in items_to_enqueue:
+            pipeline.manager.addChange(
+                item.change,
+                enqueue_time=item.enqueue_time,
+                quiet=True,
+                ignore_requirements=True)
+
+    def _doEnqueueEvent(self, event):
+        project = self.layout.projects.get(event.project_name)
+        pipeline = self.layout.pipelines[event.forced_pipeline]
+        change = pipeline.source.getChange(event, project)
+        self.log.debug("Event %s for change %s was directly assigned "
+                       "to pipeline %s" % (event, change, self))
+        self.log.info("Adding %s, %s to %s" %
+                      (project, change, pipeline))
+        pipeline.manager.addChange(change, ignore_requirements=True)
+
+    def _areAllBuildsComplete(self):
+        self.log.debug("Checking if all builds are complete")
+        waiting = False
+        if self.merger.areMergesOutstanding():
+            waiting = True
+        for pipeline in self.layout.pipelines.values():
+            for item in pipeline.getAllItems():
+                for build in item.current_build_set.getBuilds():
+                    if build.result is None:
+                        self.log.debug("%s waiting on %s" %
+                                       (pipeline.manager, build))
+                        waiting = True
+        if not waiting:
+            self.log.debug("All builds are complete")
+            return True
+        self.log.debug("All builds are not complete")
+        return False
+
+    def run(self):
+        if statsd:
+            self.log.debug("Statsd enabled")
+        else:
+            self.log.debug("Statsd disabled because python statsd "
+                           "package not found")
+        while True:
+            self.log.debug("Run handler sleeping")
+            self.wake_event.wait()
+            self.wake_event.clear()
+            if self._stopped:
+                self.log.debug("Run handler stopping")
+                return
+            self.log.debug("Run handler awake")
+            self.run_handler_lock.acquire()
+            try:
+                while not self.management_event_queue.empty():
+                    self.process_management_queue()
+
+                # Give result events priority -- they let us stop builds,
+                # whereas trigger evensts cause us to launch builds.
+                while not self.result_event_queue.empty():
+                    self.process_result_queue()
+
+                if not self._pause:
+                    while not self.trigger_event_queue.empty():
+                        self.process_event_queue()
+
+                if self._pause and self._areAllBuildsComplete():
+                    self._doPauseEvent()
+
+                for pipeline in self.layout.pipelines.values():
+                    while pipeline.manager.processQueue():
+                        pass
+
+            except Exception:
+                self.log.exception("Exception in run handler:")
+                # There may still be more events to process
+                self.wake_event.set()
+            finally:
+                self.run_handler_lock.release()
+
+    def maintainConnectionCache(self):
+        relevant = set()
+        for pipeline in self.layout.pipelines.values():
+            self.log.debug("Gather relevant cache items for: %s" % pipeline)
+            for item in pipeline.getAllItems():
+                relevant.add(item.change)
+                relevant.update(item.change.getRelatedChanges())
+        for connection in self.connections.values():
+            connection.maintainCache(relevant)
+            self.log.debug(
+                "End maintain connection cache for: %s" % connection)
+        self.log.debug("Connection cache size: %s" % len(relevant))
+
+    def process_event_queue(self):
+        self.log.debug("Fetching trigger event")
+        event = self.trigger_event_queue.get()
+        self.log.debug("Processing trigger event %s" % event)
+        try:
+            project = self.layout.projects.get(event.project_name)
+
+            for pipeline in self.layout.pipelines.values():
+                # Get the change even if the project is unknown to us for the
+                # use of updating the cache if there is another change
+                # depending on this foreign one.
+                try:
+                    change = pipeline.source.getChange(event, project)
+                except exceptions.ChangeNotFound as e:
+                    self.log.debug("Unable to get change %s from source %s. "
+                                   "(most likely looking for a change from "
+                                   "another connection trigger)",
+                                   e.change, pipeline.source)
+                    continue
+                if not project or project.foreign:
+                    self.log.debug("Project %s not found" % event.project_name)
+                    continue
+                if event.type == 'patchset-created':
+                    pipeline.manager.removeOldVersionsOfChange(change)
+                elif event.type == 'change-abandoned':
+                    pipeline.manager.removeAbandonedChange(change)
+                if pipeline.manager.eventMatches(event, change):
+                    self.log.info("Adding %s, %s to %s" %
+                                  (project, change, pipeline))
+                    pipeline.manager.addChange(change)
+        finally:
+            self.trigger_event_queue.task_done()
+
+    def process_management_queue(self):
+        self.log.debug("Fetching management event")
+        event = self.management_event_queue.get()
+        self.log.debug("Processing management event %s" % event)
+        try:
+            if isinstance(event, ReconfigureEvent):
+                self._doReconfigureEvent(event)
+            elif isinstance(event, PromoteEvent):
+                self._doPromoteEvent(event)
+            elif isinstance(event, EnqueueEvent):
+                self._doEnqueueEvent(event.trigger_event)
+            else:
+                self.log.error("Unable to handle event %s" % event)
+            event.done()
+        except Exception as e:
+            event.exception(e, sys.exc_info()[2])
+        self.management_event_queue.task_done()
+
+    def process_result_queue(self):
+        self.log.debug("Fetching result event")
+        event = self.result_event_queue.get()
+        self.log.debug("Processing result event %s" % event)
+        try:
+            if isinstance(event, BuildStartedEvent):
+                self._doBuildStartedEvent(event)
+            elif isinstance(event, BuildCompletedEvent):
+                self._doBuildCompletedEvent(event)
+            elif isinstance(event, MergeCompletedEvent):
+                self._doMergeCompletedEvent(event)
+            else:
+                self.log.error("Unable to handle event %s" % event)
+        finally:
+            self.result_event_queue.task_done()
+
+    def _doBuildStartedEvent(self, event):
+        build = event.build
+        if build.build_set is not build.build_set.item.current_build_set:
+            self.log.warning("Build %s is not in the current build set" %
+                             (build,))
+            return
+        pipeline = build.build_set.item.pipeline
+        if not pipeline:
+            self.log.warning("Build %s is not associated with a pipeline" %
+                             (build,))
+            return
+        pipeline.manager.onBuildStarted(event.build)
+
+    def _doBuildCompletedEvent(self, event):
+        build = event.build
+        if build.build_set is not build.build_set.item.current_build_set:
+            self.log.warning("Build %s is not in the current build set" %
+                             (build,))
+            return
+        pipeline = build.build_set.item.pipeline
+        if not pipeline:
+            self.log.warning("Build %s is not associated with a pipeline" %
+                             (build,))
+            return
+        pipeline.manager.onBuildCompleted(event.build)
+
+    def _doMergeCompletedEvent(self, event):
+        build_set = event.build_set
+        if build_set is not build_set.item.current_build_set:
+            self.log.warning("Build set %s is not current" % (build_set,))
+            return
+        pipeline = build_set.item.pipeline
+        if not pipeline:
+            self.log.warning("Build set %s is not associated with a pipeline" %
+                             (build_set,))
+            return
+        pipeline.manager.onMergeCompleted(event)
+
+    def formatStatusJSON(self):
+        if self.config.has_option('zuul', 'url_pattern'):
+            url_pattern = self.config.get('zuul', 'url_pattern')
+        else:
+            url_pattern = None
+
+        data = {}
+
+        data['zuul_version'] = self.zuul_version
+
+        if self._pause:
+            ret = '<p><b>Queue only mode:</b> preparing to '
+            if self._exit:
+                ret += 'exit'
+            ret += ', queue length: %s' % self.trigger_event_queue.qsize()
+            ret += '</p>'
+            data['message'] = ret
+
+        data['trigger_event_queue'] = {}
+        data['trigger_event_queue']['length'] = \
+            self.trigger_event_queue.qsize()
+        data['result_event_queue'] = {}
+        data['result_event_queue']['length'] = \
+            self.result_event_queue.qsize()
+
+        if self.last_reconfigured:
+            data['last_reconfigured'] = self.last_reconfigured * 1000
+
+        pipelines = []
+        data['pipelines'] = pipelines
+        for pipeline in self.layout.pipelines.values():
+            pipelines.append(pipeline.formatStatusJSON(url_pattern))
+        return json.dumps(data)
+
+
+class BasePipelineManager(object):
+    log = logging.getLogger("zuul.BasePipelineManager")
+
+    def __init__(self, sched, pipeline):
+        self.sched = sched
+        self.pipeline = pipeline
+        self.event_filters = []
+        self.changeish_filters = []
+
+    def __str__(self):
+        return "<%s %s>" % (self.__class__.__name__, self.pipeline.name)
+
+    def _postConfig(self, layout):
+        self.log.info("Configured Pipeline Manager %s" % self.pipeline.name)
+        self.log.info("  Source: %s" % self.pipeline.source)
+        self.log.info("  Requirements:")
+        for f in self.changeish_filters:
+            self.log.info("    %s" % f)
+        self.log.info("  Events:")
+        for e in self.event_filters:
+            self.log.info("    %s" % e)
+        self.log.info("  Projects:")
+
+        def log_jobs(tree, indent=0):
+            istr = '    ' + ' ' * indent
+            if tree.job:
+                efilters = ''
+                for b in tree.job._branches:
+                    efilters += str(b)
+                for f in tree.job._files:
+                    efilters += str(f)
+                if tree.job.skip_if_matcher:
+                    efilters += str(tree.job.skip_if_matcher)
+                if efilters:
+                    efilters = ' ' + efilters
+                tags = []
+                if tree.job.hold_following_changes:
+                    tags.append('[hold]')
+                if not tree.job.voting:
+                    tags.append('[nonvoting]')
+                if tree.job.mutex:
+                    tags.append('[mutex: %s]' % tree.job.mutex)
+                tags = ' '.join(tags)
+                self.log.info("%s%s%s %s" % (istr, repr(tree.job),
+                                             efilters, tags))
+            for x in tree.job_trees:
+                log_jobs(x, indent + 2)
+
+        for p in layout.projects.values():
+            tree = self.pipeline.getJobTree(p)
+            if tree:
+                self.log.info("    %s" % p)
+                log_jobs(tree)
+        self.log.info("  On start:")
+        self.log.info("    %s" % self.pipeline.start_actions)
+        self.log.info("  On success:")
+        self.log.info("    %s" % self.pipeline.success_actions)
+        self.log.info("  On failure:")
+        self.log.info("    %s" % self.pipeline.failure_actions)
+        self.log.info("  On merge-failure:")
+        self.log.info("    %s" % self.pipeline.merge_failure_actions)
+        self.log.info("  When disabled:")
+        self.log.info("    %s" % self.pipeline.disabled_actions)
+
+    def getSubmitAllowNeeds(self):
+        # Get a list of code review labels that are allowed to be
+        # "needed" in the submit records for a change, with respect
+        # to this queue.  In other words, the list of review labels
+        # this queue itself is likely to set before submitting.
+        allow_needs = set()
+        for action_reporter in self.pipeline.success_actions:
+            allow_needs.update(action_reporter.getSubmitAllowNeeds())
+        return allow_needs
+
+    def eventMatches(self, event, change):
+        if event.forced_pipeline:
+            if event.forced_pipeline == self.pipeline.name:
+                self.log.debug("Event %s for change %s was directly assigned "
+                               "to pipeline %s" % (event, change, self))
+                return True
+            else:
+                return False
+        for ef in self.event_filters:
+            if ef.matches(event, change):
+                self.log.debug("Event %s for change %s matched %s "
+                               "in pipeline %s" % (event, change, ef, self))
+                return True
+        return False
+
+    def isChangeAlreadyInPipeline(self, change):
+        # Checks live items in the pipeline
+        for item in self.pipeline.getAllItems():
+            if item.live and change.equals(item.change):
+                return True
+        return False
+
+    def isChangeAlreadyInQueue(self, change, change_queue):
+        # Checks any item in the specified change queue
+        for item in change_queue.queue:
+            if change.equals(item.change):
+                return True
+        return False
+
+    def reportStart(self, item):
+        if not self.pipeline._disabled:
+            try:
+                self.log.info("Reporting start, action %s item %s" %
+                              (self.pipeline.start_actions, item))
+                ret = self.sendReport(self.pipeline.start_actions,
+                                      self.pipeline.source, item)
+                if ret:
+                    self.log.error("Reporting item start %s received: %s" %
+                                   (item, ret))
+            except:
+                self.log.exception("Exception while reporting start:")
+
+    def sendReport(self, action_reporters, source, item,
+                   message=None):
+        """Sends the built message off to configured reporters.
+
+        Takes the action_reporters, item, message and extra options and
+        sends them to the pluggable reporters.
+        """
+        report_errors = []
+        if len(action_reporters) > 0:
+            for reporter in action_reporters:
+                ret = reporter.report(source, self.pipeline, item)
+                if ret:
+                    report_errors.append(ret)
+            if len(report_errors) == 0:
+                return
+        return report_errors
+
+    def isChangeReadyToBeEnqueued(self, change):
+        return True
+
+    def enqueueChangesAhead(self, change, quiet, ignore_requirements,
+                            change_queue):
+        return True
+
+    def enqueueChangesBehind(self, change, quiet, ignore_requirements,
+                             change_queue):
+        return True
+
+    def checkForChangesNeededBy(self, change, change_queue):
+        return True
+
+    def getFailingDependentItems(self, item):
+        return None
+
+    def getDependentItems(self, item):
+        orig_item = item
+        items = []
+        while item.item_ahead:
+            items.append(item.item_ahead)
+            item = item.item_ahead
+        self.log.info("Change %s depends on changes %s" %
+                      (orig_item.change,
+                       [x.change for x in items]))
+        return items
+
+    def getItemForChange(self, change):
+        for item in self.pipeline.getAllItems():
+            if item.change.equals(change):
+                return item
+        return None
+
+    def findOldVersionOfChangeAlreadyInQueue(self, change):
+        for item in self.pipeline.getAllItems():
+            if not item.live:
+                continue
+            if change.isUpdateOf(item.change):
+                return item
+        return None
+
+    def removeOldVersionsOfChange(self, change):
+        if not self.pipeline.dequeue_on_new_patchset:
+            return
+        old_item = self.findOldVersionOfChangeAlreadyInQueue(change)
+        if old_item:
+            self.log.debug("Change %s is a new version of %s, removing %s" %
+                           (change, old_item.change, old_item))
+            self.removeItem(old_item)
+
+    def removeAbandonedChange(self, change):
+        self.log.debug("Change %s abandoned, removing." % change)
+        for item in self.pipeline.getAllItems():
+            if not item.live:
+                continue
+            if item.change.equals(change):
+                self.removeItem(item)
+
+    def reEnqueueItem(self, item, last_head):
+        with self.getChangeQueue(item.change, last_head.queue) as change_queue:
+            if change_queue:
+                self.log.debug("Re-enqueing change %s in queue %s" %
+                               (item.change, change_queue))
+                change_queue.enqueueItem(item)
+
+                # Re-set build results in case any new jobs have been
+                # added to the tree.
+                for build in item.current_build_set.getBuilds():
+                    if build.result:
+                        self.pipeline.setResult(item, build)
+                # Similarly, reset the item state.
+                if item.current_build_set.unable_to_merge:
+                    self.pipeline.setUnableToMerge(item)
+                if item.dequeued_needing_change:
+                    self.pipeline.setDequeuedNeedingChange(item)
+
+                self.reportStats(item)
+                return True
+            else:
+                self.log.error("Unable to find change queue for project %s" %
+                               item.change.project)
+                return False
+
+    def addChange(self, change, quiet=False, enqueue_time=None,
+                  ignore_requirements=False, live=True,
+                  change_queue=None):
+        self.log.debug("Considering adding change %s" % change)
+
+        # If we are adding a live change, check if it's a live item
+        # anywhere in the pipeline.  Otherwise, we will perform the
+        # duplicate check below on the specific change_queue.
+        if live and self.isChangeAlreadyInPipeline(change):
+            self.log.debug("Change %s is already in pipeline, "
+                           "ignoring" % change)
+            return True
+
+        if not self.isChangeReadyToBeEnqueued(change):
+            self.log.debug("Change %s is not ready to be enqueued, ignoring" %
+                           change)
+            return False
+
+        if not ignore_requirements:
+            for f in self.changeish_filters:
+                if not f.matches(change):
+                    self.log.debug("Change %s does not match pipeline "
+                                   "requirement %s" % (change, f))
+                    return False
+
+        with self.getChangeQueue(change, change_queue) as change_queue:
+            if not change_queue:
+                self.log.debug("Unable to find change queue for "
+                               "change %s in project %s" %
+                               (change, change.project))
+                return False
+
+            if not self.enqueueChangesAhead(change, quiet, ignore_requirements,
+                                            change_queue):
+                self.log.debug("Failed to enqueue changes "
+                               "ahead of %s" % change)
+                return False
+
+            if self.isChangeAlreadyInQueue(change, change_queue):
+                self.log.debug("Change %s is already in queue, "
+                               "ignoring" % change)
+                return True
+
+            self.log.debug("Adding change %s to queue %s" %
+                           (change, change_queue))
+            item = change_queue.enqueueChange(change)
+            if enqueue_time:
+                item.enqueue_time = enqueue_time
+            item.live = live
+            self.reportStats(item)
+            if not quiet:
+                if len(self.pipeline.start_actions) > 0:
+                    self.reportStart(item)
+            self.enqueueChangesBehind(change, quiet, ignore_requirements,
+                                      change_queue)
+            for trigger in self.sched.triggers.values():
+                trigger.onChangeEnqueued(item.change, self.pipeline)
+            return True
+
+    def dequeueItem(self, item):
+        self.log.debug("Removing change %s from queue" % item.change)
+        item.queue.dequeueItem(item)
+
+    def removeItem(self, item):
+        # Remove an item from the queue, probably because it has been
+        # superseded by another change.
+        self.log.debug("Canceling builds behind change: %s "
+                       "because it is being removed." % item.change)
+        self.cancelJobs(item)
+        self.dequeueItem(item)
+        self.reportStats(item)
+
+    def _makeMergerItem(self, item):
+        # Create a dictionary with all info about the item needed by
+        # the merger.
+        number = None
+        patchset = None
+        oldrev = None
+        newrev = None
+        if hasattr(item.change, 'number'):
+            number = item.change.number
+            patchset = item.change.patchset
+        elif hasattr(item.change, 'newrev'):
+            oldrev = item.change.oldrev
+            newrev = item.change.newrev
+        connection_name = self.pipeline.source.connection.connection_name
+        return dict(project=item.change.project.name,
+                    url=self.pipeline.source.getGitUrl(
+                        item.change.project),
+                    connection_name=connection_name,
+                    merge_mode=item.change.project.merge_mode,
+                    refspec=item.change.refspec,
+                    branch=item.change.branch,
+                    ref=item.current_build_set.ref,
+                    number=number,
+                    patchset=patchset,
+                    oldrev=oldrev,
+                    newrev=newrev,
+                    )
+
+    def prepareRef(self, item):
+        # Returns True if the ref is ready, false otherwise
+        build_set = item.current_build_set
+        if build_set.merge_state == build_set.COMPLETE:
+            return True
+        if build_set.merge_state == build_set.PENDING:
+            return False
+        ref = build_set.ref
+        if hasattr(item.change, 'refspec') and not ref:
+            self.log.debug("Preparing ref for: %s" % item.change)
+            item.current_build_set.setConfiguration()
+            dependent_items = self.getDependentItems(item)
+            dependent_items.reverse()
+            all_items = dependent_items + [item]
+            merger_items = map(self._makeMergerItem, all_items)
+            self.sched.merger.mergeChanges(merger_items,
+                                           item.current_build_set,
+                                           self.pipeline.precedence)
+        else:
+            self.log.debug("Preparing update repo for: %s" % item.change)
+            url = self.pipeline.source.getGitUrl(item.change.project)
+            self.sched.merger.updateRepo(item.change.project.name,
+                                         url, build_set,
+                                         self.pipeline.precedence)
+        # merge:merge has been emitted properly:
+        build_set.merge_state = build_set.PENDING
+        return False
+
+    def _launchJobs(self, item, jobs):
+        self.log.debug("Launching jobs for change %s" % item.change)
+        dependent_items = self.getDependentItems(item)
+        for job in jobs:
+            self.log.debug("Found job %s for change %s" % (job, item.change))
+            try:
+                build = self.sched.launcher.launch(job, item,
+                                                   self.pipeline,
+                                                   dependent_items)
+                self.log.debug("Adding build %s of job %s to item %s" %
+                               (build, job, item))
+                item.addBuild(build)
+            except:
+                self.log.exception("Exception while launching job %s "
+                                   "for change %s:" % (job, item.change))
+
+    def launchJobs(self, item):
+        jobs = self.pipeline.findJobsToRun(item, self.sched.mutex)
+        if jobs:
+            self._launchJobs(item, jobs)
+
+    def cancelJobs(self, item, prime=True):
+        self.log.debug("Cancel jobs for change %s" % item.change)
+        canceled = False
+        old_build_set = item.current_build_set
+        if prime and item.current_build_set.ref:
+            item.resetAllBuilds()
+        for build in old_build_set.getBuilds():
+            try:
+                self.sched.launcher.cancel(build)
+            except:
+                self.log.exception("Exception while canceling build %s "
+                                   "for change %s" % (build, item.change))
+            build.result = 'CANCELED'
+            canceled = True
+        self.updateBuildDescriptions(old_build_set)
+        for item_behind in item.items_behind:
+            self.log.debug("Canceling jobs for change %s, behind change %s" %
+                           (item_behind.change, item.change))
+            if self.cancelJobs(item_behind, prime=prime):
+                canceled = True
+        return canceled
+
+    def _processOneItem(self, item, nnfi):
+        changed = False
+        item_ahead = item.item_ahead
+        if item_ahead and (not item_ahead.live):
+            item_ahead = None
+        change_queue = item.queue
+        failing_reasons = []  # Reasons this item is failing
+
+        if self.checkForChangesNeededBy(item.change, change_queue) is not True:
+            # It's not okay to enqueue this change, we should remove it.
+            self.log.info("Dequeuing change %s because "
+                          "it can no longer merge" % item.change)
+            self.cancelJobs(item)
+            self.dequeueItem(item)
+            self.pipeline.setDequeuedNeedingChange(item)
+            if item.live:
+                try:
+                    self.reportItem(item)
+                except exceptions.MergeFailure:
+                    pass
+            return (True, nnfi)
+        dep_items = self.getFailingDependentItems(item)
+        actionable = change_queue.isActionable(item)
+        item.active = actionable
+        ready = False
+        if dep_items:
+            failing_reasons.append('a needed change is failing')
+            self.cancelJobs(item, prime=False)
+        else:
+            item_ahead_merged = False
+            if (item_ahead and item_ahead.change.is_merged):
+                item_ahead_merged = True
+            if (item_ahead != nnfi and not item_ahead_merged):
+                # Our current base is different than what we expected,
+                # and it's not because our current base merged.  Something
+                # ahead must have failed.
+                self.log.info("Resetting builds for change %s because the "
+                              "item ahead, %s, is not the nearest non-failing "
+                              "item, %s" % (item.change, item_ahead, nnfi))
+                change_queue.moveItem(item, nnfi)
+                changed = True
+                self.cancelJobs(item)
+            if actionable:
+                ready = self.prepareRef(item)
+                if item.current_build_set.unable_to_merge:
+                    failing_reasons.append("it has a merge conflict")
+                    ready = False
+        if actionable and ready and self.launchJobs(item):
+            changed = True
+        if self.pipeline.didAnyJobFail(item):
+            failing_reasons.append("at least one job failed")
+        if (not item.live) and (not item.items_behind):
+            failing_reasons.append("is a non-live item with no items behind")
+            self.dequeueItem(item)
+            changed = True
+        if ((not item_ahead) and self.pipeline.areAllJobsComplete(item)
+            and item.live):
+            try:
+                self.reportItem(item)
+            except exceptions.MergeFailure:
+                failing_reasons.append("it did not merge")
+                for item_behind in item.items_behind:
+                    self.log.info("Resetting builds for change %s because the "
+                                  "item ahead, %s, failed to merge" %
+                                  (item_behind.change, item))
+                    self.cancelJobs(item_behind)
+            self.dequeueItem(item)
+            changed = True
+        elif not failing_reasons and item.live:
+            nnfi = item
+        item.current_build_set.failing_reasons = failing_reasons
+        if failing_reasons:
+            self.log.debug("%s is a failing item because %s" %
+                           (item, failing_reasons))
+        return (changed, nnfi)
+
+    def processQueue(self):
+        # Do whatever needs to be done for each change in the queue
+        self.log.debug("Starting queue processor: %s" % self.pipeline.name)
+        changed = False
+        for queue in self.pipeline.queues:
+            queue_changed = False
+            nnfi = None  # Nearest non-failing item
+            for item in queue.queue[:]:
+                item_changed, nnfi = self._processOneItem(
+                    item, nnfi)
+                if item_changed:
+                    queue_changed = True
+                self.reportStats(item)
+            if queue_changed:
+                changed = True
+                status = ''
+                for item in queue.queue:
+                    status += item.formatStatus()
+                if status:
+                    self.log.debug("Queue %s status is now:\n %s" %
+                                   (queue.name, status))
+        self.log.debug("Finished queue processor: %s (changed: %s)" %
+                       (self.pipeline.name, changed))
+        return changed
+
+    def updateBuildDescriptions(self, build_set):
+        for build in build_set.getBuilds():
+            try:
+                desc = self.formatDescription(build)
+                self.sched.launcher.setBuildDescription(build, desc)
+            except:
+                # Log the failure and let loop continue
+                self.log.error("Failed to update description for build %s" %
+                               (build))
+
+        if build_set.previous_build_set:
+            for build in build_set.previous_build_set.getBuilds():
+                try:
+                    desc = self.formatDescription(build)
+                    self.sched.launcher.setBuildDescription(build, desc)
+                except:
+                    # Log the failure and let loop continue
+                    self.log.error("Failed to update description for "
+                                   "build %s in previous build set" % (build))
+
+    def onBuildStarted(self, build):
+        self.log.debug("Build %s started" % build)
+        return True
+
+    def onBuildCompleted(self, build):
+        self.log.debug("Build %s completed" % build)
+        item = build.build_set.item
+
+        self.pipeline.setResult(item, build)
+        self.sched.mutex.release(item, build.job)
+        self.log.debug("Item %s status is now:\n %s" %
+                       (item, item.formatStatus()))
+        return True
+
+    def onMergeCompleted(self, event):
+        build_set = event.build_set
+        item = build_set.item
+        build_set.merge_state = build_set.COMPLETE
+        build_set.zuul_url = event.zuul_url
+        if event.merged:
+            build_set.commit = event.commit
+        elif event.updated:
+            if not isinstance(item.change, NullChange):
+                build_set.commit = item.change.newrev
+        if not build_set.commit and not isinstance(item.change, NullChange):
+            self.log.info("Unable to merge change %s" % item.change)
+            self.pipeline.setUnableToMerge(item)
+
+    def reportItem(self, item):
+        if not item.reported:
+            # _reportItem() returns True if it failed to report.
+            item.reported = not self._reportItem(item)
+        if self.changes_merge:
+            succeeded = self.pipeline.didAllJobsSucceed(item)
+            merged = item.reported
+            if merged:
+                merged = self.pipeline.source.isMerged(item.change,
+                                                       item.change.branch)
+            self.log.info("Reported change %s status: all-succeeded: %s, "
+                          "merged: %s" % (item.change, succeeded, merged))
+            change_queue = item.queue
+            if not (succeeded and merged):
+                self.log.debug("Reported change %s failed tests or failed "
+                               "to merge" % (item.change))
+                change_queue.decreaseWindowSize()
+                self.log.debug("%s window size decreased to %s" %
+                               (change_queue, change_queue.window))
+                raise exceptions.MergeFailure(
+                    "Change %s failed to merge" % item.change)
+            else:
+                change_queue.increaseWindowSize()
+                self.log.debug("%s window size increased to %s" %
+                               (change_queue, change_queue.window))
+
+                for trigger in self.sched.triggers.values():
+                    trigger.onChangeMerged(item.change, self.pipeline.source)
+
+    def _reportItem(self, item):
+        self.log.debug("Reporting change %s" % item.change)
+        ret = True  # Means error as returned by trigger.report
+        if not self.pipeline.getJobs(item):
+            # We don't send empty reports with +1,
+            # and the same for -1's (merge failures or transient errors)
+            # as they cannot be followed by +1's
+            self.log.debug("No jobs for change %s" % item.change)
+            actions = []
+        elif self.pipeline.didAllJobsSucceed(item):
+            self.log.debug("success %s" % (self.pipeline.success_actions))
+            actions = self.pipeline.success_actions
+            item.setReportedResult('SUCCESS')
+            self.pipeline._consecutive_failures = 0
+        elif not self.pipeline.didMergerSucceed(item):
+            actions = self.pipeline.merge_failure_actions
+            item.setReportedResult('MERGER_FAILURE')
+        else:
+            actions = self.pipeline.failure_actions
+            item.setReportedResult('FAILURE')
+            self.pipeline._consecutive_failures += 1
+        if self.pipeline._disabled:
+            actions = self.pipeline.disabled_actions
+        # Check here if we should disable so that we only use the disabled
+        # reporters /after/ the last disable_at failure is still reported as
+        # normal.
+        if (self.pipeline.disable_at and not self.pipeline._disabled and
+            self.pipeline._consecutive_failures >= self.pipeline.disable_at):
+            self.pipeline._disabled = True
+        if actions:
+            try:
+                self.log.info("Reporting item %s, actions: %s" %
+                              (item, actions))
+                ret = self.sendReport(actions, self.pipeline.source, item)
+                if ret:
+                    self.log.error("Reporting item %s received: %s" %
+                                   (item, ret))
+            except:
+                self.log.exception("Exception while reporting:")
+                item.setReportedResult('ERROR')
+        self.updateBuildDescriptions(item.current_build_set)
+        return ret
+
+    def formatDescription(self, build):
+        concurrent_changes = ''
+        concurrent_builds = ''
+        other_builds = ''
+
+        for change in build.build_set.other_changes:
+            concurrent_changes += '<li><a href="{change.url}">\
+              {change.number},{change.patchset}</a></li>'.format(
+                change=change)
+
+        change = build.build_set.item.change
+
+        for build in build.build_set.getBuilds():
+            if build.url:
+                concurrent_builds += """\
+<li>
+  <a href="{build.url}">
+  {build.job.name} #{build.number}</a>: {build.result}
+</li>
+""".format(build=build)
+            else:
+                concurrent_builds += """\
+<li>
+  {build.job.name}: {build.result}
+</li>""".format(build=build)
+
+        if build.build_set.previous_build_set:
+            other_build = build.build_set.previous_build_set.getBuild(
+                build.job.name)
+            if other_build:
+                other_builds += """\
+<li>
+  Preceded by: <a href="{build.url}">
+  {build.job.name} #{build.number}</a>
+</li>
+""".format(build=other_build)
+
+        if build.build_set.next_build_set:
+            other_build = build.build_set.next_build_set.getBuild(
+                build.job.name)
+            if other_build:
+                other_builds += """\
+<li>
+  Succeeded by: <a href="{build.url}">
+  {build.job.name} #{build.number}</a>
+</li>
+""".format(build=other_build)
+
+        result = build.build_set.result
+
+        if hasattr(change, 'number'):
+            ret = """\
+<p>
+  Triggered by change:
+    <a href="{change.url}">{change.number},{change.patchset}</a><br/>
+  Branch: <b>{change.branch}</b><br/>
+  Pipeline: <b>{self.pipeline.name}</b>
+</p>"""
+        elif hasattr(change, 'ref'):
+            ret = """\
+<p>
+  Triggered by reference:
+    {change.ref}</a><br/>
+  Old revision: <b>{change.oldrev}</b><br/>
+  New revision: <b>{change.newrev}</b><br/>
+  Pipeline: <b>{self.pipeline.name}</b>
+</p>"""
+        else:
+            ret = ""
+
+        if concurrent_changes:
+            ret += """\
+<p>
+  Other changes tested concurrently with this change:
+  <ul>{concurrent_changes}</ul>
+</p>
+"""
+        if concurrent_builds:
+            ret += """\
+<p>
+  All builds for this change set:
+  <ul>{concurrent_builds}</ul>
+</p>
+"""
+
+        if other_builds:
+            ret += """\
+<p>
+  Other build sets for this change:
+  <ul>{other_builds}</ul>
+</p>
+"""
+        if result:
+            ret += """\
+<p>
+  Reported result: <b>{result}</b>
+</p>
+"""
+
+        ret = ret.format(**locals())
+        return ret
+
+    def reportStats(self, item):
+        if not statsd:
+            return
+        try:
+            # Update the gauge on enqueue and dequeue, but timers only
+            # when dequeing.
+            if item.dequeue_time:
+                dt = int((item.dequeue_time - item.enqueue_time) * 1000)
+            else:
+                dt = None
+            items = len(self.pipeline.getAllItems())
+
+            # stats.timers.zuul.pipeline.NAME.resident_time
+            # stats_counts.zuul.pipeline.NAME.total_changes
+            # stats.gauges.zuul.pipeline.NAME.current_changes
+            key = 'zuul.pipeline.%s' % self.pipeline.name
+            statsd.gauge(key + '.current_changes', items)
+            if dt:
+                statsd.timing(key + '.resident_time', dt)
+                statsd.incr(key + '.total_changes')
+
+            # stats.timers.zuul.pipeline.NAME.ORG.PROJECT.resident_time
+            # stats_counts.zuul.pipeline.NAME.ORG.PROJECT.total_changes
+            project_name = item.change.project.name.replace('/', '.')
+            key += '.%s' % project_name
+            if dt:
+                statsd.timing(key + '.resident_time', dt)
+                statsd.incr(key + '.total_changes')
+        except:
+            self.log.exception("Exception reporting pipeline stats")
+
+
+class DynamicChangeQueueContextManager(object):
+    def __init__(self, change_queue):
+        self.change_queue = change_queue
+
+    def __enter__(self):
+        return self.change_queue
+
+    def __exit__(self, etype, value, tb):
+        if self.change_queue and not self.change_queue.queue:
+            self.change_queue.pipeline.removeQueue(self.change_queue.queue)
+
+
+class IndependentPipelineManager(BasePipelineManager):
+    log = logging.getLogger("zuul.IndependentPipelineManager")
+    changes_merge = False
+
+    def _postConfig(self, layout):
+        super(IndependentPipelineManager, self)._postConfig(layout)
+
+    def getChangeQueue(self, change, existing=None):
+        # creates a new change queue for every change
+        if existing:
+            return DynamicChangeQueueContextManager(existing)
+        if change.project not in self.pipeline.getProjects():
+            self.pipeline.addProject(change.project)
+        change_queue = ChangeQueue(self.pipeline)
+        change_queue.addProject(change.project)
+        self.pipeline.addQueue(change_queue)
+        self.log.debug("Dynamically created queue %s", change_queue)
+        return DynamicChangeQueueContextManager(change_queue)
+
+    def enqueueChangesAhead(self, change, quiet, ignore_requirements,
+                            change_queue):
+        ret = self.checkForChangesNeededBy(change, change_queue)
+        if ret in [True, False]:
+            return ret
+        self.log.debug("  Changes %s must be merged ahead of %s" %
+                       (ret, change))
+        for needed_change in ret:
+            # This differs from the dependent pipeline by enqueuing
+            # changes ahead as "not live", that is, not intended to
+            # have jobs run.  Also, pipeline requirements are always
+            # ignored (which is safe because the changes are not
+            # live).
+            r = self.addChange(needed_change, quiet=True,
+                               ignore_requirements=True,
+                               live=False, change_queue=change_queue)
+            if not r:
+                return False
+        return True
+
+    def checkForChangesNeededBy(self, change, change_queue):
+        if self.pipeline.ignore_dependencies:
+            return True
+        self.log.debug("Checking for changes needed by %s:" % change)
+        # Return true if okay to proceed enqueing this change,
+        # false if the change should not be enqueued.
+        if not hasattr(change, 'needs_changes'):
+            self.log.debug("  Changeish does not support dependencies")
+            return True
+        if not change.needs_changes:
+            self.log.debug("  No changes needed")
+            return True
+        changes_needed = []
+        for needed_change in change.needs_changes:
+            self.log.debug("  Change %s needs change %s:" % (
+                change, needed_change))
+            if needed_change.is_merged:
+                self.log.debug("  Needed change is merged")
+                continue
+            if self.isChangeAlreadyInQueue(needed_change, change_queue):
+                self.log.debug("  Needed change is already ahead in the queue")
+                continue
+            self.log.debug("  Change %s is needed" % needed_change)
+            if needed_change not in changes_needed:
+                changes_needed.append(needed_change)
+                continue
+            # This differs from the dependent pipeline check in not
+            # verifying that the dependent change is mergable.
+        if changes_needed:
+            return changes_needed
+        return True
+
+    def dequeueItem(self, item):
+        super(IndependentPipelineManager, self).dequeueItem(item)
+        # An independent pipeline manager dynamically removes empty
+        # queues
+        if not item.queue.queue:
+            self.pipeline.removeQueue(item.queue)
+
+
+class StaticChangeQueueContextManager(object):
+    def __init__(self, change_queue):
+        self.change_queue = change_queue
+
+    def __enter__(self):
+        return self.change_queue
+
+    def __exit__(self, etype, value, tb):
+        pass
+
+
+class DependentPipelineManager(BasePipelineManager):
+    log = logging.getLogger("zuul.DependentPipelineManager")
+    changes_merge = True
+
+    def __init__(self, *args, **kwargs):
+        super(DependentPipelineManager, self).__init__(*args, **kwargs)
+
+    def _postConfig(self, layout):
+        super(DependentPipelineManager, self)._postConfig(layout)
+        self.buildChangeQueues()
+
+    def buildChangeQueues(self):
+        self.log.debug("Building shared change queues")
+        change_queues = []
+
+        for project in self.pipeline.getProjects():
+            change_queue = ChangeQueue(
+                self.pipeline,
+                window=self.pipeline.window,
+                window_floor=self.pipeline.window_floor,
+                window_increase_type=self.pipeline.window_increase_type,
+                window_increase_factor=self.pipeline.window_increase_factor,
+                window_decrease_type=self.pipeline.window_decrease_type,
+                window_decrease_factor=self.pipeline.window_decrease_factor)
+            change_queue.addProject(project)
+            change_queues.append(change_queue)
+            self.log.debug("Created queue: %s" % change_queue)
+
+        # Iterate over all queues trying to combine them, and keep doing
+        # so until they can not be combined further.
+        last_change_queues = change_queues
+        while True:
+            new_change_queues = self.combineChangeQueues(last_change_queues)
+            if len(last_change_queues) == len(new_change_queues):
+                break
+            last_change_queues = new_change_queues
+
+        self.log.info("  Shared change queues:")
+        for queue in new_change_queues:
+            self.pipeline.addQueue(queue)
+            self.log.info("    %s containing %s" % (
+                queue, queue.generated_name))
+
+    def combineChangeQueues(self, change_queues):
+        self.log.debug("Combining shared queues")
+        new_change_queues = []
+        for a in change_queues:
+            merged_a = False
+            for b in new_change_queues:
+                if not a.getJobs().isdisjoint(b.getJobs()):
+                    self.log.debug("Merging queue %s into %s" % (a, b))
+                    b.mergeChangeQueue(a)
+                    merged_a = True
+                    break  # this breaks out of 'for b' and continues 'for a'
+            if not merged_a:
+                self.log.debug("Keeping queue %s" % (a))
+                new_change_queues.append(a)
+        return new_change_queues
+
+    def getChangeQueue(self, change, existing=None):
+        if existing:
+            return StaticChangeQueueContextManager(existing)
+        return StaticChangeQueueContextManager(
+            self.pipeline.getQueue(change.project))
+
+    def isChangeReadyToBeEnqueued(self, change):
+        if not self.pipeline.source.canMerge(change,
+                                             self.getSubmitAllowNeeds()):
+            self.log.debug("Change %s can not merge, ignoring" % change)
+            return False
+        return True
+
+    def enqueueChangesBehind(self, change, quiet, ignore_requirements,
+                             change_queue):
+        to_enqueue = []
+        self.log.debug("Checking for changes needing %s:" % change)
+        if not hasattr(change, 'needed_by_changes'):
+            self.log.debug("  Changeish does not support dependencies")
+            return
+        for other_change in change.needed_by_changes:
+            with self.getChangeQueue(other_change) as other_change_queue:
+                if other_change_queue != change_queue:
+                    self.log.debug("  Change %s in project %s can not be "
+                                   "enqueued in the target queue %s" %
+                                   (other_change, other_change.project,
+                                    change_queue))
+                    continue
+            if self.pipeline.source.canMerge(other_change,
+                                             self.getSubmitAllowNeeds()):
+                self.log.debug("  Change %s needs %s and is ready to merge" %
+                               (other_change, change))
+                to_enqueue.append(other_change)
+
+        if not to_enqueue:
+            self.log.debug("  No changes need %s" % change)
+
+        for other_change in to_enqueue:
+            self.addChange(other_change, quiet=quiet,
+                           ignore_requirements=ignore_requirements,
+                           change_queue=change_queue)
+
+    def enqueueChangesAhead(self, change, quiet, ignore_requirements,
+                            change_queue):
+        ret = self.checkForChangesNeededBy(change, change_queue)
+        if ret in [True, False]:
+            return ret
+        self.log.debug("  Changes %s must be merged ahead of %s" %
+                       (ret, change))
+        for needed_change in ret:
+            r = self.addChange(needed_change, quiet=quiet,
+                               ignore_requirements=ignore_requirements,
+                               change_queue=change_queue)
+            if not r:
+                return False
+        return True
+
+    def checkForChangesNeededBy(self, change, change_queue):
+        self.log.debug("Checking for changes needed by %s:" % change)
+        # Return true if okay to proceed enqueing this change,
+        # false if the change should not be enqueued.
+        if not hasattr(change, 'needs_changes'):
+            self.log.debug("  Changeish does not support dependencies")
+            return True
+        if not change.needs_changes:
+            self.log.debug("  No changes needed")
+            return True
+        changes_needed = []
+        # Ignore supplied change_queue
+        with self.getChangeQueue(change) as change_queue:
+            for needed_change in change.needs_changes:
+                self.log.debug("  Change %s needs change %s:" % (
+                    change, needed_change))
+                if needed_change.is_merged:
+                    self.log.debug("  Needed change is merged")
+                    continue
+                with self.getChangeQueue(needed_change) as needed_change_queue:
+                    if needed_change_queue != change_queue:
+                        self.log.debug("  Change %s in project %s does not "
+                                       "share a change queue with %s "
+                                       "in project %s" %
+                                       (needed_change, needed_change.project,
+                                        change, change.project))
+                        return False
+                if not needed_change.is_current_patchset:
+                    self.log.debug("  Needed change is not the "
+                                   "current patchset")
+                    return False
+                if self.isChangeAlreadyInQueue(needed_change, change_queue):
+                    self.log.debug("  Needed change is already ahead "
+                                   "in the queue")
+                    continue
+                if self.pipeline.source.canMerge(needed_change,
+                                                 self.getSubmitAllowNeeds()):
+                    self.log.debug("  Change %s is needed" % needed_change)
+                    if needed_change not in changes_needed:
+                        changes_needed.append(needed_change)
+                        continue
+                # The needed change can't be merged.
+                self.log.debug("  Change %s is needed but can not be merged" %
+                               needed_change)
+                return False
+        if changes_needed:
+            return changes_needed
+        return True
+
+    def getFailingDependentItems(self, item):
+        if not hasattr(item.change, 'needs_changes'):
+            return None
+        if not item.change.needs_changes:
+            return None
+        failing_items = set()
+        for needed_change in item.change.needs_changes:
+            needed_item = self.getItemForChange(needed_change)
+            if not needed_item:
+                continue
+            if needed_item.current_build_set.failing_reasons:
+                failing_items.add(needed_item)
+        if failing_items:
+            return failing_items
+        return None
diff --git a/zuul/source/__init__.py b/zuul/source/__init__.py
index cb4501a..1575927 100644
--- a/zuul/source/__init__.py
+++ b/zuul/source/__init__.py
@@ -1,65 +1,65 @@
-# Copyright 2014 Rackspace Australia
-#
-# Licensed under the Apache License, Version 2.0 (the "License"); you may
-# not use this file except in compliance with the License. You may obtain
-# a copy of the License at
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-# License for the specific language governing permissions and limitations
-# under the License.
-
-import abc
-
-import six
-
-
-@six.add_metaclass(abc.ABCMeta)
-class BaseSource(object):
-    """Base class for sources.
-
-    A source class gives methods for fetching and updating changes. Each
-    pipeline must have (only) one source. It is the canonical provider of the
-    change to be tested.
-
-    Defines the exact public methods that must be supplied."""
-
-    def __init__(self, source_config={}, sched=None, connection=None):
-        self.source_config = source_config
-        self.sched = sched
-        self.connection = connection
-
-    def stop(self):
-        """Stop the source."""
-
-    @abc.abstractmethod
-    def getRefSha(self, project, ref):
-        """Return a sha for a given project ref."""
-
-    @abc.abstractmethod
-    def isMerged(self, change, head=None):
-        """Determine if change is merged.
-
-        If head is provided the change is checked if it is at head."""
-
-    @abc.abstractmethod
-    def canMerge(self, change, allow_needs):
-        """Determine if change can merge."""
-
-    def postConfig(self):
-        """Called after configuration has been processed."""
-
-    @abc.abstractmethod
-    def getChange(self, event, project):
-        """Get the change representing an event."""
-
-    @abc.abstractmethod
-    def getProjectOpenChanges(self, project):
-        """Get the open changes for a project."""
-
-    @abc.abstractmethod
-    def getGitUrl(self, project):
-        """Get the git url for a project."""
+# Copyright 2014 Rackspace Australia
+#
+# Licensed under the Apache License, Version 2.0 (the "License"); you may
+# not use this file except in compliance with the License. You may obtain
+# a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+# License for the specific language governing permissions and limitations
+# under the License.
+
+import abc
+
+import six
+
+
+@six.add_metaclass(abc.ABCMeta)
+class BaseSource(object):
+    """Base class for sources.
+
+    A source class gives methods for fetching and updating changes. Each
+    pipeline must have (only) one source. It is the canonical provider of the
+    change to be tested.
+
+    Defines the exact public methods that must be supplied."""
+
+    def __init__(self, source_config={}, sched=None, connection=None):
+        self.source_config = source_config
+        self.sched = sched
+        self.connection = connection
+
+    def stop(self):
+        """Stop the source."""
+
+    @abc.abstractmethod
+    def getRefSha(self, project, ref):
+        """Return a sha for a given project ref."""
+
+    @abc.abstractmethod
+    def isMerged(self, change, head=None):
+        """Determine if change is merged.
+
+        If head is provided the change is checked if it is at head."""
+
+    @abc.abstractmethod
+    def canMerge(self, change, allow_needs):
+        """Determine if change can merge."""
+
+    def postConfig(self):
+        """Called after configuration has been processed."""
+
+    @abc.abstractmethod
+    def getChange(self, event, project):
+        """Get the change representing an event."""
+
+    @abc.abstractmethod
+    def getProjectOpenChanges(self, project):
+        """Get the open changes for a project."""
+
+    @abc.abstractmethod
+    def getGitUrl(self, project):
+        """Get the git url for a project."""
diff --git a/zuul/source/gerrit.py b/zuul/source/gerrit.py
index 73cf726..8347861 100644
--- a/zuul/source/gerrit.py
+++ b/zuul/source/gerrit.py
@@ -1,357 +1,357 @@
-# Copyright 2012 Hewlett-Packard Development Company, L.P.
-#
-# Licensed under the Apache License, Version 2.0 (the "License"); you may
-# not use this file except in compliance with the License. You may obtain
-# a copy of the License at
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-# License for the specific language governing permissions and limitations
-# under the License.
-
-import logging
-import re
-import time
-from zuul import exceptions
-from zuul.model import Change, Ref, NullChange
-from zuul.source import BaseSource
-
-
-# Walk the change dependency tree to find a cycle
-def detect_cycle(change, history=None):
-    if history is None:
-        history = []
-    else:
-        history = history[:]
-    history.append(change.number)
-    for dep in change.needs_changes:
-        if dep.number in history:
-            raise Exception("Dependency cycle detected: %s in %s" % (
-                dep.number, history))
-        detect_cycle(dep, history)
-
-
-class GerritSource(BaseSource):
-    name = 'gerrit'
-    log = logging.getLogger("zuul.source.Gerrit")
-    replication_timeout = 300
-    replication_retry_interval = 5
-
-    depends_on_re = re.compile(r"^Depends-On: (I[0-9a-f]{40})\s*$",
-                               re.MULTILINE | re.IGNORECASE)
-
-    def getRefSha(self, project, ref):
-        refs = {}
-        try:
-            refs = self.connection.getInfoRefs(project)
-        except:
-            self.log.exception("Exception looking for ref %s" %
-                               ref)
-        sha = refs.get(ref, '')
-        return sha
-
-    def _waitForRefSha(self, project, ref, old_sha=''):
-        # Wait for the ref to show up in the repo
-        start = time.time()
-        while time.time() - start < self.replication_timeout:
-            sha = self.getRefSha(project.name, ref)
-            if old_sha != sha:
-                return True
-            time.sleep(self.replication_retry_interval)
-        return False
-
-    def isMerged(self, change, head=None):
-        self.log.debug("Checking if change %s is merged" % change)
-        if not change.number:
-            self.log.debug("Change has no number; considering it merged")
-            # Good question.  It's probably ref-updated, which, ah,
-            # means it's merged.
-            return True
-
-        data = self.connection.query(change.number)
-        change._data = data
-        change.is_merged = self._isMerged(change)
-        if change.is_merged:
-            self.log.debug("Change %s is merged" % (change,))
-        else:
-            self.log.debug("Change %s is not merged" % (change,))
-        if not head:
-            return change.is_merged
-        if not change.is_merged:
-            return False
-
-        ref = 'refs/heads/' + change.branch
-        self.log.debug("Waiting for %s to appear in git repo" % (change))
-        if self._waitForRefSha(change.project, ref, change._ref_sha):
-            self.log.debug("Change %s is in the git repo" %
-                           (change))
-            return True
-        self.log.debug("Change %s did not appear in the git repo" %
-                       (change))
-        return False
-
-    def _isMerged(self, change):
-        data = change._data
-        if not data:
-            return False
-        status = data.get('status')
-        if not status:
-            return False
-        if status == 'MERGED':
-            return True
-        return False
-
-    def canMerge(self, change, allow_needs):
-        if not change.number:
-            self.log.debug("Change has no number; considering it merged")
-            # Good question.  It's probably ref-updated, which, ah,
-            # means it's merged.
-            return True
-        data = change._data
-        if not data:
-            return False
-        if 'submitRecords' not in data:
-            return False
-        try:
-            for sr in data['submitRecords']:
-                if sr['status'] == 'OK':
-                    return True
-                elif sr['status'] == 'NOT_READY':
-                    for label in sr['labels']:
-                        if label['status'] in ['OK', 'MAY']:
-                            continue
-                        elif label['status'] in ['NEED', 'REJECT']:
-                            # It may be our own rejection, so we ignore
-                            if label['label'].lower() not in allow_needs:
-                                return False
-                            continue
-                        else:
-                            # IMPOSSIBLE
-                            return False
-                else:
-                    # CLOSED, RULE_ERROR
-                    return False
-        except:
-            self.log.exception("Exception determining whether change"
-                               "%s can merge:" % change)
-            return False
-        return True
-
-    def postConfig(self):
-        pass
-
-    def getChange(self, event, project):
-        if event.change_number:
-            refresh = False
-            change = self._getChange(event.change_number, event.patch_number,
-                                     refresh=refresh)
-        elif event.ref:
-            change = Ref(project)
-            change.ref = event.ref
-            change.oldrev = event.oldrev
-            change.newrev = event.newrev
-            change.url = self._getGitwebUrl(project, sha=event.newrev)
-        else:
-            change = NullChange(project)
-        return change
-
-    def _getChange(self, number, patchset, refresh=False, history=None):
-        key = '%s,%s' % (number, patchset)
-        change = self.connection.getCachedChange(key)
-        if change and not refresh:
-            return change
-        if not change:
-            change = Change(None)
-            change.number = number
-            change.patchset = patchset
-        key = '%s,%s' % (change.number, change.patchset)
-        self.connection.updateChangeCache(key, change)
-        try:
-            self._updateChange(change, history)
-        except Exception:
-            self.connection.deleteCachedChange(key)
-            raise
-        return change
-
-    def getProjectOpenChanges(self, project):
-        # This is a best-effort function in case Gerrit is unable to return
-        # a particular change.  It happens.
-        query = "project:%s status:open" % (project.name,)
-        self.log.debug("Running query %s to get project open changes" %
-                       (query,))
-        data = self.connection.simpleQuery(query)
-        changes = []
-        for record in data:
-            try:
-                changes.append(
-                    self._getChange(record['number'],
-                                    record['currentPatchSet']['number']))
-            except Exception:
-                self.log.exception("Unable to query change %s" %
-                                   (record.get('number'),))
-        return changes
-
-    def _getDependsOnFromCommit(self, message, change):
-        records = []
-        seen = set()
-        for match in self.depends_on_re.findall(message):
-            if match in seen:
-                self.log.debug("Ignoring duplicate Depends-On: %s" %
-                               (match,))
-                continue
-            seen.add(match)
-            query = "change:%s" % (match,)
-            self.log.debug("Updating %s: Running query %s "
-                           "to find needed changes" %
-                           (change, query,))
-            records.extend(self.connection.simpleQuery(query))
-        return records
-
-    def _getNeededByFromCommit(self, change_id, change):
-        records = []
-        seen = set()
-        query = 'message:%s' % change_id
-        self.log.debug("Updating %s: Running query %s "
-                       "to find changes needed-by" %
-                       (change, query,))
-        results = self.connection.simpleQuery(query)
-        for result in results:
-            for match in self.depends_on_re.findall(
-                result['commitMessage']):
-                if match != change_id:
-                    continue
-                key = (result['number'], result['currentPatchSet']['number'])
-                if key in seen:
-                    continue
-                self.log.debug("Updating %s: Found change %s,%s "
-                               "needs %s from commit" %
-                               (change, key[0], key[1], change_id))
-                seen.add(key)
-                records.append(result)
-        return records
-
-    def _updateChange(self, change, history=None):
-        self.log.info("Updating %s" % (change,))
-        data = self.connection.query(change.number)
-        change._data = data
-
-        if change.patchset is None:
-            change.patchset = data['currentPatchSet']['number']
-
-        if 'project' not in data:
-            raise exceptions.ChangeNotFound(change.number, change.patchset)
-        # If updated changed came as a dependent on
-        # and its project is not defined,
-        # then create a 'foreign' project for it in layout
-        change.project = self.sched.getProject(data['project'],
-                                               create_foreign=bool(history))
-        change.branch = data['branch']
-        change.url = data['url']
-        max_ps = 0
-        files = []
-        for ps in data['patchSets']:
-            if ps['number'] == change.patchset:
-                change.refspec = ps['ref']
-                for f in ps.get('files', []):
-                    files.append(f['file'])
-            if int(ps['number']) > int(max_ps):
-                max_ps = ps['number']
-        if max_ps == change.patchset:
-            change.is_current_patchset = True
-        else:
-            change.is_current_patchset = False
-        change.files = files
-
-        change.is_merged = self._isMerged(change)
-        change.approvals = data['currentPatchSet'].get('approvals', [])
-        change.open = data['open']
-        change.status = data['status']
-        change.owner = data['owner']
-
-        if change.is_merged:
-            # This change is merged, so we don't need to look any further
-            # for dependencies.
-            self.log.debug("Updating %s: change is merged" % (change,))
-            return change
-
-        if history is None:
-            history = []
-        else:
-            history = history[:]
-        history.append(change.number)
-
-        needs_changes = []
-        if 'dependsOn' in data:
-            parts = data['dependsOn'][0]['ref'].split('/')
-            dep_num, dep_ps = parts[3], parts[4]
-            if dep_num in history:
-                raise Exception("Dependency cycle detected: %s in %s" % (
-                    dep_num, history))
-            self.log.debug("Updating %s: Getting git-dependent change %s,%s" %
-                           (change, dep_num, dep_ps))
-            dep = self._getChange(dep_num, dep_ps, history=history)
-            # Because we are not forcing a refresh in _getChange, it
-            # may return without executing this code, so if we are
-            # updating our change to add ourselves to a dependency
-            # cycle, we won't detect it.  By explicitly performing a
-            # walk of the dependency tree, we will.
-            detect_cycle(dep, history)
-            if (not dep.is_merged) and dep not in needs_changes:
-                needs_changes.append(dep)
-
-        for record in self._getDependsOnFromCommit(data['commitMessage'],
-                                                   change):
-            dep_num = record['number']
-            dep_ps = record['currentPatchSet']['number']
-            if dep_num in history:
-                raise Exception("Dependency cycle detected: %s in %s" % (
-                    dep_num, history))
-            self.log.debug("Updating %s: Getting commit-dependent "
-                           "change %s,%s" %
-                           (change, dep_num, dep_ps))
-            dep = self._getChange(dep_num, dep_ps, history=history)
-            # Because we are not forcing a refresh in _getChange, it
-            # may return without executing this code, so if we are
-            # updating our change to add ourselves to a dependency
-            # cycle, we won't detect it.  By explicitly performing a
-            # walk of the dependency tree, we will.
-            detect_cycle(dep, history)
-            if (not dep.is_merged) and dep not in needs_changes:
-                needs_changes.append(dep)
-        change.needs_changes = needs_changes
-
-        needed_by_changes = []
-        if 'neededBy' in data:
-            for needed in data['neededBy']:
-                parts = needed['ref'].split('/')
-                dep_num, dep_ps = parts[3], parts[4]
-                self.log.debug("Updating %s: Getting git-needed change %s,%s" %
-                               (change, dep_num, dep_ps))
-                dep = self._getChange(dep_num, dep_ps)
-                if (not dep.is_merged) and dep.is_current_patchset:
-                    needed_by_changes.append(dep)
-
-        for record in self._getNeededByFromCommit(data['id'], change):
-            dep_num = record['number']
-            dep_ps = record['currentPatchSet']['number']
-            self.log.debug("Updating %s: Getting commit-needed change %s,%s" %
-                           (change, dep_num, dep_ps))
-            # Because a commit needed-by may be a cross-repo
-            # dependency, cause that change to refresh so that it will
-            # reference the latest patchset of its Depends-On (this
-            # change).
-            dep = self._getChange(dep_num, dep_ps, refresh=True)
-            if (not dep.is_merged) and dep.is_current_patchset:
-                needed_by_changes.append(dep)
-        change.needed_by_changes = needed_by_changes
-
-        return change
-
-    def getGitUrl(self, project):
-        return self.connection.getGitUrl(project)
-
-    def _getGitwebUrl(self, project, sha=None):
-        return self.connection.getGitwebUrl(project, sha)
+# Copyright 2012 Hewlett-Packard Development Company, L.P.
+#
+# Licensed under the Apache License, Version 2.0 (the "License"); you may
+# not use this file except in compliance with the License. You may obtain
+# a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+# License for the specific language governing permissions and limitations
+# under the License.
+
+import logging
+import re
+import time
+from zuul import exceptions
+from zuul.model import Change, Ref, NullChange
+from zuul.source import BaseSource
+
+
+# Walk the change dependency tree to find a cycle
+def detect_cycle(change, history=None):
+    if history is None:
+        history = []
+    else:
+        history = history[:]
+    history.append(change.number)
+    for dep in change.needs_changes:
+        if dep.number in history:
+            raise Exception("Dependency cycle detected: %s in %s" % (
+                dep.number, history))
+        detect_cycle(dep, history)
+
+
+class GerritSource(BaseSource):
+    name = 'gerrit'
+    log = logging.getLogger("zuul.source.Gerrit")
+    replication_timeout = 300
+    replication_retry_interval = 5
+
+    depends_on_re = re.compile(r"^Depends-On: (I[0-9a-f]{40})\s*$",
+                               re.MULTILINE | re.IGNORECASE)
+
+    def getRefSha(self, project, ref):
+        refs = {}
+        try:
+            refs = self.connection.getInfoRefs(project)
+        except:
+            self.log.exception("Exception looking for ref %s" %
+                               ref)
+        sha = refs.get(ref, '')
+        return sha
+
+    def _waitForRefSha(self, project, ref, old_sha=''):
+        # Wait for the ref to show up in the repo
+        start = time.time()
+        while time.time() - start < self.replication_timeout:
+            sha = self.getRefSha(project.name, ref)
+            if old_sha != sha:
+                return True
+            time.sleep(self.replication_retry_interval)
+        return False
+
+    def isMerged(self, change, head=None):
+        self.log.debug("Checking if change %s is merged" % change)
+        if not change.number:
+            self.log.debug("Change has no number; considering it merged")
+            # Good question.  It's probably ref-updated, which, ah,
+            # means it's merged.
+            return True
+
+        data = self.connection.query(change.number)
+        change._data = data
+        change.is_merged = self._isMerged(change)
+        if change.is_merged:
+            self.log.debug("Change %s is merged" % (change,))
+        else:
+            self.log.debug("Change %s is not merged" % (change,))
+        if not head:
+            return change.is_merged
+        if not change.is_merged:
+            return False
+
+        ref = 'refs/heads/' + change.branch
+        self.log.debug("Waiting for %s to appear in git repo" % (change))
+        if self._waitForRefSha(change.project, ref, change._ref_sha):
+            self.log.debug("Change %s is in the git repo" %
+                           (change))
+            return True
+        self.log.debug("Change %s did not appear in the git repo" %
+                       (change))
+        return False
+
+    def _isMerged(self, change):
+        data = change._data
+        if not data:
+            return False
+        status = data.get('status')
+        if not status:
+            return False
+        if status == 'MERGED':
+            return True
+        return False
+
+    def canMerge(self, change, allow_needs):
+        if not change.number:
+            self.log.debug("Change has no number; considering it merged")
+            # Good question.  It's probably ref-updated, which, ah,
+            # means it's merged.
+            return True
+        data = change._data
+        if not data:
+            return False
+        if 'submitRecords' not in data:
+            return False
+        try:
+            for sr in data['submitRecords']:
+                if sr['status'] == 'OK':
+                    return True
+                elif sr['status'] == 'NOT_READY':
+                    for label in sr['labels']:
+                        if label['status'] in ['OK', 'MAY']:
+                            continue
+                        elif label['status'] in ['NEED', 'REJECT']:
+                            # It may be our own rejection, so we ignore
+                            if label['label'].lower() not in allow_needs:
+                                return False
+                            continue
+                        else:
+                            # IMPOSSIBLE
+                            return False
+                else:
+                    # CLOSED, RULE_ERROR
+                    return False
+        except:
+            self.log.exception("Exception determining whether change"
+                               "%s can merge:" % change)
+            return False
+        return True
+
+    def postConfig(self):
+        pass
+
+    def getChange(self, event, project):
+        if event.change_number:
+            refresh = False
+            change = self._getChange(event.change_number, event.patch_number,
+                                     refresh=refresh)
+        elif event.ref:
+            change = Ref(project)
+            change.ref = event.ref
+            change.oldrev = event.oldrev
+            change.newrev = event.newrev
+            change.url = self._getGitwebUrl(project, sha=event.newrev)
+        else:
+            change = NullChange(project)
+        return change
+
+    def _getChange(self, number, patchset, refresh=False, history=None):
+        key = '%s,%s' % (number, patchset)
+        change = self.connection.getCachedChange(key)
+        if change and not refresh:
+            return change
+        if not change:
+            change = Change(None)
+            change.number = number
+            change.patchset = patchset
+        key = '%s,%s' % (change.number, change.patchset)
+        self.connection.updateChangeCache(key, change)
+        try:
+            self._updateChange(change, history)
+        except Exception:
+            self.connection.deleteCachedChange(key)
+            raise
+        return change
+
+    def getProjectOpenChanges(self, project):
+        # This is a best-effort function in case Gerrit is unable to return
+        # a particular change.  It happens.
+        query = "project:%s status:open" % (project.name,)
+        self.log.debug("Running query %s to get project open changes" %
+                       (query,))
+        data = self.connection.simpleQuery(query)
+        changes = []
+        for record in data:
+            try:
+                changes.append(
+                    self._getChange(record['number'],
+                                    record['currentPatchSet']['number']))
+            except Exception:
+                self.log.exception("Unable to query change %s" %
+                                   (record.get('number'),))
+        return changes
+
+    def _getDependsOnFromCommit(self, message, change):
+        records = []
+        seen = set()
+        for match in self.depends_on_re.findall(message):
+            if match in seen:
+                self.log.debug("Ignoring duplicate Depends-On: %s" %
+                               (match,))
+                continue
+            seen.add(match)
+            query = "change:%s" % (match,)
+            self.log.debug("Updating %s: Running query %s "
+                           "to find needed changes" %
+                           (change, query,))
+            records.extend(self.connection.simpleQuery(query))
+        return records
+
+    def _getNeededByFromCommit(self, change_id, change):
+        records = []
+        seen = set()
+        query = 'message:%s' % change_id
+        self.log.debug("Updating %s: Running query %s "
+                       "to find changes needed-by" %
+                       (change, query,))
+        results = self.connection.simpleQuery(query)
+        for result in results:
+            for match in self.depends_on_re.findall(
+                result['commitMessage']):
+                if match != change_id:
+                    continue
+                key = (result['number'], result['currentPatchSet']['number'])
+                if key in seen:
+                    continue
+                self.log.debug("Updating %s: Found change %s,%s "
+                               "needs %s from commit" %
+                               (change, key[0], key[1], change_id))
+                seen.add(key)
+                records.append(result)
+        return records
+
+    def _updateChange(self, change, history=None):
+        self.log.info("Updating %s" % (change,))
+        data = self.connection.query(change.number)
+        change._data = data
+
+        if change.patchset is None:
+            change.patchset = data['currentPatchSet']['number']
+
+        if 'project' not in data:
+            raise exceptions.ChangeNotFound(change.number, change.patchset)
+        # If updated changed came as a dependent on
+        # and its project is not defined,
+        # then create a 'foreign' project for it in layout
+        change.project = self.sched.getProject(data['project'],
+                                               create_foreign=bool(history))
+        change.branch = data['branch']
+        change.url = data['url']
+        max_ps = 0
+        files = []
+        for ps in data['patchSets']:
+            if ps['number'] == change.patchset:
+                change.refspec = ps['ref']
+                for f in ps.get('files', []):
+                    files.append(f['file'])
+            if int(ps['number']) > int(max_ps):
+                max_ps = ps['number']
+        if max_ps == change.patchset:
+            change.is_current_patchset = True
+        else:
+            change.is_current_patchset = False
+        change.files = files
+
+        change.is_merged = self._isMerged(change)
+        change.approvals = data['currentPatchSet'].get('approvals', [])
+        change.open = data['open']
+        change.status = data['status']
+        change.owner = data['owner']
+
+        if change.is_merged:
+            # This change is merged, so we don't need to look any further
+            # for dependencies.
+            self.log.debug("Updating %s: change is merged" % (change,))
+            return change
+
+        if history is None:
+            history = []
+        else:
+            history = history[:]
+        history.append(change.number)
+
+        needs_changes = []
+        if 'dependsOn' in data:
+            parts = data['dependsOn'][0]['ref'].split('/')
+            dep_num, dep_ps = parts[3], parts[4]
+            if dep_num in history:
+                raise Exception("Dependency cycle detected: %s in %s" % (
+                    dep_num, history))
+            self.log.debug("Updating %s: Getting git-dependent change %s,%s" %
+                           (change, dep_num, dep_ps))
+            dep = self._getChange(dep_num, dep_ps, history=history)
+            # Because we are not forcing a refresh in _getChange, it
+            # may return without executing this code, so if we are
+            # updating our change to add ourselves to a dependency
+            # cycle, we won't detect it.  By explicitly performing a
+            # walk of the dependency tree, we will.
+            detect_cycle(dep, history)
+            if (not dep.is_merged) and dep not in needs_changes:
+                needs_changes.append(dep)
+
+        for record in self._getDependsOnFromCommit(data['commitMessage'],
+                                                   change):
+            dep_num = record['number']
+            dep_ps = record['currentPatchSet']['number']
+            if dep_num in history:
+                raise Exception("Dependency cycle detected: %s in %s" % (
+                    dep_num, history))
+            self.log.debug("Updating %s: Getting commit-dependent "
+                           "change %s,%s" %
+                           (change, dep_num, dep_ps))
+            dep = self._getChange(dep_num, dep_ps, history=history)
+            # Because we are not forcing a refresh in _getChange, it
+            # may return without executing this code, so if we are
+            # updating our change to add ourselves to a dependency
+            # cycle, we won't detect it.  By explicitly performing a
+            # walk of the dependency tree, we will.
+            detect_cycle(dep, history)
+            if (not dep.is_merged) and dep not in needs_changes:
+                needs_changes.append(dep)
+        change.needs_changes = needs_changes
+
+        needed_by_changes = []
+        if 'neededBy' in data:
+            for needed in data['neededBy']:
+                parts = needed['ref'].split('/')
+                dep_num, dep_ps = parts[3], parts[4]
+                self.log.debug("Updating %s: Getting git-needed change %s,%s" %
+                               (change, dep_num, dep_ps))
+                dep = self._getChange(dep_num, dep_ps)
+                if (not dep.is_merged) and dep.is_current_patchset:
+                    needed_by_changes.append(dep)
+
+        for record in self._getNeededByFromCommit(data['id'], change):
+            dep_num = record['number']
+            dep_ps = record['currentPatchSet']['number']
+            self.log.debug("Updating %s: Getting commit-needed change %s,%s" %
+                           (change, dep_num, dep_ps))
+            # Because a commit needed-by may be a cross-repo
+            # dependency, cause that change to refresh so that it will
+            # reference the latest patchset of its Depends-On (this
+            # change).
+            dep = self._getChange(dep_num, dep_ps, refresh=True)
+            if (not dep.is_merged) and dep.is_current_patchset:
+                needed_by_changes.append(dep)
+        change.needed_by_changes = needed_by_changes
+
+        return change
+
+    def getGitUrl(self, project):
+        return self.connection.getGitUrl(project)
+
+    def _getGitwebUrl(self, project, sha=None):
+        return self.connection.getGitwebUrl(project, sha)
diff --git a/zuul/trigger/__init__.py b/zuul/trigger/__init__.py
index 16fb0b1..ef54fec 100644
--- a/zuul/trigger/__init__.py
+++ b/zuul/trigger/__init__.py
@@ -1,46 +1,46 @@
-# Copyright 2014 Rackspace Australia
-#
-# Licensed under the Apache License, Version 2.0 (the "License"); you may
-# not use this file except in compliance with the License. You may obtain
-# a copy of the License at
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-# License for the specific language governing permissions and limitations
-# under the License.
-
-import abc
-
-import six
-
-
-@six.add_metaclass(abc.ABCMeta)
-class BaseTrigger(object):
-    """Base class for triggers.
-
-    Defines the exact public methods that must be supplied."""
-
-    def __init__(self, trigger_config={}, sched=None, connection=None):
-        self.trigger_config = trigger_config
-        self.sched = sched
-        self.connection = connection
-
-    def stop(self):
-        """Stop the trigger."""
-
-    @abc.abstractmethod
-    def getEventFilters(self, trigger_conf):
-        """Return a list of EventFilter's for the scheduler to match against.
-        """
-
-    def postConfig(self):
-        """Called after config is loaded."""
-
-    def onChangeMerged(self, change, source):
-        """Called when a change has been merged."""
-
-    def onChangeEnqueued(self, change, pipeline):
-        """Called when a change has been enqueued."""
+# Copyright 2014 Rackspace Australia
+#
+# Licensed under the Apache License, Version 2.0 (the "License"); you may
+# not use this file except in compliance with the License. You may obtain
+# a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+# License for the specific language governing permissions and limitations
+# under the License.
+
+import abc
+
+import six
+
+
+@six.add_metaclass(abc.ABCMeta)
+class BaseTrigger(object):
+    """Base class for triggers.
+
+    Defines the exact public methods that must be supplied."""
+
+    def __init__(self, trigger_config={}, sched=None, connection=None):
+        self.trigger_config = trigger_config
+        self.sched = sched
+        self.connection = connection
+
+    def stop(self):
+        """Stop the trigger."""
+
+    @abc.abstractmethod
+    def getEventFilters(self, trigger_conf):
+        """Return a list of EventFilter's for the scheduler to match against.
+        """
+
+    def postConfig(self):
+        """Called after config is loaded."""
+
+    def onChangeMerged(self, change, source):
+        """Called when a change has been merged."""
+
+    def onChangeEnqueued(self, change, pipeline):
+        """Called when a change has been enqueued."""
diff --git a/zuul/trigger/gerrit.py b/zuul/trigger/gerrit.py
index 8a3fe42..3844b02 100644
--- a/zuul/trigger/gerrit.py
+++ b/zuul/trigger/gerrit.py
@@ -1,117 +1,117 @@
-# Copyright 2012 Hewlett-Packard Development Company, L.P.
-#
-# Licensed under the Apache License, Version 2.0 (the "License"); you may
-# not use this file except in compliance with the License. You may obtain
-# a copy of the License at
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-# License for the specific language governing permissions and limitations
-# under the License.
-
-import logging
-import voluptuous as v
-from zuul.model import EventFilter
-from zuul.trigger import BaseTrigger
-
-
-class GerritTrigger(BaseTrigger):
-    name = 'gerrit'
-    log = logging.getLogger("zuul.trigger.Gerrit")
-
-    def getEventFilters(self, trigger_conf):
-        def toList(item):
-            if not item:
-                return []
-            if isinstance(item, list):
-                return item
-            return [item]
-
-        efilters = []
-        for trigger in toList(trigger_conf):
-            approvals = {}
-            for approval_dict in toList(trigger.get('approval')):
-                for key, val in approval_dict.items():
-                    approvals[key] = val
-            # Backwards compat for *_filter versions of these args
-            comments = toList(trigger.get('comment'))
-            if not comments:
-                comments = toList(trigger.get('comment_filter'))
-            emails = toList(trigger.get('email'))
-            if not emails:
-                emails = toList(trigger.get('email_filter'))
-            usernames = toList(trigger.get('username'))
-            if not usernames:
-                usernames = toList(trigger.get('username_filter'))
-            ignore_deletes = trigger.get('ignore-deletes', True)
-            f = EventFilter(
-                trigger=self,
-                types=toList(trigger['event']),
-                branches=toList(trigger.get('branch')),
-                refs=toList(trigger.get('ref')),
-                event_approvals=approvals,
-                comments=comments,
-                emails=emails,
-                usernames=usernames,
-                required_approvals=(
-                    toList(trigger.get('require-approval'))
-                ),
-                reject_approvals=toList(
-                    trigger.get('reject-approval')
-                ),
-                ignore_deletes=ignore_deletes
-            )
-            efilters.append(f)
-
-        return efilters
-
-
-def validate_conf(trigger_conf):
-    """Validates the layout's trigger data."""
-    events_with_ref = ('ref-updated', )
-    for event in trigger_conf:
-        if event['event'] not in events_with_ref and event.get('ref', False):
-            raise v.Invalid(
-                "The event %s does not include ref information, Zuul cannot "
-                "use ref filter 'ref: %s'" % (event['event'], event['ref']))
-
-
-def getSchema():
-    def toList(x):
-        return v.Any([x], x)
-    variable_dict = v.Schema({}, extra=True)
-
-    approval = v.Schema({'username': str,
-                         'email-filter': str,
-                         'email': str,
-                         'older-than': str,
-                         'newer-than': str,
-                         }, extra=True)
-
-    gerrit_trigger = {
-        v.Required('event'):
-            toList(v.Any('patchset-created',
-                         'draft-published',
-                         'change-abandoned',
-                         'change-restored',
-                         'change-merged',
-                         'comment-added',
-                         'ref-updated')),
-        'comment_filter': toList(str),
-        'comment': toList(str),
-        'email_filter': toList(str),
-        'email': toList(str),
-        'username_filter': toList(str),
-        'username': toList(str),
-        'branch': toList(str),
-        'ref': toList(str),
-        'ignore-deletes': bool,
-        'approval': toList(variable_dict),
-        'require-approval': toList(approval),
-        'reject-approval': toList(approval),
-    }
-
-    return gerrit_trigger
+# Copyright 2012 Hewlett-Packard Development Company, L.P.
+#
+# Licensed under the Apache License, Version 2.0 (the "License"); you may
+# not use this file except in compliance with the License. You may obtain
+# a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+# License for the specific language governing permissions and limitations
+# under the License.
+
+import logging
+import voluptuous as v
+from zuul.model import EventFilter
+from zuul.trigger import BaseTrigger
+
+
+class GerritTrigger(BaseTrigger):
+    name = 'gerrit'
+    log = logging.getLogger("zuul.trigger.Gerrit")
+
+    def getEventFilters(self, trigger_conf):
+        def toList(item):
+            if not item:
+                return []
+            if isinstance(item, list):
+                return item
+            return [item]
+
+        efilters = []
+        for trigger in toList(trigger_conf):
+            approvals = {}
+            for approval_dict in toList(trigger.get('approval')):
+                for key, val in approval_dict.items():
+                    approvals[key] = val
+            # Backwards compat for *_filter versions of these args
+            comments = toList(trigger.get('comment'))
+            if not comments:
+                comments = toList(trigger.get('comment_filter'))
+            emails = toList(trigger.get('email'))
+            if not emails:
+                emails = toList(trigger.get('email_filter'))
+            usernames = toList(trigger.get('username'))
+            if not usernames:
+                usernames = toList(trigger.get('username_filter'))
+            ignore_deletes = trigger.get('ignore-deletes', True)
+            f = EventFilter(
+                trigger=self,
+                types=toList(trigger['event']),
+                branches=toList(trigger.get('branch')),
+                refs=toList(trigger.get('ref')),
+                event_approvals=approvals,
+                comments=comments,
+                emails=emails,
+                usernames=usernames,
+                required_approvals=(
+                    toList(trigger.get('require-approval'))
+                ),
+                reject_approvals=toList(
+                    trigger.get('reject-approval')
+                ),
+                ignore_deletes=ignore_deletes
+            )
+            efilters.append(f)
+
+        return efilters
+
+
+def validate_conf(trigger_conf):
+    """Validates the layout's trigger data."""
+    events_with_ref = ('ref-updated', )
+    for event in trigger_conf:
+        if event['event'] not in events_with_ref and event.get('ref', False):
+            raise v.Invalid(
+                "The event %s does not include ref information, Zuul cannot "
+                "use ref filter 'ref: %s'" % (event['event'], event['ref']))
+
+
+def getSchema():
+    def toList(x):
+        return v.Any([x], x)
+    variable_dict = v.Schema({}, extra=True)
+
+    approval = v.Schema({'username': str,
+                         'email-filter': str,
+                         'email': str,
+                         'older-than': str,
+                         'newer-than': str,
+                         }, extra=True)
+
+    gerrit_trigger = {
+        v.Required('event'):
+            toList(v.Any('patchset-created',
+                         'draft-published',
+                         'change-abandoned',
+                         'change-restored',
+                         'change-merged',
+                         'comment-added',
+                         'ref-updated')),
+        'comment_filter': toList(str),
+        'comment': toList(str),
+        'email_filter': toList(str),
+        'email': toList(str),
+        'username_filter': toList(str),
+        'username': toList(str),
+        'branch': toList(str),
+        'ref': toList(str),
+        'ignore-deletes': bool,
+        'approval': toList(variable_dict),
+        'require-approval': toList(approval),
+        'reject-approval': toList(approval),
+    }
+
+    return gerrit_trigger
diff --git a/zuul/trigger/timer.py b/zuul/trigger/timer.py
index f81312e..1dddce6 100644
--- a/zuul/trigger/timer.py
+++ b/zuul/trigger/timer.py
@@ -1,94 +1,94 @@
-# Copyright 2012 Hewlett-Packard Development Company, L.P.
-# Copyright 2013 OpenStack Foundation
-#
-# Licensed under the Apache License, Version 2.0 (the "License"); you may
-# not use this file except in compliance with the License. You may obtain
-# a copy of the License at
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-# License for the specific language governing permissions and limitations
-# under the License.
-
-from apscheduler.schedulers.background import BackgroundScheduler
-from apscheduler.triggers.cron import CronTrigger
-import logging
-import voluptuous as v
-from zuul.model import EventFilter, TriggerEvent
-from zuul.trigger import BaseTrigger
-
-
-class TimerTrigger(BaseTrigger):
-    name = 'timer'
-    log = logging.getLogger("zuul.Timer")
-
-    def __init__(self, trigger_config={}, sched=None, connection=None):
-        super(TimerTrigger, self).__init__(trigger_config, sched, connection)
-        self.apsched = BackgroundScheduler()
-        self.apsched.start()
-
-    def _onTrigger(self, pipeline_name, timespec):
-        for project in self.sched.layout.projects.values():
-            event = TriggerEvent()
-            event.type = 'timer'
-            event.timespec = timespec
-            event.forced_pipeline = pipeline_name
-            event.project_name = project.name
-            self.log.debug("Adding event %s" % event)
-            self.sched.addEvent(event)
-
-    def stop(self):
-        self.apsched.shutdown()
-
-    def getEventFilters(self, trigger_conf):
-        def toList(item):
-            if not item:
-                return []
-            if isinstance(item, list):
-                return item
-            return [item]
-
-        efilters = []
-        for trigger in toList(trigger_conf):
-            f = EventFilter(trigger=self,
-                            types=['timer'],
-                            timespecs=toList(trigger['time']))
-
-            efilters.append(f)
-
-        return efilters
-
-    def postConfig(self):
-        for job in self.apsched.get_jobs():
-            job.remove()
-        for pipeline in self.sched.layout.pipelines.values():
-            for ef in pipeline.manager.event_filters:
-                if ef.trigger != self:
-                    continue
-                for timespec in ef.timespecs:
-                    parts = timespec.split()
-                    if len(parts) < 5 or len(parts) > 6:
-                        self.log.error(
-                            "Unable to parse time value '%s' "
-                            "defined in pipeline %s" % (
-                                timespec,
-                                pipeline.name))
-                        continue
-                    minute, hour, dom, month, dow = parts[:5]
-                    if len(parts) > 5:
-                        second = parts[5]
-                    else:
-                        second = None
-                    trigger = CronTrigger(day=dom, day_of_week=dow, hour=hour,
-                                          minute=minute, second=second)
-
-                    self.apsched.add_job(self._onTrigger, trigger=trigger,
-                                         args=(pipeline.name, timespec,))
-
-
-def getSchema():
-    timer_trigger = {v.Required('time'): str}
-    return timer_trigger
+# Copyright 2012 Hewlett-Packard Development Company, L.P.
+# Copyright 2013 OpenStack Foundation
+#
+# Licensed under the Apache License, Version 2.0 (the "License"); you may
+# not use this file except in compliance with the License. You may obtain
+# a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+# License for the specific language governing permissions and limitations
+# under the License.
+
+from apscheduler.schedulers.background import BackgroundScheduler
+from apscheduler.triggers.cron import CronTrigger
+import logging
+import voluptuous as v
+from zuul.model import EventFilter, TriggerEvent
+from zuul.trigger import BaseTrigger
+
+
+class TimerTrigger(BaseTrigger):
+    name = 'timer'
+    log = logging.getLogger("zuul.Timer")
+
+    def __init__(self, trigger_config={}, sched=None, connection=None):
+        super(TimerTrigger, self).__init__(trigger_config, sched, connection)
+        self.apsched = BackgroundScheduler()
+        self.apsched.start()
+
+    def _onTrigger(self, pipeline_name, timespec):
+        for project in self.sched.layout.projects.values():
+            event = TriggerEvent()
+            event.type = 'timer'
+            event.timespec = timespec
+            event.forced_pipeline = pipeline_name
+            event.project_name = project.name
+            self.log.debug("Adding event %s" % event)
+            self.sched.addEvent(event)
+
+    def stop(self):
+        self.apsched.shutdown()
+
+    def getEventFilters(self, trigger_conf):
+        def toList(item):
+            if not item:
+                return []
+            if isinstance(item, list):
+                return item
+            return [item]
+
+        efilters = []
+        for trigger in toList(trigger_conf):
+            f = EventFilter(trigger=self,
+                            types=['timer'],
+                            timespecs=toList(trigger['time']))
+
+            efilters.append(f)
+
+        return efilters
+
+    def postConfig(self):
+        for job in self.apsched.get_jobs():
+            job.remove()
+        for pipeline in self.sched.layout.pipelines.values():
+            for ef in pipeline.manager.event_filters:
+                if ef.trigger != self:
+                    continue
+                for timespec in ef.timespecs:
+                    parts = timespec.split()
+                    if len(parts) < 5 or len(parts) > 6:
+                        self.log.error(
+                            "Unable to parse time value '%s' "
+                            "defined in pipeline %s" % (
+                                timespec,
+                                pipeline.name))
+                        continue
+                    minute, hour, dom, month, dow = parts[:5]
+                    if len(parts) > 5:
+                        second = parts[5]
+                    else:
+                        second = None
+                    trigger = CronTrigger(day=dom, day_of_week=dow, hour=hour,
+                                          minute=minute, second=second)
+
+                    self.apsched.add_job(self._onTrigger, trigger=trigger,
+                                         args=(pipeline.name, timespec,))
+
+
+def getSchema():
+    timer_trigger = {v.Required('time'): str}
+    return timer_trigger
diff --git a/zuul/trigger/zuultrigger.py b/zuul/trigger/zuultrigger.py
index 00b21f2..be4ad47 100644
--- a/zuul/trigger/zuultrigger.py
+++ b/zuul/trigger/zuultrigger.py
@@ -1,148 +1,148 @@
-# Copyright 2012-2014 Hewlett-Packard Development Company, L.P.
-# Copyright 2013 OpenStack Foundation
-#
-# Licensed under the Apache License, Version 2.0 (the "License"); you may
-# not use this file except in compliance with the License. You may obtain
-# a copy of the License at
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-# License for the specific language governing permissions and limitations
-# under the License.
-
-import logging
-import voluptuous as v
-from zuul.model import EventFilter, TriggerEvent
-from zuul.trigger import BaseTrigger
-
-
-class ZuulTrigger(BaseTrigger):
-    name = 'zuul'
-    log = logging.getLogger("zuul.ZuulTrigger")
-
-    def __init__(self, trigger_config={}, sched=None, connection=None):
-        super(ZuulTrigger, self).__init__(trigger_config, sched, connection)
-        self._handle_parent_change_enqueued_events = False
-        self._handle_project_change_merged_events = False
-
-    def getEventFilters(self, trigger_conf):
-        def toList(item):
-            if not item:
-                return []
-            if isinstance(item, list):
-                return item
-            return [item]
-
-        efilters = []
-        for trigger in toList(trigger_conf):
-            f = EventFilter(
-                trigger=self,
-                types=toList(trigger['event']),
-                pipelines=toList(trigger.get('pipeline')),
-                required_approvals=(
-                    toList(trigger.get('require-approval'))
-                ),
-                reject_approvals=toList(
-                    trigger.get('reject-approval')
-                ),
-            )
-            efilters.append(f)
-
-        return efilters
-
-    def onChangeMerged(self, change, source):
-        # Called each time zuul merges a change
-        if self._handle_project_change_merged_events:
-            try:
-                self._createProjectChangeMergedEvents(change, source)
-            except Exception:
-                self.log.exception(
-                    "Unable to create project-change-merged events for "
-                    "%s" % (change,))
-
-    def onChangeEnqueued(self, change, pipeline):
-        # Called each time a change is enqueued in a pipeline
-        if self._handle_parent_change_enqueued_events:
-            try:
-                self._createParentChangeEnqueuedEvents(change, pipeline)
-            except Exception:
-                self.log.exception(
-                    "Unable to create parent-change-enqueued events for "
-                    "%s in %s" % (change, pipeline))
-
-    def _createProjectChangeMergedEvents(self, change, source):
-        changes = source.getProjectOpenChanges(
-            change.project)
-        for open_change in changes:
-            self._createProjectChangeMergedEvent(open_change)
-
-    def _createProjectChangeMergedEvent(self, change):
-        event = TriggerEvent()
-        event.type = 'project-change-merged'
-        event.trigger_name = self.name
-        event.project_name = change.project.name
-        event.change_number = change.number
-        event.branch = change.branch
-        event.change_url = change.url
-        event.patch_number = change.patchset
-        event.refspec = change.refspec
-        self.sched.addEvent(event)
-
-    def _createParentChangeEnqueuedEvents(self, change, pipeline):
-        self.log.debug("Checking for changes needing %s:" % change)
-        if not hasattr(change, 'needed_by_changes'):
-            self.log.debug("  Changeish does not support dependencies")
-            return
-        for needs in change.needed_by_changes:
-            self._createParentChangeEnqueuedEvent(needs, pipeline)
-
-    def _createParentChangeEnqueuedEvent(self, change, pipeline):
-        event = TriggerEvent()
-        event.type = 'parent-change-enqueued'
-        event.trigger_name = self.name
-        event.pipeline_name = pipeline.name
-        event.project_name = change.project.name
-        event.change_number = change.number
-        event.branch = change.branch
-        event.change_url = change.url
-        event.patch_number = change.patchset
-        event.refspec = change.refspec
-        self.sched.addEvent(event)
-
-    def postConfig(self):
-        self._handle_parent_change_enqueued_events = False
-        self._handle_project_change_merged_events = False
-        for pipeline in self.sched.layout.pipelines.values():
-            for ef in pipeline.manager.event_filters:
-                if ef.trigger != self:
-                    continue
-                if 'parent-change-enqueued' in ef._types:
-                    self._handle_parent_change_enqueued_events = True
-                elif 'project-change-merged' in ef._types:
-                    self._handle_project_change_merged_events = True
-
-
-def getSchema():
-    def toList(x):
-        return v.Any([x], x)
-
-    approval = v.Schema({'username': str,
-                         'email-filter': str,
-                         'email': str,
-                         'older-than': str,
-                         'newer-than': str,
-                         }, extra=True)
-
-    zuul_trigger = {
-        v.Required('event'):
-        toList(v.Any('parent-change-enqueued',
-                     'project-change-merged')),
-        'pipeline': toList(str),
-        'require-approval': toList(approval),
-        'reject-approval': toList(approval),
-    }
-
-    return zuul_trigger
+# Copyright 2012-2014 Hewlett-Packard Development Company, L.P.
+# Copyright 2013 OpenStack Foundation
+#
+# Licensed under the Apache License, Version 2.0 (the "License"); you may
+# not use this file except in compliance with the License. You may obtain
+# a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+# License for the specific language governing permissions and limitations
+# under the License.
+
+import logging
+import voluptuous as v
+from zuul.model import EventFilter, TriggerEvent
+from zuul.trigger import BaseTrigger
+
+
+class ZuulTrigger(BaseTrigger):
+    name = 'zuul'
+    log = logging.getLogger("zuul.ZuulTrigger")
+
+    def __init__(self, trigger_config={}, sched=None, connection=None):
+        super(ZuulTrigger, self).__init__(trigger_config, sched, connection)
+        self._handle_parent_change_enqueued_events = False
+        self._handle_project_change_merged_events = False
+
+    def getEventFilters(self, trigger_conf):
+        def toList(item):
+            if not item:
+                return []
+            if isinstance(item, list):
+                return item
+            return [item]
+
+        efilters = []
+        for trigger in toList(trigger_conf):
+            f = EventFilter(
+                trigger=self,
+                types=toList(trigger['event']),
+                pipelines=toList(trigger.get('pipeline')),
+                required_approvals=(
+                    toList(trigger.get('require-approval'))
+                ),
+                reject_approvals=toList(
+                    trigger.get('reject-approval')
+                ),
+            )
+            efilters.append(f)
+
+        return efilters
+
+    def onChangeMerged(self, change, source):
+        # Called each time zuul merges a change
+        if self._handle_project_change_merged_events:
+            try:
+                self._createProjectChangeMergedEvents(change, source)
+            except Exception:
+                self.log.exception(
+                    "Unable to create project-change-merged events for "
+                    "%s" % (change,))
+
+    def onChangeEnqueued(self, change, pipeline):
+        # Called each time a change is enqueued in a pipeline
+        if self._handle_parent_change_enqueued_events:
+            try:
+                self._createParentChangeEnqueuedEvents(change, pipeline)
+            except Exception:
+                self.log.exception(
+                    "Unable to create parent-change-enqueued events for "
+                    "%s in %s" % (change, pipeline))
+
+    def _createProjectChangeMergedEvents(self, change, source):
+        changes = source.getProjectOpenChanges(
+            change.project)
+        for open_change in changes:
+            self._createProjectChangeMergedEvent(open_change)
+
+    def _createProjectChangeMergedEvent(self, change):
+        event = TriggerEvent()
+        event.type = 'project-change-merged'
+        event.trigger_name = self.name
+        event.project_name = change.project.name
+        event.change_number = change.number
+        event.branch = change.branch
+        event.change_url = change.url
+        event.patch_number = change.patchset
+        event.refspec = change.refspec
+        self.sched.addEvent(event)
+
+    def _createParentChangeEnqueuedEvents(self, change, pipeline):
+        self.log.debug("Checking for changes needing %s:" % change)
+        if not hasattr(change, 'needed_by_changes'):
+            self.log.debug("  Changeish does not support dependencies")
+            return
+        for needs in change.needed_by_changes:
+            self._createParentChangeEnqueuedEvent(needs, pipeline)
+
+    def _createParentChangeEnqueuedEvent(self, change, pipeline):
+        event = TriggerEvent()
+        event.type = 'parent-change-enqueued'
+        event.trigger_name = self.name
+        event.pipeline_name = pipeline.name
+        event.project_name = change.project.name
+        event.change_number = change.number
+        event.branch = change.branch
+        event.change_url = change.url
+        event.patch_number = change.patchset
+        event.refspec = change.refspec
+        self.sched.addEvent(event)
+
+    def postConfig(self):
+        self._handle_parent_change_enqueued_events = False
+        self._handle_project_change_merged_events = False
+        for pipeline in self.sched.layout.pipelines.values():
+            for ef in pipeline.manager.event_filters:
+                if ef.trigger != self:
+                    continue
+                if 'parent-change-enqueued' in ef._types:
+                    self._handle_parent_change_enqueued_events = True
+                elif 'project-change-merged' in ef._types:
+                    self._handle_project_change_merged_events = True
+
+
+def getSchema():
+    def toList(x):
+        return v.Any([x], x)
+
+    approval = v.Schema({'username': str,
+                         'email-filter': str,
+                         'email': str,
+                         'older-than': str,
+                         'newer-than': str,
+                         }, extra=True)
+
+    zuul_trigger = {
+        v.Required('event'):
+        toList(v.Any('parent-change-enqueued',
+                     'project-change-merged')),
+        'pipeline': toList(str),
+        'require-approval': toList(approval),
+        'reject-approval': toList(approval),
+    }
+
+    return zuul_trigger
diff --git a/zuul/version.py b/zuul/version.py
index 480812d..7c7a654 100644
--- a/zuul/version.py
+++ b/zuul/version.py
@@ -1,20 +1,20 @@
-# vim: tabstop=4 shiftwidth=4 softtabstop=4
-
-#    Copyright 2011 OpenStack LLC
-#    Copyright 2012 Hewlett-Packard Development Company, L.P.
-#
-#    Licensed under the Apache License, Version 2.0 (the "License"); you may
-#    not use this file except in compliance with the License. You may obtain
-#    a copy of the License at
-#
-#         http://www.apache.org/licenses/LICENSE-2.0
-#
-#    Unless required by applicable law or agreed to in writing, software
-#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-#    License for the specific language governing permissions and limitations
-#    under the License.
-
-import pbr.version
-
-version_info = pbr.version.VersionInfo('zuul')
+# vim: tabstop=4 shiftwidth=4 softtabstop=4
+
+#    Copyright 2011 OpenStack LLC
+#    Copyright 2012 Hewlett-Packard Development Company, L.P.
+#
+#    Licensed under the Apache License, Version 2.0 (the "License"); you may
+#    not use this file except in compliance with the License. You may obtain
+#    a copy of the License at
+#
+#         http://www.apache.org/licenses/LICENSE-2.0
+#
+#    Unless required by applicable law or agreed to in writing, software
+#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+#    License for the specific language governing permissions and limitations
+#    under the License.
+
+import pbr.version
+
+version_info = pbr.version.VersionInfo('zuul')
diff --git a/zuul/webapp.py b/zuul/webapp.py
index 44c333b..577d295 100644
--- a/zuul/webapp.py
+++ b/zuul/webapp.py
@@ -1,130 +1,130 @@
-# Copyright 2012 Hewlett-Packard Development Company, L.P.
-# Copyright 2013 OpenStack Foundation
-#
-# Licensed under the Apache License, Version 2.0 (the "License"); you may
-# not use this file except in compliance with the License. You may obtain
-# a copy of the License at
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-# License for the specific language governing permissions and limitations
-# under the License.
-
-import copy
-import json
-import logging
-import re
-import threading
-import time
-from paste import httpserver
-import webob
-from webob import dec
-
-"""Zuul main web app.
-
-Zuul supports HTTP requests directly against it for determining the
-change status. These responses are provided as json data structures.
-
-The supported urls are:
-
- - /status: return a complex data structure that represents the entire
-   queue / pipeline structure of the system
- - /status.json (backwards compatibility): same as /status
- - /status/change/X,Y: return status just for gerrit change X,Y
-
-When returning status for a single gerrit change you will get an
-array of changes, they will not include the queue structure.
-"""
-
-
-class WebApp(threading.Thread):
-    log = logging.getLogger("zuul.WebApp")
-
-    def __init__(self, scheduler, port=8001, cache_expiry=1):
-        threading.Thread.__init__(self)
-        self.scheduler = scheduler
-        self.port = port
-        self.cache_expiry = cache_expiry
-        self.cache_time = 0
-        self.cache = None
-        self.daemon = True
-        self.server = httpserver.serve(dec.wsgify(self.app), host='0.0.0.0',
-                                       port=self.port, start_loop=False)
-
-    def run(self):
-        self.server.serve_forever()
-
-    def stop(self):
-        self.server.server_close()
-
-    def _changes_by_func(self, func):
-        """Filter changes by a user provided function.
-
-        In order to support arbitrary collection of subsets of changes
-        we provide a low level filtering mechanism that takes a
-        function which applies to changes. The output of this function
-        is a flattened list of those collected changes.
-        """
-        status = []
-        jsonstruct = json.loads(self.cache)
-        for pipeline in jsonstruct['pipelines']:
-            for change_queue in pipeline['change_queues']:
-                for head in change_queue['heads']:
-                    for change in head:
-                        if func(change):
-                            status.append(copy.deepcopy(change))
-        return json.dumps(status)
-
-    def _status_for_change(self, rev):
-        """Return the statuses for a particular change id X,Y."""
-        def func(change):
-            return change['id'] == rev
-        return self._changes_by_func(func)
-
-    def _normalize_path(self, path):
-        # support legacy status.json as well as new /status
-        if path == '/status.json' or path == '/status':
-            return "status"
-        m = re.match('/status/change/(\d+,\d+)$', path)
-        if m:
-            return m.group(1)
-        return None
-
-    def app(self, request):
-        path = self._normalize_path(request.path)
-        if path is None:
-            raise webob.exc.HTTPNotFound()
-
-        if (not self.cache or
-            (time.time() - self.cache_time) > self.cache_expiry):
-            try:
-                self.cache = self.scheduler.formatStatusJSON()
-                # Call time.time() again because formatting above may take
-                # longer than the cache timeout.
-                self.cache_time = time.time()
-            except:
-                self.log.exception("Exception formatting status:")
-                raise
-
-        if path == 'status':
-            response = webob.Response(body=self.cache,
-                                      content_type='application/json')
-        else:
-            status = self._status_for_change(path)
-            if status:
-                response = webob.Response(body=status,
-                                          content_type='application/json')
-            else:
-                raise webob.exc.HTTPNotFound()
-
-        response.headers['Access-Control-Allow-Origin'] = '*'
-
-        response.cache_control.public = True
-        response.cache_control.max_age = self.cache_expiry
-        response.last_modified = self.cache_time
-        response.expires = self.cache_time + self.cache_expiry
-
-        return response.conditional_response_app
+# Copyright 2012 Hewlett-Packard Development Company, L.P.
+# Copyright 2013 OpenStack Foundation
+#
+# Licensed under the Apache License, Version 2.0 (the "License"); you may
+# not use this file except in compliance with the License. You may obtain
+# a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+# License for the specific language governing permissions and limitations
+# under the License.
+
+import copy
+import json
+import logging
+import re
+import threading
+import time
+from paste import httpserver
+import webob
+from webob import dec
+
+"""Zuul main web app.
+
+Zuul supports HTTP requests directly against it for determining the
+change status. These responses are provided as json data structures.
+
+The supported urls are:
+
+ - /status: return a complex data structure that represents the entire
+   queue / pipeline structure of the system
+ - /status.json (backwards compatibility): same as /status
+ - /status/change/X,Y: return status just for gerrit change X,Y
+
+When returning status for a single gerrit change you will get an
+array of changes, they will not include the queue structure.
+"""
+
+
+class WebApp(threading.Thread):
+    log = logging.getLogger("zuul.WebApp")
+
+    def __init__(self, scheduler, port=8001, cache_expiry=1):
+        threading.Thread.__init__(self)
+        self.scheduler = scheduler
+        self.port = port
+        self.cache_expiry = cache_expiry
+        self.cache_time = 0
+        self.cache = None
+        self.daemon = True
+        self.server = httpserver.serve(dec.wsgify(self.app), host='0.0.0.0',
+                                       port=self.port, start_loop=False)
+
+    def run(self):
+        self.server.serve_forever()
+
+    def stop(self):
+        self.server.server_close()
+
+    def _changes_by_func(self, func):
+        """Filter changes by a user provided function.
+
+        In order to support arbitrary collection of subsets of changes
+        we provide a low level filtering mechanism that takes a
+        function which applies to changes. The output of this function
+        is a flattened list of those collected changes.
+        """
+        status = []
+        jsonstruct = json.loads(self.cache)
+        for pipeline in jsonstruct['pipelines']:
+            for change_queue in pipeline['change_queues']:
+                for head in change_queue['heads']:
+                    for change in head:
+                        if func(change):
+                            status.append(copy.deepcopy(change))
+        return json.dumps(status)
+
+    def _status_for_change(self, rev):
+        """Return the statuses for a particular change id X,Y."""
+        def func(change):
+            return change['id'] == rev
+        return self._changes_by_func(func)
+
+    def _normalize_path(self, path):
+        # support legacy status.json as well as new /status
+        if path == '/status.json' or path == '/status':
+            return "status"
+        m = re.match('/status/change/(\d+,\d+)$', path)
+        if m:
+            return m.group(1)
+        return None
+
+    def app(self, request):
+        path = self._normalize_path(request.path)
+        if path is None:
+            raise webob.exc.HTTPNotFound()
+
+        if (not self.cache or
+            (time.time() - self.cache_time) > self.cache_expiry):
+            try:
+                self.cache = self.scheduler.formatStatusJSON()
+                # Call time.time() again because formatting above may take
+                # longer than the cache timeout.
+                self.cache_time = time.time()
+            except:
+                self.log.exception("Exception formatting status:")
+                raise
+
+        if path == 'status':
+            response = webob.Response(body=self.cache,
+                                      content_type='application/json')
+        else:
+            status = self._status_for_change(path)
+            if status:
+                response = webob.Response(body=status,
+                                          content_type='application/json')
+            else:
+                raise webob.exc.HTTPNotFound()
+
+        response.headers['Access-Control-Allow-Origin'] = '*'
+
+        response.cache_control.public = True
+        response.cache_control.max_age = self.cache_expiry
+        response.last_modified = self.cache_time
+        response.expires = self.cache_time + self.cache_expiry
+
+        return response.conditional_response_app
